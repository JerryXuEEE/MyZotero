{"indexedPages":10,"totalPages":10,"version":"261","text":"ORIGINAL ARTICLE Theory\n\nNDBT}\n\nWhat does the free energy principle tell us about the brain?\n\nSamuel J. Gershman1∗\n1Department of Psychology and Center for Brain Science, Harvard University, Cambridge, MA, 02138, USA\nCorrespondence Northwest Laboratories, 52 Oxford St., Room 295.05, Cambridge, MA, 02138, USA Email: gershman@fas.harvard.edu\nFunding information This work was supported by a research fellowship from the Alfred P. Sloan Foundation.\n\nThe free energy principle has been proposed as a unifying account of brain function. It is closely related, and in some cases subsumes, earlier unifying ideas such as Bayesian inference, predictive coding, and active learning. This article clariﬁes these connections, teasing apart distinctive and shared predictions.\nKEYWORDS Bayesian brain, decision theory, variational inference, predictive coding\n\n1 | INTRODUCTION\nThe free energy principle (FEP) states, in a nutshell, that the brain seeks to minimize surprise [1]. It is arguably the most ambitious theory of the brain available today, claiming to subsume many other important ideas, such as predictive coding, efﬁcient coding, Bayesian inference, and optimal control theory. However, it is precisely this generality that raises a concern: what exactly does FEP predict, and what does it not predict? Addressing this concern is not easy, because the assumptions underlying applications of FEP are malleable (e.g., different applications use different generative models, different algorithmic approximations, and different neural implementations). Moreover, some of these assumptions are shared with other theories, and some are idiosyncratic; some assumptions are central to the theory, and others are ad hoc or made for analytical convenience.\nThis article systematically deconstructs the assumptions underlying FEP, with the goal of identifying what its distinctive theoretical claims are. As will become clear, FEP does not have a ﬁxed set of distinctive claims. Rather, it makes different claims under different sets of assumptions. This is not necessarily a bad thing, provided we can verify these assumptions in any particular application and thus render the theoretical assumptions falsiﬁable.\n1\n\n2\n\nSAMUEL J. GERSHMAN\n\nBefore proceeding, we must address two qualms with this deconstructive approach. Some proponents of FEP might reasonably argue that identifying distinctive theoretical claims is pointless; the whole point of a unifying theory is to unify claims, not distinguish them. However, the fundamental issue here is not whether one theory is better than another, but how to assign credit and blame to different theoretical assumptions. If FEP fails to account for the data, is that attributable to the assumption that the brain is Bayesian, or a particular algorithmic implementation of Bayesian inference, or particular assumptions about the probabilistic model? Only by answering such questions can we understand the successes and failures of a unifying theory, devise suitable tests of its assumptions, and identify ways to improve the theory.\nAnother qualm with this approach is based on the argument that FEP is not a theory at all, in the sense that a theory constitutes a set of falsiﬁable claims about empirical phenomena. What makes it a principle, rather than a theory, is that it constitutes a set of self-consistent statements in a formal mathematical system. In this sense, a principle cannot be falsiﬁed through the study of empirical phenomena. A theory designates correspondences between formal statements and empirical phenomena, and thus can be falsiﬁed if the theory makes incorrect predictions on the basis of these correspondences. Viewed in this way, FEP is unobjectionable: its mathematical soundness is sufﬁcient demonstration of its credentials as a principle. Here we will be concerned with its credentials as a theory, and therefore we will pay particular attention to speciﬁc implementations (process models).\n\n2 | THE BAYESIAN BRAIN HYPOTHESIS\n\nAs a prelude to FEP, it will be helpful to brieﬂy describe the Bayesian brain hypothesis [2, 3, 4], which can be expressed in terms that are more familiar to neuroscientists, and is in fact equivalent to FEP under certain conditions (as elaborated in the next section). The ﬁrst claim of the Bayesian brain hypothesis is that the brain is equipped with an internal (or “generative”) model of the environment, which speciﬁes a “recipe” for generating sensory observations (denoted by o) from hidden states (denoted by s). This internal model may not be represented explicitly anywhere in the brain; the claim is that the brain computes “as if” it had an internal model. In order for the Bayesian brain hypothesis to have any predictive power, it is necessary to make speciﬁc assumptions about the structure of the internal model.\nThere are two components of the internal model that need to be speciﬁed. First, hidden variables are drawn from a prior distribution, p(s). For example, the hidden state might be the orientation of a line segment on the surface of an object, and the prior might be a distribution that favors cardinal over oblique orientations [5]. Second, the sensory observations are drawn from an observation distribution conditional on the hidden state, p(o s). For example, the hidden line orientation is projected onto the retina and then encoded by the ﬁring of retinal ganglion cells. This encoding process might be noisy (due to stochasticity of neural ﬁring) or ambiguous (due to the optical projection of three dimensions onto the two-dimensional retinal image), such that different settings of the hidden state could plausibly “explain” the observations to varying degrees. These degrees of plausibility are quantiﬁed by the likelihood, the probability of the observations under the observation distribution given a hypothetical setting of the hidden state.\nThe second claim of the Bayesian brain hypothesis is that the the prior and the likelihood are combined to infer the hidden state given the observations, as stipulated by Bayes’ rule:\n\np (s\n\no) =\n\np(o s)p p (o )\n\n(s\n\n)\n\n,\n\n(1)\n\nwhere p(s o) is the posterior distribution and p(o) = s p(o s)p(s) is the marginal likelihood (for continuous states, the summation is replaced with integration). We can think of Bayes’ rule as “inverting” the internal model to compute a\n\nSAMUEL J. GERSHMAN\n\n3\n\nbelief about the hidden state of the environment given the observations. The Bayesian brain hypothesis can be naturally extended to settings where an agent can inﬂuence its observations\nby taking actions according to a policy π, which is a mapping from observations to a distribution over actions. In the simplest variant, an agent chooses a policy that maximizes information gain:\n\nI(π) = p(o π)D[p(s o, π) p(s π)],\n\n(2)\n\no\n\nwhere o now denotes a future observation, and D denotes the Kullback-Leibler (KL) divergence (also known as relative entropy):\n\nD[p(s o, π) p(s o)] =\n\ns\n\np (s\n\no,\n\nπ)\n\nlog\n\np(s o, π) p(s π)\n\n.\n\n(3)\n\nThe expression for I(π) is equivalent to “Bayesian surprise” [6], and to the mutual information between s and o conditional on π [7, 8]. Information maximization has been studied extensively in the cognitive psychology literature [9, 10, 11]. More generally, information maximization can be understood as a form of active learning that has been studied extensively in the machine learning and statistics literature [12].\nInformation gain maximization is a special case of Bayesian decision theory, where the utility u(o) = D[p(s o, π) p(s π)] of an observation corresponds to information gain. If the observations are valenced (rewards or punishments), then utilities may reﬂect their goodness to the agent, who seeks to maximize the expected utility:\n\n¾[u(o) π] = p(o π)u(o).\n\n(4)\n\no\n\nThis analysis can be generalized to sequential decision problems [see 13], where an agent’s actions and observations unfold over time. Typically, the goal in sequential decision problems is to maximize discounted cumulative utility (return):\n\nR (o) = u(o1) + γu(o2) + γ2u(o3) + · · ·\n\n(5)\n\nwhere we have introduced a subscript denoting time-step and the bold notation o = [o1, o2, . . .] denotes the time-series of observations. The discount factor γ down-weights future utility exponentially as a function of temporal distance. The expected return under the posterior is then deﬁned analogously to expected utility:\n\n¾[R (o) π] = p(o π)R (o).\n\n(6)\n\no\n\nIn sequential decision problems, an agent needs to trade off gathering information to reduce uncertainty (exploration) and taking actions that yield immediate reward (exploitation). This means that preferences for information will arise instrumentally in the sequential decision setting; they need not be built explicitly into the utility function.\nThere are several points worth noting here before moving on:\n\n• Although the Bayesian brain hypothesis has received considerable support, there are numerous empirical deviations from its claims (e.g., [14, 15],) some of which may be rationalized by considering approximate inference algorithms [16]. The variational algorithms we consider below are examples of such approximations. We will not evaluate the empirical validity of the (approximate) Bayesian brain hypothesis, focusing instead on more conceptual issues\n\n4\n\nSAMUEL J. GERSHMAN\n\nrelated to the free energy principle. • The Bayesian brain hypothesis does not make any speciﬁc claims about the priors and likelihoods of an individual.\nRather, the central claim concerns consistency of beliefs: a Bayesian agent will convert prior beliefs into posterior beliefs in accordance with Bayes’ rule. • The Bayesian brain hypothesis abstracts away from any particular algorithmic or neural claims: it is purely a “computational-level” hypothesis. All algorithms that compute the posterior exactly give equivalent predictions with regard to the central claims of the Bayesian brain hypothesis, and likewise any neural implementation will give equivalent predictions. These equivalences do not hold, however, when we consider approximate inference schemes, which may systematically deviate from the Bayesian ideal. We will return to this point below.\n\n3 | THE UNRESTRICTED FREE ENERGY PRINCIPLE IS BAYESIAN INFERENCE\n\nThe basic idea of the FEP is to convert Bayesian inference into an optimization problem (see [17] for a tutorial introduction). This idea was ﬁrst developed in physics, and later in machine learning, to handle computationally intractable inference problems. The key algorithmic trick, as we will see, is to restrict the optimization problem in such a way that it is not searching over all possible posterior distributions.\nAssume we have available a family of distributions Q (discussed further in the next section), and we can choose one distribution q ∈ Q to approximate p(s o). This leads to the following “variational” optimization problem:\n\nq ∗(s) = argmin D[q (s) p(s o)].\n\n(7)\n\nq (s)\n\nThe KL divergence is 0 when q (s) = p(s o). Thus, if p(s o) is contained in the variational family Q, then the solution of the optimization problem yields the exact posterior: q ∗(s) = p(s o). This holds true when the variational family is unrestricted (i.e., contains all possible distributions with support on the hypothesis space).\nAlgorithmically, this optimization problem is not very practical because to compute the KL divergence we need access to p(s o)—precisely the problem we are trying to solve! However, it turns out that one can reformulate this problem in a way that is more practical, based on the following identity:\n\nlog p(o) = D[q (s) p(s o)] − F [q (s)],\n\n(8)\n\nwhere F [q (s)] is the variational free energy:\n\nF [q (s)] =\n\ns\n\nq (s)\n\nlog\n\nq (s) p(o, s)\n\n.\n\n(9)\n\nThe free energy is equivalent to the negative of the evidence lower bound, the more common term in the machine learning literature [18].\nNote that the free energy only requires knowledge of p(s o) up to a normalizing constant, since p(s o) ∝ p(o s)p(s). This is typically unproblematic, since we can often compute the prior p(s) and likelihood p(o s) of any particular state s. Critically, the identity above implies that minimizing the free energy is equivalent to minimizing KL divergence, since the two must balance each other out to match the marginal likelihood, which is ﬁxed as a function of q . Thus, minimizing free energy when the variational family is unrestricted is equivalent to exact Bayesian inference.\nIf FEP = Bayes, then we cannot distinguish its predictions from other asymptotically correct inference algorithms,\n\nSAMUEL J. GERSHMAN\n\n5\n\nsuch as Monte Carlo sampling, except when these algorithms are restricted in some way. Monte Carlo methods may, for example, be restricted in terms of the number of samples they generate or how they generate the samples (e.g., [19]). Optimization of free energy is typically restricted by placing constraints on the variational family, as we discuss next.\n\n4 | RESTRICTING THE VARIATIONAL FAMILY\n\nIf the hypothesis space is vast, then summing (or integrating) over all possible hypotheses to compute the free energy will be intractable. Thus, essentially all practical applications of free energy optimization make use of a restriction on Q that renders the optimization tractable (as will be discussed below). The important point for present purposes is that as long as the true posterior is in Q, the optimal q ∗ will be equal to the posterior. Thus, FEP in its most general form is indistinguishable from Bayesian inference.\nPractical applications of free energy optimization restrict Q in some way to make the problem tractable. These restrictions typically mean that the posterior is no longer contained in Q, and thus the distribution that minimizes free energy will deviate from Bayes-optimality: q ∗(s) p(s o).\nThe widely used “mean-ﬁeld” approximation assumes that the posterior factorizes across components of s (i.e., dimensions of the state space):\n\nq (s) = qi (si ).\n\n(10)\n\ni\n\nFor example, if I’m trying to infer the posterior over the height and weight of an individual given their gender, I could assume that the posterior factorizes into q (height gender) and q (weight gender). Because the true posterior rarely factorizes, the mean-ﬁeld approximation will produce systematic errors. For example, if the factorization is across a sequence of states, the posterior may be biased by the order of the data. Intriguingly, these errors can be discerned in human behavior [20, 21]. On the other hand, the mean-ﬁeld approximation may work well in many cases, which is why it is widely adopted in machine learning. This effectiveness can render it difﬁcult to test as a process model, because it often makes similar predictions to exact Bayesian inference.\nWhen s is continuous, another common restriction is to assume that the posterior is Gaussian [22], parametrized by a mean µ and covariance matrix Σ:\n\nq (s) = N (s; µ, Σ).\n\n(11)\n\nThese parameters are then chosen to minimize the free energy, typically by gradient descent. The Gaussian approximation can be motivated by the “Bayesian central limit theorem,” which states that the posterior is approximately Gaussian around the mode when the amount of data is large relative to the dimensionality of s. It can also be generalized to mixtures of Gaussians to approximate multimodal posteriors [23].\nOne challenge facing applications of the Gaussian approximation is that the free energy is not, in general, tractable (except in the case where the exact posterior is Gaussian). To deal with this issue, a common technique, known as the Laplace approximation, is to use a second-order Taylor series expansion around the posterior mode. This replaces the nonlinear free energy with a quadratic function, rendering the free energy tractable. The price we pay for this approximation is that we are no longer optimizing the free energy, and we have no guarantee that this will produce sensible answers, or even converge. It turns out, however, that the Laplace approximation has intriguing implications for the neurobiological implementation of Bayesian inference.\n\n6\n5 | PREDICTIVE CODING\n\nSAMUEL J. GERSHMAN\n\nThe Laplace approximation can be used to derive arguably the most inﬂuential and distinctive aspect of FEP—predictive coding, according to which feedback pathways convey predictions, and feedforward pathways in the brain convey prediction errors (discrepancies between data and predictions). The idea of predictive coding has a long history in signal processing [24], and was previously proposed as a theory of redundancy reduction (efﬁcient coding) in neural signals [25]. Friston and colleagues showed how predictive coding could be derived within the framework of free energy minimization [22, 26, 27], how it could be mapped onto the structure of biologically realistic microcircuits [28], and how it could be applied to motor control [29] and action selection more generally (a topic we visit in the next section).\nFriston and colleagues started from the following assumptions:\n\n• The internal (generative) model is hierarchically structured, such that hidden states at higher levels generate hidden states at lower levels.\n• The approximate posterior factorizes across hidden state dimensions within and between levels of the internal model (i.e., the mean-ﬁeld approximation).\n• Each component of the factorized posterior is modeled as a Gaussian.\n\nThey then used the Laplace approximation to approximate the free energy and derive update rules for optimization based on gradient descent. They showed that this optimization scheme corresponds to a form of predictive coding, which is found ubiquitously in the engineering literature (e.g., Kalman ﬁltering).\nIt is important to emphasize that predictive coding is not a generic consequence of FEP, or even of FEP with a speciﬁc approximation family. It is derived from a combination of assumptions about the internal model (hierarchical organization), the approximation family (factorized and Gaussian), the approximation of the free energy (quadratic around the mode), and the optimization scheme (gradient descent). With all of these assumptions in place, FEP does make claims that go beyond the general Bayesian brain hypothesis, and have received ample empirical support [30, 31, 32, 33, 34]. Alternatively, some authors have explored variants of FEP that do not invoke predictive coding, or combine it with other neural message passing schemes (e.g., [35]).\n\n6 | ACTIVE INFERENCE\n\nLet us return now to the setting in which an agent can take actions (according to policy π) to inﬂuence its observations. In this setting, FEP posits that the agent seeks to minimize expected free energy under future observations o and future state s [36]:\n\no\n\np(o s, π)\n\ns\n\nq (s\n\nπ) log\n\nq (s π) p(o, s π)\n\n=\n\n−\n\no\n\nq (o π)D[q (s o, π) q (s π)] −\n\no\n\nq (o π) log p(o π),\n\n(12)\n\nwhere q (s π) is the approximate belief about future state s prior to observing o. Friston and colleagues refer to the minimization of expected free energy with respect to actions as active inference. Note that here the likelihood is stipulated to be p(o s, π) = q (o s, π), and we have assumed that the predictive posterior q (s o, π) ≈ p(s o, π).\nWhen the approximate posterior is exact, the ﬁrst term in the expression is the negative information gain and the\n\nSAMUEL J. GERSHMAN\n\n7\n\nsecond term is the entropy H [p(o π)] of the future observations conditional on the policy:\n\no\n\np(o s, π)\n\ns\n\np (s\n\nπ) log\n\np(s π) p(o, s π)\n\n=\n\n−I(π) +\n\nH [p(o π)].\n\n(13)\n\nIf in addition observations are deterministic functions of the policy, then the entropy term is 0, and minimizing expected free energy is equivalent to maximizing information gain. Thus, under certain conditions active inference is equivalent to the information gain policy studied in standard Bayesian treatments of information acquisition [10]. When the observations are stochastic and can be interpreted as reward outcomes (see next section), active inference instantiates a form of risk-sensitive control, since actions that reduce outcome variability will be favored (see [36] for more discussion). Another way of thinking about the entropy term is that it reﬂects the “coding cost” of unpredictable data, since entropy is a lower bound on the average number of bits needed to communicate observations via a sensory channel without loss of information [37]. Thus, active inference prefers actions that produce observations which are both informative and predictable.\nAs in the previous sections, we can ask which aspects of this analysis are generic implications of the Bayesian brain hypothesis (with an information gain policy), and which are speciﬁc to FEP. We showed that FEP is equivalent to Bayesian information gain only under the special case of an exact posterior and deterministic outcomes in the future. When the determinism constraint is relaxed, information gain and expected free energy will be substantively different.\n\n7 | PLANNING AS INFERENCE\nA number of papers on active inference make an additional conceptual move (e.g., [38, 39, 36]), reinterpreting the entropy term as a form of extrinsic value, contrasting it with the epistemic value of the information gain term. Central to this reinterpretation is the postulate that the utility of an outcome is equal to its log prior probability, u(o) = log p(o π), usually referred to as its prior preference. (Note that we are conditioning on the policy here to emphasize that the free energy is being computed for a ﬁxed policy.) This leads to a form of planning as inference [40, 41], whereby minimizing free energy optimizes a combination of expected utility (extrinsic value) and information gain (epistemic value).\nAt ﬁrst glance, this seems rather odd; why should utility be proportional to probability? Undoubtedly there are high probability events that have low utility (e.g., if you are born into poverty then lacking access to basic goods may be highly probable). However, note that this is potentially just Bayesian decision theory in disguise: as long as I’m allowed to choose probabilities that are proportional to utilities, FEP will coincide with Bayesian decision theory. The critical step in this logic is the assumption that evolution has equipped us with the belief that low utility states are low probability, due to the fact that if our ancestors spent a lot of time in those states they would be less likely to reproduce. Whether or not this is a reasonable assumption, the technical point is that planning as inference can be understood as a notational variant of Bayesian decision theory, provided the utilities and probabilities coincide (free energy theorists typically stipulate that they coincide). FEP can make distinctive predictions when they don’t coincide, or when the planning as inference transformation leads to different algorithmic approximations or neural implementations, provided the utilities and prior preferences coincide (i.e., effectively, replacing utility with prior preferences).\n\n8 | CONCLUSIONS\nThere are several take-home messages from this article:\n\n8\n\nSAMUEL J. GERSHMAN\n\n• For passive observations (no actions), the predictions of FEP are indistinguishable from the predictions of the Bayesian brain hypothesis when the variational family is unrestricted (i.e., the when the exact posterior is in the variational family, and hence minimizing free energy is equivalent to exact inference).\n• Predictive coding is not a generic consequence of FEP; it arises only under certain restrictions of the variational family and a speciﬁc choice of optimization scheme.\n• In the active setting (observations can be inﬂuenced by actions), active inference is equivalent to an information gain policy when the approximate posterior is exact and the observations are deterministic functions of actions. When observations are stochastic, active inference induces a form of risk-aversion not found in the information gain policy.\n• When utilities are interpreted as log probabilities, FEP corresponds to a form of planning as inference, a class of algorithms for utility maximization. The predictions of FEP are distinguished from utility maximization when utilities don’t correspond to log probabilities.1\n• When utilities are interpreted as prior preferences, FEP places value on information gain. This also arises naturally in Bayesian decision theory applied to sequential decision problems and hence is not a distinctive prediction.\n\nThese take-home messages do not exhaust the set of ideas that have been introduced under the banner of FEP. For example, FEP has been offered as a ﬁrst-principle account of self-organization [43] and ecological niche construction [44]. We have focused here on issues that are more central to neuroscience.\nThe broader point of this article is that a unifying theory like FEP needs to be deconstructed in order to be properly evaluated and compared to alternative theories. By undertaking part of this deconstruction, we hope to make the elegant synthesis offered by FEP more accessible to the broader neuroscience community.\n\n| Acknowledgments\nI am grateful to Ben Vincent, Momchil Tomov, Chris Summerﬁeld, Giovanni Pezzulo, Peter Battaglia, Jan Drugowitsch, Rani Moran, Yuqing Hou, Jascha Achterberg, Robert Rosenbaum, Sabya Shivkumar, and Nathaniel Daw for comments on an earlier draft of the paper.\n\nREFERENCES [1] Friston K. The free-energy principle: a uniﬁed brain theory? Nature Reviews Neuroscience 2010;11:127.\n[2] Lee TS, Mumford D. Hierarchical Bayesian inference in the visual cortex. JOSA A 2003;20:1434–1448.\n[3] Knill DC, Pouget A. The Bayesian brain: the role of uncertainty in neural coding and computation. TRENDS in Neurosciences 2004;27:712–719.\n[4] Doya K, Ishii S, Pouget A, Rao RP. Bayesian Brain: Probabilistic Approaches to Neural Coding. MIT Press; 2007.\n[5] Girshick AR, Landy MS, Simoncelli EP. Cardinal rules: visual orientation perception reﬂects knowledge of environmental statistics. Nature Neuroscience 2011;14:926.\n[6] Itti L, Baldi P. Bayesian surprise attracts human attention. Vision Research 2009;49:1295–1306.\n[7] Cover TM, Thomas JA. Elements of Information Theory. John Wiley & Sons; 1991.\n1Technically, utilities can always be formulated as log probabilities. But it is an empirical question whether subjective beliefs and preferences disclosed by behavior coincide [42].\n\nSAMUEL J. GERSHMAN\n\n9\n\n[8] Lindley DV. On a measure of the information provided by an experiment. The Annals of Mathematical Statistics 1956;27:986–1005.\n[9] Oaksford M, Chater N. A rational analysis of the selection task as optimal data selection. Psychological Review 1994;101:608–631.\n[10] Nelson JD. Finding useful questions: on Bayesian diagnosticity, probability, impact, and information gain. Psychological Review 2005;112:979–999.\n[11] Tsividis P, Gershman S, Tenenbaum J, Schulz L. Information selection in noisy environments with large action spaces. In: Proceedings of the Annual Meeting of the Cognitive Science Society, vol. 36; 2014. .\n[12] Settles B. Active learning. Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning 2012;6:1–114.\n[13] Dayan P, Daw ND. Decision theory, reinforcement learning, and the brain. Cognitive, Affective, & Behavioral Neuroscience 2008;8:429–453.\n[14] Soltani A, Khorsand P, Guo C, Farashahi S, Liu J. Neural substrates of cognitive biases during probabilistic inference. Nature Communications 2016;7:11393.\n[15] Rahnev D, Denison RN. Suboptimality in Perceptual Decision Making. Behavioral and Brain Sciences 2018;p. 1–107.\n[16] Gershman SJ, Horvitz EJ, Tenenbaum JB. Computational rationality: A converging paradigm for intelligence in brains, minds, and machines. Science 2015;349:273–278.\n[17] Bogacz R. A tutorial on the free-energy framework for modelling perception and learning. Journal of Mathematical Psychology 2017;76:198–211.\n[18] Blei DM, Kucukelbir A, McAuliffe JD. Variational inference: A review for statisticians. Journal of the American Statistical Association 2017;112:859–877.\n[19] Dasgupta I, Schulz E, Gershman SJ. Where do hypotheses come from? Cognitive Psychology 2017;96:1–25.\n[20] Daw ND, Courville AC, Dayan P. Semi-rational models of conditioning: The case of trial order 2008;p. 431–452.\n[21] Sanborn AN, Silva R. Constraining bridges between levels of analysis: A computational justiﬁcation for locally Bayesian learning. Journal of Mathematical Psychology 2013;57:94–106.\n[22] Friston K, Mattout J, Trujillo-Barreto N, Ashburner J, Penny W. Variational free energy and the Laplace approximation. Neuroimage 2007;34:220–234.\n[23] Gershman SJ, Hoffman MD, Blei DM. Nonparametric variational inference. In: Proceedings of the 29th International Conference on International Conference on Machine Learning Omnipress; 2012. p. 235–242.\n[24] Elias P. Predictive coding–I. IRE Transactions on Information Theory 1955;1:16–24.\n[25] Rao RP, Ballard DH. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptiveﬁeld effects. Nature Neuroscience 1999;2:79.\n[26] Friston K. Hierarchical models in the brain. PLoS Computational Biology 2008;4:e1000211.\n[27] Friston K, Kiebel S. Predictive coding under the free-energy principle. Philosophical Transactions of the Royal Society of London B: Biological Sciences 2009;364:1211–1221.\n[28] Bastos AM, Usrey WM, Adams RA, Mangun GR, Fries P, Friston KJ. Canonical microcircuits for predictive coding. Neuron 2012;76:695–711.\n\n10\n\nSAMUEL J. GERSHMAN\n\n[29] Friston K. What is optimal about motor control? Neuron 2011;72:488–498.\n[30] Aitchison L, Lengyel M. With or without you: predictive coding and Bayesian inference in the brain. Current Opinion in Neurobiology 2017;46:219–227.\n[31] Murray SO, Kersten D, Olshausen BA, Schrater P, Woods DL. Shape perception reduces activity in human primary visual cortex. Proceedings of the National Academy of Sciences 2002;99:15164–15169.\n[32] Summerﬁeld C, Trittschuh EH, Monti JM, Mesulam MM, Egner T. Neural repetition suppression reﬂects fulﬁlled perceptual expectations. Nature Neuroscience 2008;11:1004.\n[33] Egner T, Monti JM, Summerﬁeld C. Expectation and surprise determine neural population responses in the ventral visual stream. Journal of Neuroscience 2010;30:16601–16608.\n[34] Kok P, de Lange FP. Predictive coding in sensory cortex. In: An Introduction to Model-based Cognitive Neuroscience Springer; 2015.p. 221–244.\n[35] Friston KJ, Parr T, de Vries B. The graphical brain: belief propagation and active inference. Network Neuroscience 2017;1:381–414.\n[36] Friston K, Rigoli F, Ognibene D, Mathys C, Fitzgerald T, Pezzulo G. Active inference and epistemic value. Cognitive Neuroscience 2015;6:187–214.\n[37] Shannon CE. A mathematical theory of communication. Bell System Technical Journal 1948;27:379–423.\n[38] Friston KJ, Daunizeau J, Kiebel SJ. Reinforcement learning or active inference? PloS one 2009;4:e6421.\n[39] Friston K, Samothrakis S, Montague R. Active inference and agency: optimal control without cost functions. Biological Cybernetics 2012;106:523–541.\n[40] Botvinick M, Toussaint M. Planning as inference. Trends in Cognitive Sciences 2012;16:485–488.\n[41] Kappen HJ, Gómez V, Opper M. Optimal control as a graphical model inference problem. Machine Learning 2012;87:159–182.\n[42] Gershman SJ, Daw ND, M Rabinovich PV K Friston, editor, Perception, action and utility: The tangled skein. MIT Press; 2012.\n[43] Friston K. Life as we know it. Journal of the Royal Society Interface 2013;10:20130475.\n[44] Constant A, Ramstead MJ, Veissiere SP, Campbell JO, Friston KJ. A variational approach to niche construction. Journal of The Royal Society Interface 2018;15:20170685.\n\n"}