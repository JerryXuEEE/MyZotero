{"indexedPages":100,"totalPages":590,"version":"239","text":"NEURONAL DYNAMICS\nWhat happens in our brain when we make a decision? What triggers a neuron to send out a signal? What is the neural code?\nThis textbook for advanced undergraduate and beginning graduate students provides a thorough and up-to-date introduction to the ﬁelds of computational and theoretical neuroscience. It covers classical topics, including the Hodgkin–Huxley equations and Hopﬁeld model, as well as modern developments in the ﬁeld such as Generalized Linear Models and decision theory. Concepts are introduced using clear step-by-step explanations suitable for readers with only a basic knowledge of differential equations and probabilities, and richly illustrated by ﬁgures and worked-out examples.\nEnd-of-chapter summaries and classroom-tested exercises make the book ideal for courses or for self-study. The authors also give pointers to the literature and an extensive bibliography, which will prove invaluable to readers interested in further study.\nWulfram Gerstner is Director of the Laboratory of Computational Neuroscience and a Professor of Life Sciences and Computer Science at the E´ cole Polytechnique Fe´de´rale de Lausanne (EPFL) in Switzerland. He studied physics in Tu¨bingen and Munich and holds a PhD from the Technical University of Munich. His research in computational neuroscience concentrates on models of spiking neurons and synaptic plasticity. He teaches computational neuroscience to physicists, computer scientists, mathematicians, and life scientists. He is co-author of Spiking Neuron Models (Cambridge University Press, 2002).\nWerner M. Kistler received a Master’s and PhD in physics from the Technical University of Munich. He previously worked as Assistant Professor in Rotterdam for computational neuroscience and he is co-author of Spiking Neuron Models. He is now working in Munich as a patent attorney. His scientiﬁc contributions are related to spiking neuron models, synaptic plasticity, and network models of the cerebellum and the inferior olive.\nRichard Naud holds a PhD in computational neuroscience from the EPFL in Switzerland and a Bachelor’s degree in Physics from McGill University, Canada. He has published several scientiﬁc articles and book chapters on the dynamics of neurons. He is now a postdoctoral researcher.\nLiam Paninski is a Professor in the statistics department at Columbia University and codirector of the Grossman Center for the Statistics of Mind. He is also a member of the Center for Theoretical Neuroscience, the Kavli Institute for Brain Science and the doctoral program in neurobiology and behavior. He holds a PhD in neuroscience from New York University and a Bachelor’s from Brown University. His work focuses on neuron models, estimation methods, neural coding and neural decoding. He teaches courses on computational statistics, inference, and statistical analysis of neural data.\n\nNEURONAL DYNAMICS\nFrom Single Neurons to Networks and Models of Cognition\nWULFRAM GERSTNER WERNER M. KISTLER\nRICHARD NAUD LIAM PANINSKI\n\nUniversity Printing House, Cambridge CB2 8BS, United Kingdom\nCambridge University Press is part of the University of Cambridge.\nIt furthers the University’s mission by disseminating knowledge in the pursuit of education, learning and research at the highest international levels of excellence.\nwww.cambridge.org Information on this title: www.cambridge.org/9781107060838\n© Cambridge University Press 2014\nThis publication is in copyright. Subject to statutory exception and to the provisions of relevant collective licensing agreements, no reproduction of any part may take place without the written\npermission of Cambridge University Press.\nFirst published 2014\nPrinted in the United Kingdom by TJ International Ltd. Padstow Cornwall\nA catalogue record for this publication is available from the British Library\nLibrary of Congress Cataloging-in-Publication Data Gerstner, Wulfram.\nNeuronal dynamics : from single neurons to networks and models of cognition / Wulfram Gerstner, Werner M. Kistler, Richard Naud, Liam Paninski. pages cm ISBN 978-1-107-06083-8 (Hardback : alk. paper) ISBN 978-1-107-63519-7 (Paperback : alk. paper)\n1. Neurobiology. 2. Neural networks (Neurobiology). 3. Cognitive neuroscience. I. Kistler, Werner M., 1969– II. Naud, Richard. III. Paninski, Liam. IV. Title.\nQP363.G474 2014 612.8–dc23 2013047693\nISBN 978-1-107-06083-8 Hardback ISBN 978-1-107-63519-7 Paperback\nAdditional resources for this publication at www.cambridge.org/gerstner\nCambridge University Press has no responsibility for the persistence or accuracy of URLs for external or third-party internet websites referred to in this publication, and does not guarantee that any content on such websites is, or will remain, accurate or appropriate.\n\nContents\n\nPreface\nPART ONE FOUNDATIONS OF NEURONAL DYNAMICS\n1 Introduction: neurons and mathematics 1.1 Elements of neuronal systems 1.2 Elements of neuronal dynamics 1.3 Integrate-and-ﬁre models 1.4 Limitations of the leaky integrate-and-ﬁre model 1.5 What can we expect from integrate-and-ﬁre models? 1.6 Summary\n2 Ion channels and the Hodgkin–Huxley model 2.1 Equilibrium potential 2.2 Hodgkin–Huxley model 2.3 The zoo of ion channels 2.4 Summary\n3 Dendrites and synapses 3.1 Synapses 3.2 Spatial structure: the dendritic tree 3.3 Spatial structure: axons 3.4 Compartmental models 3.5 Summary\n4 Dimensionality reduction and phase plane analysis 4.1 Threshold effects 4.2 Reduction to two dimensions 4.3 Phase plane analysis 4.4 Type I and type II neuron models 4.5 Threshold and excitability 4.6 Separation of time scales and reduction to one dimension 4.7 Summary\n\npage ix\n1\n3 3 7 10 19 23 25\n28 28 31 42 56\n58 58 64 72 75 79\n81 81 84 91 96 103 108 111\n\nvi\n\nContents\n\nPART TWO GENERALIZED INTEGRATE-AND-FIRE NEURONS 115\n\n5 Nonlinear integrate-and-ﬁre models\n\n119\n\n5.1 Thresholds in a nonlinear integrate-and-ﬁre model\n\n120\n\n5.2 Exponential integrate-and-ﬁre model\n\n124\n\n5.3 Quadratic integrate and ﬁre\n\n129\n\n5.4 Summary\n\n132\n\n6 Adaptation and ﬁring patterns\n\n136\n\n6.1 Adaptive exponential integrate-and-ﬁre\n\n136\n\n6.2 Firing patterns\n\n140\n\n6.3 Biophysical origin of adaptation\n\n149\n\n6.4 Spike Response Model (SRM)\n\n154\n\n6.5 Summary\n\n165\n\n7 Variability of spike trains and neural codes\n\n168\n\n7.1 Spike-train variability\n\n169\n\n7.2 Mean ﬁring rate\n\n172\n\n7.3 Interval distribution and coefﬁcient of variation\n\n178\n\n7.4 Autocorrelation function and noise spectrum\n\n179\n\n7.5 Renewal statistics\n\n181\n\n7.6 The problem of neural coding\n\n190\n\n7.7 Summary\n\n199\n\n8 Noisy input models: barrage of spike arrivals\n\n202\n\n8.1 Noise input\n\n202\n\n8.2 Stochastic spike arrival\n\n207\n\n8.3 Subthreshold vs. superthreshold regime\n\n212\n\n8.4 Diffusion limit and Fokker–Planck equation (*)\n\n215\n\n8.5 Summary\n\n221\n\n9 Noisy output: escape rate and soft threshold\n\n224\n\n9.1 Escape noise\n\n224\n\n9.2 Likelihood of a spike train\n\n229\n\n9.3 Renewal approximation of the Spike Response Model\n\n232\n\n9.4 From noisy inputs to escape noise\n\n235\n\n9.5 Summary\n\n241\n\n10 Estimating parameters of probabilistic neuron models\n\n243\n\n10.1 Parameter optimization in linear and nonlinear models\n\n244\n\n10.2 Statistical formulation of encoding models\n\n249\n\n10.3 Evaluating goodness-of-ﬁt\n\n255\n\n10.4 Closed-loop stimulus design\n\n263\n\n10.5 Summary\n\n264\n\nContents\n\nvii\n\n11 Encoding and decoding with stochastic neuron models\n\n267\n\n11.1 Encoding models for intracellular recordings\n\n268\n\n11.2 Encoding models in systems neuroscience\n\n273\n\n11.3 Decoding\n\n278\n\n11.4 Summary\n\n285\n\nPART THREE NETWORKS OF NEURONS AND\n\nPOPULATION ACTIVITY\n\n287\n\n12 Neuronal populations\n\n291\n\n12.1 Columnar organization\n\n293\n\n12.2 Identical neurons: a mathematical abstraction\n\n297\n\n12.3 Connectivity schemes\n\n300\n\n12.4 From microscopic to macroscopic\n\n309\n\n12.5 Summary\n\n322\n\n13 Continuity equation and the Fokker–Planck approach\n\n325\n\n13.1 Continuity equation\n\n326\n\n13.2 Stochastic spike arrival\n\n328\n\n13.3 Fokker–Planck equation\n\n332\n\n13.4 Networks of leaky integrate-and-ﬁre neurons\n\n335\n\n13.5 Networks of nonlinear integrate-and-ﬁre neurons\n\n341\n\n13.6 Neuronal adaptation and synaptic conductance\n\n347\n\n13.7 Summary\n\n353\n\n14 Quasi-renewal theory and the integral-equation approach\n\n357\n\n14.1 Population activity equations\n\n358\n\n14.2 Recurrent networks and interacting populations\n\n367\n\n14.3 Linear response to time-dependent input\n\n375\n\n14.4 Density equations vs. integral equations\n\n381\n\n14.5 Adaptation in population equations\n\n386\n\n14.6 Heterogeneity and ﬁnite size\n\n390\n\n14.7 Summary\n\n392\n\n15 Fast transients and rate models\n\n395\n\n15.1 How fast are population responses?\n\n397\n\n15.2 Fast transients vs. slow transients in models\n\n399\n\n15.3 Rate models\n\n408\n\n15.4 Summary\n\n414\n\nviii\n\nContents\n\nPART FOUR DYNAMICS OF COGNITION\n\n417\n\n16 Competing populations and decision making\n\n421\n\n16.1 Perceptual decision making\n\n422\n\n16.2 Competition through common inhibition\n\n426\n\n16.3 Dynamics of decision making\n\n428\n\n16.4 Alternative decision models\n\n433\n\n16.5 Human decisions, determinism, and free will\n\n436\n\n16.6 Summary\n\n439\n\n17 Memory and attractor dynamics\n\n442\n\n17.1 Associations and memory\n\n442\n\n17.2 Hopﬁeld model\n\n446\n\n17.3 Memory networks with spiking neurons\n\n458\n\n17.4 Summary\n\n464\n\n18 Cortical ﬁeld models for perception\n\n467\n\n18.1 Spatial continuum model\n\n468\n\n18.2 Input-driven regime and sensory cortex models\n\n472\n\n18.3 Bump attractors and spontaneous pattern formation\n\n484\n\n18.4 Summary\n\n488\n\n19 Synaptic plasticity and learning\n\n491\n\n19.1 Hebb rule and experiments\n\n492\n\n19.2 Models of Hebbian learning\n\n495\n\n19.3 Unsupervised learning\n\n505\n\n19.4 Reward-based learning\n\n516\n\n19.5 Summary\n\n519\n\n20 Outlook: dynamics in plastic networks\n\n524\n\n20.1 Reservoir computing\n\n524\n\n20.2 Oscillations: good or bad?\n\n529\n\n20.3 Helping patients\n\n541\n\n20.4 Summary\n\n544\n\nReferences\n\n547\n\nIndex\n\n573\n\nPreface\nThis textbook for advanced undergraduate and beginning graduate students provides a systematic introduction into the ﬁelds of neuron modeling, neuronal dynamics, neural coding, and neural networks. It can be used as a text for introductory courses on Computational and Theoretical Neuroscience or as main text for a more focused course on Neural Dynamics and Neural Modeling at the graduate level. The book is also a useful resource for researchers and students who want to learn how different models of neurons and descriptions of neural activity are related to each other.\nAll mathematical concepts are introduced the pedestrian way: step by step. All chapters are richly illustrated by ﬁgures and worked examples. Each chapter closes with a short summary and a series of mathematical Exercises. On the authors’ webpage Python source code is provided for numerical simulations that illustrate the main ideas and models of the chapter (http://lcn.epﬂ.ch/∼gerstner/NeuronalDynamics.html).\nThe book is organized into four parts with a total of 20 chapters. Part I provides a general introduction to the foundations of computational neuroscience and its mathematical tools. It covers classic material such as the Hodgkin–Huxley model, ion channels and dendrites, or phase plane analysis of two-dimensional systems of differential equations. A special focus is put on the ﬁring threshold for the generation of action potentials, in the Hodgkin– Huxley models, as well as in reduced two-dimensional neuron models such as the Morris– Lecar model.\nPart II focuses on simpliﬁed models for the dynamics of a single neuron. It covers nonlinear integrate-and-ﬁre models with and without adaptation, in particular the quadratic and exponential integrate-and-ﬁre model, as well as the Izhikevich model and adaptive exponential integrate-and-ﬁre model. The question of noise in the neural dynamics is posed and two classic descriptions of noise are presented. First, stochasticity arising from random spike arrival: this approach leads to a noise term in the differential equation of the voltage, and can be formulated as a Langevin equation. Second, intrinsic stochasticity of neurons leading to an “escape” across the ﬁring threshold even when the neuron is in the subthreshold regime: this approach leads to the framework of a Generalized Linear Model which is systematically introduced and discussed in applications of neuronal coding and decoding. The relation between the neuron models of Part II and biological data is highlighted and systematic parameter optimization algorithms are presented.\n\nx\n\nPreface\n\nPart III takes the simpliﬁed models derived in Part II and builds networks out of these. The collective properties of the network dynamics are described in terms of equations for the population activity also called the population ﬁring rate. The conditions under which population activity can be described by a standard rate model are identiﬁed.\nPart IV makes the link from dynamics to cognition. The population activity equations are used for an analysis of famous paradigms of computational and cognitive neuroscience, such as the neural activity during decision making or memory retrieval. In Part IV we also sketch the theory of learning in relation to synaptic plasticity. The book closes with a fascinating application of the principles of neuronal dynamics to help patients suffering from Parkinson’s disease.\nA small fraction of the text of the present book is based on Spiking Neuron Models (Cambridge University Press) which was ﬁrst published in 2002 and has been reprinted several times since then. In the meantime, the ﬁeld has changed and we felt that a simple update of Spiking Neuron Models for a second edition would not be enough to give credit to the developments that have occurred.\nScientiﬁcally, the scope of Spiking Neuron Models was limited in several respects. First, it mainly focused on linear integrate-and-ﬁre models, and mentioned their nonlinear counterparts only in passing. In the present book, nonlinear integrate-and-ﬁre models are treated in a full chapter. Second, adaptation was neglected in the treatment 10 years ago – mainly because population equations for adaptive neurons were not yet available. In the present book, adaptive integrate-and-ﬁre models are covered at length in a separate chapter and the population activity equations for adaptive neurons are derived. Third, while the Spike Response Model with escape noise has always contained all the features of a Generalized Linear Model (GLM), by the year 2002 the theory of GLMs had not yet found its way into the ﬁeld of neuroscience and was therefore simply absent from the original book. Given the phenomenal rise of GLMs in neuroscience, the theory of GLMs for ﬁtting neuronal data is given a prominent role in this book. Finally, during teaching we always felt the need to show famous applications of the principles of neuronal dynamics, such as retrieval of contents from associative memories or decision dynamics and the neuroscience of free will. The present book covers these topics.\nOn a more general level, we felt that it would be useful to have a book that is, from the beginning, designed as a textbook rather than a monograph. Therefore, the present book makes the link to experimental data more visible, has more explanatory text, and, last but not least, provides a series of exercises that have already been tested in the classroom over several years.\nWe hope that this book will be useful for students and researchers alike.\n\nWulfram Gerstner, Werner Kistler, Richard Naud, Liam Paninski\n\nPreface\n\nxi\n\nAdvice to the reader\nEach chapter starts with a speciﬁc question and gives ﬁrst intuitive answers in the ﬁrst section. As the chapter proceeds, the material gets more advanced, and the presentation becomes more technical. For a ﬁrst reading of the book, it is possible to read only the ﬁrst section, or ﬁrst two sections, of each chapter and just glance at the subsequent sections.\nMore speciﬁc advice depends on the background. For example, readers who are new to the ﬁeld of computational neuroscience are advised to spend enough time with the classic material of Part I, before they move on to Parts II and IV. The expert reader may skip Part I completely and start directly with Part II.\nIn Part III, the main ideas are exposed in Chapters 12 and 15, which present the foundations for the rate models in Part IV. The more technical chapters and sections of Part III can be skipped at a ﬁrst reading, but are necessary for a thorough understanding of the current developments in the ﬁeld of computational neuroscience.\nPart IV contains applications of neuronal dynamics to questions of cognition and can be read in any arbitrary order.\nSections marked by an asterisk (∗) are mathematically more advanced and can be omitted during a ﬁrst reading of the book.\n\nAcknowledgements\nWe would like to thank our students, visitors, exchange students, and postdocs who carefully read and commented on at least two chapters each, some many more: Dane Corneil, Andrea De Antoni, Mortiz Deger, Mohammad Faraji, Nicolas Fre´maux, Felipe Gerhard, Laureline Logiaco, Skander Mensi, Alexandre Payeur, Christian Pozzorini, Kerstin Preuschoff, Tilo Schwalger, Alex Seeholzer, Hesam Setareh, Carlos Stein, Tim Vogels, Friedemann Zenke, Lorric Ziegler.\nThe writing of the text was a joint work of the four authors. Werner Kistler and Wulfram Gerstner were the authors of Spiking Neuron Models from which several sections survived. Liam Paninski was mainly involved in writing Chapters 9–11 of the present book and gave valuable input to other chapters of Part II. Richard Naud contributed to writing Chapters 1–11 and 14 with a leading role in some of these, made valuable comments and suggestions for all other chapters, and was responsible for all the ﬁgures. Wulfram Gerstner wrote the ﬁrst drafts of Parts III and IV and contributed text to all other chapters.\nWulfram Gerstner, Werner Kistler, Richard Naud, Liam Paninski\n\nPART ONE\nFOUNDATIONS OF NEURONAL DYNAMICS\n\n1\nIntroduction: neurons and mathematics\nThe primary aim of this chapter is to introduce several elementary notions of neuroscience, in particular the concepts of action potentials, postsynaptic potentials, ﬁring thresholds, refractoriness, and adaptation. Based on these notions a preliminary model of neuronal dynamics is built and this simple model (the leaky integrate-and-ﬁre model) will be used as a starting point and reference for the generalized integrate-and-ﬁre models, which are the main topic of the book, to be discussed in Parts II and III. Since the mathematics used for the simple model is essentially that of a one-dimensional linear differential equation, we take this ﬁrst chapter as an opportunity to introduce some of the mathematical notation that will be used throughout the rest of the book.\nOwing to the limitations of space, we cannot – and do not want to – give a comprehensive introduction to such a complex ﬁeld as neurobiology. The presentation of the biological background in this chapter is therefore highly selective and focuses on those aspects needed to appreciate the biological background of the theoretical work presented in this book. For an in-depth discussion of neurobiology we refer the reader to the literature mentioned at the end of this chapter.\nAfter the review of neuronal properties in Sections 1.1 and 1.2 we will turn, in Section 1.3, to our ﬁrst mathematical neuron model. The last two sections are devoted to a discussion of the strengths and limitations of simpliﬁed models.\n1.1 Elements of neuronal systems Over the past hundred years, biological research has accumulated an enormous amount of detailed knowledge about the structure and function of the brain. The elementary processing units in the central nervous system are neurons, which are connected to each other in an intricate pattern. A tiny portion of such a network of neurons is sketched in Fig. 1.1, which shows a drawing by Ramo´n y Cajal, one of the pioneers of neuroscience around 1900. We can distinguish several neurons with triangular or circular cell bodies and long wire-like extensions. This picture can only give a glimpse of the network of neurons in the cortex. In reality, cortical neurons and their connections are packed into a dense network with more than 104 cell bodies and several kilometers of “wires” per cubic millimeter. Across areas of\n\n4\n\nIntroduction\n\nFig. 1.1 This reproduction of a drawing of Ramo´n y Cajal shows a few neurons in the mammalian cortex that he observed under the microscope. Only a small portion of the neurons contained in the sample of cortical tissue have been made visible by the staining procedure; the density of neurons is in reality much higher. Cell b is a typical example of a pyramidal cell with a triangularly shaped cell body. Dendrites, which leave the cell laterally and upwards, can be recognized by their rough surface. The axons are recognizable as thin, smooth lines which extend downwards with a few branches to the left and right. From Ramo`n y Cajal (1909).\nthe brain the wiring pattern may look different. In all areas, however, neurons of different sizes and shapes form the basic elements.\nStill, the cortex does not consist exclusively of neurons. Beside the various types of neuron, there are a large number of “supporter” cells, so-called glia cells, that are required for energy supply and structural stabilization of brain tissue. Since glia cells are not directly involved in information processing, we will not discuss them any further. We will also neglect a few rare subtypes of neuron, such as non-spiking neurons in the mammalian retina. Throughout this book we concentrate on spiking neurons only.\n1.1.1 The ideal spiking neuron\nA typical neuron can be divided into three functionally distinct parts, called dendrites, the soma, and the axon; see Fig. 1.2. Roughly speaking, the dendrites play the role of the “input device” that collects signals from other neurons and transmits them to the soma. The soma is the “central processing unit” that performs an important nonlinear processing step: if the total input arriving at the soma exceeds a certain threshold, then an output signal is generated. The output signal is taken over by the “output device,” the axon, which delivers the signal to other neurons.\nThe junction between two neurons is called a synapse. Let us suppose that a neuron sends a signal across a synapse. It is common to refer to the sending neuron as the presynaptic cell and to the receiving neuron as the postsynaptic cell. A single neuron in vertebrate\n\n1.1 Elements of neuronal systems\n\n5\n\n(a)\n\n(b)\n\nDendrites\n\nSoma\n\nAxon\n\nElectrode\n\nDendrites\n\nAction potential\nj 10 mV\n\n1 ms\n\nAxon\n\ni Synapse\n\nFig. 1.2 (a) Single neuron in a drawing by Ramo´n y Cajal. Dendrites, soma, and axon can be clearly distinguished. The inset shows an example of a neuronal action potential (schematic). The action potential is a short voltage pulse of 1–2 ms duration and an amplitude of about 100 mV. (b) Signal transmission from a presynaptic neuron j to a postsynaptic neuron i. The synapse is marked by the dashed circle. The axons at the lower right end lead to other neurons. (Schematic ﬁgure.)\ncortex often connects to more than 104 postsynaptic neurons. Many of its axonal branches end in the direct neighborhood of the neuron, but the axon can also stretch over several centimeters so as to reach neurons in other areas of the brain.\n\n1.1.2 Spike trains\nThe neuronal signals consist of short electrical pulses and can be observed by placing a ﬁne electrode either on the soma or close to the soma or axon of a neuron; see Fig. 1.2. The pulses, so-called action potentials or spikes, have an amplitude of about 100 mV and typically a duration of 1–2 ms. The form of the pulse does not change as the action potential propagates along the axon. A chain of action potentials emitted by a single neuron is called a spike train – a sequence of stereotyped events which occur at regular or irregular intervals; see Fig. 1.3. Since isolated spikes of a given neuron look alike, the form of the action potential does not carry any information. Rather, it is the number and the timing of spikes which matter. The action potential is the elementary unit of signal transmission.\nAction potentials in a spike train are usually well separated. Even with very strong input, it is impossible to excite a second spike during or immediately after a ﬁrst one. The minimal distance between two spikes deﬁnes the absolute refractory period of the neuron. The absolute refractory period is followed by a phase of relative refractoriness where it is difﬁcult, but not impossible, to excite an action potential.\n\n6\n\nIntroduction\n\nFig. 1.3 Action potentials are stereotypical events. Membrane potential recordings aligned on the time of maximum voltage show little variability of the action potential shape. Data is courtesy of Maria Toledo-Rodriguez and Henry Markram (Toledo-Rodriguez et al., 2004).\n1.1.3 Synapses\nThe site where the axon of a presynaptic neuron makes contact with the dendrite (or soma) of a postsynaptic cell is the synapse. The most common type of synapse in the vertebrate brain is a chemical synapse. At a chemical synapse, the axon terminal comes very close to the postsynaptic neuron, leaving only a tiny gap between pre- and postsynaptic cell membrane. This is called the synaptic cleft. When an action potential arrives at a synapse, it triggers a complex chain of biochemical processing steps that lead to a release of neurotransmitter from the presynaptic terminal into the synaptic cleft. As soon as transmitter molecules have reached the postsynaptic side, they will be detected by specialized receptors in the postsynaptic cell membrane and lead (either directly or via a biochemical signaling chain) to an opening of speciﬁc channels causing ions from the extracellular ﬂuid to ﬂow into the cell. The ion inﬂux, in turn, changes the membrane potential at the postsynaptic site so that, in the end, the chemical signal is translated into an electrical response. The voltage response of the postsynaptic neuron to a presynaptic spike is called the postsynaptic potential.\nApart from chemical synapses neurons can also be coupled by electrical synapses, sometimes called gap junctions. Specialized membrane proteins make a direct electrical connection between the two neurons. Not much is known about the functional aspects of gap junctions, but they are thought to be involved in the synchronization of neurons.\n1.1.4 Neurons are part of a big system\nNeurons are embedded in a network of billions of other neurons and glial cells that make up the brain tissue. The brain is organized into different regions and areas. The cortex can be thought of as a thin but extended sheet of neurons, folded over other brain structures. Some cortical areas are mainly involved in processing sensory input, other areas are involved in working memory or motor control.\nNeurons in sensory cortices can be experimentally characterized by the stimuli to which they exhibit a strong response. For example, neurons in the primary visual cortex respond\n\n1.2 Elements of neuronal dynamics\n\n7\n\nReceptive field\n\nElectrode\n\nV1\n\nFig. 1.4 Receptive ﬁelds in the visual cortex. An electrode probes the activity of a neuron while light dots are presented on a gray screen. The neuron responds whenever the stimulus falls into its receptive ﬁeld, schematically indicated as an oval.\nto dots of lights only within a small region of the visual space. The limited zone where a neuron is sensitive to stimuli is called the neuron’s receptive ﬁeld (Fig. 1.4).\nThe receptive ﬁeld of so-called simple cells in the visual cortex is not homogeneous, but has typically two or three elongated subﬁelds. When a light dot falls into one of the positive subﬁelds, the neuron increases its activity, i.e., it emits more spikes than in the absence of a stimulus. When a light dot falls into a negative subﬁeld, it decreases the activity compared to its spontaneous activity in the presence of a gray screen. A spot of light is in fact not the best stimulus. The neuron responds maximally to a moving light bar with an orientation aligned with the elongation of the positive subﬁeld (Hubel and Wiesel, 1968).\nA large body of the neuroscience literature consists in determining the receptive ﬁelds of neurons in sensory cortices. While neurons in the visual cortex respond to appropriate visual stimuli, neurons in the auditory cortex or somatosensory cortex respond to auditory or tactile stimuli. The concept of receptive ﬁeld becomes less well deﬁned if one moves away from the sensory cortex. For example, in the inferotemporal cortex, neurons respond to objects independently of their size and location; in working memory tasks, frontal cortex neurons are active during periods where no stimulus is present at all. In Parts II, III, and IV of this book we touch on aspects of receptive ﬁelds and memory of neuronal networks embedded in a big system. For the moment, we return to a simple, idealized neuron.\n1.2 Elements of neuronal dynamics\nThe effect of a spike on the postsynaptic neuron can be recorded with an intracellular electrode which measures the potential difference u(t) between the interior of the cell and its surroundings. This potential difference is called the membrane potential. Without any input, the neuron is at rest corresponding to a constant membrane potential urest. After the\n\n8\n\nIntroduction\n\narrival of a spike, the potential changes and ﬁnally decays back to the resting potential; see Fig. 1.5a. If the change is positive, the synapse is said to be excitatory. If the change is negative, the synapse is inhibitory.\nAt rest, the cell membrane has already a strongly negative polarization of about –65 mV. An input at an excitatory synapse reduces the negative polarization of the membrane and is therefore called depolarizing. An input that increases the negative polarization of the membrane even further is called hyperpolarizing.\n\n1.2.1 Postsynaptic potentials\n\nLet us formalize the above observation. We study the time course ui(t) of the membrane potential of neuron i. Before the input spike has arrived, we have ui(t) = urest. At t = 0 the presynaptic neuron j ﬁres its spike. For t > 0, we see at the electrode a response of neuron i\n\nui(t) − urest =: εi j(t) .\n\n(1.1)\n\nThe right-hand side of Eq. (1.1) deﬁnes the postsynaptic potential (PSP). If the voltage difference ui(t) − urest is positive (negative) we have an excitatory (inhibitory) postsynaptic potential or short EPSP (IPSP). In Fig. 1.5a we have sketched the EPSP caused by the arrival of a spike from neuron j at an excitatory synapse of neuron i.\n\n1.2.2 Firing threshold and action potential\n\nConsider two presynaptic neurons j = 1, 2, which both send spikes to the postsynaptic neuron i. Neuron j = 1 ﬁres spikes at t1(1),t1(2), . . . , similarly neuron j = 2 ﬁres at t2(1),t2(2), . . . . Each spike evokes a postsynaptic potential εi1 or εi2, respectively. As long as there are\nonly few input spikes, the total change of the potential is approximately the sum of the\n\nindividual PSPs,\n\n∑∑ ui(t) =\n\nεi\n\nj\n\n(t\n\n−\n\nt\n\nf j\n\n)\n\n+\n\nurest\n\n,\n\njf\n\n(1.2)\n\ni.e., the membrane potential responds linearly to input spikes; see Fig. 1.5b. On the other hand, linearity breaks down if too many input spikes arrive during a short\ninterval. As soon as the membrane potential reaches a critical value ϑ , its trajectory shows a behavior that is quite different from a simple summation of PSPs: the membrane potential exhibits a pulse-like excursion with an amplitude of about 100 mV. This short voltage pulse will propagate along the axon of neuron i to the synapses with other neurons. After the pulse the membrane potential does not directly return to the resting potential, but passes, for many neuron types, through a phase of hyperpolarization below the resting value. This hyperpolarization is called “spike-afterpotential.”\nSingle EPSPs have amplitudes in the range of 1 mV. The critical value for spike initiation is about 20 to 30 mV above the resting potential. In most neurons, four spikes – as shown\n\n(a) j=1\n\n1.2 Elements of neuronal dynamics\n\n(b)\n\nui (t )\n\nj=1\n\nj=2\n\n9 ui (t )\n\nJ\n\nJ\n\nurest\n\nei1\n\nu(t )\n\nt\n\nurest\n\nu(t ) t\n\nt\n\nf 1\n\n(c) j=1\n\nt1f t2f\nui(t )\n\nj=2\n\nu(t )\n\nurest\n\nt\n\nt\n\n1 1\n\nt\n\n2 1\n\nt\n\n1 2\n\nt\n\n2 2\n\nFig. 1.5 A postsynaptic neuron i receives input from two presynaptic neurons j = 1, 2. (a) Each\npresynaptic spike evokes an excitatory postsynaptic potential (EPSP) that can be measured with an electrode as a potential difference ui(t) − urest. The time course of the EPSP caused by the spike of neuron j = 1 is εi1(t − t1f ). (b) An input spike from a second presynaptic neuron j = 2 that arrives shortly after the spike from neuron j = 1 causes a second postsynaptic potential that adds to the ﬁrst one. (c) If ui(t) reaches the threshold ϑ , an action potential is triggered. As a consequence, the membrane potential starts a large positive pulse-like excursion (arrow). On the voltage scale\nof the graph, the peak of the pulse is out of bounds. After the pulse the voltage returns to a value\nbelow the resting potential urest.\n\n10\n\nIntroduction\n\nschematically in Fig. 1.5c – are thus not sufﬁcient to trigger an action potential. Instead, about 20–50 presynaptic spikes have to arrive within a short time window to trigger a postsynaptic action potential.\n\n1.3 Integrate-and-ﬁre models\nWe have seen in the previous section that, to a ﬁrst and rough approximation, neuronal dynamics can be conceived as a summation process (sometimes also called “integration” process) combined with a mechanism that triggers action potentials above some critical voltage. Indeed in experiments ﬁring times are often deﬁned as the moment when the membrane potential reaches some threshold value from below. In order to build a phenomenological model of neuronal dynamics, we describe the critical voltage for spike initiation by a formal threshold ϑ . If the voltage ui(t) (that contains the summed effect of all inputs) reaches ϑ from below, we say that neuron i ﬁres a spike. The moment of threshold crossing deﬁnes the ﬁring time tif .\nThe model makes use of the fact that neuronal action potentials of a given neuron always have roughly the same form. If the shape of an action potential is always the same, then the shape cannot be used to transmit information: rather information is contained in the presence or absence of a spike. Therefore action potentials are reduced to “events” that happen at a precise moment in time.\nNeuron models where action potentials are described as events are called “integrate-andﬁre” models. No attempt is made to describe the shape of an action potential. Integrate-andﬁre models have two separate components that are both necessary to deﬁne their dynamics: ﬁrst, an equation that describes the evolution of the membrane potential ui(t); and second, a mechanism to generate spikes.\nIn the following we introduce the simplest model in the class of integrate-and-ﬁre models using the following two ingredients: (i) a linear differential equation to describe the evolution of the membrane potential; (ii) a threshold for spike ﬁring. This model is called the “leaky integrate-and-ﬁre” model. Generalized integrate-and-ﬁre models, which will be discussed in Part II of the book, can be seen as variations of this basic model.\n\n1.3.1 Integration of inputs\nThe variable ui describes the momentary value of the membrane potential of neuron i. In the absence of any input, the potential is at its resting value urest. If an experimenter injects a current I(t) into the neuron, or if the neuron receives synaptic input from other neurons, the potential ui will be deﬂected from its resting value.\nIn order to arrive at an equation that links the momentary voltage ui(t) − urest to the input current I(t), we use elementary laws from the theory of electricity. A neuron is surrounded by a cell membrane, which is a rather good insulator. If a short current pulse I(t) is injected into the neuron, the additional electrical charge q = I(t )dt has to go somewhere: it will\n\n(a)\n\n+\n\n+\n\n-\n\n-\n\n+\n\n-\n\n+\n\n+\n\n-\n\n-\n+\n\n-+ -+\n\n++\n\n-\n\n--\n\nI (t)\n+ +\n+\n\nI (t)\nR C u(t)\nurest\n\n1.3 Integrate-and-ﬁre models\n\n11\n\n(b)\n\nI (t )\n\nFig. 1.6 Electrical properties of\n\nneurons: the passive membrane.\n\n(a) A neuron, which is enclosed\n\nby the cell membrane (big circle),\n\nreceives a (positive) input current\n\nI(t) which increases the electri-\n\ncal charge inside the cell. The cell\n\nu (t )\n\nt membrane acts like a capacitor in\n\nt parallel with a resistor which is in\n\nline with a battery of potential urest\n\n(zoomed inset). (b) The cell mem-\n\nbrane reacts to a step current (top)\n\nwith a smooth voltage trace (bot-\n\nurest\n\ntom).\n\ncharge the cell membrane (Fig. 1.6a). The cell membrane therefore acts like a capacitor of capacity C. Because the insulator is not perfect, the charge will, over time, slowly leak through the cell membrane. The cell membrane can therefore be characterized by a ﬁnite leak resistance R.\nThe basic electrical circuit representing a leaky integrate-and-ﬁre model consists of a capacitor C in parallel with a resistor R driven by a current I(t); see Fig. 1.6. If the driving current I(t) vanishes, the voltage across the capacitor is given by the battery voltage urest. For a biological explanation of the battery we refer the reader to the next chapter. Here we have simply inserted the battery “by hand” into the circuit so as to account for the resting potential of the cell (Fig. 1.6a).\nIn order to analyze the circuit, we use the law of current conservation and split the driving current into two components,\n\nI(t) = IR + IC.\n\n(1.3)\n\nThe ﬁrst component is the resistive current IR which passes through the linear resistor R. It can be calculated from Ohm’s law as IR = uR/R where uR = u − urest is the voltage across the resistor. The second component IC charges the capacitor C. From the deﬁnition of the capacity as C = q/u (where q is the charge and u the voltage), we ﬁnd a capacitive current IC = dq/dt = C du/dt. Thus\n\nI(t) = u(t) − urest +C du .\n\nR\n\ndt\n\n(1.4)\n\nWe multiply Eq. (1.4) by R and introduce the time constant τm = RC of the “leaky integra-\n\ntor.” This yields the standard form\n\nτm\n\ndu dt\n\n=\n\n−[u(t)\n\n− urest] +\n\nR I(t) .\n\n(1.5)\n\nWe refer to u as the membrane potential and to τm as the membrane time constant of the\n\nneuron.\n\n12\nI (t)\n\nIntroduction\n\n0 t\nΔ u(t)\nq/C\n\n0\n\nt\n\nFig. 1.7 Short pulses and total charged delivered on the passive membrane. The amplitude of the voltage response (bottom) of a leaky integrator driven by a short current pulse I(t) (top) depends only on the total charge q = I(t)dt, not on the height of the current pulse.\n\nFrom a mathematical point of view, Eq. (1.5) is a linear differential equation. From the point of view of an electrical engineer, it is the equation of a leaky integrator or RC-circuit where resistor R and capacitor C are arranged in parallel. From the point of view of the neuroscientist, Eq. (1.5) is called the equation of a passive membrane.\nWhat is the solution of Eq. (1.5)? We suppose that, for whatever reason, at time t = 0 the membrane potential takes a value urest + Δu. For t > 0 the input vanishes I(t) = 0. Intuitively we expect that, if we wait long enough, the membrane potential relaxes to its resting value urest. Indeed, the solution of the differential equation with initial condition u(t0) = urest + Δu is\n\nu(t) − urest = Δu exp\n\n−\n\nt\n\n− t0 τm\n\nfor t > t0 .\n\n(1.6)\n\nThus, in the absence of input, the membrane potential decays exponentially to its resting value. The membrane time constant τm = RC is the characteristic time of the decay. For a typical neuron it is in the range of 10 ms, and hence rather long compared with the duration of a spike which is of the order of 1 ms.\nThe validity of the solution (1.6) can be checked by taking the derivative on both sides of the equation. Since it is the solution in the absence of input, it is sometimes called the “free” solution.\n\n1.3.2 Pulse input\nBefore we continue with the deﬁnition of the integrate-and-ﬁre model and its variants, let us study the dynamics of the passive membrane deﬁned by Eq. (1.5) in a simple example. Suppose that the passive membrane is stimulated by a constant input current I(t) = I0 which starts at t = 0 and ends at time t = Δ. For the sake of simplicity we assume that the membrane potential at time t = 0 is at its resting value u(0) = urest.\n\n1.3 Integrate-and-ﬁre models\n\n13\n\nAs a ﬁrst step, let us calculate the time course of the membrane potential. The trajec-\n\ntory of the membrane potential can be found by integrating (1.5) with the initial condition\n\nu(0) = urest. The solution for 0 < t < Δ is\n\nt u(t) = urest + R I0 1 − exp − τm .\n\n(1.7)\n\nIf the input current never stopped, the membrane potential (1.7) would approach for t → ∞\n\nthe asymptotic value u(∞) = urest + R I0. We can understand this result by looking at the\n\nelectrical diagram of the RC-circuit in Fig. 1.6. Once a steady state is reached, the charge\n\non the capacitor no longer changes. All input current must then ﬂow through the resistor.\n\nThe steady-state voltage at the resistor is therefore RI0 so that the total membrane voltage is urest + RI0.\n\nExample: Short pulses and the Dirac δ -function\n\nFor short pulses the steady-state value is never reached. At the end of the pulse, the\n\nvalue of the membrane potential is given according to Eq. (1.7) by u(Δ) = urest +\n\nR I0\n\n1 − exp\n\n−\n\nΔ τm\n\n. For pulse durations Δ\n\nτm (where\n\nmeans much smaller than)\n\nwe can expand the exponential term into a Taylor series: exp(x) = 1 + x + x2/2 + · · · . To\n\nﬁrst\n\norder\n\nin\n\nx=\n\n−\n\nΔ τm\n\nwe\n\nﬁnd\n\nu(Δ)\n\n=\n\nurest\n\n+\n\nR\n\nI0\n\nΔ τm\n\nfor Δ\n\nτm.\n\n(1.8)\n\nThus, the voltage deﬂection depends linearly on the amplitude and the duration of the\n\npulse (Fig. 1.7, thick line). We now make the duration Δ of the pulse shorter and shorter while increasing the\namplitude of the current pulse to a value I0 = q/Δ, so that the integral I(t)dt = q remains constant. In other words, the total charge q delivered by the current pulse is\n\nalways the same. Interestingly, the voltage deﬂection at the end of the pulse calcu-\n\nlated from Eq. (1.8) remains unaltered, however short we make the pulse. Indeed, from Eq. (1.8) we ﬁnd u(Δ) − urest = q R/τm = q/C where we have used τm = RC. Thus we can consider the limit of an inﬁnitely short pulse\n\nI(t\n\n)\n\n=\n\nq\n\nδ\n\n(t\n\n)\n\n=\n\nlim\nΔ→0\n\nq Δ\n\nfor 0 < t < Δ,\n\nand 0 otherwise.\n\n(1.9)\n\nδ (t) is called the Dirac δ -function. It is deﬁned by δ (x) = 0 for x = 0 and\n\n∞ −∞\n\nδ\n\n(x)dx\n\n=\n\n1.\n\nObviously, the Dirac δ -function is a mathematical abstraction since it is practically\n\nimpossible to inject a current with an inﬁnitely short and inﬁnitely strong current pulse into a neuron. Whenever we encounter a δ -function, we should remember that, as a\n\nstand-alone object, it looks strange, but it becomes meaningful as soon as we integrate\n\nover it. Indeed the input current deﬁned in Eq. (1.9) needs to be inserted into the\n\ndifferential equation (1.5) and integrated. The mathematical abstraction of the Dirac δ -function suddenly makes a lot of sense, because the voltage change induced by a short current pulse is always the same, whenever the duration of the pulse Δ is much\n\n14\n\nIntroduction\n\nu δ (t − ti1)\n\nϑ\n\nurest\n\nt\n\nur ti1\nFig. 1.8 In formal models of spiking neurons the shape of an action potential (dashed line) is usually replaced by a δ -pulse (vertical line). The negative overshoot (spike-afterpotential) after the pulse is replaced by a “reset” of the membrane potential to the value ur. The pulse is triggered by the threshold crossing at ti1.\n\nshorter than the time constant τm. Thus, the exact duration of the pulse is irrelevant, as long as it is short enough.\nWith the help of the δ -function, we no longer have to worry about the time course\n\nof the membrane potential during the application of the current pulse: the membrane\n\npotential simply jumps at time t = 0 by an amount q/C. Thus, it is as if we added\n\ninstantaneously a charge q onto the capacitor of the RC circuit. What happens for times t > Δ? The membrane potential evolves from its new initial\n\nvalue urest + q/C in the absence of any further input. Thus we can use the “free” solution from Eq. (1.6) with t0 = Δ and Δu = q/C.\nWe can summarize the considerations of this section by the following statement. The\n\nsolution of the linear differential equation with pulse input\n\nτm\n\ndu dt\n\n=\n\n−[u(t)\n\n− urest] +\n\nRqδ\n\n(t)\n\n(1.10)\n\nis u(t) = urest for t ≤ 0 and given by\n\nR\n\nt\n\nu(t) − urest = q τm exp − τm\n\nfor t > 0.\n\n(1.11)\n\nThe right-hand side of the equation is called the impulse-response function or Green’s\n\nfunction of the linear differential equation.\n\n1.3.3 The threshold for spike ﬁring\n\nThroughout this book, the term “ﬁring time” refers to the moment when a given neuron emits an action potential t f . The ﬁring time t f in the leaky integrate-and-ﬁre model is\n\ndeﬁned by a threshold criterion\n\nt f : u(t f ) = ϑ .\n\n(1.12)\n\n1.3 Integrate-and-ﬁre models\n\n15\n\n(a)\n\n(b)\n\nΔ u (t)/J Δ u (t)/J\n\n1.2 1.0 0.8 0.6 0.4 0.2 0.0\n0\n\nt1\n\nt2\n\nt3\n\nt4\n\nt5\n\n20\n\n40\n\n60\n\n80 100\n\nt [ms]\n\n1.2\n\n1.0\n\nt1\n\nt2\n\nt3\n\nt4\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.0\n\n0\n\n20\n\n40\n\n60\n\n80 100\n\nt [ms]\n\nFig. 1.9 Integrate-and-ﬁre model. (a) Time course of the membrane potential of an integrate-and-ﬁre\nneuron driven by constant input current I0 = 1.5. The voltage Δu(t) = u − urest is normalized by the value of the threshold ϑ . Units of input current are chosen so that I0 = 1 corresponds to a trajectory that reaches the threshold for t → ∞. After a spike, the potential is reset to ur = urest. (b) Voltage\nresponse to a time-dependent input current.\n\nThe form of the spike is not described explicitly. Rather, the ﬁring time is noted and immediately after t f the potential is reset to a new value ur < ϑ ,\n\nlim u(t f + δ ) = ur .\nδ →0;δ >0\n\n(1.13)\n\nFor t > t f the dynamics is again given by (1.5) until the next threshold crossing occurs. The\ncombination of leaky integration (1.5) and reset (1.13) deﬁnes the leaky integrate-and-ﬁre\nmodel (Stein, 1967b). The voltage trajectory of a leaky integrate-and-ﬁre model driven by\na constant current I0 is shown in Fig. 1.9. For the ﬁring times of neuron i we write tif where f = 1, 2, . . . is the label of the spike.\nFormally, we may denote the spike train of a neuron i as the sequence of ﬁring times\n\nSi(t) = ∑ δ (t − tif ) f\n\n(1.14)\n\nwhere δ (x) is the Dirac δ -function introduced earlier, with δ (x) = 0 for x = 0 and\n\n∞ −∞\n\nδ\n\n(x)dx\n\n=\n\n1.\n\nSpikes\n\nare\n\nthus\n\nreduced\n\nto\n\npoints\n\nin\n\ntime\n\n(Fig.\n\n1.8).\n\nWe\n\nremind\n\nthe\n\nreader\n\nthat the δ -function is a mathematical object that needs to be inserted into an integral in\n\norder to give meaningful results.\n\n1.3.4 Time-dependent input (*)1\nWe study a leaky integrate-and-ﬁre model which is driven by an arbitrary time-dependent input current I(t); see Fig. 1.9b. The ﬁring threshold has a value ϑ and after ﬁring the potential is reset to a value ur < ϑ .\n1Sections marked by an asterisk are mathematically more advanced and can be omitted during a ﬁrst reading of the book.\n\n16\n\nIntroduction\n\nIn the absence of a threshold, the linear differential equation (1.5) has a solution\n\nR u(t) = urest + τm\n\n∞\nexp\n0\n\n−\n\ns τm\n\nI(t − s) ds,\n\n(1.15)\n\nwhere I(t) is an arbitrary input current and τm = RC is the membrane time constant. We assume here that the input current is deﬁned for a long time back into the past: t → −∞ so that we do not have to worry about the initial condition. A sinusoidal current I(t) = I0 sin(ω t) or a step current pulse I(t) = I0Θ(t), where Θ denotes the Heaviside step function with Θ(t) = 0 for t ≤ 0 and Θ(t) = 1 for t > 0, are two examples of a time-dependent current,\nbut the solution, Eq. (1.15), is also valid for every other time-dependent input current.\nSo far our leaky integrator does not have a threshold. What happens to the solution Eq. (1.15) if we add a threshold ϑ ? Each time the membrane potential hits the threshold, the variable u is reset from ϑ to ur. In the electrical circuit diagram, the reset of the potential corresponds to removing a charge qr = C (ϑ − ur) from the capacitor (Fig. 1.6) or, equivalently, adding a negative charge −qr onto the capacitor. Therefore, the reset corresponds to a short current pulse Ir = −qr δ (t − t f ) at the moment of the ﬁring t f . Indeed, it is not unusual to say that a neuron “discharges” instead of “ﬁres.” Since the reset happens\neach time the neuron ﬁres, the reset current is\n\nIr = −qr ∑ δ (t − t f ) = −C (ϑ − ur)S(t) , f\n\n(1.16)\n\nwhere S(t) denotes the spike train, deﬁned in Eq. (1.14). The short current pulse corresponding to the “discharging” is treated mathematically just\nlike any other time-dependent input current. The total current I(t) + Ir(t), consisting of the stimulating current and the reset current, is inserted into the solution (1.15) to give the ﬁnal result\n\n∑ u(t) = urest + (ur − ϑ ) exp f\n\n−\n\nt\n\n−t τm\n\nf\n\nR∞\n\ns\n\n+ τm 0 exp − τm\n\nI(t − s) ds ,\n\n(1.17)\n\nwhere the ﬁring times t f are deﬁned by the threshold condition\n\nt f = {t|u(t) = ϑ } .\n\n(1.18)\n\nNote that with our deﬁnition of the Dirac δ -function in Eq. (1.9), the discharging reset follows immediately after the threshold crossing, so that the natural sequence of events – ﬁrst ﬁring, then reset – is respected.\nEquation (1.17) looks rather complicated. It has, however, a simple explanation. In Section 1.3.2 we have seen that a short input pulse at time t causes at time t a response of the membrane proportional to exp [−(t − t /τm)], sometimes called the impulse response function or Green’s function; see Eq. (1.11). The second term on the right-hand side of Eq. (1.17) is the effect of the discharging current pulses at the moment of the reset.\nIn order to interpret the last term on the right-hand side, we think of a stimulating current I(t) as consisting of a rapid sequence of discrete and short current pulses. In discrete time,\n\n1.3 Integrate-and-ﬁre models\n\n17\n\nthere would be a different current pulse in each time step. Because of the linearity of the differential equation, the effect of all these short current pulses can be added. When we return from discrete time to continuous time, the sum of the impulse response functions turns into the integral on the right-hand side of Eq. (1.17).\n\n1.3.5 Linear differential equation vs. linear ﬁlter: two equivalent pictures (*)\n\nThe leaky integrate-and-ﬁre model is deﬁned by the differential equation (1.5), i.e.,\n\nτm\n\ndu dt\n\n=\n\n−[u(t) − urest] + R I(t),\n\ncombined with the reset condition\n\n(1.19)\n\nlim u(t f + δ ) = ur,\nδ →0;δ >0\n\n(1.20)\n\nwhere t f are the ﬁring times\n\nt f = {t|u(t) = ϑ } .\n\n(1.21)\n\nAs we have seen in the previous section, the linear equation can be integrated and yields\n\nthe solution (1.17). It is convenient to rewrite the solution in the form\n\n∞\n\n∞\n\nu(t) = η(s)S(t − s)ds + κ(s)I(t − s) ds,\n\n0\n\n0\n\n(1.22)\n\nwhere we have introduced ﬁlters η(s) = (ur − ϑ ) exp\n\n−\n\ns τm\n\nand\n\nκ (s)\n\n=\n\n1 C\n\nexp\n\n−\n\ns τm\n\n.\n\nInterestingly, Eq. (1.22) is much more general than the leaky integrate-and-ﬁre model,\n\nbecause the ﬁlters do not need to be exponentials but could have any arbitrary shape.\n\nThe ﬁlter η describes the reset of the membrane potential and, more generally, accounts\n\nfor neuronal refractoriness. The ﬁlter κ summarizes the linear electrical properties of the\n\nmembrane. Eq. (1.22) in combination with the threshold condition (1.21) is the basis of\n\nthe Spike Response Model and Generalized Linear Models, which will be discussed in\n\nPart II.\n\n1.3.6 Periodic drive and Fourier transform (*)\n\nFormally, the complex Fourier transform of a real-valued function f (t) with argument t on\n\nthe real line is\n\n∞\nfˆ(ω) = f (t) e−iωt dt = | fˆ(ω)| eiφ f (ω),\n−∞\n\n(1.23)\n\nwhere | fˆ(ω)| and φ f (ω) are called the amplitude and phase of the Fourier transform at\n\nfrequency ω. The mathematical condition for a well-deﬁned Fourier transform is that the\n\nfunction f be Lebesgue integrable with integral\n\n∞ −∞\n\n|\n\nf\n\n(t)|dt\n\n< ∞.\n\nIf\n\nf\n\nis\n\na\n\nfunction\n\nof\n\ntime, then fˆ(ω) is a function of frequency. An inverse Fourier transform leads back from\n\nfrequency-space to the original space, i.e., time.\n\n18\n\nIntroduction\n\nFor a linear system, the above deﬁnition gives rise to several convenient rules for Fourier-\n\ntransformed equations. For example, let us consider the system\n\n∞\nu(t) = κ(s) I(t − s) ds ,\n−∞\n\n(1.24)\n\nwhere I(t) is a real-valued input (e.g., a current), u(t) the real-valued system output (e.g., a voltage) and κ a linear response ﬁlter, or kernel, with κ(s) = 0 for s < 0 because of causality. The convolution on the right-hand side of Eq. (1.24) turns after Fourier transformation into a simple multiplication, as shown by the following calculation steps:\n\n∞∞\n\nuˆ(ω) =\n\nκ(s) I(t − s) ds e−iωt dt\n\n−∞ −∞\n\n= ∞ ∞ κ(s)e−iωs I(t − s) e−iω(t−s)dsdt\n\n−∞ −∞\n\n= κˆ (ω) Iˆ(ω),\n\n(1.25)\n\nwhere we introduced in the last step the variable t = t − s and used the deﬁnition (1.23) of the Fourier transform.\nSimilarly, the derivative du/dt of a function u(t) can be Fourier-transformed using the product rule of integration. The Fourier transform of the derivative of u(t) is iωuˆ(ω).\nWhile introduced here as a purely mathematical operation, it is often convenient to visualize the Fourier transform in the context of a physical system driven by a periodic input. Consider the linear system of Eq. (1.24) with an input\n\nI(t) = I0 eiωt .\n\n(1.26)\n\nA short comment on the notation. If the input is a current, it should be real-valued, as opposed to a complex number. We therefore take I0 as a real and positive number and focus on the real part of the complex equation (1.26) as our physical input. When we perform a calculation with complex numbers, we therefore implicitly assume that, at the very end, we take only the real part of solution. However, the calculation with complex numbers turns out to be convenient for the steps in between.\nInserting the periodic drive, Eq. (1.26), into Eq. (1.24) yields\n\nu(t) = ∞ κ(s) I0eiω(t−s) ds = ∞ κ(s) e−iωs ds I0eiωt .\n\n−∞\n\n−∞\n\n(1.27)\n\nHence, if the input is periodic at frequency ω the output is too. The term in square brackets is the Fourier transform of the linear ﬁlter. We write u(t) = u0 eiφκ(ω) eiωt. The ratio between\nthe amplitude of the output and that of the input is\n\nu0 = |κˆ (ω)| . I0\n\n(1.28)\n\nThe phase φκ (ω) of the Fourier-transformed linear ﬁlter κ corresponds to the phase shift between input and output or, to say it differently, a delay Δ = φκ /ω = φκ T /2π where T is the period of the oscillation. Fourier transforms will play a role in the discussion of signal\n\n1.4 Limitations of the leaky integrate-and-ﬁre model\n\n19\n\nprocessing properties of connected networks of neurons in Part III of the book.\n\nExample: Periodic drive of a passive membrane\n\nWe consider the differential equation of the passive membrane deﬁned in Eq. (1.5) and choose voltage units such that urest = 0, i.e.,\n\nτm\n\ndu dt\n\n=\n\n−u(t) + R I(t) .\n\n(1.29)\n\nThe solution, given by Eq. (1.15), corresponds to the convolution of the input I(t) with a causal linear ﬁlter κ(s) = (1/C) e(−s/τm) for s > 0. In order to determine the response amplitude u0 to a periodic drive I(t) = I0 eiωt we need to calculate the Fourier transform\nof κ:\n\n|κˆ (ω)| =\n\n1 C\n\n∞\n\n−t\ne τm\n\ne−iωt\n\ndt\n\n0\n\n1 =\nC\n\nτm 1 + iωτm\n\n.\n\n(1.30)\n\nFor ωτm 1 the right-hand side is proportional to ω−1. Therefore the amplitude of the response to a periodic input decreases at high frequencies.\n\n1.4 Limitations of the leaky integrate-and-ﬁre model\n\nThe leaky integrate-and-ﬁre model presented in Section 1.3 is highly simpliﬁed and neglects many aspects of neuronal dynamics. In particular, input, which may arise from presynaptic neurons or from current injection, is integrated linearly, independently of the state of the postsynaptic neuron:\n\nτm\n\ndu dt\n\n=\n\n−[u(t) − urest] + R I(t),\n\n(1.31)\n\nwhere I(t) is the input current. Furthermore, after each output spike the membrane potential is reset,\n\nif u(t) = ϑ then lim u(t + δ ) = ur,\nδ →0;δ >0\n\n(1.32)\n\nso that no memory of previous spikes is kept. Let us list the major limitations of the simpliﬁed model discussed so far. All of these limitations will be addressed in the extension of the leaky integrate-and-ﬁre model presented in Part II of the book.\n\n1.4.1 Adaptation, bursting, and inhibitory rebound\nTo study neuronal dynamics experimentally, neurons can be isolated and stimulated by current injection through an intracellular electrode. In a standard experimental protocol we could, for example, impose a stimulating current that is switched at time t0 from a value I1 to a new value I2. Let us suppose that I1 = 0 so that the neuron is quiescent for t < t0. If the current I2 is sufﬁciently large, it will evoke spikes for t > t0. Most neurons will respond to the current step with a spike train where intervals between spikes increase\n\n20 (a)\n\nIntroduction (b)\n\nI=0\n\nI2 I=0\n\nI2\n\n(c)\n\n(d)\n\nI2\n\nI=0\n\nI=0\n\nt0\n\nI1\n\nt0\n\nFig. 1.10 Response to a current step. In (a)–(c), the current is switched on at t = t0 to a value I2 > 0. Fast-spiking neurons (a) have short interspike intervals without adaptation while regularspiking neurons (c) exhibit adaptation, visible as an increase in the duration of interspike intervals.\nAn example of a stuttering neuron is shown in (b). Many neurons emit an inhibitory rebound spike (d) after an inhibitory current I1 < 0 is switched off. Data is courtesy of Henry Markram and Maria Toledo-Rodriguez (Markram et al., 2004; Toledo-Rodriguez et al., 2004).\n\nsuccessively until a steady state of periodic ﬁring is reached; see Fig. 1.10c. Neurons that show this type of adaptation are called regularly ﬁring neurons (Connors and Gutnick, 1990). Adaptation is a slow process that builds up over several spikes. Since the standard leaky integrate-and-ﬁre model resets the voltage after each spike to the same value and restarts the integration process, no memory is kept beyond the most recent spike. Therefore, the leaky integrate-and-ﬁre neuron cannot capture adaptation. Detailed neuron models, which will be discussed in Chapter 2, explicitly describe the slow processes that lead to adaptation. To mimic these processes in integrate-and-ﬁre neurons, we need to add up the contributions to refractoriness of several spikes back in the past. As we shall see in Chapter 6, this can be done in the “ﬁlter” framework of Eq. (1.22) by using a ﬁlter η for refractoriness with a time constant much slower than that of the membrane potential, or by combining the differential equation of the leaky integrate-and-ﬁre model with a second differential equation describing the evolution of a slow variable; see Chapter 6.\nA second class of neurons are fast-spiking neurons. These neurons show no adaptation (see Fig. 1.10a) and can therefore be well approximated by non-adapting integrate-and-ﬁre models. Many inhibitory neurons are fast-spiking neurons. Apart from regular-spiking and fast-spiking neurons, there are also bursting and stuttering neurons which form a separate group (Connors and Gutnick, 1990). These neurons respond to constant stimulation by a sequence of spikes that is periodically (bursting) or aperiodically (stuttering) interrupted by rather long intervals; see Fig. 1.10b. Again, a neuron model that has no memory beyond the most recent spike cannot describe bursting, but the framework in Eq. (1.22) with arbitrary “ﬁlters” is general enough to account for bursting as well.\nAnother frequently observed behavior is post-inhibitory rebound. Consider a step current with I1 < 0 and I2 = 0, i.e., an inhibitory input that is switched off at time t0; see Fig. 1.10d.\n\n1.4 Limitations of the leaky integrate-and-ﬁre model\n\n21\n\nMany neurons respond to such a change with one or more “rebound spikes”; even the release of inhibition can trigger action potentials. We will return to inhibitory rebound in Chapter 3.\n\n1.4.2 Shunting inhibition and reversal potential\n\nIn the previous section we focused on an isolated neuron stimulated by an applied current.\n\nIn reality, neurons are embedded into a large network and receive input from many other\n\nneurons.\n\nSuppose\n\na\n\nspike\n\nfrom\n\na\n\npresynaptic\n\nneuron\n\nj\n\nis\n\nsent\n\nat\n\ntime\n\nt\n\nf j\n\ntowards\n\nthe\n\nsynapse\n\nof a postsynaptic neuron i. When we introduced in Fig. 1.5 the postsynaptic potential that\n\nis generated after the arrival of the spike at the synapse, its shape and amplitude did not\n\ndepend on the state of the postsynaptic neuron i. This is of course a simpliﬁcation and\n\nreality is somewhat more complicated. In Chapter 3 we will discuss detailed neuron models\n\nthat describe synaptic input as a change of the membrane conductance. Here we simply\n\nsummarize the major phenomena.\n\nIn Fig. 1.11 we have sketched schematically an experiment where the neuron is driven\n\nby a constant current I0. We assume that I0 is too weak to evoke ﬁring so that, after some relaxation time, the membrane potential settles at a constant value u0. At t = t f one of the\n\npresynaptic neurons emits a spike so that shortly afterwards the action potential arrives at\n\nthe synapse and provides additional stimulation of the postsynaptic neuron. More precisely,\n\nthe spike generates a current pulse at the postsynaptic neuron (postsynaptic current, PSC)\n\nwith amplitude\n\nPSC ∝ [u0 − Esyn]\n\n(1.33)\n\nwhere u0 is the membrane potential and Esyn is the “reversal potential” of the synapse. Since the amplitude of the current input depends on u0, the response of the postsynaptic potential does so as well. Reversal potentials are systematically introduced in Chapter 2; models of synaptic input are discussed in Section 3.1.\n\nExample: Shunting inhibition\nThe dependence of the postsynaptic response upon the momentary state of the neuron is most pronounced for inhibitory synapses. The reversal potential of inhibitory synapses Esyn is below, but usually close to the resting potential. Input spikes thus have hardly any effect on the membrane potential if the neuron is at rest; see Fig. 1.11a. However, if the membrane is depolarized, the very same input spikes evoke a larger inhibitory postsynaptic potential. If the membrane is already hyperpolarized, the input spike can even produce a depolarizing effect. There is an intermediate value u0 = Esyn – the reversal potential – where the response to inhibitory input “reverses” from hyperpolarizing to depolarizing.\nThough inhibitory input usually has only a small impact on the membrane potential, the local conductivity of the cell membrane can be signiﬁcantly increased. Inhibitory\n\n22\n\nIntroduction\n\n(a)\n\n(b)\n\nu\n\nu\n\nurest\n\nurest\n\ntf\n\ntf\n\nFig. 1.11 The shape of postsynaptic potentials depends on the momentary level of depolarization. (a) A presynaptic spike that arrives at time t f at an inhibitory synapse has hardly any effect on the membrane potential when the neuron is at rest, but a large effect if the membrane potential u is above the resting potential. If the membrane is hyperpolarized below the reversal potential of the inhibitory synapse, the response to the presynaptic input changes sign. (b) A spike at an excitatory synapse evokes a postsynaptic potential with an amplitude that depends only slightly on the momentary voltage u. For large depolarizations the amplitude saturates and becomes smaller. (Schematic ﬁgure.)\n\nsynapses are often located on the soma or on the shaft of the dendritic tree. Owing to their strategic position, a few inhibitory input spikes can “shunt” the whole input that is gathered by the dendritic tree from hundreds of excitatory synapses. This phenomenon is called “shunting inhibition.”\nThe reversal potential for excitatory synapses is usually signiﬁcantly above the resting potential. If the membrane is depolarized u0 urest the amplitude of an excitatory postsynaptic potential is reduced, but the effect is not as pronounced as for inhibition. For very high levels of depolarization a saturation of the EPSPs can be observed; see Fig. 1.11b.\n\n1.4.3 Conductance changes after a spike\n\nThe shape of the postsynaptic potentials depends not only on the level of depolarization but,\n\nmore generally, on the internal state of the neuron, e.g., on the timing relative to previous\n\naction potentials.\n\nSuppose that an action potential has occurred at time tif and that a presynaptic spike\n\narrives\n\nat\n\na\n\ntime\n\nt\n\nf j\n\n> tif\n\nat\n\nthe\n\nsynapse\n\nj.\n\nThe\n\nform\n\nof\n\nthe\n\npostsynaptic\n\npotential\n\ndepends\n\nnow\n\non\n\nthe\n\ntime\n\nt\n\nf j\n\n−\n\ntif\n\n;\n\nsee\n\nFig.\n\n1.12.\n\nIf\n\nthe\n\npresynaptic\n\nspike\n\narrives\n\nduring\n\nor\n\nshortly\n\nafter a postsynaptic action potential, it has little effect because some of the ion channels\n\nthat were involved in ﬁring the action potential are still open. If the input spike arrives\n\nmuch later, it generates a postsynaptic potential of the usual size. We will return to this\n\neffect in Chapter 2.\n\n1.4.4 Spatial structure\nThe form of postsynaptic potentials also depends on the location of the synapse on the dendritic tree. Synapses that are located far away from the soma are expected to evoke a\n\n1.5 What can we expect from integrate-and-ﬁre models?\n\n23\n\nurest\n\nt\n\nf i\n\nt\n\nf j\n\nt\n\nf j\n\nFig. 1.12 The shape of postsynaptic potentials (dashed lines) depends on the time t − tif that has\n\npassed since the last output spike of neuron i. The postsynaptic spike has been triggered at time tif .\n\nA\n\npresynaptic\n\nspike\n\nthat\n\narrives\n\nat\n\ntime\n\nt\n\nf j\n\nshortly\n\nafter\n\nthe\n\nspike\n\nof\n\nthe\n\npostsynaptic\n\nneuron\n\nhas\n\na\n\nsmaller effect than a spike that arrives much later. Data is courtesy of Thomas Berger (Berger et al.,\n\n2009).\n\nsmaller postsynaptic response at the soma than a synapse that is located directly on the soma; see Chapter 3. If several inputs occur on the same dendritic branch within a few milliseconds, the ﬁrst input will cause local changes of the membrane potential that inﬂuence the amplitude of the response to the input spikes that arrive slightly later. This may lead to saturation or, in the case of so-called “active” currents, to an enhancement of the response. Such nonlinear interactions between different presynaptic spikes are neglected in the leaky integrate-and-ﬁre model. Whereas a purely linear dendrite can be incorporated in the “ﬁlter” description of the model, as we shall see in Chapter 6, nonlinear interactions cannot. Small regions on the dendrite where a strong nonlinear boosting of synpatic currents occurs are sometimes called dendritic “hot spots.” The boosting can lead to dendritic spikes which, in contrast to normal somatic action potentials last for tens of milliseconds (Larkum and Nevian, 2008).\n\n1.5 What can we expect from integrate-and-ﬁre models?\nThe leaky integrate-and-ﬁre model is an extremely simpliﬁed neuron model. As we have seen in the previous section, it neglects many features that neuroscientists have observed when they study neurons in the living brain or in slices of brain tissue. Therefore the question arises: what should we expect from such a model? Clearly we cannot expect it to explain the complete biochemistry and biophysics of neurons. Nor do we expect it to account for highly nonlinear interactions that are caused by active currents in some “hot spots” on the dendritic tree. However, the integrate-and-ﬁre model is surprisingly accurate when it comes to generating spikes, i.e., precisely timed events in time. Thus, it could potentially be a valid model of spike generation in neurons, or more precisely, in the soma.\nIt is reasonable to require from a model of spike generation that it should be able to predict the moments in time when a real neuron spikes. Let us look at the following schematic\n\n24\n\nIntroduction\n\nNeuron\n\nI (t )\nMathematical neuron model\n\nModel optimization\n\nPrediction\n\nFig. 1.13 The challenge of spike time prediction. A current I(t) is experimentally injected into the soma of a real neuron in vitro through an electrode. The response of the neuron is recorded and half of the response is made available for model optimization while part of the response remains hidden. The challenge is then to use the input I(t) to predict the spike times of the hidden response with a mathematical neuron model.\nset-up (Fig. 1.13). An experimenter injects a time-dependent input current I(t) into the soma of a cortical neuron using a ﬁrst electrode. With an independent second electrode he or she measures the voltage at the soma of the neuron. Not surprisingly, the voltage trajectory contains from time to time sharp electrical pulses. These are the action potentials or spikes.\nA befriended mathematical neuroscientist now takes the time course I(t) of the input current that was used by the experimenter together with the time course of the membrane potential of the neuron and adjusts the parameters of a leaky integrate-and-ﬁre model so that the model generates, for the very same input current, spikes at roughly the same moments in time as the real neuron. This needs some parameter tuning, but seems feasible. The relevant and much more difﬁcult question, however, is whether the neuron model can now be used to predict the ﬁring times of the real neuron for a novel time-dependent input current that was not used during parameter optimization (Fig. 1.13).\nAs discussed above, neurons not only show refractoriness after each spike but also exhibit adaptation which builds up over hundreds of milliseconds. A simple leaky integrateand-ﬁre model does not perform well at predicting the spike times of a real neuron. However, if adaptation (and refractoriness) is added to the neuron model, the prediction works surprisingly well. A straightforward way to add adaptation is to make the ﬁring threshold of the neuron model dynamic: after each spike the threshold ϑ is increased by an amount θ , while during a quiescent period the threshold approaches its stationary value ϑ0. We can\n\nu [mV]\n\n40 20\n0 –20 –40 –60 –80\n0\n\n1.6 Summary\n\n25\n\n100\n\n200\n\n300\n\n400\n\n500\n\nMissed\n\nt [ms]\n\nExtra\n\nFig. 1.14 Comparing a generalized integrate-and-ﬁre model with experimental traces. A voltage trace (thick black trace) recorded in a real neuron driven by a ﬂuctuating current is superposed on the voltage trace generated by a generalized integrate and ﬁre model (thin line) driven by the same current. The subthreshold voltage ﬂuctuations are accurately predicted (inset) and the spike timings are well predicted on average, apart from a few additional or missed spikes (arrows).\n\nuse the Dirac δ -function to express this idea\n\n∑ τadapt\n\nd dt\n\nϑ\n\n(t\n\n)\n\n=\n\n−[ϑ\n\n(t\n\n)\n\n−\n\nϑ0]\n\n+\n\nθ\n\nf\n\nδ (t − t f )\n\n(1.34)\n\nwhere τadapt is the time constant of adaptation (a few hundred milliseconds) and t f = t(1),t(2),t(3), ... are the ﬁring times of the neuron.\nThe predictions of an integrate-and-ﬁre model with adaptive threshold agree nicely with the voltage trajectory of a real neuron, as can be seen from Fig. 1.14. The problem of how to construct practical, yet powerful, generalizations of the simple leaky integrate-and-ﬁre model is the main topic of Part II of the book. Another question arising from this is how to quantify the performance of such neuron models (see Chapter 11).\nOnce we have identiﬁed good candidate neuron models, we will ask in Part III whether we can construct big populations of neurons with these models, and whether we can use them to understand the dynamic and computational principles as well as potential neural codes used by populations of neurons. Indeed, as we shall see, it is possible to make the transition from plausible single-neuron models to large and structured populations. This does not mean that we understand the full brain, but understanding the principles of large populations of neurons from well-tested simpliﬁed neuron models is a ﬁrst and important step in this direction.\n\n1.6 Summary\nThe neuronal signal consists of short voltage pulses called action potentials or spikes. These pulses travel along the axon and are distributed to several postsynaptic neurons where they evoke postsynaptic potentials. If a postsynaptic neuron receives a sufﬁcient\n\n26\n\nIntroduction\n\nnumber of spikes from several presynaptic neurons within a short time window, its membrane potential may reach a critical value and an action potential is triggered. We say that the neuron has “ﬁred” a spike. This spike is the neuronal output signal which is, in turn, transmitted to other neurons.\nA particularly simple model of a spiking neuron is the leaky integrate-and-ﬁre model. First, a linear differential equation describes how input currents are integrated and transformed into a membrane voltage u(t). Here the input can be the input current injected by an experimenter into an isolated neuron or synaptic input currents caused by spikes arriving from other neurons in a large and highly connected network. Second, the model neuron generates an output spike if the membrane voltage reaches the threshold ϑ . Finally, after spike ﬁring, the integration of the linear differential equation resumes from a reset value ur.\nThe simple leaky integrate-and-ﬁre model does not account for long-lasting refractoriness or adaptation. However, if the voltage dynamics of the leaky integrate-and-ﬁre model is enhanced by mechanisms of adaptation, then it can be a powerful tool to accurately predict spike times of cortical neurons. Such generalized integrate-and-ﬁre models are the main topic of Part II.\n\nLiterature\nAn elementary, non-technical introduction to neurons and synapses can be found in the book by Thompson (1993). At an intermediate level is the introductory textbook of Purves et al. (2008) while the Principles of Neural Science by Kandel et al. (2000) can be considered as a standard textbook on neuroscience covering a wealth of experimental results.\nThe use of mathematics to explain neuronal activity has a long tradition in theoretical neuroscience, over one hundred years. Phenomenological spiking neuron models similar to the leaky integrate-and-ﬁre model were proposed in 1907 by Lapicque, who wanted to predict the ﬁrst spike after stimulus onset (so that his model did not yet have the reset of the membrane potential after ﬁring), and have been developed further in different variants by others (Lapicque, 1907; Hill, 1936; McCulloch and Pitts, 1943; Stein, 1965; Geisler and Goldberg, 1966; Weiss, 1966; Stein, 1967b). For the “ﬁlter” description of integrateand-ﬁre models see, for example, Gerstner et al. (1996b) and Pillow et al. (2008). The elegance and simplicity of integrate-and-ﬁre models makes them a widely used tool to describe principles of neural information processing in neural networks of a broad range of sizes.\nA different line of mathematical neuron models are biophysical models, ﬁrst developed by Hodgkin and Huxley (1952); these biophysical models are the topic of the next chapter.\n\n1.6 Summary\n\n27\n\nExercises\n\n1. Synaptic current pulse. Synaptic inputs can be approximated by an exponential current I(t) =\n\nq\n\n1 τs\n\nexp[−\n\nt\n\n−t τs\n\nf\n\n]\n\nfor t\n\n>tf\n\nwhere t f\n\nis\n\nthe\n\nmoment\n\nwhen\n\nthe\n\nspike\n\narrives\n\nat\n\nthe\n\nsynapse.\n\n(a) Use Eq. (1.5) to calculate the response of a passive membrane with time constant τm to an\n\ninput spike arriving at time t f .\n\n(b) In the solution resulting from (a), take the limit τs → τm and show that in this limit the\n\nresponse\n\nis\n\nproportional\n\nto\n\n∝\n\n[t\n\n−\n\nt\n\nf\n\n]\n\nexp[−\n\nt−t τs\n\nf\n\n].\n\nA\n\nfunction\n\nof\n\nthis\n\nform\n\nis\n\nsometimes\n\ncalled\n\nan\n\nα -function.\n\n(c) In the solution resulting from (a), take the limit τs → 0. Can you relate your result to the discussion of the Dirac-δ function?\n\n2. Time-dependent solution. Show that Eq. (1.15) is a solution of the differential equation Eq. (1.5)\n\nfor time-dependent input I(t). To do so, start by changing the variable in the integral from s to\n\nt = t − s. Then take the derivative of Eq. (1.15) and compare the terms to those on both sides of\n\nthe differential equation.\n\n3. Chain of linear equations. Suppose that arrival of a spike at time t f releases neurotransmit-\n\nter\n\ninto\n\nthe\n\nsynaptic\n\ncleft.\n\nThe\n\namount\n\nof\n\navailable\n\nneurotransmitter\n\nat\n\ntime\n\nt\n\nis\n\nτx\n\ndx dt\n\n=\n\n−x\n\n+\n\nδ (t − t f ) . The neurotransmitter binds to the postsynaptic membrane and opens channels that\n\nenable\n\na\n\nsynaptic\n\ncurrent\n\nτs\n\ndI dt\n\n=\n\n−I\n\n+ I0 x(t) .\n\nFinally,\n\nthe\n\ncurrent\n\ncharges\n\nthe\n\npostsynaptic\n\nmem-\n\nbrane\n\naccording\n\nto\n\nτm\n\ndu dt\n\n=\n\n−u + R I(t).\n\nWrite\n\nthe\n\nvoltage\n\nresponse\n\nto\n\na\n\nsingle\n\ncurrent\n\npulse\n\nas\n\nan integral.\n\n2\nIon channels and the Hodgkin–Huxley model\nFrom a biophysical point of view, action potentials are the result of currents that pass through ion channels in the cell membrane. In an extensive series of experiments on the giant axon of the squid, Hodgkin and Huxley succeeded in measuring these currents and described their dynamics in terms of differential equations. Their paper published in 1952, which presents beautiful experiments combined with an elegant mathematical theory (Hodgkin and Huxley, 1952), was rapidly recognized as groundbreaking work and eventually led to the Nobel Prize for Hodgkin and Huxley in 1963. In this chapter, the Hodgkin– Huxley model is reviewed and its behavior illustrated by several examples.\nThe Hodgkin–Huxley model in its original form describes only three types of ion channel. However, as we shall see in Section 2.3 it can be extended to include many other ion channel types. The Hodgkin–Huxley equations are the basis for detailed neuron models which account for different types of synapse, and the spatial geometry of an individual neuron. Synaptic dynamics and the spatial structure of dendrites are the topics of Chapter 3. The Hodgkin–Huxley model is also the starting point for the derivation of simpliﬁed neuron models in Chapter 4 and will serve as a reference throughout the discussion of generalized integrate-and-ﬁre models in Part II of the book.\nBefore we can turn to the Hodgkin–Huxley equations, we need to give some additional information on the equilibrium potential of ion channels.\n2.1 Equilibrium potential Neurons, just as other cells, are enclosed by a membrane which separates the interior of the cell from the extracellular space. Inside the cell the concentration of ions is different from that in the surrounding liquid. The difference in concentration generates an electrical potential which plays an important role in neuronal dynamics. In this section, we provide some background information and give an intuitive explanation of the equilibrium potential.\n\n(a) E = qu\n\n2.1 Equilibrium potential (b) n1 (inside)\n\n29 Δ u\n\nn2 (outside)\nFig. 2.1 (a) At thermal equilibrium, positive ions in an electric ﬁeld will be distributed so that fewer ions are in a state of high energy and more at low energy. Thus a voltage difference generates a gradient in concentration. (b) Similarly, a difference in ion concentration generates an electrical potential. The concentration n2 inside the neuron is different from the concentration n1 of the surround. The resulting potential is called the Nernst potential. The solid line indicates the cell membrane. Ions can pass through the gap.\n\n2.1.1 Nernst potential\n\nFrom the theory of thermodynamics, it is known that the probability of a molecule taking a state of energy E is proportional to the Boltzmann factor, p(E) ∝ exp(−E/kT ), where k is the Boltzmann constant and T the temperature. Let us consider positive ions with charge q in a static electrical ﬁeld. Their energy at location x is E(x) = q u(x) where u(x) is the potential at x. The probability of ﬁnding an ion in the region around x is therefore proportional to exp[−q u(x)/kT ]. Since the number of ions is huge, we may interpret the probability as an ion density. For ions with positive charge q > 0, the ion density is therefore higher in regions with low potential u. Let us write n(x) for the ion density at point x. The relation between the density at point x1 and point x2 is\n\nn(x1) = exp − q u(x1) − q u(x2) .\n\nn(x2)\n\nkT\n\n(2.1)\n\nA difference in the electrical potential Δu = u(x1) − u(x2) generates therefore a difference in ion density; see Fig. 2.1.\nSince this is a statement about an equilibrium state, the reverse must also be true. A difference in ion density generates a difference Δu in the electrical potential. We consider two regions of ions with concentration n1 and n2, respectively; see Fig. 2.1b. Solving (2.1) for Δu we ﬁnd that, at equilibrium, the concentration difference generates a voltage\n\nΔu = k T ln n2 q n1\n\n(2.2)\n\nwhich is called the Nernst potential (Hille, 2001).\n\n30\n\nThe Hodgkin–Huxley Model\n\n2.1.2 Reversal potential\nThe cell membrane consists of a thin bilayer of lipids and is a nearly perfect electrical insulator. Embedded in the cell membrane are, however, speciﬁc proteins which act as ion gates. A ﬁrst type of gate is the ion pumps, a second one is ion channels. Ion pumps actively transport ions from one side to the other. As a result, ion concentrations in the intracellular liquid differ from those of the surround. For example, the sodium concentration inside a mammalian neuron (≈ 10 mM) is lower than that in the extracellular liquid (≈ 145 mM). On the other hand, the potassium concentration inside is higher (≈ 140 mM) than in the surround (≈ 5 mM) (Purves et al., 2008). For the giant axon of the squid which was studied by Hodgkin and Huxley the numbers are slightly different, but the basic idea is the same: there is more sodium outside the cell than inside, while the reverse is true for potassium.\nLet us focus for the moment on sodium ions. At equilibrium the difference in concentration causes a Nernst potential ENa of about +67 mV. That is, at equilibrium the interior of the cell has a positive potential with respect to the surround. The interior of the cell and the surrounding liquid are in contact through ion channels where Na+ ions can pass from one side of the membrane to the other. If the voltage difference Δu is smaller than the value of the Nernst potential ENa, more Na+ ions ﬂow into the cell so as to decrease the concentration difference. If the voltage is larger than the Nernst potential ions would ﬂow out the cell. Thus the direction of the current is reversed when the voltage Δu passes ENa. For this reason, ENa is called the reversal potential.\n\nExample: Reversal potential for potassium\nAs mentioned above, the ion concentration of potassium is higher inside the cell (≈ 140 mM) than in the extracellular liquid (≈ 5 mM). Potassium ions have a single positive charge q = 1.6 × 10−19 C. Application of the Nernst formula, (2.2), with the Boltzmann constant k = 1.4 × 10−23 J/K yields EK ≈ −83 mV at room temperature. The reversal potential for K+ ions is therefore negative.\n\nExample: Resting potential\nSo far we have considered the presence of either sodium or potassium. In real cells, these and other ion types are simultaneously present and contribute to the voltage across the membrane. It is found experimentally that the resting potential of the membrane is about urest ≈ 65 mV. Since EK < urest < ENa, potassium ions, at the resting potential, ﬂow out of the cell while sodium ions ﬂow into the cell. In the stationary state, the active ion pumps balance this ﬂow and transport just as many ions back as pass through\n\n2.2 Hodgkin–Huxley model\n\n31\n\nInside –– –\n\nK+ –\n\n++ Outside\n\n++ Na+\n\nI\n\nC\n\nR\n\nEL\n\nRNa ENa\n\nRK u EK\n\nFig. 2.2 Schematic diagram for the Hodgkin–Huxley model.\nthe channels. The value of urest is determined by the dynamic equilibrium between the ion ﬂow through the channels (permeability of the membrane) and active ion transport (efﬁciency of the ion pump in maintaining the concentration difference).\n\n2.2 Hodgkin–Huxley model\nHodgkin and Huxley (1952) performed experiments on the giant axon of the squid and found three different types of ion current, namely, sodium, potassium, and a leak current that consists mainly of Cl− ions. Speciﬁc voltage-dependent ion channels, one for sodium and another one for potassium, control the ﬂow of those ions through the cell membrane. The leak current takes care of other channel types which are not described explicitly.\n2.2.1 Deﬁnition of the model\nThe Hodgkin–Huxley model can be understood with the help of Fig. 2.2. The semipermeable cell membrane separates the interior of the cell from the extracellular liquid and acts as a capacitor. If an input current I(t) is injected into the cell, it may add further charge on the capacitor, or leak through the channels in the cell membrane. Each channel type is represented in Fig. 2.2 by a resistor. The unspeciﬁc channel has a leak resistance R, the sodium channel a resistance RNa and the potassium channel a resistance RK. The diagonal arrow across the diagram of the resistor indicates that the value of the resistance is not ﬁxed, but changes depending on whether the ion channel is open or closed. Because of active ion transport through the cell membrane, the ion concentration inside the cell is different from that in the extracellular liquid. The Nernst potential generated by the difference in ion concentration is represented by a battery in Fig. 2.2. Since the Nernst potential is different for each ion type, there are separate batteries for sodium, potassium, and the unspeciﬁc third channel, with battery voltages ENa, EK and EL, respectively.\nLet us now translate the above schema of an electrical circuit into mathematical equations. The conservation of electric charge on a piece of membrane implies that the applied current I(t) may be split into a capacitive current IC which charges the capacitor C and\n\n32 (a)\n\nThe Hodgkin–Huxley Model (b)\n\nFig. 2.3 The Hodgkin–Huxley model. (a) The equilibrium functions for the three variables m, n, h in the Hodgkin–Huxley model. (b) The voltage-dependent time constant. The resting potential is at u = −65 mV (arrow) and parameters are those given in Table 2.1.\n\nfurther components Ik which pass through the ion channels. Thus\n\nI(t) = IC(t) + ∑ Ik(t),\n\n(2.3)\n\nk\n\nwhere the sum runs over all ion channels. In the standard Hodgkin–Huxley model there are only three types of channel: a sodium channel with index Na, a potassium channel with index K and an unspeciﬁc leakage channel with resistance R; see Fig. 2.2. From the deﬁnition of a capacity C = q/u where q is a charge and u the voltage across the capacitor, we ﬁnd the charging current IC = C du/dt. Hence from (2.3)\n\ndu C\ndt\n\n=\n\n− ∑ Ik(t) + I(t) . k\n\n(2.4)\n\nIn biological terms, u is the voltage across the membrane and ∑k Ik is the sum of the ionic currents which pass through the cell membrane.\nAs mentioned above, the Hodgkin–Huxley model describes three types of channel. All channels may be characterized by their resistance or, equivalently, by their conductance. The leakage channel is described by a voltage-independent conductance gL = 1/R. Since u is the total voltage across the cell membrane and EL the voltage of the battery, the voltage at the leak resistor in Fig. 2.2 is u − EL. Using Ohm’s law, we get a leak current IL = gL (u − EL).\nThe mathematics of the other ion channels is analogous except that their conductance is voltage- and time-dependent. If all channels are open, they transmit currents with a maximum conductance gNa or gK, respectively. Normally, however, some of the channels are blocked. The breakthrough of Hodgkin and Huxley was that they succeeded in measuring how the effective resistance of a channel changes as a function of time and voltage. Moreover, they proposed a mathematical description of their observations. Speciﬁcally, they introduced additional “gating” variables m, n and h to model the probability that a\n\n2.2 Hodgkin–Huxley model\n\n33\n\nx Ex [mV] gx [mS/cm2]\n\nNa\n\n55\n\n40\n\nK −77\n\n35\n\nL −65\n\n0.3\n\nx\n\nαx(u / mV) [ms−1]\n\nβx(u / mV) [ms−1]\n\nn 0.02(u − 25) / [1 − e−(u−25)/9] −0.002(u − 25) / [1 − e(u−25)/9]\n\nm 0.182(u + 35) / [1 − e−(u+35)/9] −0.124(u + 35) / [1 − e(u+35)/9]\n\nh\n\n1 / [1 + e−(u+62)/6]\n\n4e(u+90)/12 / [1 + e−(u+62) / 6]\n\nTable 2.1 Parameters for the Hodgkin–Huxley equations ﬁtted on pyramidal neurons of the cortex. The parameters for n and m were ﬁtted by Zach Mainen (Mainen et al., 1995) on experiments reported by Huguenard et al. (1988) and the parameters for h by Richard Naud on the experiments reported in Hamill et al. (1991). Voltage is measured in mV and the membrane capacity is C = 1 μF/cm2.\n\nchannel is open at a given moment in time. The combined action of m and h controls the Na+ channels while the K+ gates are controlled by n. For example, the effective conductance of sodium channels is modeled as 1/RNa = gNa m3 h, where m describes the activation (opening) of the channel and h its inactivation (blocking). The conductance of potassium is 1/RK = gK n4, where n describes the activation of the channel.\nIn summary, Hodgkin and Huxley formulated the three ion currents on the right-hand-\n\nside of (2.4) as\n\n∑ Ik = gNa m3h (u − ENa) + gK n4 (u − EK) + gL (u − EL).\nk\n\n(2.5)\n\nThe parameters ENa, EK, and EL are the reversal potentials. The three gating variables m, n, and h evolve according to differential equations of the\n\nform\n\nx˙\n\n=\n\n−\n\nτx\n\n1 (u)\n\n[x\n\n−\n\nx0(u)],\n\n(2.6)\n\nwith x˙ = dx/dt, and where x stands for m, n, or h. The interpretation of (2.6) is simple: for a\n\nﬁxed voltage u, the variable x approaches the target value x0(u) with a time constant τx(u). The voltage dependence of the time constant and asymptotic value is illustrated in Fig. 2.3.\n\nThe form of the functions plotted in Fig. 2.3, as well as the maximum conductances\n\nand reversal potentials in (2.5), were deduced by Hodgkin and Huxley from empirical\n\nmeasurements.\n\nExample: Voltage step\nExperimenters can hold the voltage across the cell membrane at a desired value by injecting an appropriate current into the cell. Suppose that the experimenter keeps the\n\ngKn4(t)\n\n34 0\n\nThe Hodgkin–Huxley Model\n\n5 t [ms]\n\nFig. 2.4 Original data and ﬁt of\n\nHodgkin and Huxley (1952). The\n\nmeasured time course of the potassium\n\nconductance (circles) after application\n\nof a voltage step of 25 mV and after\n\nreturn to resting potential. The ﬁt (solid\n\n10\n\nline) is based on Eq. (2.8). Adapted\n\nfrom Hodgkin and Huxley (1952).\n\ncell at resting potential u0 = −65 mV for t < t0 and switches the voltage at t0 to a new value u1. Integration of the differential equation (2.6) gives, for t > t0, the dynamics\n\nm(t) = m0(u1) + [m0(u0) − m0(u1)] exp\n\n−(t − t0) τm(u1)\n\n,\n\nh(t) = h0(u1) + [h0(u0) − h0(u1)] exp\n\n−(t − t0) τh(u1)\n\n,\n\n(2.7)\n\nso that, based on the model with given functions for m0(u), h0(u), τm(u), τh(u), we can predict the sodium current INa(t) = gNa [m(t)3] h(t) (u1 − ENa) for t > t0 generated by the\nvoltage step at t = t0. Similarly, the potassium current caused by a voltage step is IK(t) = gK [n(t)4] (u1 −\nEK) with\n\nn(t) = n0(u1) + [n0(u0) − n0(u1)] exp\n\n−(t − t0) τn(u1)\n\n.\n\n(2.8)\n\nHodgkin and Huxley used Eqs. (2.7) and (2.8) to work the other way round. After\nblocking the sodium channel with appropriate pharmacological agents, they applied\na voltage step and measured the time course of the potassium current. Dividing the\nrecorded current through the driving potential (u1 − EK) yields the time-dependent conductance gK [n(t)4]; see Fig. 2.4. Using (2.8), Hodgkin and Huxley deduced the value of n0(u1) and τn(u1) as well as the exponent of 4 in n4(t) for potassium. Repeating the experiments for different values u1 gives the experimental curves for n0(u) and τn(u).\n\nExample: Activation and de-inactivation\nThe variable m is called an activation variable. To understand this terminology, we note from Fig. 2.3 that the value of m0(u) at the neuronal resting potential of u = −65 mV is close to zero. Therefore, at rest, the sodium current INa = gNa m3h (u − ENa) through the channel vanishes. In other words, the sodium channel is closed.\nWhen the membrane potential increases signiﬁcantly above the resting potential, the gating variable m increases to its new value m0(u). As long as h does not change, the\n\n2.2 Hodgkin–Huxley model\n\n35\n\n-40\n\n-140\n\nFig. 2.5 Stochastic channel activation. The current ﬂowing through a small patch of membrane after application of a voltage step (top row) shows steplike changes and is different in each trial (subsequent traces). Averaging over many trials yields the bottom trace. Adapted from Patlak and Ortiz (1985) . ©1985 Rockefeller University Press. Originally published in Journal of General Physiology, 86: 89–104.\n3 pA\n10 ms\nsodium current increases and the gate opens. Therefore the variable m “activates” the channel. If, after a return of the voltage to rest, m decays back to zero, it is said to be “de-activating.”\nThe terminology of the “inactivation” variable h is analogous. At rest, h has a large positive value. If the voltage increases to a value above −40 mV, h approaches a new value h0(u) which is close to rest. Therefore the channel “inactivates” (blocks) with a time constant that is given by τh(u). If the voltage returns to zero, h increases so that the channel undergoes “de-inactivation.” This sounds like tricky vocabulary, but it turns out to be useful to distinguish between a deactivated channel (m close to zero and h close to 1) and an inactivated channel (h close to zero).\n2.2.2 Stochastic channel opening\nThe number of ion channels in a patch of membrane is ﬁnite, and individual ion channels open and close stochastically. Thus, when an experimenter records the current ﬂowing through a small patch of membrane, he does not ﬁnd a smooth and reliable evolution of the measured variable over time but rather a highly ﬂuctuating current, which looks different at each repetition of the experiment (Fig. 2.5).\nThe Hodgkin–Huxley equations, which describe the opening and closing of ion channels with deterministic equations for the variables m, h, and n, correspond to the current density through a hypothetical, extremely large patch of membrane containing an inﬁnite number of channels or, alternatively, to the current through a small patch of membrane but averaged over many repetitions of the same experiment (Fig. 2.5). The stochastic aspects can be included by adding appropriate noise to the model.\n\n36\n\nThe Hodgkin–Huxley Model\n\nExample: Time constants, transition rates, and channel kinetics\n\nAs an alternative to the formulation of channel gating in Eq. (2.6), the activation and inactivation dynamics of each channel type can also be described in terms of voltagedependent transition rates α and β ,\n\nm˙ = αm(u) (1 − m) − βm(u) m,\n\nn˙ = αn(u) (1 − n) − βn(u) n,\n\n(2.9)\n\nh˙ = αh(u) (1 − h) − βh(u) h.\n\nThe two formulations Eqs. (2.6) and (2.9) are equivalent. The asymptotic value x0(u) and the time constant τx(u) are given by the transformation x0(u) = αx(u)/[αx(u) + βx(u)] and τx(u) = [αx(u) + βx(u)]−1. The various functions α and β , given in Table 2.1, are empirical functions of u that produce the curves in Fig. 2.3.\nEquations (2.9) are typical equations used in chemistry to describe the stochastic dynamics of an activation process with rate constants α and β . We may interpret this process as a molecular switch between two states with voltage-dependent transition rates.\nFor example, the activation variable n can be interpreted as the probability of ﬁnding a single potassium channel open. Therefore in a patch with K channels, k ≈ (1 − n)K channels are expected to be closed. We may interpret αn(u)Δt as the probability that in a short time interval Δt one of the momentarily closed channels switches to the open state.\n\n2.2.3 Dynamics\nIn this section we study the dynamics of the Hodgkin–Huxley model for different types of input. Pulse input, constant input, step current input, and time-dependent input are considered in turn. These input scenarios have been chosen so as to provide an intuitive understanding of the dynamics of the Hodgkin–Huxley model.\nThe most important property of the Hodgkin–Huxley model is its ability to generate action potentials. In Fig. 2.6a an action potential has been initiated by a short current pulse of 1 ms duration applied at t = 1 ms. The spike has an amplitude of nearly 100 mV and a width at half maximum of about 2.5 ms. After the spike, the membrane potential falls below the resting potential and returns only slowly back to its resting value of −65 mV.\nIon channel dynamics during spike generation\nIn order to understand the biophysics underlying the generation of an action potential we return to Fig. 2.3a. We ﬁnd that m0 and n0 increase with u whereas h0 decreases. Thus, if some external input causes the membrane voltage to rise, the conductance of sodium channels increases due to increasing m. As a result, positive sodium ions ﬂow into the cell and raise the membrane potential even further. If this positive feedback is large enough, an action potential is initiated. The explosive increase comes to a natural halt when the membrane potential approaches the reversal potential ENa of the sodium current.\n\n2.2 Hodgkin–Huxley model\n\n37\n\n(a)\n\n(b)\n\n(c)\n\nFig. 2.6 (a) Action potential. The Hodgkin–Huxley model is stimulated by a short, but strong, current pulse between t = 1 ms and t = 2 ms. The time course of the membrane potential u(t) for t > 2 ms shows the action potential (positive peak) followed by a relative refractory period where the potential is below the resting potential urest (dashed line). The right panel shows an expanded view of the action potential between t = 2 ms and t = 5 ms. (b) The dynamics of gating variables m, h, n illustrate how the action potential is mediated by sodium and potassium channels. (c) The sodium current INa which depends on the variables m and h has a sharp peak during the upswing of an action potential. The potassium current IK is controlled by the variable n and starts with a delay compared with INa.\nAt high values of u the sodium conductance is slowly shut off due to the factor h. As indicated in Fig. 2.3b, the “time constant” τh is always larger than τm. Thus the variable h which inactivates the channels reacts more slowly to the voltage increase than the variable m which opens the channel. On a similar slow time scale, the potassium (K+) current sets in Fig. 2.6c. Since it is a current in outward direction, it lowers the potential. The overall effect of the sodium and potassium currents is a short action potential followed by\n\n38 (a)\n\nThe Hodgkin–Huxley Model (b)\n\nFiring frequency [Hz]\n\nu [mV]\n\n0\n-20\n-40\n-60\n0 20 40 60 80 100 120 140 t [ms]\n(c) 50\n\n200 150 100 50\n00\n(d) 100\n\n2 4 6 8 10 I0 [µA/cm2]\n\nFiring frequency [Hz]\n\nu [mV]\n\n0 50\n\n-50\n0 20 40 60 80 100 t [ms]\n\n00\n\n5\n\n10\n\n15\n\n20\n\nI0 [mA/cm2]\n\nFig. 2.7 (a) Spike train of the Hodgkin–Huxley model (with the parameters used in this book) for constant input current I0. (b) Gain function. The mean ﬁring rate ν is plotted as a function of I0. The gain function of the Hodgkin–Huxley model is of type II, because it exhibits a jump. (c) Same as (a), but for the original parameters found by Hodgkin and Huxley to describe the ion currents in the giant axon of the squid. (d) Gain function for the model in (c).\n\na negative overshoot; see Fig. 2.6a. The negative overshoot, called hyperpolarizing spikeafterpotential, is due to the slow de-inactivation of the sodium channel, caused by the h-variable.\nExample: Mean ﬁring rates and gain function\nThe Hodgkin–Huxley equations (2.4)–(2.9) may also be studied for constant input I(t) = I0 for t > 0. (The input is zero for t ≤ 0.) If the value I0 is larger than a critical value Iθ ≈ 2.7 μA/cm2, we observe regular spiking; see Fig. 2.7a. We may deﬁne a ﬁring rate ν = 1/T where T is the interspike interval.\nThe ﬁring rate as a function of the constant input I0, often called the “frequency– current” relation or “f–I plot,” deﬁnes the gain function plotted in Fig. 2.7b. With the parameters given in Table 2.1, the gain function exhibits a jump at Iθ . Gain functions with a discontinuity are called “type II.”\n\n2.2 Hodgkin–Huxley model\n\n39\n\n(a)\n\n(b)\n\nFig. 2.8 (a) Spike train of the Hodgkin–Huxley model driven by a time-dependent input current. The action potentials occur irregularly. The ﬁgure shows the voltage u as a function of time. (b) Threshold effect. A short current pulse of 1 ms is applied which leads to a excursion of the membrane potential of a few millivolts (dashed line). A slight increase of the strength of the current pulse leads to the generation of an action potential (solid line) with an amplitude of about 100 mV above rest (out of bounds).\nIf we shift the curve of the inactivation variable h to more positive voltages, and keep the same parameters otherwise, the modiﬁed Hodgkin–Huxley model exhibits a smooth gain function; see Section 2.3.2 and Fig. 2.11. Neuron models or, more generally, “excitable membranes” are called “type I” or “class I” if they have a continuous frequency–current relation. The distinction between the excitability of type I and II can be traced back to Hodgkin (1948).\nExample: Stimulation by time-dependent input\nIn order to explore a more realistic input scenario, we stimulate the Hodgkin–Huxley model by a time-dependent input current I(t) that is generated by the following procedure. Every 2 ms, a random number is drawn from a Gaussian distribution with zero mean and standard deviation σ = 34 μA/cm2. To get a continuous input current, a linear interpolation was used between the target values. The resulting time-dependent input current was then applied to the Hodgkin–Huxley model (2.4)–(2.6). The response to the current is the voltage trace shown in Fig. 2.8a. Note that action potentials occur at irregular intervals.\nExample: Firing threshold\nIn Fig. 2.8b an action potential (solid line) has been initiated by a short current pulse of 1 ms duration. If the amplitude of the stimulating current pulse is reduced below some\n\n40\n\nThe Hodgkin–Huxley Model\n\ncritical value, the membrane potential (dashed line) returns to the rest value without a large spike-like excursion; see Fig. 2.8b. Thus we have a threshold-type behavior.\nIf we increased the amplitude of the current by a factor of 2, but reduced the duration of the current pulse to 0.5 ms, so that the current pulse delivers exactly the same electric charge as before, the response curves in Fig. 2.8b would look exactly the same. Thus, the threshold of spike initiation can not be deﬁned via the amplitude of the current pulse. Rather, it is the charge delivered by the pulse or, equivalently, the membrane voltage immediately after the pulse, which determines whether an action potential is triggered or not. However, while the notion of a voltage threshold for ﬁring is useful for a qualitative understanding of spike initiation in response to current pulses, it is in itself not sufﬁcient to capture the dynamics of the Hodgkin–Huxley model; see the discussion in this and the next two chapters.\n\nExample: Refractoriness\nIn order to study neuronal refractoriness, we stimulate the Hodgkin–Huxley model by a ﬁrst current pulse that is sufﬁciently strong to excite a spike. A second current pulse of the same amplitude as the ﬁrst one is used to probe the responsiveness of the neuron during the phase of hyperpolarization that follows the action potential. If the second stimulus is not sufﬁcient to trigger another action potential, we have a clear signature of neuronal refractoriness. In the simulation shown in Fig. 2.9, a second spike is not emitted if the second stimulus is given less than 40 ms after the ﬁrst one. It would, of course, be possible to trigger a second spike after a shorter interval, if a signiﬁcantly stronger stimulation pulse was used; for classical experiments along those lines (see, e.g., Fuortes and Mantegazzini 1962).\nIf we look more closely at the voltage trajectory of Fig. 2.9, we see that neuronal refractoriness manifests itself in two different forms. First, owing to the hyperpolarizing spike-afterpotential, the voltage is lower. More stimulation is therefore needed to reach the ﬁring threshold. Second, since a large portion of channels are open immediately after a spike, the resistance of the membrane is reduced compared with the situation at rest. The depolarizing effect of a stimulating current pulse therefore decays faster immediately after the spike than 10 ms later. An efﬁcient description of refractoriness plays a major role in simpliﬁed neuron models discussed in Chapter 6.\nExample: Damped oscillations and transient spiking\nWhen stimulated with a small step-increase in current, the Hodgkin–Huxley model with parameters as in Table 2.1 exhibits a damped oscillation with a maximum of about\n\n2.2 Hodgkin–Huxley model\n\n41\n\nFig. 2.9 Refractoriness of the Hodgkin–Huxley model. At t = 20 ms the model is stimulated by a short current pulse (left arrow) so as to trigger an action potential. A second current pulse of the same amplitude applied at t = 35, 45, or 55 ms (subsequent arrows) is not sufﬁcient to trigger a second action potential.\n20 ms after the onset of the current step; see Fig. 2.10. If the step size is large enough, but not sufﬁcient to cause sustained ﬁring, a single spike can be generated. Note that in Fig. 2.10 the input current returns at 200 ms to the same value it had a hundred milliseconds before. While the neuron stays quiescent after the ﬁrst step, it ﬁres a transient spike the second time not because the total input is stronger but because the step starts from a strong negative value.\nA spike which is elicited by a step current that starts from a strong negative value and then switches back to zero would be called a rebound spike. In other words, a rebound spike is triggered by release from inhibition. For example, the Hodgkin–Huxley model with the original parameters for the giant axon of the squid exhibits rebound spikes when a prolonged negative input current is stopped; the model with the set of parameters adopted in this book, however, does not.\nThe transient spike in Fig. 2.10 occurs about 20 ms after the start of the step. A simple explanation of the transient spike is that the peak of the membrane potential oscillation after the step reaches the voltage threshold for spike initiation, so that a single action potential is triggered. It is indeed the subthreshold oscillations that underly the transient spiking illustrated in Fig. 2.10.\nDamped oscillations result from subthreshold inactivation of the sodium current. At rest the sodium currents are not activated (m ≈ 0) but only partially inactivated (h ≈ 0.6). Responding to the step stimulus, the membrane potential increases, which activates slightly and de-inactivates slowly the sodium channel. When the input is not strong enough for an action potential to be initiated, the de-inactivation of INa reduces the effective drive and thus the membrane potential. The system then relaxes to an equilibrium. If, on the other hand, the current was strong enough to elicit a spike, the equilibrium may be reached only after the spike. A further increase in the step current drives sustained ﬁring (Fig. 2.10).\n\n42\n\nThe Hodgkin–Huxley Model\n\nFig. 2.10 Damped oscillations and a transient spike. Top. The voltage response to a step current shows a damped oscillation (arrow), a single rebound spike (asterisk) or repetitive ﬁring. Bottom. Time course of the stimulating current.\n\n2.3 The zoo of ion channels\nHodgkin and Huxley used their equations to describe the electrophysiological properties of the giant axon of the squid. These equations capture the essence of spike generation by sodium and potassium ion channels. The basic mechanism of generating action potentials is a short inﬂux of sodium ions that is followed by an efﬂux of potassium ions. This mechanism of spike generation is essentially preserved in higher organisms, so that, with the choice of parameters given in Table 2.1, we already have a ﬁrst approximate model of neurons in vertebrates. With a further change of parameters we could adapt the model equations to different temperatures to account for the fact that neurons at 37 degrees Celsius behave differently than neurons in a lab preparation held at a room temperature of 21 degrees Celsius.\nHowever, in order to account for the rich biophysics observed in the neurons of the vertebrate nervous system, two types of ion channel are not enough. Neurons come in different types and exhibit different electrical properties which in turn correspond to a large variety of ion channels. Today, about 200 ion channels are known and many of these have been identiﬁed genetically (Ranjan et al., 2011). In experimental laboratories where the biophysics and functional role of ion channels are investigated, speciﬁc ion channel types can be blocked through pharmacological manipulations. In order to make predictions of blocking results, it is important to develop models that incorporate multiple ion channels. As we shall see below (Section 2.3.1), the mathematical framework of the Hodgkin–Huxley model is well suited for such an endeavor.\nFor other scientiﬁc questions, we may be interested only in the ﬁring pattern of neurons and not in the biophysical mechanisms that give rise to it. Later, in Part II of this book,\n\n2.3 The zoo of ion channels\n\n43\n\nwe will show that generalized integrate-and-ﬁre models can account for a large variety of neuronal ﬁring patterns (Chapter 6) and predict spike timings of real neurons with high precision (Chapter 11). Therefore, in Parts III and IV of the book, where we focus on large networks of neurons, we mainly work with generalized integrate-and-ﬁre rather than biophysical models. Nevertheless, biophysical models, i.e., Hodgkin–Huxley equations with multiple ion channels, serve as an important reference.\n\n2.3.1 Framework for biophysical neuron models\n\nThe formalism of the Hodgkin–Huxley equation is extremely powerful, because it enables researchers to incorporate known ion channel types into a given neuron model. Just as before, the electrical properties of a patch of neuronal membrane are described by the conservation of current\n\nC\n\ndu dt\n\n=\n\n− ∑ Ik(t) + k\n\nI(t) ,\n\n(2.10)\n\nbut in contrast to the simple Hodgkin–Huxley model discussed in the previous section, the right-hand side now contains all types of ion current found in a given neuron. For each ion channel type k, we introduce activation and inactivation variables\n\nIk(t) = gk([Ca++], ...)mpk hqk (u − Ek),\n\n(2.11)\n\nwhere m and h describe activation and inactivation of the channel with equations analogous to (2.6), pk and qk are empirical parameters, Ek is the reversal potential, and gk is the maximum conductance which may depend on secondary variables such as the concentration of calcium, magnesium, dopamine or other substances. In principle, if the dynamics of each channel type (i.e., all parameters that go into Eqs. (2.11) and (2.6)) are available, then one needs only to know which channels are present in a given neuron in order to build a biophysical neuron model. Studying the composition of messenger RNA in a drop of liquid extracted from a neuron gives a strong indication of which ion channels are present in a neuron, and which are not (Toledo-Rodriguez et al., 2004). The relative importance of ion channels is not ﬁxed, but depends on the age of neuron as well as other factors. Indeed, a neuron can tune its spiking dynamics by regulating its ion channel composition via a modiﬁcation of the gene expression proﬁle.\nIon channels are complex transmembrane proteins which exist in many different forms. It is possible to classify an ion channel using (i) its genetic sequence; (ii) the ion type (sodium, potassium, calcium, ...) that can pass through the open channel; (iii) its voltage dependence; (iv) its sensitivity to second-messengers such as intracellular calcium; (v) its presumed functional role; (vi) its response to pharmacological drugs or to neuromodulators such as acetylcholine and dopamine.\nUsing a notation that mixes the classiﬁcation schemes (i)–(iii), geneticists have distinguished multiple families of voltage-gated ion channels on the basis of similarities in\n\n44\n\nThe Hodgkin–Huxley Model\n\nthe amino acid sequences. The channels are labeled with the chemical symbol of the ion of their selectivity, one or two letters denoting a distinct characteristic and a number to determine the subfamily. For instance “Kv5” is the ﬁfth subfamily of the voltage-sensitive potassium channel family “Kv”. An additional number may be inserted to indicate the channel isoforms, for instance “Nav1.1” is the ﬁrst isoform that was found within the ﬁrst subfamily of voltage-dependent sodium channels. Sometimes a lower-case letter is used to point to the splice variants (e.g., “Nav1.1a”). Strictly speaking, these names apply to a single cloned gene which corresponds to a channel subunit, whereas the full ion channel is composed of several subunits usually from a given family but possibly from different subfamilies.\nTraditionally, electrophysiologists have identiﬁed channels with subscripts that reﬂect a combination of the classiﬁcation schemes (ii)–(vi). The index of the potassium “M-current” IM points to its response to pharmacological stimulation of Muscarinic (M) acetylcholine receptors. Another potassium current, IAHP, shapes the after-hyperpolarization (AHP) of the membrane potential after a spike. Thus the subscript corresponds to the presumed functional role of the channel. Sometimes the functionally characterized current can be related to the genetic classiﬁcation; for instance IAHP is a calcium-dependent potassium channel associated with the small-conductance “SK” family, but in other cases the link between an electrophysiologically characterized channel and its composition in terms of genetically identiﬁed subunits is still uncertain. Linking genetic expression with a functionally characterized ion current is a fast-expanding ﬁeld of study (Ranjan et al., 2011).\nIn this section we select a few examples from the zoo of ion channels and illustrate how ion channels can modify the spiking dynamics. The aim is not to quantitatively specify parameters of each ionic current as this depends heavily on the genetic expression of the subunits, cell type, temperature and neurotransmitters. Rather, we would like to explore qualitatively the inﬂuence of ion channel kinetics on neuronal properties. In other words, let us bring the zoo of ion channels to the circus and explore the stunning acts that can be achieved.\n\n2.3.2 Sodium ion channels and the type-I regime\nThe parameters of the Hodgkin–Huxley model in Table 2.1 relate to only one type of sodium and potassium ion channel. There are more than 10 different types of sodium channels, each with a slightly different activation and inactivation proﬁle. However, as we shall see, even a small change in the ion channel kinetics can profoundly affect the spiking characteristics of a neuron.\nLet us consider a sodium ion channel which has its inactivation curve h0(u) (Fig. 2.3a) shifted to depolarized voltages by 20 mV compared with the parameters in Table 2.1. With maximal conductances gNa = 25 nS/cm2 and gK = 40 nS/cm2, the dynamics of a neuron with this modiﬁed sodium channel (Fig. 2.11) is qualitatively different from that of a neuron with the parameters as in Table 2.1 (Figs. 2.7 and 2.10).\n\n2.3 The zoo of ion channels\n\n45\n\n(a)\n\n(b)\n\nFig. 2.11 A modiﬁcation of sodium channel kinetics leads to different neuronal dynamics. (a) Response of a model with modiﬁed sodium channel to current steps of different amplitudes. (b) Delayed spike initiation. A short current pulse of 2 ms duration is applied at t = 8 ms. The action potential that is elicited in response to the current pulse is shown for decreasing pulse amplitudes (I = 6.25, 5.90, 5.88 μA/cm2). Note that the action potential can occur more than 10 ms after the end of the current pulse.\n\n(a)\n\n(b)\n\nFig. 2.12 Type-I regime with a modiﬁed sodium current. (a) Regular spiking response to constant input. (b) Firing frequency as a function of the constant current.\nFirst, the neuron with the modiﬁed sodium dynamics shows no damped oscillations in response to a step input (Fig. 2.11a). Second, the neuron responds to a short current pulse which is just slightly above the ﬁring threshold with a delayed action potential (Fig. 2.11b). Third, during regular spiking the shape of the action potential is slightly different in the model with the modiﬁed sodium channel (Fig. 2.12a). In particular, the membrane potential between the spikes exhibits an inﬂection point, unlike the spike train with the original set of parameters (Fig. 2.7a). Finally, the gain function (frequency–current plot) has no gap so that the neuron model can ﬁre at arbitrarily small frequencies (Fig. 2.12b). If we compare this f–I plot with the gain function of the neuron with parameters from Table\n\n46\n\nThe Hodgkin–Huxley Model\n\n2.1, we can distinguish two types of excitability: type I has a continuous input–output function (Fig. 2.12b), while type II has a discontinuity (Fig. 2.7b).\n\n2.3.3 Adaptation and refractoriness\nWe have seen in Section 2.2 that the combination of sodium and potassium channels generates spikes followed by a relative refractory period. The refractoriness is caused by the slow return of the sodium inactivation variable h and the potassium activation variable n to their equilibrium values. The time scale of recovery in the Hodgkin–Huxley model, with parameters as in Table 2.1, is 4 ms for the potassium channel activation and 20 ms for the sodium channel inactivation. Other ion channel types, not present in the original Hodgkin– Huxley model, can affect the recovery process on much longer time scales and lead to spike-frequency adaptation: after stimulation with a step current, interspike intervals get successively longer. The basic mechanism of adaptation is the same as that of refractoriness: either a hyperpolarizing current is activated during a spike (and slowly de-activates thereafter) or a depolarizing current is inactivated during a spike and de-inactivates on a much slower time scale.\nExample: Slow inactivation of a hyperpolarizing current\nLet us start with the muscarinic potassium channel IM, often called M-current. Genetically, the channel is composed of subunits of the Kv7 family. Figure 2.13a,b show the activation function as well as the voltage dependence of the time constant as characterized by Yamada et al. (1989). The activation function (Fig. 2.13a) tells us that this channel tends to be activated at voltages above 40 mV and is de-activated below 40 mV with a very sharp transition between the two regimes. Since 40 mV is well above the threshold of spike initiation, the membrane potential is never found above 40 mV except during the 1–2 ms of a spike. Therefore the channel partially activates during a spike and, after the end of the action potential, de-activates with a time constant of 40–60 ms (Fig. 2.13b). The slow deactivation of this potassium channel affects the time course of the membrane potential after a spike. Compared with the original Hodgkin–Huxley model, which has only the two currents speciﬁed in Table 2.1, a model with an additional IM current exhibits a prolonged hyperpolarizing spike-afterpotential and therefore a longer relative refractory period (Fig. 2.13c–e).\nIn the regular ﬁring regime, it is possible that the M-current caused by a previous spike is still partially activated when the next spike is emitted. The partial activation can therefore accumulate over successive spikes. By cumulating over spikes, the activation of IM gradually forces the membrane potential away from the threshold, increasing the interspike interval. This results in a spiking response that appears to “adapt” to a step input, hence the name “spike-frequency adaptation” or simply “adaptation” (Fig. 2.13c–e).\n\n2.3 The zoo of ion channels\n\n47\n\n(a)\n\n(b)\n\n(c)\n(d)\n(e)\nFig. 2.13 Spike-frequency adaptation with IM. (a) Voltage dependence of the stationary value of the activation variable m and (b) its time constants for the muscarinic potassium current IM = gM m (u − Ek) extracted from experimental observations (Yamada et al., 1989). (c) Voltage response to the current shown in (d) of the original Hodgkin–Huxley model with parameters from Table 2.1 (dashed line) and a model which also contains the IM channel. The model with IM exhibits adaptation. (e) Progressive activation of the potassium current IM during the repetitive spiking period shown in (c).\n\n48\n\nThe Hodgkin–Huxley Model\n\nExample: A-current\n\nAnother potassium ion channel with kinetics similar to IM is IA, but qualitatively different effects: IA makes the relative refractory period longer and stronger without causing much adaptation. To see the distinction between IA and IM, we compare the activation kinetics of both channels (Figs. 2.13b and 2.14b). The time constant τm of activation is much faster for IA than for IM. This implies that the A-current increases rapidly during the short time of the spike and decays quickly afterward. In other words, the effect of\nIA is short and strong whereas the effect of IM is long and small. Because the effect of IA does not last as long, it contributes to refractoriness, but only very little to spike frequency adaptation (Fig. 2.14c–e). Even though an inactivation process, with variable h, was reported for IA, its time constant τh is so long (>150 ms) that it does not play a role in the above arguments.\n\nExample: Slow extrusion of calcium\n\nMultiple ion channel families contribute to spike-frequency adaptation. In contrast to the direct action of IM, the calcium-dependent potassium channel IK[Ca] generates adaptation indirectly via its dependence on the intracellular calcium concentration. During each spike, calcium enters through the high-threshold calcium channel IHVA. As calcium accumulates inside the cell, the calcium-dependent potassium channel IK[Ca] gradually opens, lowers the membrane potential, and makes further spike generation more difﬁcult. Thus, the level of adaptation can be read out from the intracellular calcium concentration.\nIn order to understand the accumulation of calcium, we need to discuss the highthreshold calcium channel IHVA. Since it activates above −40 mV to −30 mV, the channel opens during a spike. Its dynamics is therefore similar to that of the sodium current, but the direct effect of IHVA on the shape of the spike is small. Its main role is to deliver a pulse of calcium ions into the neuron. The calcium ions have an important role as second-messengers; they can trigger various cascades of biophysical processes. Intracellular calcium is taken up by internal buffers, or slowly pumped out of the cell, leading to an intricate dynamics dependent on the properties of calcium buffers and calcium pumps. For small calcium transients, however, or when the calcium pump has high calcium afﬁnity and slow extrusion rate, the intracellular calcium dynamics follow (Helmchen et al., 2011)\n\nd[Ca] dt\n\n=\n\nφCaICa\n\n+\n\nτC−a1\n\n([Ca] −\n\n[Ca]0) ,\n\n(2.12)\n\nwhere [Ca] denotes the intracellular calcium concentration, ICa is the sum of currents coming from all calcium ion channels, φCa is a constant that scales the ionic current to changes in ionic concentration, [Ca]0 is a baseline intracellular calcium concentration,\n\n2.3 The zoo of ion channels\n\n49\n\n(a)\n\n(b)\n\n(c)\n(d)\n(e)\nFig. 2.14 IA and the refractory period. (a) Voltage dependence of the stationary values and (b) time constants of the activation variable m and inactivation variable h for the A-type potassium current IA = gA m h (u − EK) extracted from experimental observations in pyramidal neurons of the cortex (Korngreen and Sakmann, 2000). (c) Voltage response to the current shown in (d), consisting of a single pulse and a step. (e) Progressive activation (solid line) and inactivation (dashed line) of the potassium current IA during the stimulation shown in (c) and (d).\n\n50\n\nThe Hodgkin–Huxley Model\n\n(a)\n\n(b)\n\n(c)\n(d)\n(e)\nFig. 2.15 Calcium-based spike-frequency adaptation with IK[Ca] and IHVA. (a) Voltage dependence of the stationary values. (b) Time constants of the activation variable m and inactivation variable h of the high-threshold calcium current IHVA = gL h m (u − ECa) extracted from experiments (Reuveni et al., 1993). (c) Voltage response of a Hodgkin–Huxley model with the calcium current IHVA (dashed line) and a model that also contains a calcium-dependent potassium current IK[Ca]. (d) External current used for the simulation in (c) and (e). (e) Progressive accumulation of intracellular calcium in the two models.\n\n2.3 The zoo of ion channels\n\n51\n\nand τCa is the effective time constant of calcium extrusion. In our simple example the sole source of incoming calcium ions is the high-threshold calcium channel hence ICa = IHVA. Because of the short duration of the spike, each spike adds a ﬁxed amount of intracellular calcium which afterward decays exponentially (Fig. 2.15e), as observed in many cell types (Helmchen et al., 2011).\nThe calcium-dependent potassium channel IK[Ca] = gK[Ca] n (u − EK) is weakly sensitive to the membrane potential but highly sensitive to intra cellular calcium concentration. The dynamics of activation can be modeled with a calcium-dependent time constant (Destexhe et al., 1994a) of the activation variable n\n\nτn([Ca])\n\n=\n\nk3 1 + k2ek1[Ca]\n\nand a stationary value (Destexhe et al., 1994a)\n\n(2.13)\n\nn0\n\n=\n\nk2ek1[Ca] 1 + k2ek1[Ca]\n\n(2.14)\n\nwhere k1, k2, k3, and gK[Ca] are constants. Figure 2.15 shows the effect of the combined action of calcium channel and IK[Ca]. A\nsingle spike generates a transient increase in intracellular calcium which in turn causes\n\na transient increase in IK[Ca] activation which results in a hyperpolarization of the membrane potential compared with a model without IK[Ca]. During sustained stimulation, calcium accumulates over several spikes, so that the effect of IK[Ca] becomes successively larger and interspike intervals increase. The time constant associated with adaptation is\n\na combination of the calcium extrusion time constant and the potassium activation and\n\nde-activation time constant.\n\nExample: Slow de-inactivation of a persistent sodium channel\nThe stationary values of activation and inactivation of the persistent sodium channel INaP (Fig. 2.16a) are very similar to that of the normal sodium channel of the Hodgkin– Huxley model. The main difference lies in the time constant. While activation of the channel is quick, it inactivates on a much slower time scale. Hence the name: the current “persists”. The time constant of the inactivation variable h is of the order of a second.\nDuring sustained stimulation, each spike contributes to the inactivation of the sodium channel and therefore reduces the excitability of the neuron. This special type of refractoriness is not visible in the spike-afterpotential, but can be illustrated as a relative increase in the effective spiking threshold (Fig. 2.16b). Since, after a ﬁrst spike, the sodium channel is partially inactivated, it becomes more difﬁcult to make the neuron ﬁre a second time.\n\n52 (a)\n\nThe Hodgkin–Huxley Model\n(b)\n\nFig. 2.16 The persistent sodium channel INaP increases the ﬁring threshold. (a) Activation and inactivation proﬁles for INaP = gNaP m h (u − ENa). Activation is fast (a few ms) whereas inactivation is of the order of a second as measured in pyramidal neurons of the cortex (Aracri et al., 2006). (b) Slow inactivation affects the effective threshold. In a Hodgkin–Huxley model with persistent sodium current, subthreshold current pulses are injected at different moments (arrows). At t = 75 ms (asterisk) only, a suprathreshold stimulation was applied. Tuning the strength of the other current pulses so that they are just below spike initiation reveals an effective voltage threshold (dashed line) that is higher immediately after the ﬁrst spike.\n2.3.4 Subthreshold effects\nSome ion channels have an activation proﬁle m0(u) which has a signiﬁcant slope well below the spike initiation threshold. During subthreshold activation of the cell by background activity in vivo, or during injection of a ﬂuctuating current, these currents partially activate and inactivate, following the time course of membrane potential ﬂuctuations and shaping them in turn.\nWe describe two examples of subthreshold ion channel dynamics. The ﬁrst one illustrates adaptation to a depolarized membrane potential by inactivation of a depolarizing current, which results in a subsequent reduction of the membrane potential. The second example illustrates the opposite behavior: in response to a depolarized membrane potential, a depolarizing current is triggered which increases the membrane potential even further. The two examples therefore correspond to subthreshold adaptation and subthreshold facilitation, respectively.\nExample: Subthreshold adaptation by Ih\nSubthreshold adaptation through a hyperpolarization-activated current Ih is present in many cell classes. As the name implies, the current is activated only at hyperpolarized voltages as can be seen in Fig. 2.17a. Thus, the voltage dependence of the activation variable is inverted compared with the normal case and has negative slope. Therefore, the activation variable looks more like an inactivation variable and this is why we choose h as the symbol for the variable. The channel is essentially closed for prolonged membrane\n\n2.3 The zoo of ion channels\n\n53\n\n(a)\n\n(b)\n\nFig. 2.17 Subthreshold adaptation with Ih. (a) Stationary values and (b) time constants of the variable h controlling the hyperpolarization-activated mixed current Ih as measured in pyramidal neurons of the hippocampus (Magee, 1998).\npotential ﬂuctuations above −30 mV. The Ih current is a non-speciﬁc cation current, meaning that sodium, potassium, and calcium can pass through the Ih channel when it is open. The reversal potential of this ion channel is usually around −45 mV so that the h-current Ih = gh h (u − Eh) is depolarizing at resting potential and over most of the subthreshold regime.\nThe presence of Ih causes the response to step changes in input current to exhibit damped oscillations. The interaction works as follows: suppose an external driving current depolarizes the cell. In the absence of ion channels this would lead to an exponential relaxation to a new value of the membrane potential of, say, −50 mV. Since Ih was mildly activated at rest, the membrane potential increase causes the channel to de-activate. The gradual closure of Ih removes the effective depolarizing drive and the membrane potential decreases, leading to a damped oscillation (as in Fig. 2.10). This principle can also lead to a rebound spike as seen in Fig. 2.10. Subthreshold adaptation and damped oscillations will be treated in more detail in Chapter 6.\nExample: Subthreshold facilitation by INaS The sodium ion channel INaS is slow to activate. Let us consider again stimulation with a step current using a model which contains both the fast sodium current of the Hodgkin– Huxley model and the slow sodium current INaS. If the strength of the current step is such that it does not activate the fast sodium channel but is strong enough to activate INaS, then the slow activation of this sodium current increases membrane potential gradually with the time constant of activation of the slow sodium current (Fig. 2.18b). The slow depolarization continues until the fast sodium current activates and an action potential\n\n54 (a)\n\nThe Hodgkin–Huxley Model (b)\n\n(c)\n(d)\n(e)\nFig. 2.18 Subthreshold facilitation with INaS. (a) Stationary values and (b) time constants of the activation variable m for the slow sodium current INaS similar to measurements done in pyramidal neurons of the hippocampus (Hoehn et al., 1993). (c) Voltage response. (d) External current. (e) Slow sodium current activation, leading to delayed spike initiation and ﬁring frequency facilitation.\nis generated (Fig. 2.18c). Such delayed spike initiation has been observed in various types of interneurons of the cortex. If the amplitude of the step current is sufﬁcient to immediately activate the fast sodium channels, the gradual activation of INaS increases the ﬁring frequency leading to spike-frequency facilitation.\n\n2.3 The zoo of ion channels\n\n55\n\n2.3.5 Calcium spikes and postinhibitory rebound\nPostinhibitory rebound means that a hyperpolarizing current which is suddenly switched off results in an overshoot of the membrane potential or even in the triggering of one or more action potentials. Through this mechanism, action potentials can be triggered by inhibitory input. These action potentials, however, occur with a certain delay after the arrival of the inhibitory input, i.e., after the end of the IPSP (Aizenman and Linden, 1999).\nInactivating currents with a voltage threshold below the resting potential, such as the low-threshold calcium current, can give rise to a much stronger effect of inhibitory rebound than the one seen in the standard Hodgkin–Huxley model. Compared with the sodium current, the low-threshold calcium current IT has activation and inactivation curves that are shifted signiﬁcantly toward a hyperpolarized membrane potential so that the channel is\n\nh\n\nm ,h\n\n(a) 1\n0.8 0.6 0.4\n0.2 0 -100 -80 -60 -40 -20 0 u [mV]\n(c)\n\n0\n\nu[mV]\n\n-20 -40\n\n-60\n\n0\n\n500\n\nt [ms]\n\n(b)\n\n4 th\n3\n\n2\n\ntm\n\ntm ,0.01 th\n\n1\n\n0 -100 -80 -60 -40 -20 0\n\nu[mV] (d)\n\n1.0\n\n0.1\n\n0.8\n\n0.08\n\n0.6\n\n0.06\n\n0.4\n\n0.04\n\n0.2\n\n0.02\n\n0.0 0\n\n0.0 500\n\nt[ms]\n\nm\n\nFig. 2.19 Postinhibitory rebound through de-inactivation. (a) Activation m0(u) and inactivation h0(u) at equilibrium of the low-threshold calcium current IT. Small circles indicate the equilibrium values of m and h at the resting potential. To de-inactivate the current, the membrane potential must be below rest. (b) The time constants of activation and inactivation. Note that different vertical scales have been used for τm and τh since the dynamics of the inactivation variable h is slower by a factor 10–100 than that of the activation variable m. Numerical values of parameters correspond to a model of neurons in the deep cerebellar nuclei (Kistler and van Hemmen, 2000). (c) Membrane potential as a function of time. Injection of a hyperpolarizing current pulse (100 pA during 200 ms from t = 100 ms to t = 300 ms) results, at the end of current injection, in a low-threshold calcium spike that in turn triggers two sodium action potentials. (d) Time course of activation (solid line, left scale) and inactivation (dashed line, right scale) variables of the IT current that is responsible for this phenomenon.\n\n56\n\nThe Hodgkin–Huxley Model\n\ncompletely inactivated (h ≈ 0) at the resting potential; see Fig. 2.19a and b. In order to open the low-threshold calcium channels it is ﬁrst of all necessary to remove its inactivation by hyperpolarizing the membrane. The time constant of the inactivation variable h is rather high and it thus takes a while until h has reached a value sufﬁciently above zero; see Fig. 2.19b and d. But even if the channels have been successfully “de-inactivated” they remain in a closed state, because the activation variable m is zero as long as the membrane is hyperpolarized. However, the channels will be transiently opened if the membrane potential is rapidly relaxed from the hyperpolarized level to the resting potential, because activation is faster than inactivation and, thus, there is a short period when both m and h are nonzero. The current that passes through the channels is terminated (“inactivated”) as soon as the inactivation variable h has dropped to zero again, but this takes a while because of the relatively slow time scale of τh. The resulting current pulse is called a low-threshold calcium spike and is much broader than a sodium spike.\nThe increase in the membrane potential caused by the low-threshold calcium spike may be sufﬁcient to trigger ordinary sodium action potentials. These are the rebound spikes that may occur after a prolonged inhibitory input. Figure 2.19c shows an example of (sodium) rebound spikes that ride on the broad depolarization wave of the calcium spike; note that the whole sequence is triggered at the end of an inhibitory current pulse. Thus release from inhibition causes here a spike-doublet.\n\n2.4 Summary\nThe Hodgkin–Huxley model describes the generation of action potentials on the level of ion channels and ion current ﬂow. It is the starting point for detailed biophysical neuron models which in general include more than the three types of currents considered by Hodgkin and Huxley. Electrophysiologists have described an overwhelming richness of different ion channels. The set of ion channels is different from one neuron to the next. The precise channel conﬁguration in each individual neuron determines a good deal of its overall electrical properties.\n\nLiterature\nA nice review of the Hodgkin–Huxley model including some historical remarks can be found in Nelson and Rinzel (1995). A comprehensive and readable introduction to the biophysics of single neurons is provided by the book of Christof (1999). Even more detailed information on ion channels and nonlinear effects of the nervous membrane can be found in B. Hille’s book of Ionic Channels of Excitable Membranes (Hille, 2001). The rapidly growing knowledge of the genetic description of ion channel families and associated phenotypes is condensed in Channelpedia (Ranjan et al., 2011).\n\n2.4 Summary\n\n57\n\nExercises\n1. Nernst equation. Using the Nernst equation (Eq. 2.2) calculate the reversal potential of Ca2+ at room temperature (21 degrees Celsius), given an intracellular concentration of 10−4 mM and\nan extracellular concentration of 1.5 mM.\n2. Reversal potential and stationary current–voltage relation. An experimenter studies an\nunknown ion channel by applying a constant voltage u while measuring the injected current I\nneeded to balance the membrane current that passes through the ion channel.\n(a) Sketch the current–voltage relationship (I as a function of u) assuming that the current follows Iion = gionm h (u − Erev) with gion = 1 nS and Erev = 0 mV where m = 0.1 and h = 1.0 are independent of the voltage.\n(b) Sketch qualitatively the current–voltage relationship assuming that the current follows Iion = gionm h (u − Erev) with gion = 1 nS and Erev = 0 mV where m0(u) and h0(u) have the qualitative shape indicated in Fig. 2.15.\n3. Activation time constant. An experimenter holds the channel from Fig. 2.15a and b at u = −50 mV for two seconds and then suddenly switches to u = 0 mV. Sketch the current passing through the ion channel as a function of time, assuming Iion = gionm h (u − Erev) with gion = 1 nS and Erev = 0 mV.\n4. The power of the exponent. An experimenter holds an unknown potassium ion channel with activation variable n with voltage dependence n0(u) and time constant τn at u = −50 mV for two seconds and then, at time t = 0, suddenly switches to u = 0 mV. (a) Sketch the activation variable n, n2, n3 as a function of time for times smaller than τn. (b) Show mathematically that for 0 < t < τn the time course of the activation variable can be approximated n(t) = n0(−50mV) + [n0(0mV) − n0(−50mV)]t/τm. (c) Do you agree with the statement that “the exponent p in the current formula Iion = gionnp (u− Erev) determines the “delay” of activation”? Justify your answer.\n5. Hodgkin–Huxley parameter estimation. Design a set of experiments to constrain all the param-\neters of the two ion channels of the Hodgkin–Huxley model. Assume that the neuron has only the\nINa and IK currents and that you can use tetrodotoxin (TTX) to block the sodium ion channel and tetraethylammonium (TEA) to block the potassium ion channel.\nHint: Use the results of the previous exercises.\n6. Simpliﬁed expression of the activation function. Show that with the voltage-dependent parameters αm(u) = 1/ [1 − e−(u+a)/b] and βm(u) = 1/ [1 − e−(u+a)/b] (compare Table 2.1), the stationary value of the activation variable can be written as m0(u) = 0.5 [1 + tanh[β (u − θact)]]. Determine the activation threshold θact and the activation slope β . Hint: tanh(x) = [exp(x) − exp(−x)]/[exp(x) + exp(−x)].\n\n3\nDendrites and synapses\nNeurons have intricate morphologies: the central part of the cell is the soma, which contains the genetic information and a large fraction of the molecular machinery. At the soma originate long wire-like extensions which come in two different ﬂavors. First, the dendrites form a multitude of smaller or larger branches on which synapses are located. The synapses are the contact points where information from other neurons (i.e., “presynaptic” cells) arrives. Second, also originating at the soma, is the axon, which the neuron uses to send action potentials to its target neurons. Traditionally, the transition region between soma and axon is thought to be the crucial region where the decision is taken whether a spike is sent out or not.\nThe Hodgkin–Huxley model, at least in the form presented in the previous chapter, disregards this spatial structure and reduces the neuron to a point-like spike generator – despite the fact that the precise spatial layout of a neuron could potentially be important for signal processing in the brain. In this chapter we will discuss how some of the spatial aspects can be taken into account by neuron models. In particular we focus on the properties of the synaptic contact points between neurons and on the electrical function of dendrites.\n3.1 Synapses In the previous chapter, we have encountered two classes of ion channels, namely voltageactivated and calcium-activated ion channels. The third type of ion channel we have to deal with are the transmitter-activated ion channels involved in synaptic transmission (see Fig. 3.1) and generally activated from outside the cell. Activation of a presynaptic neuron results in a release of neurotransmitters into the synaptic cleft. The transmitter molecules diffuse to the other side of the cleft and activate receptors that are located in the postsynaptic membrane. So-called ionotropic receptors have a direct inﬂuence on the state of an associated ion channel. Metabotropic receptors control the state of an ion channel by means of a biochemical cascade of G proteins and second-messengers. In both cases, the activation of the receptor results in the opening of certain ion channels and, thus, in an excitatory or inhibitory postsynaptic transmembrane current (EPSC or IPSC).\nInstead of developing a mathematical model of the transmitter concentration in the synaptic cleft, we keep things simple and describe transmitter-activated ion channels as\n\n3.1 Synapses\n\n59\n\nan explicitly time-dependent conductivity gsyn(t) that will open whenever a presynaptic spike arrives. The current that passes through a synaptic channel depends, as before, on the difference between its reversal potential Esyn and the actual value of the membrane potential,\n\nIsyn(t) = gsyn(t) (u(t) − Esyn) .\n\n(3.1)\n\nThe parameter Esyn and the function gsyn(t) can be used to describe different types of synapses. For inhibitory synapses Esyn is usually set to −75 mV, whereas for excitatory synapses Esyn ≈ 0.\nTypically, a superposition of exponentials is used for gsyn(t). A simple choice for the time course of the synaptic conductance in Eq. (3.1) is an exponential decay\n\n∑ gsyn(t) = g¯syn e−(t−t f )/τ Θ(t − t f ) , f\n\n(3.2)\n\nwith a time constant of, e.g., τ = 5 ms and an amplitude of g¯syn = 40 pS. Here, t f denotes the arrival time of a presynaptic action potential and Θ(x) is the Heaviside step function.\nFor some synapse types, a single exponential decay is not sufﬁcient. Rather, the postsynaptic current is made up of two different components, a fast one with a decay time constant of a few milliseconds, and a second one that is often ten times slower. If we also take into account the smooth rise of the synaptic response, the postsynaptic conductance is of the form\n\n∑ gsyn(t) = g¯syn [1 − e−(t−t f )/τrise ] a e−(t−t f )/τfast + (1 − a) e−(t−t f )/τslow Θ(t − t f ) , (3.3) f\n\nwhere a is the relative weight of the fast component. The time constant τrise characterizes the rise time of the synaptic conductance.\n\nExample: A more detailed synapse model\n\nInstead of considering a synapse with a ﬁxed time course gsyn(t) , we can also make a model which has the ﬂavor of a Hodgkin–Huxley channel. We describe the synaptic conductance gsyn(t) = gmax R(t), by its maximal conductance gmax and a gating variable R, where R(t) is the fraction of open synaptic channels. Channels open when neurotransmitter N binds to the synapse\n\ndR = α N (1 − R) − β R,\n\n(3.4)\n\ndt\n\nwhere α is the binding constant, β the unbinding constant and (1 − R) the fraction\n\nof closed channels where binding of neurotransmitter can occur. Neurotransmitter N\n\nis released with each presynaptic spike so that the total amount of neurotransmitter at\n\nsynapse j is\n\n∞\n\nN(t) = γ(s) S j(t − s) ds,\n\n(3.5)\n\n0\n\n60\n\nDendrites and synapses\n\n(a) Presynaptic\nTransmitter release\n\n(b) Na +ion ﬂux Glutamate\n} Neuronal membrane\n\nTransmitter-gated Postsynaptic ion channels\n\nK + ion ﬂux\n\nFig. 3.1 (a) Schema of synaptic transmission. Upon arrival of a presynaptic spike, neurotransmitter\nspills into the synaptic cleft and is captured by postsynaptic receptors. (b) Schema of a postsynaptic AMPA1 receptor of an excitatory synapse. When glutamate is bound to the receptor, sodium and\npotassium ions can ﬂow through the membrane.\n\nwhere\n\nSj\n\n=\n\n∑f\n\nδ (t\n\n−\n\nt\n\nf j\n\n)\n\nis\n\nthe\n\npresynaptic\n\nspike\n\ntrain\n\n(a\n\nsequence\n\nof\n\nδ -functions,\n\nsee\n\nChapter 1) and γ(s) is the time course of the neurotransmitter density as measured at\n\nthe site of the postsynaptic receptor. More advanced synaptic signaling schemes can be\n\ndesigned along the same line of argument (Destexhe et al., 1994b).\n\n3.1.1 Inhibitory synapses\nThe effect of fast inhibitory neurons in the central nervous system of higher vertebrates is almost exclusively conveyed by a neurotransmitter called γ-aminobutyric acid, or GABA for short. A characteristic feature of inhibitory synapses is that the reversal potential Esyn is in the range of −70 to −75 mV. Thus, if the neuronal membrane potential is above the reversal potential, presynaptic spike arrival leads to a hyperpolarization of the neuron, making action potential generation less likely. However, the same presynaptic spike would lead to a depolarization of the membrane if the neuron has its membrane potential at −80 mV or below.\nThere are many different types of inhibitory interneurons (Markram et al., 2004; Klausberger and Somogyi, 2008). Biologists distinguish between two major types of inhibitory synapse, called GABAA and GABAB. Both synapse types use GABA as the neurotransmitter. GABAA channels are ionotropic and open exclusively for chloride ions, whereas GABAB synapses have metabotropic receptors that trigger a comparatively slow signaling chain ultimately leading to the opening of K+ channels. Consequently the value of the synaptic reversal potential Esyn depends for GABAA synapses on the concentration of chloride ions inside and outside the cell, while that of GABAB synapses depends on the potassium concentrations.\n1AMPA is short for α-amino-3-hydroxy-5-methyl-4-isoxalone propionic acid.\n\n3.1 Synapses\n\n61\n\nExample: GABAA synapse model\nGABAA synapses have a fast time course that can be approximated by a single term in Eq. (3.3) with a = 1, τrise ≈ 1 ms, and a time constant τfast ≈ 6 ms (Destexhe and Pare (1999); Fig. 3.2), which has also been deemed 3 times larger. More complex models are sometimes used (Destexhe et al., 1994b).\n\nExample: GABAB synapse model\nThis is a slow inhibitory synapse working via a second-messenger chain. Common models use Eq. (3.3) with a rise time of about 25–50 ms, a fast decay time in the range of 100–300 ms and a slow decay time of 500–1000 ms. The fast component accounts for about 80% of the amplitude of conductance (a = 0.8) (Destexhe et al., 1994b; McCormick et al., 1993), illustrated in Fig. 3.2.\n\n3.1.2 Excitatory synapses\nMost excitatory synapses in the vertebrate central nervous system rely on glutamate as their neurotransmitter. The postsynaptic receptors, however, can have very different pharmacological properties and different types of glutamate receptor units can be present in a single synapse. These receptors are classiﬁed using artiﬁcial drugs such as NMDA or AMPA that act as selective agonists. NMDA (N-methyl-D-aspartate) binds to channels with NMDA receptors, but not to other glutamate receptors. The most prominent among those glutamate receptors that do not respond to NMDA are the AMPA-receptors. AMPA is an artiﬁcial glutamate. Channels with AMPA-sensitive receptors are called “AMPA channels” because these channels react to AMPA, whereas channels with NMDA-sensitive receptors do not open upon application of AMPA. However, both NMDA and AMPA channels react to the natural form of glutamate that the nervous system uses as neurotransmitter.\nAMPA receptors consist of four subunits, each with a glutamate binding site. Most AMPA receptors contain the subunit called GluR2. If an AMPA-receptor channel containing GluR2 is open, sodium and potassium ions can pass, but calcium ions cannot. Synaptic channels with AMPA-receptors are characterized by a fast response to presynaptic spikes and a quickly decaying postsynaptic current.\nNMDA-receptor controlled channels are signiﬁcantly slower and have additional interesting properties that are due to a voltage-dependent block by magnesium ions (Hille, 1992). In addition to sodium and potassium ions, also calcium ions can pass through open NMDA-channels.\n\n62\n\nDendrites and synapses\n\n400 300 AMPA\n\n−Isyn [pA]\n\n200\n\n100 0\n−100 −200\n\nNMDA\n\nGABAA\n\nGABAB\n\nFig. 3.2 Dynamics of postsynaptic current (3.1) after a single presynaptic spike at t = 0. GABAA, GABAB, AMPA, and NMDA without magnesium block are shown for a postsynaptic neuron at rest (u = −65mV).\n\n−300 0\n\n100 200 300 400 500\nt [ms]\n\nExample: Conductance of glutamate channels with AMPA-receptors\nThe time course of the postsynaptic conductivity caused by an activation of AMPAreceptors at time t = t f is sometimes described by Eq. (3.2) with a decay time of about 2–5 ms (Gabbiani et al., 1994; Destexhe et al., 1994b).\n\nExample: Conductance of glutamate channels with NMDA-receptors\nNMDA-receptor controlled channels exhibit a rich repertoire of dynamic behavior because their state is controlled not only by the presence or absence of glutamate, but also by the membrane potential. At resting potential the NMDA channel is blocked by a common extracellular ion, Mg2+, even if glutamate is present (Hille, 1992). Even in the presence of glutamate, the channel remains closed at the resting potential. If the membrane is depolarized beyond −50 mV, the Mg2+-block is removed, the channel opens when glutamate binds to the receptor and, thereafter, stays open for 10–100 ms. A simple model of the voltage dependence of NMDA-receptor controlled channels is\ngNMDA(t) = g¯NMDA · 1 − e−(t−t f )/τrise e−(t−t f )/τdecay g∞(u, [Mg2+]o) Θ(t − t f ) ,\nwith g∞(u, [Mg2+]o) = 1 + β eα u [Mg2+]o −1 , (3.6)\nwith τrise in the range of 3 ms to 15 ms, τdecay in the range of 40 ms to 100 ms, g¯NMDA = 1.5 nS, α = −0.062 mV−1, β = 1/(3.57mM), and an extracellular magnesium concentration [Mg2+]o = 1.2 mM (McCormick et al., 1993; Gabbiani et al., 1994).\nWhat is the potential functional role of NMDA receptors? First, their comparatively long time constant keeps a trace of presynaptic events and acts as a low-pass ﬁlter. Second, even though NMDA-receptor controlled ion channels are permeable to sodium and potassium ions, their permeability to Ca2+ is ﬁve or ten times larger. Calcium ions are known to play an important role in intracellular signalling and are probably involved in\n\n3.1 Synapses\n\n63\n\n(a)\n\n(b)\n\nFig. 3.3 Short-term plasticity. A synapse is activated by four presynaptic spikes and a ﬁfth spike 400 ms later. (a) At a facilitating synapse, the effect of the second and third spike is larger than that of the ﬁrst spike. The effect of a spike after a pause of 400 ms is approximately that of the ﬁrst spike (time constant τP = 200 ms). (b) At a depressing synapse, successive spikes in a periodic spike train have less and less effect: 400 ms later, the synapse has partially recovered, but is still signiﬁcantly depressed (τP = 500 ms).\nlong-term modiﬁcations of synaptic efﬁcacy. Calcium inﬂux through NMDA-controlled ion channels can occur if presynaptic spike arrival (leading to glutamate release from presynaptic sites) coincides with a depolarization of the postsynaptic membrane (leading to removal of the Mg2+-block). Hence, NMDA-receptors operate as molecular coincidence detectors between pre- and postsynaptic events.\n3.1.3 Rapid synaptic dynamics\nParameters of a synaptic contact point are not ﬁxed, but can change as a function of the stimulation history. Some of these changes are long-lasting and are thought to represent the neuronal correlate of learning and memory formation. The description of these learningrelated changes will be covered in Chapter 19. Here we concentrate on dynamic changes of the synapse that do not persist but decay back to their normal values within hundreds of milliseconds or a few seconds. These changes are called short-term synaptic plasticity.\nShort-term synaptic plasticity can be measured if a presynaptic neuron is stimulated so as to generate a sequence of spikes. Synaptic facilitation means that the apparent amplitude of a postsynaptic current in response to the second spike is larger than that to the ﬁrst spike. Synaptic depression is the opposite effect (Fig. 3.3).\nAs a simple model of synaptic facilitation and depression (Dayan and Abbott, 2001), we assume that the maximal synaptic conductance g¯syn in Eq. (3.2) or (3.3) depends on the fraction Prel of presynaptic sites releasing neurotransmitter. Facilitation and depression can both be modeled as presynaptic processes that modify Prel. With each presynaptic spike, the number of available presynaptic release sites changes. Between spikes the value of Prel\n\n64\n\nDendrites and synapses\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\n(e)\n\nFig. 3.4 Reconstructed morphology of various types of neurons. (a) Pyramidal neuron from a deep cortical layer (Contreras et al., 1997). (b) Pyramidal neuron from the CA1 of the hippocampus (Golding et al., 2005). (c) Purkinje cell from the cerebellum (Rapp et al., 1994). (d) Motoneuron from the spinal cord (Cullheim et al., 1987). (e) Stellate neuron from the neocortex (Mainen and Sejnowski, 1996). Reconstructed morphologies can be downloaded from http://NeuroMorpho.Org. Scale bars represent 100 μm.\n\nreturns exponentially to its resting value P0. Thus,\n\n∑ dPrel\ndt\n\n=\n\n−\n\nPrel − τP\n\nP0\n\n+\n\nfF (1 − Prel)\n\nf\n\nδ (t − t f )\n\n(3.7)\n\nwhere τP is a time constant, fF controls the degree of facilitation, and t f denotes the times of presynaptic spike arrivals.\nThe model of a depressing synapse is completely analogous. The amount of neurotransmitter available for release develops according to the differential equation\n\n∑ dPrel\ndt\n\n=\n\n−\n\nPrel − τP\n\nP0\n\n−\n\nfD Prel\n\nf\n\nδ (t − t f )\n\n(3.8)\n\nwhere τP is a time constant and the parameter fD with 0 < fD < 1 controls the amount of depression per spike.\nThe total effect of presynaptic spikes depends on the available neurotransmitter as well as the value g0 of postsynaptic conductance if all synaptic ion channels are open, so that, for depressing or facilitating synapses, we can use Eq. (3.2) with a value g¯syn = Prel g0. This procedure has been used to generate Fig. 3.3.\n\n3.2 Spatial structure: the dendritic tree\nNeurons in the cortex and other areas of the brain often exhibit highly developed dendritic trees that may extend over several hundred microns (Fig. 3.4). Synaptic input to a neuron is mostly located on its dendritic tree. Disregarding NMDA- or calcium-based electrogenic “spikes,” action potentials are generated at the soma near the axon hillock. Up to now we\n\n3.2 Spatial structure: the dendritic tree\n\n65\n\nhave discussed point neurons only, i.e., neurons without any spatial structure. What are the consequences of the spatial separation of input and output?\nThe electrical properties of point neurons have been described as a capacitor that is charged by synaptic currents and other transversal ion currents across the membrane. A non-uniform distribution of the membrane potential on the dendritic tree and the soma induces additional longitudinal current along the dendrite. We are now going to derive the cable equation that describes the membrane potential along a dendrite as a function of time and space. In Section 3.4 we shall see how geometric and electrophysiological properties can be integrated in a comprehensive biophysical model.\n\n3.2.1 Derivation of the cable equation\n\nConsider a piece of dendrite decomposed into short cylindric segments of length dx each. The schematic drawing in Fig. 3.5 shows the corresponding circuit diagram. Using Kirchhoff’s laws we ﬁnd equations that relate the voltage u(x) across the membrane at location x with longitudinal and transversal currents. First, a longitudinal current i(x) passing through the dendrite causes a voltage drop across the longitudinal resistor RL according to Ohm’s law,\n\nu(t, x + dx) − u(t, x) = RL i(t, x) ,\n\n(3.9)\n\nwhere u(t, x + dx) is the membrane potential at the neighboring point x + dx. Second, the transversal current that passes through the RC-circuit is given by C ∂ u(t, x)/∂ t + ∑ion Iion where the sum runs over all ion channel types present in the dendrite. Kirchhoff’s law\nregarding the conservation of current at each node leads to\n\n∑ i(t,\n\nx\n\n+\n\ndx)\n\n−\n\ni(t,\n\nx)\n\n=\n\nC\n\n∂ ∂t\n\nu(t,\n\nx)\n\n+\n\nion\n\nIion\n\n−\n\nIext (t ,\n\nx)\n\n.\n\n(3.10)\n\nThe values of the longitudinal resistance RL, the capacity C, the ionic currents as well as the externally applied current can be expressed in terms of speciﬁc quantities per unit length rL, c, iion and iext, respectively, namely\n\nRL = rL dx , C = c dx , Iext(t, x) = iext(t, x) dx, Iion(t, x) = iion(t, x) dx . (3.11)\n\nThese scaling relations express the fact that the longitudinal resistance and the capacity increases with the length of the cylinder. Similarly, the total amount of transversal current increases with the length dx simply because the surface through which the current can pass is increasing. Substituting these expressions in Eqs. (3.9) and (3.10), dividing by dx, and taking the limit dx → 0 leads to\n\n∂ ∂ x u(t, x) = rL i(t, x)\n\n∑ ∂\n\n∂\n\n∂ x i(t, x) = c ∂ t u(t, x) + ion iion(t, x) − iext(t, x) .\n\n(3.12a) (3.12b)\n\n66\n\nDendrites and synapses\n\ndx\n\nRL i(x) Iext(x) i(x+dx)\n\nu(x)\n\nu(x+dx)\n\nC\n\nRT\n\nFig. 3.5 Part of a dendrite and the corresponding circuit diagram. Longitudinal and transversal resistors are denoted by RL and RT, respectively. The electrical capacity of each small piece of dendrite is symbolized by capacitors C.\n\nTaking the derivative of equation (3.12a) with respect to x and substituting the result into (3.12b) yields\n\n∑ ∂ 2\n\n∂\n\n∂ x2 u(t, x) = c rL ∂ t u(t, x) + rL ion iion(t, x) − rL iext(t, x).\n\n(3.13)\n\nEquation (3.13) is called the general cable equation.\n\nExample: Cable equation for passive dendrite\n\nThe ionic currents ∑ion iion(t, x) in Eq. (3.13) can in principle comprise many different types of ion channel, as discussed in Section 2.3. For simplicity, the dendrite is\n\nsometimes considered as passive. This means that the current density follows Ohm’s law\n\n∑ion iion(t, x) = gl(u − El) where gl = 1/rT is the leak conductance per unit length and\nEl is the leak reversal potential. We introduce the characteristic length scale λ 2 = rT/rL (“electrotonic length scale”)\nand the membrane time constant τ = rT c. If we multiply Eq. (3.13) by λ 2 we get\n\nλ\n\n2\n\n∂2 ∂ x2\n\nu(t,\n\nx)\n\n=\n\nτ\n\n∂ ∂t\n\nu(t,\n\nx)\n\n+\n\n[u(t,\n\nx)\n\n−\n\nEl ]\n\n−\n\nrT\n\niext (t ,\n\nx).\n\n(3.14)\n\nAfter a transformation to unit-free coordinates,\n\nx → xˆ = x/λ , t → tˆ = t/τ ,\n\n(3.15)\n\nand a rescaling of the current and voltage variables,\n\ni\n\n→\n\niˆ\n\n=\n\n√ rT\n\nrL\n\ni\n\n,\n\niext → iˆext = rT iext ,\n\nu → uˆ = u − El,\n\n(3.16)\n\nwe obtain the cable equation (where we have dropped the hats)\n\n∂\n\n∂2\n\n∂ t u(t, x) = ∂ x2 u(t, x) − u(t, x) + iext(t, x),\n\nin an elegant unit-free form.\n\n(3.17)\n\n3.2 Spatial structure: the dendritic tree\n\n67\n\nThe cable equation can be easily interpreted. The change in time of the voltage at location x is determined by three different contributions. The ﬁrst term on the right-hand side of Eq. (3.17) is a diffusion term that is positive if the voltage is a convex function of x. The voltage at x thus tends to increase, if the values of u are higher in a neighborhood of x than at x itself. The second term on the right-hand side of Eq. (3.17) is a simple decay term that causes the voltage to decay exponentially towards zero. The third term, ﬁnally, is a source term that acts as an inhomogeneity in the otherwise autonomous differential equation. This source can be due to an externally applied current or to synaptic input arriving at location x.\n\nExample: Stationary solutions of the cable equation\n\nIn order to get an intuitive understanding of the behavior of the cable equation of a passive dendrite we look for stationary solutions of Eq. (3.17), i.e., for solutions with ∂ u(t, x)/∂t = 0. In that case, the partial differential equation reduces to an ordinary differential equation in x, namely\n\n∂2 ∂ x2 u(t, x) − u(t, x) = −iext(t, x) . The general solution to the homogenous equation with iext(t, x) ≡ 0 is\n\n(3.18)\n\nu(t, x) = c1 sinh(x) + c2 cosh(x) ,\n\n(3.19)\n\nas can easily be checked by taking the second derivative with respect to x. Here, c1 and c2 are constants that are determined by the boundary conditions.\nSolutions for non-vanishing input current can be found by standard techniques. For a stationary input current iext(t, x) = δ (x) localized at x = 0 and boundary conditions u(±∞) = 0 we ﬁnd\n\nu(t, x) = 1 e−|x|; 2\n\n(3.20)\n\nsee Fig. 3.6. This solution is given in units of the intrinsic length scale λ = (rT/rL)1/2. If we re-substitute the physical units, we see that λ is the length over which the stationary membrane potential drops by a factor 1/e. In the literature λ is referred to as the electro-\ntonic length scale (Rall, 1989). Typical values for the speciﬁc resistance of the intracellular medium and the cell membrane are 100 Ω cm and 30k Ω cm2, respectively. In a dendrite with radius ρ = 1 μm this amounts to a longitudinal and a transversal resistance of rL = 100 Ω cm/(πρ2) = 3 × 105 Ω μm−1 and rT = 30 kΩ cm2/(2πρ) = 5 × 1011 Ω μm. The corresponding electrotonic length scale is λ = 1.2 mm. Note that the electrotonic\n\n68 0.5 0.4 0.3\nu 0.2 0.1\n−3 −2 −1\n\nDendrites and synapses\n\nFig. 3.6 Stationary solution of the cable\n\nequation with a constant current of unit\n\nl\n\nstrength being injected at x = 0, i.e.,\n\niext(t, x) = δ (x). The electrotonic length scale λ is the distance over which the membrane\n\npotential drops to 1/e of its initial value.\n\n0\n\n1\n\n2\n\n3\n\nx\n\nlength can be signiﬁcantly smaller if the transversal conductivity is increased, e.g., due to open ion channels.\nFor arbitrary stationary input current iext(x) the solution of Eq. (3.17) can be found by a superposition of translated fundamental solutions (3.20), namely\n\nu(t, x) =\n\ndx\n\n1 2\n\ne−|x−x\n\n|\n\niext(x\n\n)\n\n.\n\n(3.21)\n\nThis is an example of the Green’s function approach applied here to the stationary case. The general time-dependent case will be treated in the next section.\n\n3.2.2 Green’s function of the passive cable\n\nIn the following we will concentrate on the equation for the voltage and start our analysis by a discussion of the Green’s function for a cable extending to inﬁnity in both directions. The Green’s function is deﬁned as the solution of a linear equation such as Eq. (3.17) with a Dirac δ -pulse as its input. It can be seen as an elementary solution of the differential equation because – due to linearity – the solution for any given input can be constructed as a superposition of these Green’s functions.\nSuppose a short current pulse iext(t, x) is injected at time t = 0 at location x = 0. As we will show below, the time course of the voltage at an arbitrary position x is given by\n\nu(t, x) = √Θ(t) exp −t − x2\n\n4π t\n\n4t\n\n≡ G∞(t, x) ,\n\n(3.22)\n\nwhere G∞(t, x) is the Green’s function. Knowing the Green’s function, the general solution for an inﬁnitely long cable is given by\n\nt\n\n∞\n\nu(t, x) = dt dx G∞(t − t , x − x ) iext(t , x ) .\n\n−∞ −∞\n\n(3.23)\n\nThe Green’s function is therefore a particularly elegant and useful mathematical tool: once\n\nyou have solved the linear cable equation for a single short current pulse, you can write\n\ndown the full solution to arbitrary input as an integral over (hypothetical) pulse-inputs at\n\nall places and all times.\n\n3.2 Spatial structure: the dendritic tree\n\n69\n\nChecking the Green’s property (*)\n\nWe can check the validity of Eq. (3.22) by substituting G∞(t, x) into Eq. (3.17). After a short calculation we ﬁnd\n\n∂ ∂2 ∂ t − ∂ x2 + 1\n\nG∞(t, x)\n\n=\n\n√1 4π t\n\nexp\n\n−t − x2 4t\n\nδ (t) ,\n\n(3.24)\n\nwhere we have used ∂ Θ(t)/∂t = δ (t). As long as t = 0 the right-hand side of Eq. (3.24) vanishes, as required by Eq. (3.28). For t → 0 we ﬁnd\n\nlim √ 1 exp −t − x2 = δ (x) ,\n\nt→0 4π t\n\n4t\n\n(3.25)\n\nwhich proves that the right-hand side of Eq. (3.24) is indeed equivalent to the right-hand side of Eq. (3.28).\nHaving established that\n\n∂ ∂t\n\n−\n\n∂2 ∂ x2\n\n+\n\n1\n\nG∞(t, x) = δ (x) δ (t) ,\n\n(3.26)\n\nwe can readily show that Eq. (3.23) is the general solution of the cable equation for arbitrary input currents iext(t0, x0). We substitute Eq. (3.23) into the cable equation, exchange the order of integration and differentiation, and ﬁnd\n\n∂ ∂t\n\n−\n\n∂2 ∂ x2\n\n+\n\n1\n\nu(t, x)\n\nt\n\n∞\n\n= dt dx\n\n−∞ −∞\n\n∂ ∂2 ∂ t − ∂ x2 + 1\n\nG∞(t − t , x − x ) iext(t , x )\n\nt\n\n∞\n\n= dt dx δ (x − x ) δ (t − t ) iext(t , x ) = iext(t, x) .\n\n−∞ −∞\n\n(3.27)\n\nDerivation of the Green’s function (*)\n\nPreviously, we have just “guessed” the Green’s function and then shown that it is indeed a solution of the cable equation. However, it is also possible to derive the Green’s function step by step. In order to ﬁnd the Green’s function for the cable equation we thus have to solve Eq. (3.17) with iext(t, x) replaced by a δ -impulse at x = 0 and t = 0\n\n∂ ∂t\n\nu(t,\n\nx)\n\n−\n\n∂2 ∂ x2\n\nu(t,\n\nx)\n\n+\n\nu(t,\n\nx)\n\n=\n\nδ\n\n(t)\n\nδ\n\n(x)\n\n.\n\n(3.28)\n\nFourier transformation with respect to the spatial variable yields\n\n∂ ∂t\n\nu(t, k) +\n\nk2\n\nu(t, k)\n\n+\n\nu(t, k)\n\n=\n\n√ δ (t)/ 2π\n\n.\n\n(3.29)\n\nThis is an ordinary differential equation in t and has a solution of the form √\nu(t, k) = exp − 1 + k2 t / 2π Θ(t),\n\n(3.30)\n\n70\n\nDendrites and synapses\n\nwith Θ(t) denoting the Heaviside function. After an inverse Fourier transform we obtain\n\nthe desired Green’s function G∞(t, x),\n\nu(t, x) = √Θ(t) exp −t − x2\n\n4π t\n\n4t\n\n≡ G∞(t, x) .\n\n(3.31)\n\nExample: Finite cable\n\nReal cables do not extend from −∞ to +∞ and we have to take extra care to correctly\n\ninclude boundary conditions at the ends. We consider a ﬁnite cable extending from x = 0\n\nto\n\nx\n\n=\n\nL\n\nwith\n\nsealed\n\nends,\n\ni.e.,\n\ni(t, x\n\n=\n\n0)\n\n=\n\ni(t, x\n\n=\n\nL)\n\n=\n\n0\n\nor,\n\nequivalently,\n\n∂ ∂x\n\nu(t\n\n,\n\nx\n\n=\n\n0)\n\n=\n\n∂ ∂x\n\nu(t, x\n\n=\n\nL)\n\n=\n\n0.\n\nThe Green’s function G0,L for a cable with sealed ends can be constructed from G∞\n\nby applying a trick from electrostatics called “mirror charges” (Jackson, 1962). Similar\n\ntechniques can also be applied to treat branching points in a dendritic tree (Abbott,\n\n1991). The cable equation is linear and, therefore, a superposition of two solutions is\n\nalso a solution. Consider a δ -current pulse at time t0 and position x0 somewhere along\n\nthe\n\ncable.\n\nThe\n\nboundary\n\ncondition\n\n∂ ∂x\n\nu(t\n\n,\n\nx\n\n=\n\n0)\n\n=\n\n0\n\ncan\n\nbe\n\nsatisﬁed\n\nif\n\nwe\n\nadd\n\na\n\nsecond,\n\nvirtual current pulse at a position x = −x0 outside the interval [0, L]. Adding a current\n\npulse outside the interval [0, L] comes for free since the result is still a solution of the\n\ncable equation on that interval. Similarly, we can fulﬁll the boundary condition at x = L\n\nby adding a mirror pulse at x = 2 L −x0. In order to account for both boundary conditions simultaneously, we have to compensate for the mirror pulse at −x0 by adding another mirror pulse at 2 L + x0 and for the mirror pulse at x = 2 L − x0 by adding a fourth pulse at −2 L + x0 and so forth. Altogether we have\n\n∞\n\n∑ G0,L(t0, x0;t, x) =\n\nG∞(t − t0, x − 2 n L − x0) + G∞(t − t0, x − 2 n L + x0) . (3.32)\n\nn=−∞\n\nWe emphasize that in the above Green’s function we have to specify both (t0, x0) and (t, x) because the setup is no longer translation invariant. The general solution on the interval [0, L] is given by\n\nt\n\nL\n\nu(t, x) = dt0 dx0 G0,L(t0, x0;t, x) iext(t0, x0) .\n\n−∞\n\n0\n\n(3.33)\n\nAn example for the spatial distribution of the membrane potential along the cable is shown in Fig. 3.7a, where a current pulse has been injected at location x = 1. In addition to Fig. 3.7a, Fig. 3.7b exhibits the time course of the membrane potential measured in various distances from the point of injection. It is clearly visible that the peak of the membrane potential measured at, say, x = 3 is more delayed than at, say, x = 2. Also the amplitude of the membrane potential decreases signiﬁcantly with the distance from the injection point. This is a well-known phenomenon that is also present in neurons. In the absence of active ampliﬁcation mechanisms, synaptic input at distal dendrites produces\n\n3.2 Spatial structure: the dendritic tree\n\n71\n\n(a) 0.8\n\n0.6\n\nu 0.4\n\n0.2\n\n0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\nx\n\n(b)\n0.4 0.3 u 0.2 0.1\n0 0 0.5 1 1.5 2 2.5 3 t\n\nFig. 3.7 Spatial distribution (a) and temporal evolution (b) of the membrane potential along a\n\ndendrite (L = 5) with sealed ends\n\n∂ ∂x\n\nu\n\nx∈{0,L}\n\n=\n\n0\n\nafter injection of a unit current pulse at x = 1\n\nand t = 0. The various traces in (a) show snapshots for time t = 0.1, 0.2, . . . , 1.0, respectively (top to\n\nbottom). The traces in (b) give the membrane potential as a function of time for different locations\n\nx = 1.5, 2.0, 2.5, . . . , 5.0 (top to bottom) along the cable.\n\nbroader and weaker response at the soma as compared to synaptic input at proximal dendrites.\n\n3.2.3 Nonlinear extensions to the cable equation\n\nIn the context of a realistic modeling of “biological” neurons, two nonlinear extensions of the cable equation have to be discussed. The obvious one is the inclusion of nonlinear elements in the circuit diagram of Fig. 3.5 that account for specialized ion channels. As we have seen in the Hodgkin–Huxley model, ion channels can exhibit a complex dynamics that is in itself governed by a system of (ordinary) differential equations. The current through one of these channels is thus not simply a (nonlinear) function of the actual value of the membrane potential but may also depend on the time course of the membrane potential in the past. Using the symbolic notation iion[u](t, x) for this functional dependence, the extended cable equation takes the form\n\n∂\n\n∂2\n\n∂ t u(t, x) = ∂ x2 u(t, x) − u(t, x) − iion[u](t, x) + iext(t, x) .\n\n(3.34)\n\nA more subtle complication arises from the fact that a synapse cannot be treated as an ideal current source. The effect of an incoming action potential is the opening of ion channels. The resulting current is proportional to the difference of the membrane potential and the corresponding ionic reversal potential. Hence, a time-dependent conductivity as in Eq. (3.1) provides a more realistic description of synaptic input than an ideal current source with a ﬁxed time course.\nIf we replace in Eq. (3.17) the external input current iext(t, x) by an appropriate synaptic input current −isyn(t, x) = −gsyn(t, x)[u(t, x) − Esyn] with gsyn being the synaptic\n\n72\n\nDendrites and synapses\n\nconductivity and Esyn the corresponding reversal potential, we obtain2\n\n∂ ∂t\n\nu(t, x)\n\n=\n\n∂2 ∂ x2\n\nu(t, x) −\n\nu(t, x)\n\n− gsyn(t, x)[u(t, x)\n\n−\n\nEsyn] .\n\n(3.35)\n\nThis is still a linear differential equation but its coefﬁcients are now time-dependent. If\n\nthe time course of the synaptic conductivity can be written as a solution of a differential\n\nequation, then the cable equation can be reformulated so that synaptic input reappears as\n\nan inhomogeneity to an autonomous equation. For example, if the synaptic conductivity is simply given by an exponential decay with time constant τsyn we have\n\n∂\n\n∂2\n\n∂ t u(t, x) − ∂ x2 u(t, x) + u(t, x) + gsyn(t, x)[u(t, x) − Esyn] = 0 ,\n\n∂ ∂t\n\ngsyn(t\n\n,\n\nx)\n\n−\n\nτs−y1n\n\ngsyn(t\n\n,\n\nx)\n\n=\n\nS(t\n\n,\n\nx)\n\n.\n\n(3.36a) (3.36b)\n\nHere, S(t, x) is a sum of Dirac δ -functions which describe the presynaptic spike train that\n\narrives at a synapse located at position x. Note that this equation is nonlinear because it\n\ncontains a product of gsyn and u which are both unknown functions of the differential equation. Consequently, the formalism based on Green’s functions cannot be applied. We\n\nhave reached the limit of what we can do with analytical analysis alone. To study the\n\neffect of ion channels distributed on the dendrites numerical approaches in compartmental\n\nmodels become invaluable (Section 3.4).\n\n3.3 Spatial structure: axons\nAny given neuron has a single axon that leaves the soma to make synaptic contacts. Like dendrites, axons have a range of different morphologies. Some axons project mainly to neurons close by. This is the case for neurons in the layer 2–3 of cortex; their axons branch out in all directions from the soma forming a star-shaped axonal arbor called a “daisy.” Other neurons such as pyramidal neurons situated deeper in the cortex have axons that plunge in the white matter and may cross the whole brain to reach another brain area. There are even longer axons that leave the central nervous system and travel down the spinal cord to reach muscles at the tip of the foot.\nIn terms of propagation dynamics, we distinguish two types of axons: the myelinated and the unmyelinated axons. We will see that myelin is useful to increase propagation speed in far-reaching projections. This is the case for cortical projections passing through the white matter, or for axons crossing the spinal cord. Short projections on the other hand use axons devoid of myelin.\n\n3.3.1 Unmyelinated axons\nMathematical description of the membrane potential in the axon is identical to that of dendrites with active ion channels. Unmyelinated axons contain sodium and potassium\n2We want outward currents to be positive, hence the change in the sign of iext and isyn.\n\n3.3 Spatial structure: axons\n\n73\n\nchannels uniformly distributed over their entire length. The classical example is the squid giant axon investigated by Hodgkin and Huxley. The Hodgkin–Huxley model described in Chapter 2 was developed for a small axon segment. The general equation for ion channels imbedded on a passive membrane is\n\n∂\n\n∂2\n\nc rL ∂ t u(t, x) = ∂ x2 u(t, x) − rL(u(t, x) − El) − rLiion[u](t, x)\n\n(3.37)\n\nwhere we have reverted to a variable u in units of mV from the equation of active dendrites\n\nseen in Section 3.2.3. For the giant squid axon, the ionic current are described by the\n\nHodgkin–Huxley model\n\niion[u](t, x) = gNa m3(t, x)h(t, x) (u(t, x) − ENa) + gK n4(t, x) (u(t, x) − EK). (3.38)\n\nIn other systems, the axon may be covered with other types of sodium or potassium ion channels.\nWhen an action potential is ﬁred in the axon initial segment, the elevated membrane potential will depolarize the adjacent axonal segments. Sodium channels farther down the axon, which were previously closed, will start to open, thereby depolarizing the membrane further. The action potential propagates by activating sodium channels along the cable rather than by spreading the charges as in a passive dendrite. The properties of the ion channels strongly inﬂuence conduction velocity. In the unmyelinated axons of the hippocampus, the conduction velocity of the axons is 0.25 m/s.\nThe dynamics described by Eqs. (3.37)–(3.38) reproduces many properties of real axons. In particular, two spikes traveling in opposite direction will collide and annihilate each other. This is unlike waves propagating on water. Another property is reﬂection at branch points. When the impedance mismatch at the point where a single axon splits into two is signiﬁcant, the action potential can reﬂect and start traveling in the direction it came from.\nThe solution of Eq. (3.37) with sodium and postassium ion channels as in Eq. (3.38) cannot be written in a closed form. Properties of axonal propagation are either studied numerically (see Section 3.4) or with reduced models of ion channels.\n\nExample: Speed of propagation with simpliﬁed action potential dynamics\n\nFor the sake of studying propagation properties, we can replace the active properties of a small axonal segment by a bistable switch (FitzHugh, 1961; Nagumo et al., 1962) . We can write the time- and space-dependent membrane potential as\n\nc rL\n\n∂ ∂t\n\nu(t, x)\n\n=\n\n∂2 ∂ x2\n\nu(t, x) −\n\nrLg 1−a\n\nu(t\n\n,\n\nx)(u(t\n\n,\n\nx)\n\n−\n\n1)(u(t\n\n,\n\nx)\n\n−\n\na)\n\n,\n\n(3.39)\n\nwhere a < 1/2 and g are parameters. The membrane potential is scaled such that it rests at zero but may be activated to u = 1. The reduced model can switch between u = 0 and u = 1 if it is pushed above u = a, but does not reproduce the full upswing followed by downswing of action potentials.\n\n74\n\nDendrites and synapses\n\nIt turns out that Eq. (3.39) can also be interpreted as a model of ﬂame front propagation. The solution of this equation follows (Zeldovich and Frank-Kamenetskii, 1938)\n\nwith traveling speed\n\n1 u(x,t) =\n1 + exp √x−2λvt∗\n\nc(1 − 2a)\n\nv=\n\n.\n\n2(1 − a)rL/g\n\n(3.40) (3.41)\n\nThe propagation velocity depends on the capacitance per unit length c, the longitudinal\n\nresistance per unit length rL, and the excitability parameters g and a.\n\nHow does the conduction velocity scale with axon size? Since rL, c and g themselves\n\ndepend on the diameter of the axon, we expect the velocity to reﬂect that relationship.\n\nThe parameters c and g scale with the circumference of the cellular membrane and\n\ntherefore scale linearly with the radius ρ. The cytoplasmic resistance per unit length,\n\nhowever, scales with the cross-sectional area, rL ∝ ρ2. With these relations in mind,\n\nEdiqa.m(e3t.e4r1)vs∝ho√wsρ\n\nthat the conduction velocity is proportional to . Therefore, increasing the diameter improves\n\nthe square root of the propagation velocity.\n\nThis is thought to be the reason why the unmyelinated axons that Hodgkin and Huxley\n\nstudied were so large (up to ρ = 500 μm).\n\n3.3.2 Myelinated axons\n\nMyelinated axons have sodium and potassium channels only in restricted segments called nodes of Ranvier. These nodes form only 0.2% of the axonal length, the rest is considered\n\na passive membrane that is wrapped into a myelin sheath. Myelin mainly decreases the\n\nmembrane capacitance C and increase the resistance RT by a factor of up to 300 (Debanne et al., 2011). Ions are trapped by myelin since it prevents them from either ﬂowing outside\n\nthe axon or accumulating on the membrane. Instead, ions ﬂow in and out of the nodes such\n\nthat an ion leaving a node of Ranvier forces another to enter the following node. Assuming that the nodes are equally separated by a myelinated segment of length L, we can model the evolution of the membrane potential at each node un. The dynamics of idealized myelinated axons follow Kirchoff’s equation with a resistance RL = L rL replacing the myelinated segment\n\n∑ C dun\ndt\n\n=\n\n1 L rL\n\n(un+1\n\n(t\n\n)\n\n−\n\n2un(t\n\n)\n\n+\n\nun−1\n\n(t\n\n))\n\n−\n\nion\n\nIion,n(t\n\n)\n\n(3.42)\n\nwhere C is the total capacitance of the node. This equation was encountered in the derivation of the cable equation (Section 3.2.1). The conduction velocity is greatly increased by myelin such that some nerves reach 70–80 m/s (Debanne et al., 2011).\n\n3.4 Compartmental models\n\n75\n\nExample: Propagation speed with simpliﬁed action potential dynamics\n\nUsing the simpliﬁcation of the ion channel dynamics in Eq. (3.39) for each node\n\nC dun dt\n\n=\n\n1 L rL\n\n(un+1\n\n(t\n\n)\n\n−\n\n2un(t\n\n)\n\n+\n\nun−1\n\n(t\n\n))\n\n−\n\n1\n\ng −\n\na\n\nun (t )(un (t\n\n)\n\n−\n\n1)(un\n\n(t\n\n)\n\n−\n\na)\n\n(3.43)\n\nwhere g and a < 1/2 are parameters regulating the excitability of the node. Unlike Eq. (3.39), the parameter g has units of conductance per node since the nodes of Ranvier are discrete segments. An activated node may fail to excite the adjacent nodes if the membrane potential does not reach u = a. In this model, the internodal distance must satisfy (Erneux and Nicolis, 1993)\n\nL\n\n<\n\nL∗\n\n=\n\n1−a 4a2rLg\n\n(3.44)\n\nfor propagation to be sustained. When the internodal distance L is smaller than L∗,\n\npropagation will fail. When the internodal distance is larger, propagation will succeed. The propagation velocity for small L∗ − L follows (Binczak et al., 2001)\n\nv ≈ πg a L(L∗ − L) (1 − a)C\n\n(3.45)\n\nwhich is maximum at L = L∗/2. Since, in most myelinated axons, internodal distance scales linearly with their radius (Waxman, 1980), the velocity of myelinated axons also scales linearly with radius, v ∝ L ∝ ρ.\n\n3.4 Compartmental models\nWe have seen that analytical solutions can be given for the voltage along a passive cable with uniform geometrical and electrical properties. If we want to apply the above results in order to describe the membrane potential along the dendritic tree of a neuron we face several problems. Even if we neglect “active” conductances formed by nonlinear ion channels, a dendritic tree is at most locally equivalent to a uniform cable. Numerous bifurcations and variations in diameter and electrical properties along the dendrite render it difﬁcult to ﬁnd a solution for the membrane potential analytically (Abbott et al., 1991).\nNumerical treatment of partial differential equations such as the cable equation requires a discretization of the spatial variable. Hence, all derivatives with respect to spatial variables are approximated by the corresponding quotient of differences. Essentially we are led back to the discretized model of Fig. 3.5 that has been used as the starting point for the derivation of the cable equation. After the discretization we have a large system of ordinary differential equations for the membrane potential at the chosen discretization points as a function of time. This system of ordinary differential equations can be treated by standard numerical methods.\n\n76\n\nDendrites and synapses\n\n1\n2 RL\n\nIm\n1\n2 RL\n\nCm\n\nR\n\nm T\n\nFig. 3.8 Multi-compartment neuron model. Dendritic compartments with membrane capacitance Cμ\n\nainnpduttrtaonscvoemrspaalrrtemseisnttanμceisRdTμenaorteedcobuyplIeμd.\n\nby a longitudinal resistance rμν Some or all compartments may\n\n= (RLμ + RνL )/2. External also contain nonlinear ion\n\nchannels (variable resistor in leftmost compartment).\n\nIn order to solve for the membrane potential of a complex dendritic tree numerically, compartmental models are used that are the result of the above mentioned discretization. The dendritic tree is divided into small cylindric compartments with an approximatively uniform membrane potential. Each compartment is characterized by its capacity and transversal conductivity. Adjacent compartments are coupled by the longitudinal resistance determined by their geometrical properties (see Fig. 3.8).\nOnce numerical methods are used to solve for the membrane potential along the dendritic tree, some or all compartments can be equipped with nonlinear ion channels as well. In this way, effects of nonlinear integration of synaptic input can be studied. Apart from practical problems that arise from a growing complexity of the underlying differential equations, conceptual problems are related to a drastically increasing number of free parameters. To avoid these problems, all nonlinear ion channels responsible for generating spikes are usually lumped together at the soma and the dendritic tree is treated as a passive cable. For a review of the compartmental approach we refer the reader to Bower and Beeman (1995). In the following we illustrate the compartmental approach by a model of a pyramidal cell.\nExample: A multi-compartment model of a deep-layer pyramidal cell\nSoftware tools such as NEURON (Carnevale and Hines, 2006) or GENESIS (Bower and Beeman, 1995) enable researchers to construct detailed compartmental models of any type of neuron. The morphology of such a detailed model is constrained by the anatomical reconstruction of the corresponding “real” neuron. This is possible if length,\n\n3.4 Compartmental models\n\n77\n\n(a)\n\n(b)\n\nVm Istim 2\n\n2 1\n\n(c)\n\n1\n\n2\nVm 1 Istim 2\n\n1\n(d)\n\nVm\n\n50 mV 4 nA 10 ms\n2\n\n2 1 Istim\n\n(e)\n\n2\n\n1\n\nV\n\nm\n\n1\n\nIstim 2\n\nFig. 3.9 Bursting in a computational model of a deep layer cortical neuron. (a) Reconstruction of the complete morphology indicating the location of injection and measurement sites marked “1” at the soma, and “2” at the dendrite. (b) A current is injected into the dendrite (lower trace, marked “2”) mimicking an excitatatory postsynaptic current (EPSC). The model responds to the current injection in the dendrite with a voltage deﬂection at the dendrite (upper trace marked “2”) but hardly any deﬂection at the soma (“1”). (c). A current pulse in the soma (lower trace, marked “1”) causes an action potential at the soma (voltage trace marked “1”) that back-propagates as a broader voltage pulse (“2”) into the dendrite. (d) Coincidence of somatic current pulse and dendritic EPSC activates calcium currents in the dendrites and causes a burst of spikes in the soma. (e) A single, but large EPSC-shaped dendritic current can also activate calcium currents and leads to a delayed burst of spikes in the soma. Image modiﬁed from Hay et al. (2011).\n\nsize and orientation of each dendritic segment are measured under a microscope, after the neuron has been ﬁlled with a suitable dye. Before the anatomical reconstruction, the electrophysiological properties of the neuron can be characterized by stimulating the neuron with a time-dependent electric current. The presence of speciﬁc ion channel types can be inferred, with genetic methods, from the composition of the intracellular liquid, extracted from the neuron (Toledo-Rodriguez et al., 2004). The distribution of ion\n\n78\n\nDendrites and synapses\n\nchannels across the dendrite is probably the least constrained parameter. It is sometimes inferred from another set of experiments on neurons belonging to the same class. All the experimental knowledge about the neuron is then condensed in a computational neuron model. A good example is the model of a deep-layer cortical neuron with active dendrites as modeled by Hay et al. (2011).\nThe complete morphology is divided into 200 compartments, none exceeding 20 μm in length. Each compartment has its speciﬁc dynamics deﬁned by intracellular ionic concentration, transversal ion ﬂux through modeled ion channels, and longitudinal current ﬂux to connected compartments. The membrane capacitance is set to 1 μF/cm2 for the soma and axon and to 2 μF/cm2 in the dendrites to compensate for the presence of dendritic spines. A cocktail of ionic currents is distributed across the different compartments. These are:\n• the fast inactivating sodium current INa (Section 2.2.1), • the persistent sodium current INaP (Section 2.3.3), • the non-speciﬁc cation current Ih (Section 2.3.4), • the muscarinic potassium current IM (Section 2.3.3), • the small conductance calcium-activated potassium current IK[Ca] (Section 2.3.4), • the fast non-inactivating potassium current IKv3.1 (Rettig et al., 1992) which is very\nsimilar to the model of potassium current of the Hodgkin–Huxley model in Table 2.1, • the high-voltage activated calcium current IHVA (mentioned in Section 2.3.3), • the low-voltage activated calcium current IL(Avery and Johnston, 1996; Randall and\nTsien, 1997) (similar to the HVA channel but different parameters), • and a calcium pump (Section 2.3.3).\nIn addition, the model contains a slow and a fast inactivating potassium current IKp, IKt, respectively (Korngreen and Sakmann, 2000).\nIn the dendrites all these currents are modeled as uniformly distributed except Ih, IHVA and IL. The ﬁrst one of these, Ih, is exponentially distributed along the main dendrite that ascends from the deep layers with low Ih concentration to the top layers with large Ih concentration (Kole et al., 2006). The two calcium channels were distributed with a uniform distribution in all dendrites except for a single hotspot with a concentration 100 and 10 times higher for IL and IHVA, respectively. Finally, the strength of each ionic current was scaled by choosing the maximal conductance gion that best ﬁt experimental data.\nThis detailed compartmental model can reproduce quantitatively some features of the deep-layer pyramidal neurons (Fig. 3.9). For example, a small dendritic current injection results in a transient increase of the dendritic voltage, but only a small effect in the soma (Fig. 3.9b). A sufﬁciently large current pulse in the soma initiates not only a spike at the soma but also a back-propagating action potential traveling into the dendrites (Fig. 3.9c). Note that it is the presence of sodium and potassium currents throughout the dendrites that support the back-propagation. In order to activate the dendritic calcium channels at the hotspot, either a large dendritic injection or a coincidence between the\n\n3.5 Summary\n\n79\n\nback-propagating action potential and a small dendritic injection is required (Fig. 3.9d, e). The activation of calcium channels in the hotspot introduces a large and long (around 40 ms) depolarizing current that propagates forward to the soma where it eventually causes a burst of action potentials.\n\n3.5 Summary\n“Real” neurons are complex biophysical and biochemical entities. Before designing a model it is therefore necessary to develop an intuition for what is important and what can be safely neglected. Synapses are usually modeled as speciﬁc ion channels that open for a certain time after presynaptic spike arrival. The geometry of the neuron can play an important role in the integration of incoming signals because the effect of synaptic input on the somatic membrane potential depends on the location of the synapses on the dendritic tree. Though some analytic results can be obtained for passive dendrites, it is usually necessary to resort to numerical methods and multi-compartment models in order to account for the complex geometry and presence of active ion channels on neuronal dendrites.\nLiterature\nThe book Dendrites (Stuart et al., 2007) offers a comprehensive review of the role and importance of dendrites from multiple points of view. An extensive description of cable theory as applied to neuronal dendrites can be found in the collected works of Wilfrid Rall (Segev et al., 1994). NEURON (Carnevale and Hines, 2006) and GENESIS (Bower and Beeman, 1995) are important tools to numerically solve the system of differential equations of compartmental neuron models. There are useful repositories of neuronal morphologies (see http://NeuroMorpho.Org for instance) and of published models on ModelDB (http://senselab.med.yale.edu/modeldb). The deep-layer cortical neuron discussed in this chapter is described in Hay et al. (2011). Potential computational consequences of nonlinear dendrites are described in Mel (1994).\nExercises 1. Biophysical synapse model and its relation to other models\n(a) Consider Eq. (3.4) and discuss its relation to Eq. (3.2). Hint: (i) Assume that the time course γ(t) can be described by a short pulse (duration of 1 ms) and that the unbinding is on a time scale β −1 > 10 ms. (ii) Assume that the interval between two presynaptic spike arrivals is much larger than β −1. (b) Discuss the relation of the depressive synapse model in Eq. (3.8) with the biophysically model in Eq. (3.4). Hint: (i) Assume that the interval between two presynaptic spikes is of the same order β −1. (ii) In Eq. (3.8) consider a variable x = Prel/P0.\n\n80\n\nDendrites and synapses\n\n2. Transmitter-gated ion channel For each of the following statements state whether it is correct or wrong: (a) AMPA channels are activated by glutamate. (b) AMPA channels are activated by AMPA. (c) If the AMPA channel is open, AMPA can pass through the channel. (d) If the AMPA channel is open, glutamate can pass through the channel. (e) If the AMPA channel is open, potassium can pass through the channel.\n3. Cable equation (a) Show that the passive cable equation for the current is\n\n∂ ∂t\n\ni(t, x)\n\n=\n\n∂2 ∂ x2\n\ni(t, x) − i(t, x) +\n\n∂ ∂x\n\niext(t, x).\n\n(3.46)\n\n(b) Set the external current to zero and ﬁnd the mapping to the heat equation\n\n∂\n\n∂2\n\n∂ t y(t, x) = ∂ x2 y(t, x).\n\n(3.47)\n\nHint: Try y(t, x) = f (t) i(t, x) with some function f . (c) Find the solution to the current equation in (a) for the inﬁnite cable receiving a short current pulse at time t = 0 and show that the corresponding equation for y satisﬁes the heat equation in (b). 4. Non-leaky cable (a) Redo the derivation of the cable equation for the case of an inﬁnite one-dimensional passive dendrite without transversal leak and show that the solution to the equation is of the form\n\nt\n\n∞\n\nu(x,t) = dt dx Gd (t − t , x − x ) iext(t , x )\n−∞ −∞\n\n(3.48)\n\nwhere Gd is a Gaussian of the form Gd (x,t) =\n\n1 2π σ\n\n(t\n\n)\n\nexp\n\n−\n\nx2 2σ 2(t)\n\n.\n\n(3.49)\n\nDetermine σ (t) and discuss the result.\n\n(b) Use the method of mirror charges to discuss how the solution changes if the cable is semi-\n\ninﬁnite and extends from zero to inﬁnity.\n\n(c) Take the integral over space of the elementary solution of the non-leaky cable equation and\n\nshow that the value of the integral does not change over time. Give an interpretation of this result.\n\n(d) Take the integral over space of the elementary solution of the normal leaky cable equation of\n\na passive dendrite and derive an expression for its temporal evolution. Give an interpretation of\n\nyour result.\n\n5. Conduction velocity in unmyelinated axons\n\n(a) Using the simpliﬁed ion channel dynamics of Eq. (3.39), transform x and t to dimensionless\n\nvariables using effective time and electrotonic constants.\n\n(b) A traveling pulse solution will have the form u(x,t) = u˜(x − vt) where v is the conduction\n\nvelocity. Find the ordinary differential equation that rules u˜.\n\n(c)\n\nShow\n\nthat\n\nu˜(y) =\n\n1 1+exp(y)\n\nwith\n\ntraveling\n\nspeed\n\nv\n\n=\n\n1√−2a 2\n\nis\n\na\n\nsolution.\n\n4\nDimensionality reduction and phase plane analysis\nThe ﬁring of action potentials has been successfully described by the Hodgkin–Huxley model, originally for the spikes in the giant axon of the squid but also, with appropriate modiﬁcations of the model, for other neuron types. The Hodgkin–Huxley model is deﬁned by four nonlinear differential equations. The behavior of high-dimensional systems of nonlinear differential equations is difﬁcult to visualize – and even more difﬁcult to analyze. For an understanding of the ﬁring behavior of the Hodgkin–Huxley model, we therefore need to turn to numerical simulations of the model. In Section 4.1 we show, as an example, some simulation results in search of the ﬁring threshold of the Hodgkin–Huxley model. However, it remains to show whether we can get some deeper insights into the observed behavior of the model.\nFour equations are in fact just two more than two: in Section 4.2 we exploit the temporal properties of the gating variables of the Hodgkin–Huxley model so as to approximate the four-dimensional differential equation by a two-dimensional one. Two-dimensional differential equations can be studied in a transparent manner by means of a technique known as “phase plane analysis.” Section 4.3 is devoted to the phase plane analysis of generic neuron models consisting of two coupled differential equations, one for the membrane potential and the other for an auxiliary variable.\nThe mathematical tools of dimension reduction and phase plane analysis that are presented in Sections 4.2 and 4.3 will be repeatedly used throughout this book, in particular in Chapters 5, 6, 16 and 18. As a ﬁrst application of phase plane analysis, we study in Section 4.4 the classiﬁcation of neurons into type I and type II according to their frequency– current relation. As a second application of phase plane analysis, we return in Section 4.5 to some issues around the notion of a “ﬁring threshold,” which will be sketched now.\n4.1 Threshold effects Many introductory textbooks of neuroscience state that neurons ﬁre an action potential if the membrane potential reaches a threshold. Since the onset of an action potential is characterized by a rapid rise of the voltage trace, the onset points can be detected in exper-\n\n82\n\nDimensionality reduction and phase plane analysis\n\niment recordings (Fig. 4.1a). Intuitively, the onset of an action potential occurs when the membrane potential crosses the ﬁring threshold.\nThe ﬁring threshold is not only a useful concept for experimental neuroscience, it is also at the heart of most integrate-and-ﬁre models and therefore central to Parts II and III of this book. But does a ﬁring threshold really exist?\nExperimenters inject currents into a single neuron to probe its ﬁring characteristics. There is a large choice of potential current wave forms, but only few of these are routinely used in many labs. In this section we use current pulses and steps in order to explore the threshold behavior of the Hodgkin–Huxley model.\n\n4.1.1 Pulse input\nA Hodgkin–Huxley model is stimulated by a short current pulse of 1 ms duration. If the stimulus is strong enough, it elicits a spike. In Fig. 4.1a,b the amplitude of the stimulating current is only slightly increased between the ﬁrst and second current pulse. The membrane potential returns directly to the resting potential after the stimulus, while the neuron ﬁres a spike in response to the second pulse. This seems to suggest that the voltage threshold for spike ﬁring is just above the maximum voltage that is reached after the ﬁrst current injection (upper horizontal dashed line in Fig. 4.1b.\nUnfortunately, however, such an interpretation is incorrect. If we use a longer current pulse of 100 ms duration and apply the same argument as before, we would ﬁnd a different voltage threshold, indicated by the lower horizontal dashed line in Fig. 4.1b.\nDespite the fact that neuronal action potential ﬁring is often treated as a thresholdlike behavior, such a threshold is not well deﬁned mathematically (Rinzel and Ermentrout, 1998; Koch et al., 1995). For practical purposes, however, the transition can be treated as a threshold effect. However, the threshold we ﬁnd depends on the stimulation protocol.\nFor a mathematical discussion of the threshold phenomenon, it is helpful to reduce the system of four differential equations to two equations; this is the topic of Section 4.2. We will return to pulse currents in Section 4.5.\nExample: Short current pulse as voltage step\nThe above argument excludes a voltage threshold, but could there be a current threshold? Instead of injecting a current of 1 ms duration we can use a shorter pulse that lasts only half as long. Numerically, we ﬁnd that the minimal current necessary to trigger an action potential of the Hodgkin–Huxley model is now twice as large as before. We conclude that, for short current pulses, it is not the amplitude of the current that sets the effective ﬁring threshold, but rather the integral of the current pulse or the charge. Indeed, the more charge we put on the membrane the higher the voltage, simply because of the capacitance C of the membrane. For very short current pulses\n\n4.1 Threshold effects\n\n83\n\n(a)\n\n(b)\n\nFig. 4.1 Firing threshold. (a) Experimental recording of membrane voltage (top) during stimulation with a time-dependent current (bottom). The onset of spikes, deﬁned as the moment when the voltage starts its upswing, is marked by open circles. The lowest and highest onset voltage during a recording of 20 s are marked by dotted horizontal lines. Inset: Histogram of onset voltages; adapted from Mensi et al. (2013). (b) Stimulation of the Hodgkin–Huxley model with pulse and step current. Voltage (top) in response to pulses and steps (bottom). The apparent voltage threshold is higher (dotted lines in top panel) for a pulse than for a step current. The critical currents are 16.6 μA/cm2 for the 1 ms pulse and 3.31 μA/cm2 for the step. Parameters of the Hodgkin–Huxley model as in Table 2.1.\n\nI(t)\n\n=\n\nq\n\nδ\n\n(t)\n\n=\n\nlim\nΔ→0\n\nq Δ\n\nfor 0 < t < Δ\n\nand 0 otherwise.\n\n(4.1)\n\nThe voltage of the Hodgkin–Huxley model increases at the moment of current injection by an amount Δu = q/C where q is the charge of the current pulse (see Section 1.3.2). If the model was at rest for t < 0, the new voltage u = urest + Δu can be used as the\ninitial condition for the numerical integration of the Hodgkin–Huxley model for times\n\n84\n\nDimensionality reduction and phase plane analysis\n\nt > 0. Thus, a short current pulse amounts to a step-like increase of the voltage by a ﬁxed amount.\n\n4.1.2 Step current input\n\nIn Chapter 2 we have seen that a constant input current I0 > Iθ can generate regular ﬁring. In this paragraph we study the response of the Hodgkin–Huxley model to a step current of the form\n\nI(t) = I1 + ΔI Θ(t) .\n\n(4.2)\n\nHere Θ(t) denotes the Heaviside step function, i.e., Θ(t) = 0 for t ≤ 0 and Θ(t) = 1 for t > 0. At t = 0 the input jumps from a ﬁxed value I1 to a new value I2 = I1 + ΔI; see Fig. 4.2a. We may wonder whether spiking for t > 0 depends only on the ﬁnal value I2 or also on the step size ΔI.\nThe answer to this question is given by Fig. 4.2d. A large step ΔI facilitates the spike initiation. For example, a target value I2 = 2 μA/cm2 elicits a spike, provided that the step size is large enough, but does not cause a spike if the step is small. The letter S in Fig. 4.2d\ndenotes the regime where only a single spike is initiated. Repetitive ﬁring (regime R) is possible for I2 > 2.6 μA/cm2, but must be triggered by sufﬁciently large current steps.\nWe may conclude from Fig. 4.2d that, when probing with step currents, there is neither a\nunique current threshold for spike initiation nor for repetitive ﬁring. The trigger mechanism for action potentials depends not only on I2 but also on the size of the current step ΔI.\nBiologically, the dependence upon the step size arises from the different time constants\nof activation and inactivation of the ion channels. Mathematically, the stimulation with step\ncurrents can be analyzed transparently in two dimensions (Sections 4.3 and 4.4).\n\n4.2 Reduction to two dimensions\nA system of four differential equations, such as the Hodgkin–Huxley model, is difﬁcult to analyze, so that normally we are limited to numerical simulations. A mathematical analysis is, however, possible for a system of two differential equations.\nIn this section we perform a systematic reduction of the four-dimensional Hodgkin– Huxley model to two dimensions. To do so, we have to eliminate two of the four variables. The essential ideas of the reduction can also be applied to detailed neuron models that may contain many different ion channels. In these cases, more than two variables would have to be eliminated, but the procedure would be completely analogous (Kepler et al., 1992).\n\n4.2.1 General approach\nWe focus on the Hodgkin–Huxley model discussed in Chapter 2 and start with two qualitative observations. First, we see from Fig. 2.3b that the time scale of the dynamics of the\n\n4.2 Reduction to two dimensions\n\n85\n\n(a)\nI\n2\nI1 0\n(c)\n−40 −60 −80\n0\n\n(b)\n\n20\n\n0\n\nu (mV)\n\n−20\n\n−40\n\n−60\n\n50 100\n\n−800\n\n50 100\n\nt (ms)\n\nt (ms)\n\n(d) 6\n\n(e)\n\n50 100 t (ms)\n\nΔI [mA/cm2]\n\n5\n\n4\n\nQ\n\n3\n\n2\n\n1\n\nS\n\nR\n\n20\n\n0\n\nu (mV)\n\n−20\n\n−40\n\n−60\n\n−80 0\n\n0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5\nI2 [µA/cm2]\n\n50 100 t (ms)\n\nu (mV)\n\nFig. 4.2 Phase diagram for stimulation with a step current. (a) The input current I(t) changes at t = 0\nfrom I1 to I2. (b), (c), (e). Sample responses of the Hodgkin–Huxley model to step current input. (d) The outcome of the simulation experiment as a function of the ﬁnal current I2 and the step size ΔI = I2 − I1. Three regimes denoted by S, R, and Q may be distinguished. In Q no action potential is initiated (quiet regime). In S, a single spike is initiated by the current step (single spike regime). In R,\nperiodic spike trains are triggered by the current step (repetitive ﬁring). Examples of voltage traces\nin the different regimes are presented in the smaller graphs (b, c, e) with stimulation parameters indicated by the ﬁlled circles in (d). Note that for the same ﬁnal current I2 (e.g., (d) 2.0 μA/cm2), the neuron model emits a spike if the current step ΔI is large enough (regime S), or no spike if the step is too small. For a ﬁnal current I2 = 3 μA/cm2, the model exhibits bistability between repetitive ﬁring and quiescence.\n\ngating variable m is much faster than that of the variables n and h.Moreover, the time scale of m is fast compared with the membrane time constant τ = C/gL of a passive membrane, which characterizes the evolution of the voltage u when all channels are closed. The relatively rapid time scale of m suggests that we may treat m as an instantaneous variable. The variable m in the ion current equation (2.5) of the Hodgkin–Huxley model can therefore be replaced by its steady-state value, m(t) → m0[u(t)]. This is what we call a quasi steadystate approximation which is possible because of the “separation of time scales” between fast and slow variables.\nSecond, we see from Fig. 2.3b that the time constants τn(u) and τh(u) have similar dynamics over the voltage u. Moreover, the graphs of n0(u) and 1 − h0(u) in Fig. 2.3a are also similar. This suggests that we may approximate the two variables n and (1 − h) by a single effective variable w. To keep the formalism slightly more general we use a linear approximation (b − h) ≈ a n with some constants a, b and set w = b − h = an. With h = b − w, n = w/a, and m = m0(u), Eqs. (2.4)–(2.5) become\n\nC\n\ndu dt\n\n=\n\n−gNa[m0(u)]3\n\n(b\n\n−\n\nw) (u\n\n−\n\nENa)\n\n−\n\ngK\n\nw a\n\n4\n(u − EK) − gL (u − EL) + I ,\n\n(4.3)\n\n86\n\nDimensionality reduction and phase plane analysis\n\n(a)\n\n(b)\n\nFig. 4.3 Phase plane u, w of a Hodgkin–Huxley model reduced to two dimensions. (a) The reduction of the Hodgkin–Huxley model leads to a system of two equations, τ du/dt = F(u, w) + RI and τw dw/dt = G(u, w). The ensemble of points with F(u, w) = 0 and G(u, w) = 0 is shown as a function of voltage u and recovery variable w, based on Eqs. (4.3), (4.20) and (4.21). (b) As in (a), but for the Morris–Lecar model (see text for details).\n\n(a)\n\n(b)\n\nFig. 4.4 Close-up of Fig. 4.3a and b. Solid lines: the sets of points with F(u, w) = 0 and G(u, w) = 0 are determined in the absence of stimulation (I = 0). Dashed line: in the presence of a constant current I = I0 > 0, the set of points with du/dt = 0 is given F(u, w) = −RI0. The curve G(u, w) = 0 characterizing the points with dw/dt = 0 starts (for u → −∞) nearly horizontally at w = 0 and does not change under stimulation.\n\nor\n\ndu 1\n\ndt = τ [F(u, w) + R I] ,\n\n(4.4)\n\nwith R = g−L 1, τ = RC and some function F. We now turn to the three equations (2.9). The m equation has disappeared since m is treated as instantaneous. Instead of the two\nequations (2.9) for n and h, we are left with a single effective equation\n\ndw 1\n\ndt = τw G(u, w) ,\n\n(4.5)\n\nwhere τw is a parameter and G a function that interpolates between dn/dt and dh/dt (see\n\n4.2 Reduction to two dimensions\n\n87\n\nSection 4.2.2). Equations (4.4) and (4.5) deﬁne a general two-dimensional neuron model. If we start with the Hodgkin–Huxley model and implement the above reduction steps we arrive at functions F(u, w) and G(u, w) which are illustrated in Figs. 4.3a and 4.4a. The mathematical details of the reduction of the four-dimensional Hodgkin–Huxley model to the two equations (4.4) and (4.5) are given below.\n\nBefore we go through the mathematical steps, we present two examples of two-dimensional neuron dynamics which are not directly derived from the Hodgkin–Huxley model, but are attractive because of their mathematical simplicity. We will return to these examples repeatedly throughout this chapter.\n\nExample: Morris–Lecar model\n\nMorris and Lecar (1981) proposed a simpliﬁed two-dimensional description of neuronal spike dynamics. A ﬁrst equation describes the evolution of the membrane potential u, the second equation the evolution of a slow “recovery” variable wˆ. The Morris–Lecar equations read\n\nC\n\ndu dt\n\n=\n\n−g1\n\nmˆ 0(u) (u\n\n−V1) −\n\ng2\n\nwˆ (u\n\n− V2)\n\n−\n\ngL\n\n(u\n\n− VL )\n\n+\n\nI\n\n,\n\ndwˆ dt\n\n=\n\n−\n\n1 τ (u)\n\n[wˆ\n\n− w0(u)]\n\n.\n\n(4.6) (4.7)\n\nIf we compare Eq. (4.6) with Eq. (4.3), we note that the ﬁrst current term on the righthand side of Eq. (4.3) has a factor (b − w) which closes the channel for high voltage and which is absent in (4.6). Another difference is that neither mˆ 0 nor wˆ in Eq. (4.6) have exponents. To clarify the relation between the two models, we could set mˆ 0(u) = [m0(u)]3 and wˆ = (w/a)4. In the following we consider Eqs. (4.6) and (4.7) as a model in its own right and drop the hats over m0 and w.\nThe equilibrium functions shown in Fig. 2.3a typically have a sigmoidal shape. It is\nreasonable to approximate the voltage dependence by\n\n1 m0(u) = 2\n\n1 + tanh\n\nu − u1 u2\n\n,\n\n(4.8)\n\n1 w0(u) = 2\n\n1 + tanh\n\nu − u3 u4\n\n,\n\n(4.9)\n\nwith parameters u1, . . . , u4, and to approximate the time constant by\n\nτ(u) =\n\nτw\n\ncosh\n\nu−u3 2u4\n\n(4.10)\n\nwith a further parameter τw. With the above assumptions, the zero-crossings of functions F(u, w) and G(u, w) of the Morris–Lecar model have the shape illustrated in Fig. 4.3b.\n\nThe Morris–Lecar model (4.6)–(4.10) gives a phenomenological description of action\n\n"}