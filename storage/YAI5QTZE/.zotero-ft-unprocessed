{"indexedChars":3381,"totalChars":3381,"version":"242","text":" \nSkip to main content \nCornell University \nWe gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate \narxiv logo > cs > arXiv:2210.03310 \n \nHelp | Advanced Search \nSearch \nComputer Science > Machine Learning \n(cs) \n[Submitted on 7 Oct 2022 ( v1 ), last revised 2 Mar 2023 (this version, v3)] \nTitle: Scaling Forward Gradient With Local Losses \nAuthors: Mengye Ren , Simon Kornblith , Renjie Liao , Geoffrey Hinton \nDownload a PDF of the paper titled Scaling Forward Gradient With Local Losses, by Mengye Ren and 3 other authors \nDownload PDF \n \n    Abstract: Forward gradient learning computes a noisy directional gradient and is a biologically plausible alternative to backprop for learning deep neural networks. However, the standard forward gradient algorithm, when applied naively, suffers from high variance when the number of parameters to be learned is large. In this paper, we propose a series of architectural and algorithmic modifications that together make forward gradient learning practical for standard deep learning benchmark tasks. We show that it is possible to substantially reduce the variance of the forward gradient estimator by applying perturbations to activations rather than weights. We further improve the scalability of forward gradient by introducing a large number of local greedy loss functions, each of which involves only a small number of learnable parameters, and a new MLPMixer-inspired architecture, LocalMixer, that is more suitable for local learning. Our approach matches backprop on MNIST and CIFAR-10 and significantly outperforms previously proposed backprop-free algorithms on ImageNet.  \n \nComments: \t31 pages, ICLR 2023 \nSubjects: \tMachine Learning (cs.LG) ; Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE) \nCite as: \tarXiv:2210.03310 [cs.LG] \n  \t(or arXiv:2210.03310v3 [cs.LG] for this version) \n  \thttps://doi.org/10.48550/arXiv.2210.03310 \nFocus to learn more \narXiv-issued DOI via DataCite \nSubmission history \nFrom: Mengye Ren [ view email ] \n[v1] Fri, 7 Oct 2022 03:52:27 UTC (1,894 KB) \n[v2] Fri, 17 Feb 2023 02:15:53 UTC (1,888 KB) \n[v3] Thu, 2 Mar 2023 03:08:10 UTC (1,896 KB) \nFull-text links: \nDownload: \n \n    Download a PDF of the paper titled Scaling Forward Gradient With Local Losses, by Mengye Ren and 3 other authors \n    PDF \n    Other formats  \n \nCurrent browse context: \ncs.LG \n< prev   |   next > \nnew | recent | 2210 \nChange to browse by: \ncs \ncs.CV \ncs.NE \nReferences & Citations \n \n    NASA ADS \n    Google Scholar \n    Semantic Scholar \n \na export BibTeX citation Loading... \nBookmark \nBibSonomy logo Reddit logo \nBibliographic Tools \nBibliographic and Citation Tools \nBibliographic Explorer Toggle \nBibliographic Explorer ( What is the Explorer? ) \nLitmaps Toggle \nLitmaps ( What is Litmaps? ) \nscite.ai Toggle \nscite Smart Citations ( What are Smart Citations? ) \nCode, Data, Media \nDemos \nRelated Papers \nAbout arXivLabs \nWhich authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) \n \n    About \n    Help \n \n    contact arXiv Click here to contact arXiv Contact \n    subscribe to arXiv mailings Click here to subscribe Subscribe \n \n    Copyright \n    Privacy Policy \n \n    Web Accessibility Assistance \n \n    arXiv Operational Status \n    Get status notifications via email or slack \n \n"}