PRL 97, 188104 (2006)

PHYSICAL REVIEW LETTERS

week ending 3 NOVEMBER 2006

Eigenvalue Spectra of Random Matrices for Neural Networks

Kanaka Rajan and L. F. Abbott
Center for Neurobiology and Behavior, Columbia University,, College of Physicians and Surgeons, New York, New York 10032, USA (Received 18 July 2006; published 2 November 2006)
The dynamics of neural networks is inï¬‚uenced strongly by the spectrum of eigenvalues of the matrix describing their synaptic connectivity. In large networks, elements of the synaptic connectivity matrix can be chosen randomly from appropriate distributions, making results from random matrix theory highly relevant. Unfortunately, classic results on the eigenvalue spectra of random matrices do not apply to synaptic connectivity matrices because of the constraint that individual neurons are either excitatory or inhibitory. Therefore, we compute eigenvalue spectra of large random matrices with excitatory and inhibitory columns drawn from distributions with different means and equal or different variances.

DOI: 10.1103/PhysRevLett.97.188104

PACS numbers: 87.18.Sn, 02.10.Yn, 05.90.+m, 87.19.La

Knowledge of the statistical properties of eigenvalues of large random matrices has proven valuable in a wide range of applications [1,2]. In neuroscience, networks of neurons are often studied using models in which interconnections are represented by a synaptic matrix with elements drawn randomly [3,4]. The distribution of eigenvalues of this matrix is useful for studying spontaneous activity and evoked responses in such models [3â€“7]. For example, the existence of spontaneous activity depends on whether the real parts of any of the eigenvalues are large enough to destabilize the silent state in a linear analysis, and the spectrum of eigenvalues with large real parts provides strong clues about the nature of the spontaneous activity in the full, nonlinear models. A classic result in random matrix theory is Girkoâ€™s circle law [8], which states that, for large N, the eigenvalues of an N  N asymmetric random matrix lie uniformly within the unit circle in the complex plane, if the elements are chosen from a distribution with zero mean and variance 1=N. When partial symmetry is included, the circle changes to an ellipse [9].
Unfortunately, these results do not apply to the synaptic matrices used in realistic neural network models. Neurons are either excitatory or inhibitory, which means that the synapses they make on their targets are all of one type or the other. Thus, the elements of the synaptic matrix must be drawn from two distributions with different means and perhaps different variances. Furthermore, each column of the matrix must have all excitatory (positive) or all inhibitory (negative) elements. Our goal is to determine the eigenvalue spectra of such matrices.
We construct a random synaptic matrix by choosing the elements of fN â€˜â€˜excitatoryâ€™â€™ columns from an excitatory distribution and the elements of the remaining Â…1 Ã¿ fÂ†N â€˜â€˜inhibitoryâ€™â€™ columns from an inhibitory distribution. Initially, we consider distributions for the excpitÂaÂÂtÂory and inhibitory columnspwÂÂiÂÂth different means (E= N > 0 for excitatory and I= N < 0 for inhibitory) but the same variance, 1=N. We primarily study a â€˜â€˜balancedâ€™â€™ situation [10,11] in which the average of the combined excitatory and inhibitory distributions is 0, i.e., fE Â‡ Â…1 Ã¿ fÂ†I Âˆ 0.

The result of numerically calculating the eigenvalues of such a matrix is shown in Fig. 1(a). Most of the eigenvalues lie within the unit circle, but there are a number of outliers. The location of these outlying eigenvalues varies from matrix to matrix, and their number does not appear to go to zero as N increases. This makes it difï¬cult to study them analytically. Interestingly, the circle shown in Fig. 1(a) that contains most of the eigenvalues has unit radius. If each element of the matrix were chosen to be either excitatory or inhibitory and then assigned a value from the appropriate distribution, the eigenvalues of the synaptic strength matrix woupldÂÂÂÂoÂÂÂbÂÂeÂÂyÂÂÂÂaÂÂÂÂcÂÂiÂÂrÂcÂÂÂlÂeÂÂÂÂlÂaÂÂwÂÂÂÂ,ÂÂbÂÂÂuÂ t the radius of the circle would be 1 Â‡ f2E Â‡ Â…1 Ã¿ fÂ†2I , rather than 1. Thus, the columnwise assignment of distributions has a dramatic effect on the distribution of eigenvalues.
It is possible to remove the outliers in Fig. 1(a) [see Fig. 1(b)] by imposing a constraint that we now derive. When the variances of the excitatory and inhibitory distributions are equal, we can write our N  N synaptic matrix as J Â‡ M, where the elements of J obey hJiji Âˆ 0 and hJi2ji Âˆ 1=N, with the angle brackets representing an average over the distribution from which these elements are

a.

Im(Î»)

b.

Im(Î»)

Re(Î»)

Re(Î»)

FIG. 1. Numerical results for the distribution of eigenvalues in the complex plane for N Âˆ 1000. (a) If excitatory and inhibitory elements are drawn from distributions with different means but the same variance, a few eigenvalues lie outside the unit circle. (b) When Eq. (2) is imposed, the eigenvalues lie inside the unit circle.

0031-9007= 06=97(18)=188104(4)

188104-1

Â© 2006 The American Physical Society

PRL 97, 188104 (2006)

PHYSICAL REVIEW LETTERS

week ending 3 NOVEMBER 2006

drawn. Thus, the eigenvalues of J lie uniformly inside the
unit circle in the complex plane [8].
M pisÂÂaÂÂ matrix in which every row is identical and is equal to 1= N times a vector m that has fN elements equal to E and Â…1 Ã¿ fÂ†N elements equal to I. The result of multiplying any vector v by M is

Mv Âˆ p1ÂÂÂÂ Â…m  vÂ†u N

where ui Âˆ 1

for i Âˆ 1; . . . ; N:

(1)

All of the eigenvalues of M are zero and, in particular, the vecptoÂÂÂrÂ u ips ÂaÂÂÂ right eigenvector of M with eigenvalue m  u= N Âˆ NÂ‰fE Â‡ Â…1 Ã¿ fÂ†IÂŠ Âˆ 0. Note that the peÂlÂeÂÂments of M are of the same order of magnitude (1= N) as the elements of J, so the presence of the outliers in Fig. 1(a) is not surprising.
To force the outliers inside the unit circle, we impose the condition

Ju Âˆ 0;

(2)

which implies that the strengths of the synapses to each neuron (corresponding to one row of J) independently sum to zero [12]. This corresponds to a balance condition not only on the average synaptic strength but also on the speciï¬c sum for each neuron [10,11].
Equation (2) implies that u is a right eigenvector of J. If we denote the ath left eigenvector of J by La, this means that La  u Âˆ 0 for all the other eigenvectors of J. Thus, according to Eq. (1), LaM Âˆ 0 for all these eigenvectors, so they are left eigenvectors of both J and J Â‡ M with the same eigenvalues. Furthermore, because the vector u is a right eigenvector of J with eigenvalue p0Â,ÂÂÂit is a right eigenvector of J Â‡ M with eigenvalue NÂ‰fE Â‡ Â…1 Ã¿ fÂ†IÂŠ Âˆ 0. Thus, the eigenvalues of J and J Â‡ M are identical, and shifting the means of the excitatory and inhibitory distributions away from zero in a balanced
manner has no effect on the eigenvalues, provided that
Eq. (2) is satisï¬ed. This is illustrated in Fig. 1(b), which shows the eigenvalue distribution computed numerically for a matrix constructed as in Fig. 1(a), but with the balance condition imposed row by row by subtracting the same constant from each element in the row.
Although the balance constraint of Eq. (2) has a signiï¬cant effect on the interaction between J and M, it does not change the distribution of eigenvalues [13] of J, for large N. This is because J acts as a random Â…N Ã¿ 1Â†  Â…N Ã¿ 1Â† matrix in the subspace spanned by the left eigenvectors of J orthogonal to u. It thus has N Ã¿ 1 eigenvectors distributed upnÂÂiÂÂfÂoÂÂÂrÂmÂÂÂÂlÂÂyÂÂÂÂwÂÂÂithin a circle in the complex plane of radius
Â…N Ã¿ 1Â†=N, which goes to 1 as N ! 1. The additional eigenvalue of J is the zero eigenvalue from Eq. (2).
In addition to having different means, the distributions for excitatory and inhibitory synapses are likely to have different variances. The inset in Fig. 2(a) shows that in this case, the eigenvalues lie inside a circle, but they are no

density, Ï

a.
0.25

f = 0.2

0.2

0.15

0.1

f = 0.5

0.05 f = 0.7

0

0 0.5 1

Ï‰ 1.5 2 2.5 | |

3

3.5

b.

0.3

Î± = 0.8

0.25

0.2

0.15

Î±= 0.3

0.1

0.05

Î± = 0.06

00

0.5 1

1.5

2

2.5 3 3.5

| Ï‰ |

FIG. 2. Density  of eigenvalues as a function of radius in the complex plane j!j, for N Âˆ 1000. The solid lines are the result
of the analytic calculation and symbols are numerical results.
(a) Results for different fractions f of excitatory and inhibitory elements, and  Âˆ 0:06. The inset shows eigenvalues in the complex plane computed numerically for f Âˆ 0:5 and  Âˆ 0:06. (b) Results for different excitatory variances 1=Â…N) and f Âˆ 0:5, with the inhibitory variance equal to 1=N.

longer distributed uniformly. We now consider this novel effect analytically.
According to the above results, the eigenvalues of J Â‡ M are identical to those of J provided Eq. (2) is satisï¬ed. Therefore, we can compute the eigenvalues of J rather than J Â‡ M (except that we will include different variances for different columns). In addition, imposing the constraint of Eq. (2) does not change the distribution of eigenvalues of J, for large N. These two observations imply that we can compute the eigenvalue spectrum we seek using distributions with different variances but 0 means, without having to impose Eq. (2).
We now proceed to calculate the density of eigenvalues  in the complex plane for such a matrix, averaged over the underlying probability distributions. Our calculation follows the approach used in Ref. [9], which begins by considering the quRantity GÂ…!Â† Âˆ TrÂ‰1=Â…!I Ã¿ JÂ†ÂŠ, which is also equal to d2=Â…! Ã¿ Â†, where I is the identity matrix, ! is a complex variable, and the integral in the second expression is over complex . As shown in Ref. [9], for example, the real and complex parts of GÂ…!Â† act like the x and y components of an electric ï¬eld in two dimensions produced by a charge density given by . As a result,  can be related to a potential  through Poissonâ€™s equation. Finally, the potential can be written in terms of a determinant of a square of the operator !I Ã¿ J which can, in turn, be expressed in terms of a Gaussian integral [9] (see below).
We consider cases in which the density and potential are functions of j!j2 so that

 Âˆ 1 Â…j!j200 Â‡ 0Â†;

(3)



where the primes denote derivatives with respect to j!j2. We construct a matrix with different variances for different columns by expressing its elements as Jijj with the elements of J drawn from a Gaussian distribution with

188104-2

PRL 97, 188104 (2006)

PHYSICAL REVIEW LETTERS

week ending 3 NOVEMBER 2006

zero mean and variance 1=N. This matrix satisï¬es hÂ…JijjÂ†2i Âˆ 2j =N. The potential we need is determined
by

expÂ…Ã¿NÂ† Âˆ Z YN dzi dzi expÂ…Ã¿NQ~Â†;

(4)

iÂˆ1 

with the average over the distribution of J values given by

expÂ…Ã¿NQ~Â†

Âˆ

Z

sÂÂÂÂÂÂ YN N i;jÂˆ1 2 dJij

expÂ…Ã¿NQÂ†:

(5)

In our case,

Q

Âˆ

Xj!j2 i 2i

Â‡

 i

zi zi N

Â‡

1 2

X JkiAijJkj
i;j;k

Ã¿

X BkjJkj;
k;j

Aij

Âˆ

zi zj N

Â‡

zj zi N

Â‡

ij;

Bkj

Âˆ

!zkzj kN

Â‡

!zj zk kN

(6)

and the term involving i is introduced to assure convergence of the integrals, with the understanding that all i ! 0 from the posiptivÂÂÂeÂ side at the end of the calculation.
To order 1= N, the matrix A has all but two of its
eigenvalues equal to one. The two exceptions are the eigenvectors zi and zi , both with eigenvalue 1 Â‡ r. This observation allows us to compute the Gaussian integral
over J in Eq. (5) by completing the square [shifting J by Jij ! Jij Ã¿ Bij=Â…1 Â‡ rÂ†] and calculating the determinant
of A. The result is

Q~

Âˆ

lnÂ…1

Â‡

rÂ†

Â‡

j!j2r~ 1Â‡r

Â‡

r;

where

r

Âˆ

1 N

X zi zi;
i

r~

Âˆ

1 N

X
i

zi zi 2i

and

r

Âˆ

1 N

X izi zi:
i

(7)

The potential  is then determined by computing the

integrals over z and z by saddle-point approximation [14].

In the case of excitatory and inhibitory neurons, there

are two variances. Without loss of generality, we choose

fN columns from a distribution with variance 2E Âˆ 1=Â…NÂ† and Â…1 Ã¿ fÂ†N columns from a distribution with

variance 2I Âˆ 1=N. In this case, we deï¬ne r1 as 1=Â…NÂ† times the sum of jzij2 over all i values corresponding to excitatory columns, and r2 as 1=N times the sum of jzij2 over all inhibitory i values. Then,



Âˆ

lnÂ…1rÂ‡1 Â†frÂ…1r2Â‡Â†1Ã¿r2f 

Â‡

j!j2Â…r1 Â‡ r2Â† 1 Â‡ r1 Â‡ r2

Â‡

1r1

Â‡

2r2;

where this expression is to be evaluated at values of r1 and r2 that minimize . The extra factors in the logarithm in  as compared to Q~ come from expressing the integrals over z and z in spherical coordinates.
The equations @=@r1 Âˆ @=@r2 Âˆ 0 that determine r1 and r2 are

Ã¿1

Âˆ

1 Â‡ j!j2 1 Â‡ r1 Â‡ r2

Ã¿

f r1

Ã¿

j!j2Â…r1 Â‡ r2Â† Â…1 Â‡ r1 Â‡ r2Â†2

and

Ã¿

2

Âˆ

1 Â‡ j!j2 1 Â‡ r1 Â‡ r2

Ã¿

1

Ã¿ r2

f

Ã¿

j!j2Â…r1 Â‡ r2Â† Â…1 Â‡ r1 Â‡ r2Â†2

:

(8)

Here the convergence factors have reduced to two, 1 and 2, and these equations must be solved in the limit where these both go to zero from the positive side.
Equations (8) have two solutions depending on whether j!j2 is greater than (outside solution) or less than (inside solution) a critical value 1 Ã¿ f Â‡ f=. It is easiest to express the solutions by writing r1 Âˆ qr2. Then, outside the circle

r2

Âˆ

Â…j!j2

Â…1 Ã¿ fÂ† Ã¿ 1Â† Â‡ Â…

Ã¿

1Â†f

and

q

Âˆ

f Â…1 Ã¿

fÂ†

;

(9)

which gives the potential



Âˆ

1

Â‡



f

ln Â…fÂ†fÂ…1 Ã¿

 fÂ†1Ã¿f

Â‡

lnÂ…j!j2Â†;

(10)

and a zero density,  Âˆ 0. Inside the circle,

r2 ! 1 and

q

Âˆ

Â…1

Ã¿

Â†j!j2 Â‡ 2f 2Â…1 Ã¿ fÂ†

Ã¿

1

(11)

pÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂ

Â‡ Â‰Â…1 Ã¿ Â†j!j2 Ã¿ 1ÂŠ2 Â‡ 4fÂ…1 Ã¿ Â†j!j2

2Â…1 Ã¿ fÂ†

and



Âˆ

1 ln

Â‡ qf

q

Â‡

j!j2Â…q Â‡ qÂ‡1

1Â†

:

(12)

The density of eigenvalues, , is obtained from this using Eq. (3) and

0

Âˆ

q0

 q

1 Â‡

1

Ã¿

f q

Â‡

j!j2 qÂ‡1

Ã¿

j!j2Â…q Â‡ Â…q Â‡ 1Â†2

1Â†

Â‡

q Â‡ 1 qÂ‡1

and

00

Âˆ

Â‡q002qq0Â‡1q1Â‡Ã¿1fqÃ¿Â‡Â…qqqÂ‡jÂ‡!Â‡1j12Â†12Ã¿ Â‡j!Â…Â…jq2q0Â…Â†Â‡2q1Â…Â‡Â†q2Ã¿Â‡1Â†11Â†2

Â‡

f q2

Ã¿

2j!j2 Â…q Â‡ 1Â†2

Â‡

2j!j2Â…q Â‡ Â…q Â‡ 1Â†3

1Â†

(13)

where, from Eq. (11),

188104-3

PRL 97, 188104 (2006)

PHYSICAL REVIEW LETTERS

week ending 3 NOVEMBER 2006

q0

Âˆ

1Ã¿ 2Â…1 Ã¿ fÂ†



 1

Â‡

pÂÂÂÂÂÂÂÂÂÂÂÂÂÂ…ÂÂ1ÂÂÂÃ¿ÂÂÂÂÂÂÂÂÂ†ÂÂjÂ!ÂÂÂÂjÂÂ2ÂÂÃ¿ÂÂÂÂÂ1ÂÂÂÂ‡ÂÂÂÂÂ2ÂÂÂfÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂ

Â‰Â…1 Ã¿ Â†j!j2 Ã¿ 1ÂŠ2 Â‡ 4fÂ…1 Ã¿ Â†j!j2

and q00 Âˆ

pÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂ…ÂÂ1ÂÂÂÃ¿ÂÂÂÂÂÂÂÂÂÂ†Â2ÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂ

2Â…1 Ã¿ fÂ† Â‰Â…1 Ã¿ Â†j!j2 Ã¿ 1ÂŠ2 Â‡ 4fÂ…1 Ã¿ Â†j!j2



 1

Ã¿

Â‰Â…1

Ã¿

Â‰Â…1 Ã¿ Â†j!j2 Â†j!j2 Ã¿ 1ÂŠ2

Ã¿ Â‡

1 Â‡ 2fÂŠ2



4fÂ…1 Ã¿ Â†j!j2 :

(14)

Figure 2 compares the analytic expression we have computed for the density of eigenvalues with numerical results for N Âˆ 1000. The numerical densities were calculated by counting the number of eigenvalues in successive concentric ringsp. AÂÂÂlÂÂlÂÂoÂÂÂfÂÂÂtÂhÂÂÂeÂÂÂeÂÂiÂgÂÂÂeÂ nvalues are located inside a circle of radius 1 Ã¿ f Â‡ f= because  Âˆ 0 for the outside solution. Note that, since we are considering the case N2E Âˆ 1= and N2I Âˆ 1, the square of this radius can also be written as NÂ‰f2E Â‡ Â…1 Ã¿ fÂ†2I ÂŠ. Thus, the square of the radius of the circle containing the eigenvalues is N
times the average of the variances of the excitatory and
inhibitory distributions. The distributions are characterized by a high-density central repgÂÂiÂoÂ n with a radius given approximately by minÂ…E; IÂ† N, which is equal to 1 in the cases shown in Fig. 2.
Figure 2(a) shows results with  Âˆ 0:06 for different values of f. The value of f determines how many eigenvalues fall into the high-density central region, with higher central densities for smaller f values. There are two limits in which our results reduce to the usual circle law, f ! 0 and f ! 1. When f Âˆ 0, all the elements of the matrix come from a single distribution with variance 1=N, and the eigenvalues all fall uniformly inside the inner circle of radius 1. When f Âˆ 1, all the elements come from a distribution with variance 1=Â…NÂ†, andpaÂÂlÂÂlÂÂtÂÂhe eigenvalues lie uniformly inside a circle of radius 1=.
Figure 2(b) shows results with f Âˆ 0:5 for different values of . We only consider   1 because  > 1 can be reduced to this case by rescaling the matrix and changing f ! 1 Ã¿ f. When  Âˆ 1, both of the distributions have the same variance, 2E Âˆ 2I Âˆ 1=N, and the density is uniform inside the unit circle.
Our results were obtained using Gaussian distributions, but they apply more generally. For example, they apply even though synaptic strength distributions are nonGaussian [15] and include a zero-strength  function due to the sparseness of neuronal connectivity. Neurons do not normally form synaptic connections with themselves, but we did not eliminate diagonal terms in the synaptic matrix for our calculations because their effect is negligible for large N. Finally, if Eq. (2) is imposed but the balance

condition on the means is not satisï¬ed, the eigenvalues will be identical to what we have comppÂÂÂuÂ ted except that the eigenvalue at zero will be shifted to NÂ‰fE Â‡ Â…1 Ã¿ fÂ†IÂŠ.
The eigenvalue distributions we have obtained have several implications for neural network dynamics. First and most surprisingly, modifying the mean strengths of excitatory and inhibitory synapses has no effect on stability or small-ï¬‚uctuation dynamics under balanced conditions. Instead, the key elements in determining the spontaneous dynamics of networks constructed in this way are the widths of the distributions of excitatory and inhibitory synaptic strengths. Furthermore, if these widths are different, fewer eigenvalues will appear at the edge of the eigenvalue circle, meaning that there will be fewer slowly oscillating and long-lasting modes. In summary, the calculations we present suggest that having different cell types with different distributions of synaptic strengths has a large impact on network dynamics, and that the critical element to measure, and the critical element that may be modiï¬ed by the modulatory and plasticity mechanisms that control neural circuit dynamics, are the variances of the synaptic strength distributions.
We thank Bill Bialek, Michael Eisele, Edward Farhi, Vikram Rajan, Evan Schaffer, and Haim Sompolinksy for helpful comments and suggestions. Research supported by the Swartz Foundation and NIH (5-DP1-OD114-02).

[1] M. L. Mehta, Random Matrices and the Statistical Theory

of Energy Levels (Academic, New York, 1967).

[2] T. A. Brody, J. Flores, J. B. French, P. A. Mello, A. Pandey,

and S. S. M. Wong, Rev. Mod. Phys. 53, 385 (1981).

[3] H. Sompolinsky, A. Crisanti, and H. J. Sommers, Phys.

Rev. Lett. 61, 259 (1988).

[4] B. Cessac and J. A. Sepulchre, Chaos 16, 013104 (2006).

[5] H. R. Wilson and J. D. Cowan, Biophys. J. 12, 1, (1972).

[6] O. Shriki, D. Hansel, and H. Sompolinsky, Neural

Comput. 15, 1809 (2003).

[7] Additional references in: T. P. Vogels, K. Rajan, and L. F.

Abbott, Annu. Rev. Neurosci. 28, 357 (2005).

[8] V. L. Girko, Theory Probab. Appl. 29, 694 (1984).

[9] H. J. Sommers, A. Crisanti, H. Sompolinsky, and Y. Stein,

Phys. Rev. Lett. 60, 1895 (1988).

[10] M. N. Shadlen and W. T. Newsome, Curr. Opin. Neurobiol.

4, 569 (1994).

[11] T. W. Troyer and K. D. Miller, Neural Comput. 9, 971

(1997).

pÂÂÂÂ

[12] It is actually sufï¬cient to make Ju of order 1= N.

[13] We thank Michael Eisele for assistance with this point.

[14] The quadratic integral over ï¬‚uctuations about the saddle-

point is ignored because it produces a term of OÂ…1=NÂ†. Note that the saddle-node value of Q~ we obtain is OÂ…1Â† as

required.

[15] S. Song S., P. J. SjoÂ¨stroÂ¨m, M. Reigl, S. B. Nelson, and

D. B. Chklovskii, PLoS Biol. 3, 505, (2005).

188104-4

