PRL 97, 188104 (2006)

PHYSICAL REVIEW LETTERS

week ending 3 NOVEMBER 2006

Eigenvalue Spectra of Random Matrices for Neural Networks

Kanaka Rajan and L. F. Abbott
Center for Neurobiology and Behavior, Columbia University,, College of Physicians and Surgeons, New York, New York 10032, USA (Received 18 July 2006; published 2 November 2006)
The dynamics of neural networks is inﬂuenced strongly by the spectrum of eigenvalues of the matrix describing their synaptic connectivity. In large networks, elements of the synaptic connectivity matrix can be chosen randomly from appropriate distributions, making results from random matrix theory highly relevant. Unfortunately, classic results on the eigenvalue spectra of random matrices do not apply to synaptic connectivity matrices because of the constraint that individual neurons are either excitatory or inhibitory. Therefore, we compute eigenvalue spectra of large random matrices with excitatory and inhibitory columns drawn from distributions with different means and equal or different variances.

DOI: 10.1103/PhysRevLett.97.188104

PACS numbers: 87.18.Sn, 02.10.Yn, 05.90.+m, 87.19.La

Knowledge of the statistical properties of eigenvalues of large random matrices has proven valuable in a wide range of applications [1,2]. In neuroscience, networks of neurons are often studied using models in which interconnections are represented by a synaptic matrix with elements drawn randomly [3,4]. The distribution of eigenvalues of this matrix is useful for studying spontaneous activity and evoked responses in such models [3–7]. For example, the existence of spontaneous activity depends on whether the real parts of any of the eigenvalues are large enough to destabilize the silent state in a linear analysis, and the spectrum of eigenvalues with large real parts provides strong clues about the nature of the spontaneous activity in the full, nonlinear models. A classic result in random matrix theory is Girko’s circle law [8], which states that, for large N, the eigenvalues of an N  N asymmetric random matrix lie uniformly within the unit circle in the complex plane, if the elements are chosen from a distribution with zero mean and variance 1=N. When partial symmetry is included, the circle changes to an ellipse [9].
Unfortunately, these results do not apply to the synaptic matrices used in realistic neural network models. Neurons are either excitatory or inhibitory, which means that the synapses they make on their targets are all of one type or the other. Thus, the elements of the synaptic matrix must be drawn from two distributions with different means and perhaps different variances. Furthermore, each column of the matrix must have all excitatory (positive) or all inhibitory (negative) elements. Our goal is to determine the eigenvalue spectra of such matrices.
We construct a random synaptic matrix by choosing the elements of fN ‘‘excitatory’’ columns from an excitatory distribution and the elements of the remaining 1 ÿ fN ‘‘inhibitory’’ columns from an inhibitory distribution. Initially, we consider distributions for the excpitatory and inhibitory columnspwith different means (E= N > 0 for excitatory and I= N < 0 for inhibitory) but the same variance, 1=N. We primarily study a ‘‘balanced’’ situation [10,11] in which the average of the combined excitatory and inhibitory distributions is 0, i.e., fE  1 ÿ fI  0.

The result of numerically calculating the eigenvalues of such a matrix is shown in Fig. 1(a). Most of the eigenvalues lie within the unit circle, but there are a number of outliers. The location of these outlying eigenvalues varies from matrix to matrix, and their number does not appear to go to zero as N increases. This makes it difﬁcult to study them analytically. Interestingly, the circle shown in Fig. 1(a) that contains most of the eigenvalues has unit radius. If each element of the matrix were chosen to be either excitatory or inhibitory and then assigned a value from the appropriate distribution, the eigenvalues of the synaptic strength matrix woupldobeyacirclelaw,bu t the radius of the circle would be 1  f2E  1 ÿ f2I , rather than 1. Thus, the columnwise assignment of distributions has a dramatic effect on the distribution of eigenvalues.
It is possible to remove the outliers in Fig. 1(a) [see Fig. 1(b)] by imposing a constraint that we now derive. When the variances of the excitatory and inhibitory distributions are equal, we can write our N  N synaptic matrix as J  M, where the elements of J obey hJiji  0 and hJi2ji  1=N, with the angle brackets representing an average over the distribution from which these elements are

a.

Im(λ)

b.

Im(λ)

Re(λ)

Re(λ)

FIG. 1. Numerical results for the distribution of eigenvalues in the complex plane for N  1000. (a) If excitatory and inhibitory elements are drawn from distributions with different means but the same variance, a few eigenvalues lie outside the unit circle. (b) When Eq. (2) is imposed, the eigenvalues lie inside the unit circle.

0031-9007= 06=97(18)=188104(4)

188104-1

© 2006 The American Physical Society

PRL 97, 188104 (2006)

PHYSICAL REVIEW LETTERS

week ending 3 NOVEMBER 2006

drawn. Thus, the eigenvalues of J lie uniformly inside the
unit circle in the complex plane [8].
M pisa matrix in which every row is identical and is equal to 1= N times a vector m that has fN elements equal to E and 1 ÿ fN elements equal to I. The result of multiplying any vector v by M is

Mv  p1 m  vu N

where ui  1

for i  1; . . . ; N:

(1)

All of the eigenvalues of M are zero and, in particular, the vecptor u ips a right eigenvector of M with eigenvalue m  u= N  NfE  1 ÿ fI  0. Note that the pelements of M are of the same order of magnitude (1= N) as the elements of J, so the presence of the outliers in Fig. 1(a) is not surprising.
To force the outliers inside the unit circle, we impose the condition

Ju  0;

(2)

which implies that the strengths of the synapses to each neuron (corresponding to one row of J) independently sum to zero [12]. This corresponds to a balance condition not only on the average synaptic strength but also on the speciﬁc sum for each neuron [10,11].
Equation (2) implies that u is a right eigenvector of J. If we denote the ath left eigenvector of J by La, this means that La  u  0 for all the other eigenvectors of J. Thus, according to Eq. (1), LaM  0 for all these eigenvectors, so they are left eigenvectors of both J and J  M with the same eigenvalues. Furthermore, because the vector u is a right eigenvector of J with eigenvalue p0,it is a right eigenvector of J  M with eigenvalue NfE  1 ÿ fI  0. Thus, the eigenvalues of J and J  M are identical, and shifting the means of the excitatory and inhibitory distributions away from zero in a balanced
manner has no effect on the eigenvalues, provided that
Eq. (2) is satisﬁed. This is illustrated in Fig. 1(b), which shows the eigenvalue distribution computed numerically for a matrix constructed as in Fig. 1(a), but with the balance condition imposed row by row by subtracting the same constant from each element in the row.
Although the balance constraint of Eq. (2) has a signiﬁcant effect on the interaction between J and M, it does not change the distribution of eigenvalues [13] of J, for large N. This is because J acts as a random N ÿ 1  N ÿ 1 matrix in the subspace spanned by the left eigenvectors of J orthogonal to u. It thus has N ÿ 1 eigenvectors distributed upniformlywithin a circle in the complex plane of radius
N ÿ 1=N, which goes to 1 as N ! 1. The additional eigenvalue of J is the zero eigenvalue from Eq. (2).
In addition to having different means, the distributions for excitatory and inhibitory synapses are likely to have different variances. The inset in Fig. 2(a) shows that in this case, the eigenvalues lie inside a circle, but they are no

density, ρ

a.
0.25

f = 0.2

0.2

0.15

0.1

f = 0.5

0.05 f = 0.7

0

0 0.5 1

ω 1.5 2 2.5 | |

3

3.5

b.

0.3

α = 0.8

0.25

0.2

0.15

α= 0.3

0.1

0.05

α = 0.06

00

0.5 1

1.5

2

2.5 3 3.5

| ω |

FIG. 2. Density  of eigenvalues as a function of radius in the complex plane j!j, for N  1000. The solid lines are the result
of the analytic calculation and symbols are numerical results.
(a) Results for different fractions f of excitatory and inhibitory elements, and   0:06. The inset shows eigenvalues in the complex plane computed numerically for f  0:5 and   0:06. (b) Results for different excitatory variances 1=N) and f  0:5, with the inhibitory variance equal to 1=N.

longer distributed uniformly. We now consider this novel effect analytically.
According to the above results, the eigenvalues of J  M are identical to those of J provided Eq. (2) is satisﬁed. Therefore, we can compute the eigenvalues of J rather than J  M (except that we will include different variances for different columns). In addition, imposing the constraint of Eq. (2) does not change the distribution of eigenvalues of J, for large N. These two observations imply that we can compute the eigenvalue spectrum we seek using distributions with different variances but 0 means, without having to impose Eq. (2).
We now proceed to calculate the density of eigenvalues  in the complex plane for such a matrix, averaged over the underlying probability distributions. Our calculation follows the approach used in Ref. [9], which begins by considering the quRantity G!  Tr1=!I ÿ J, which is also equal to d2=! ÿ , where I is the identity matrix, ! is a complex variable, and the integral in the second expression is over complex . As shown in Ref. [9], for example, the real and complex parts of G! act like the x and y components of an electric ﬁeld in two dimensions produced by a charge density given by . As a result,  can be related to a potential  through Poisson’s equation. Finally, the potential can be written in terms of a determinant of a square of the operator !I ÿ J which can, in turn, be expressed in terms of a Gaussian integral [9] (see below).
We consider cases in which the density and potential are functions of j!j2 so that

  1 j!j200  0;

(3)



where the primes denote derivatives with respect to j!j2. We construct a matrix with different variances for different columns by expressing its elements as Jijj with the elements of J drawn from a Gaussian distribution with

188104-2

PRL 97, 188104 (2006)

PHYSICAL REVIEW LETTERS

week ending 3 NOVEMBER 2006

zero mean and variance 1=N. This matrix satisﬁes hJijj2i  2j =N. The potential we need is determined
by

expÿN  Z YN dzi dzi expÿNQ~;

(4)

i1 

with the average over the distribution of J values given by

expÿNQ~



Z

s YN N i;j1 2 dJij

expÿNQ:

(5)

In our case,

Q



Xj!j2 i 2i



 i

zi zi N



1 2

X JkiAijJkj
i;j;k

ÿ

X BkjJkj;
k;j

Aij



zi zj N



zj zi N



ij;

Bkj



!zkzj kN



!zj zk kN

(6)

and the term involving i is introduced to assure convergence of the integrals, with the understanding that all i ! 0 from the posiptive side at the end of the calculation.
To order 1= N, the matrix A has all but two of its
eigenvalues equal to one. The two exceptions are the eigenvectors zi and zi , both with eigenvalue 1  r. This observation allows us to compute the Gaussian integral
over J in Eq. (5) by completing the square [shifting J by Jij ! Jij ÿ Bij=1  r] and calculating the determinant
of A. The result is

Q~



ln1



r



j!j2r~ 1r



r;

where

r



1 N

X zi zi;
i

r~



1 N

X
i

zi zi 2i

and

r



1 N

X izi zi:
i

(7)

The potential  is then determined by computing the

integrals over z and z by saddle-point approximation [14].

In the case of excitatory and inhibitory neurons, there

are two variances. Without loss of generality, we choose

fN columns from a distribution with variance 2E  1=N and 1 ÿ fN columns from a distribution with

variance 2I  1=N. In this case, we deﬁne r1 as 1=N times the sum of jzij2 over all i values corresponding to excitatory columns, and r2 as 1=N times the sum of jzij2 over all inhibitory i values. Then,





ln1r1 fr1r21ÿr2f 



j!j2r1  r2 1  r1  r2



1r1



2r2;

where this expression is to be evaluated at values of r1 and r2 that minimize . The extra factors in the logarithm in  as compared to Q~ come from expressing the integrals over z and z in spherical coordinates.
The equations @=@r1  @=@r2  0 that determine r1 and r2 are

ÿ1



1  j!j2 1  r1  r2

ÿ

f r1

ÿ

j!j2r1  r2 1  r1  r22

and

ÿ

2



1  j!j2 1  r1  r2

ÿ

1

ÿ r2

f

ÿ

j!j2r1  r2 1  r1  r22

:

(8)

Here the convergence factors have reduced to two, 1 and 2, and these equations must be solved in the limit where these both go to zero from the positive side.
Equations (8) have two solutions depending on whether j!j2 is greater than (outside solution) or less than (inside solution) a critical value 1 ÿ f  f=. It is easiest to express the solutions by writing r1  qr2. Then, outside the circle

r2



j!j2

1 ÿ f ÿ 1  

ÿ

1f

and

q



f 1 ÿ

f

;

(9)

which gives the potential





1





f

ln ff1 ÿ

 f1ÿf



lnj!j2;

(10)

and a zero density,   0. Inside the circle,

r2 ! 1 and

q



1

ÿ

j!j2  2f 21 ÿ f

ÿ

1

(11)

p

 1 ÿ j!j2 ÿ 12  4f1 ÿ j!j2

21 ÿ f

and





1 ln

 qf

q



j!j2q  q1

1

:

(12)

The density of eigenvalues, , is obtained from this using Eq. (3) and

0



q0

 q

1 

1

ÿ

f q



j!j2 q1

ÿ

j!j2q  q  12

1



q  1 q1

and

00



q002qq01q1ÿ1fqÿqqqj!1j1212ÿ j!jq2q02q1q2ÿ1112



f q2

ÿ

2j!j2 q  12



2j!j2q  q  13

1

(13)

where, from Eq. (11),

188104-3

PRL 97, 188104 (2006)

PHYSICAL REVIEW LETTERS

week ending 3 NOVEMBER 2006

q0



1ÿ 21 ÿ f



 1



p1ÿj!j2ÿ12f

1 ÿ j!j2 ÿ 12  4f1 ÿ j!j2

and q00 

p1ÿ2

21 ÿ f 1 ÿ j!j2 ÿ 12  4f1 ÿ j!j2



 1

ÿ

1

ÿ

1 ÿ j!j2 j!j2 ÿ 12

ÿ 

1  2f2



4f1 ÿ j!j2 :

(14)

Figure 2 compares the analytic expression we have computed for the density of eigenvalues with numerical results for N  1000. The numerical densities were calculated by counting the number of eigenvalues in successive concentric ringsp. Alloftheeige nvalues are located inside a circle of radius 1 ÿ f  f= because   0 for the outside solution. Note that, since we are considering the case N2E  1= and N2I  1, the square of this radius can also be written as Nf2E  1 ÿ f2I . Thus, the square of the radius of the circle containing the eigenvalues is N
times the average of the variances of the excitatory and
inhibitory distributions. The distributions are characterized by a high-density central repgio n with a radius given approximately by minE; I N, which is equal to 1 in the cases shown in Fig. 2.
Figure 2(a) shows results with   0:06 for different values of f. The value of f determines how many eigenvalues fall into the high-density central region, with higher central densities for smaller f values. There are two limits in which our results reduce to the usual circle law, f ! 0 and f ! 1. When f  0, all the elements of the matrix come from a single distribution with variance 1=N, and the eigenvalues all fall uniformly inside the inner circle of radius 1. When f  1, all the elements come from a distribution with variance 1=N, andpallthe eigenvalues lie uniformly inside a circle of radius 1=.
Figure 2(b) shows results with f  0:5 for different values of . We only consider   1 because  > 1 can be reduced to this case by rescaling the matrix and changing f ! 1 ÿ f. When   1, both of the distributions have the same variance, 2E  2I  1=N, and the density is uniform inside the unit circle.
Our results were obtained using Gaussian distributions, but they apply more generally. For example, they apply even though synaptic strength distributions are nonGaussian [15] and include a zero-strength  function due to the sparseness of neuronal connectivity. Neurons do not normally form synaptic connections with themselves, but we did not eliminate diagonal terms in the synaptic matrix for our calculations because their effect is negligible for large N. Finally, if Eq. (2) is imposed but the balance

condition on the means is not satisﬁed, the eigenvalues will be identical to what we have comppu ted except that the eigenvalue at zero will be shifted to NfE  1 ÿ fI.
The eigenvalue distributions we have obtained have several implications for neural network dynamics. First and most surprisingly, modifying the mean strengths of excitatory and inhibitory synapses has no effect on stability or small-ﬂuctuation dynamics under balanced conditions. Instead, the key elements in determining the spontaneous dynamics of networks constructed in this way are the widths of the distributions of excitatory and inhibitory synaptic strengths. Furthermore, if these widths are different, fewer eigenvalues will appear at the edge of the eigenvalue circle, meaning that there will be fewer slowly oscillating and long-lasting modes. In summary, the calculations we present suggest that having different cell types with different distributions of synaptic strengths has a large impact on network dynamics, and that the critical element to measure, and the critical element that may be modiﬁed by the modulatory and plasticity mechanisms that control neural circuit dynamics, are the variances of the synaptic strength distributions.
We thank Bill Bialek, Michael Eisele, Edward Farhi, Vikram Rajan, Evan Schaffer, and Haim Sompolinksy for helpful comments and suggestions. Research supported by the Swartz Foundation and NIH (5-DP1-OD114-02).

[1] M. L. Mehta, Random Matrices and the Statistical Theory

of Energy Levels (Academic, New York, 1967).

[2] T. A. Brody, J. Flores, J. B. French, P. A. Mello, A. Pandey,

and S. S. M. Wong, Rev. Mod. Phys. 53, 385 (1981).

[3] H. Sompolinsky, A. Crisanti, and H. J. Sommers, Phys.

Rev. Lett. 61, 259 (1988).

[4] B. Cessac and J. A. Sepulchre, Chaos 16, 013104 (2006).

[5] H. R. Wilson and J. D. Cowan, Biophys. J. 12, 1, (1972).

[6] O. Shriki, D. Hansel, and H. Sompolinsky, Neural

Comput. 15, 1809 (2003).

[7] Additional references in: T. P. Vogels, K. Rajan, and L. F.

Abbott, Annu. Rev. Neurosci. 28, 357 (2005).

[8] V. L. Girko, Theory Probab. Appl. 29, 694 (1984).

[9] H. J. Sommers, A. Crisanti, H. Sompolinsky, and Y. Stein,

Phys. Rev. Lett. 60, 1895 (1988).

[10] M. N. Shadlen and W. T. Newsome, Curr. Opin. Neurobiol.

4, 569 (1994).

[11] T. W. Troyer and K. D. Miller, Neural Comput. 9, 971

(1997).

p

[12] It is actually sufﬁcient to make Ju of order 1= N.

[13] We thank Michael Eisele for assistance with this point.

[14] The quadratic integral over ﬂuctuations about the saddle-

point is ignored because it produces a term of O1=N. Note that the saddle-node value of Q~ we obtain is O1 as

required.

[15] S. Song S., P. J. Sjo¨stro¨m, M. Reigl, S. B. Nelson, and

D. B. Chklovskii, PLoS Biol. 3, 505, (2005).

188104-4

