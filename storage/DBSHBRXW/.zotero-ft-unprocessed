{"indexedPages":100,"totalPages":302,"version":"261","text":"Haiping Huang\nStatistical Mechanics of Neural Networks\n\nStatistical Mechanics of Neural Networks\n\nHaiping Huang\nStatistical Mechanics of Neural Networks\n\nHaiping Huang Sun Yat-sen University Guangzhou, China\n\nISBN 978-981-16-7569-0\n\nISBN 978-981-16-7570-6 (eBook)\n\nhttps://doi.org/10.1007/978-981-16-7570-6\n\nJointly published with Higher Education Press The print edition is not for sale in China (Mainland). Customers from China (Mainland) please order the print book from: Higher Education Press.\n\n© Higher Education Press 2021 This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether the whole or part of the material is concerned, speciﬁcally the rights of reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publishers, the authors, and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publishers nor the authors or the editors give a warranty, express or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publishers remain neutral with regard to jurisdictional claims in published maps and institutional afﬁliations.\n\nThis Springer imprint is published by the registered company Springer Nature Singapore Pte Ltd. The registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721, Singapore\n\nTo my wife, Huihong Pan, and to my children, Qing Huang and Yun Huang\n\nPreface\nNeural networks have become a powerful tool in various domains of scientiﬁc research and industrial applications. However, the inner workings of this tool remain unknown, which prohibits us from a deep understanding and further principled design of more powerful network architectures and optimization algorithms. To crack the black box, different disciplines including physics, statistics, information theory, nonconvex optimization and so on must be integrated, which may also bridge the gap between the artiﬁcial neural networks and the brain. However, in this highly interdisciplinary ﬁeld, there are few monographs providing a systematic introduction of theoretical physics basics for understanding neural networks, especially covering recent cutting-edge topics of neural networks.\nIn this book, we provide a physics perspective on the theory of neural networks, and even neural computation in models of the brain. The book covers the basics of statistical mechanics, statistical inference, neural networks, and especially classic and recent mean-ﬁeld analysis of neural networks of different nature. These mathematically beautiful examples of statistical mechanics analysis of neural networks are expected to inspire further techniques to provide an analytic theory for more complex networks. Future important directions along the line of scientiﬁc machine learning and theoretical models of brain computation are also reviewed.\nWe remark that this book is not a complete review of both ﬁelds of artiﬁcial neural networks and mean-ﬁeld theory of neural networks, instead, a biased-viewpoint of statistical physics methods toward understanding the black box of deep learning, especially for beginner-level students and researchers who get interested in the meanﬁeld theory of learning in neural networks.\nThis book stemmed from a series of lectures about the interplay between statistical mechanics and neural networks. These lectures were given by the author in his PMI (physics, machine and intelligence) group during the years from 2018 to 2020. The book is organized into two parts—basics of statistical mechanics related to the theory of neural networks, and theoretical studies of neural networks including cortical models.\nThe ﬁrst part is further divided into nine chapters. Chapter 1 gives a brief history of neural network studies. Chapter 2 introduces multi-spin interaction models and\nvii\n\nviii\n\nPreface\n\nthe cavity method to compute the partition function of disordered systems. Chapter 3 introduces the variational mean-ﬁeld methods including the Bethe approximation and belief propagation algorithms. Chapter 4 introduces the Monte Carlo simulation methods that are used to acquire low-energy conﬁgurations of a statistical mechanical system. Chapter 5 introduces high-temperature expansion techniques. Chapter 6 introduces the spin glass model where the Nishimori line was discovered. Chapter 7 introduces the random energy model which is an inﬁnite-body interaction limit of multi-spin disordered systems. Chapter 8 introduces a statistical mechanical theory of the Hopﬁeld model that was designed for associative memory of random patterns based on the Hebbian local learning rule. Chapter 9 introduces the concepts of replica symmetry and replica symmetry breaking in the spin glass theory of disordered systems.\nThe second part is divided into nine chapters. Chapter 10 introduces the Boltzmann machine learning (also called the inverse Ising problem in physics or maximum entropy method in statistics) and the statistical mechanics of the restricted Boltzmann machine learning. In this chapter, a variational mean-ﬁeld theory for learning a generic RBM of discrete synapses is also introduced in depth. Chapter 11 introduces the simplest model of unsupervised learning. Chapter 12 introduces the nature of unsupervised learning with RBM (only two hidden neurons are considered), i.e., the unsupervised learning process can be understood in terms of a series of continuous phase transitions, including both weight-reversal symmetry breaking and hidden-neuron-permutation symmetry breaking. Chapter 13 introduces a single-layer discrete perceptron and its mean-ﬁeld theory. Chapter 14 introduces the mean-ﬁeld model of multi-layered perceptron and its analysis via the cavity method. In this chapter, a mean-ﬁeld training algorithm of multi-layered perceptron with discrete synapses is introduced, together with mean-ﬁeld training from an ensemble perspective. Chapter 15 introduces the mean-ﬁeld theory of dimension reduction in deep random neural networks. Chapter 16 introduces the chaos theory of random recurrent neural networks. In this chapter, the excitatory-inhibitory balance theory of cortical circuits is also introduced, together with the backpropagation through time for training a generic RNN. Chapter 17 introduces how the statistical mechanics technique can be applied to compute the asymptotic behavior of the spectral density for the Hermitian and the non-Hermitian random matrices. Finally, perspectives on a statistical mechanical theory toward deep learning and even other interesting aspects of intelligence are provided, hopefully inspiring future developments of the interdisciplinary ﬁelds across physics, machine learning and theoretical neuroscience and other involved disciplines.\nI am grateful for the students’ efforts in drafting the lecture notes, including preparing ﬁgures. Here, I list their contributions to associated chapters. These students in my PMI group are Zhenye Huang (Chaps. 4 and 10), Zijian Jiang (Chaps. 2, 13 and 16), Chan Li (Chaps. 11, 15 and 16), Jianwen Zhou (Chaps. 5, 8 and 17), Wenxuan Zou (Chaps. 3, 6 and 14) and Tianqi Hou (Chap. 12). I also thank the other PMI members, Ziming Chen, Yiming Jiao, Junbin Qiu, Mingshan Xie, Xianbo Xu and Yang Zhao for their reading feedbacks on the draft. I also would like to thank Haijun Zhou, K. Y. Michael Wong, Yoshiyuki Kabashima and Taro Toyoizumi\n\nPreface\n\nix\n\nfor their encouragements and supports during my Ph.D. and Post-doctoral research career. I ﬁnally acknowledge the ﬁnancial support from the National Natural Science Foundation of China (Grant No. 11805284 Grant No. 12122515).\n\nGuangzhou, China December 2021\n\nHaiping Huang\n\nContents\n1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2 Spin Glass Models and Cavity Method . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.1 Multi-spin Interaction Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2 Cavity Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3 From Cavity Method to Message Passing Algorithms . . . . . . . . . . 12 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3 Variational Mean-Field Theory and Belief Propagation . . . . . . . . . . . 17 3.1 Variational Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3.2 Variational Free Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.2.1 Mean-Field Approximation . . . . . . . . . . . . . . . . . . . . . . . . 20 3.2.2 Bethe Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 3.2.3 From the Bethe to Naive Mean-Field Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 3.3 Mean-Field Inverse Ising Problem . . . . . . . . . . . . . . . . . . . . . . . . . . 29 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n4 Monte Carlo Simulation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 4.1 Monte Carlo Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 4.2 Importance Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 4.3 Markov Chain Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 4.4 Monte Carlo Simulations in Statistical Physics . . . . . . . . . . . . . . . 36 4.4.1 Metropolis Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 4.4.2 Parallel Tempering Monte Carlo . . . . . . . . . . . . . . . . . . . . 39 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n5 High-Temperature Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 5.1 Statistical Physics Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 5.2 High-Temperature Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 5.3 Properties of the TAP Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nxi\n\nxii\n\nContents\n\n6 Nishimori Line . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 6.1 Model Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 6.2 Exact Result for Internal Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 6.3 Proof of No RSB Effects on the Nishimori Line . . . . . . . . . . . . . . . 57 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n7 Random Energy Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 7.1 Model Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 7.2 Phase Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n8 Statistical Mechanical Theory of Hopﬁeld Model . . . . . . . . . . . . . . . . . 63 8.1 Hopﬁeld Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 8.2 Replica Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 8.2.1 Replica-Symmetric Ansätz . . . . . . . . . . . . . . . . . . . . . . . . . 73 8.2.2 Zero-Temperature Limit . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 8.3 Phase Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 8.4 Hopﬁeld Model with Arbitrary Hebbian Length . . . . . . . . . . . . . . . 81 8.4.1 Computation of the Disorder-Averaged Free Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 8.4.2 Derivation of Saddle-Point Equations . . . . . . . . . . . . . . . . 91 8.4.3 Computation Transformation to Solve the SDE . . . . . . . . 92 8.4.4 Zero-Temperature Limit . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n9 Replica Symmetry and Replica Symmetry Breaking . . . . . . . . . . . . . 99 9.1 Generalized Free Energy and Complexity of States . . . . . . . . . . . . 99 9.2 Applications to Constraint Satisfaction Problems . . . . . . . . . . . . . . 102 9.3 More Steps of Replica Symmetry Breaking . . . . . . . . . . . . . . . . . . 106 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n10 Statistical Mechanics of Restricted Boltzmann Machine . . . . . . . . . . 111 10.1 Boltzmann Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 10.2 Restricted Boltzmann Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 10.3 Free Energy Calculation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 10.4 Thermodynamic Quantities Related to Learning . . . . . . . . . . . . . . 117 10.5 Stability Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 10.6 Variational Mean-Field Theory for Training Binary RBMs . . . . . 123 10.6.1 RBMs with Binary Weights . . . . . . . . . . . . . . . . . . . . . . . . 124 10.6.2 Variational Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 10.6.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n\nContents\n\nxiii\n\n11 Simplest Model of Unsupervised Learning with Binary Synapses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 11.1 Model Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 11.2 Derivation of sMP and AMP Equations . . . . . . . . . . . . . . . . . . . . . . 135 11.3 Replica Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 11.3.1 Explicit form of Z n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 11.3.2 Estimation of Z n Under Replica Symmetry Ansätz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140 11.3.3 Derivation of Free Energy and Saddle-Point Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 11.4 Phase Transitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 11.5 Measuring the Temperature of Dataset . . . . . . . . . . . . . . . . . . . . . . . 148 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n12 Inherent-Symmetry Breaking in Unsupervised Learning . . . . . . . . . . 153 12.1 Model Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153 12.1.1 Cavity Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 12.1.2 Replica Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161 12.1.3 Stability Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182 12.2 Phase Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 12.3 Hyper-Parameters Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\n13 Mean-Field Theory of Ising Perceptron . . . . . . . . . . . . . . . . . . . . . . . . . 195 13.1 Ising Perceptron model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195 13.2 Message-Passing-Based Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 197 13.3 Replica Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199 13.3.1 Replica Symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201 13.3.2 Replica Symmetry Breaking . . . . . . . . . . . . . . . . . . . . . . . . 205 13.4 Further Theory Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\n14 Mean-Field Model of Multi-layered Perceptron . . . . . . . . . . . . . . . . . . 213 14.1 Random Active Path Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213 14.1.1 Results from Cavity Method . . . . . . . . . . . . . . . . . . . . . . . . 215 14.1.2 An Inﬁnite Depth Analysis . . . . . . . . . . . . . . . . . . . . . . . . . 216 14.2 Mean-Field Training Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 220 14.3 Spike and Slab Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221 14.3.1 Ensemble Perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221 14.3.2 Training Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\n15 Mean-Field Theory of Dimension Reduction . . . . . . . . . . . . . . . . . . . . . 227 15.1 Mean-Field Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227 15.2 Linear Dimensionality and Correlation Strength . . . . . . . . . . . . . . 231 15.2.1 Iteration Equations for Correlation Strength . . . . . . . . . . 232 15.2.2 Mechanism of Dimension Reduction . . . . . . . . . . . . . . . . 234\n\nxiv\n\nContents\n\n15.3 Dimension Reduction with Correlated Synapses . . . . . . . . . . . . . . 237 15.3.1 Model Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238 15.3.2 Mean-Field Calculation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239 15.3.3 Numerical Results Compared with Theory . . . . . . . . . . . 247\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250\n16 Chaos Theory of Random Recurrent Neural Networks . . . . . . . . . . . 253 16.1 Spiking and Rate Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253 16.2 Dynamical Mean-Field Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255 16.2.1 Dynamical Mean-Field Equation . . . . . . . . . . . . . . . . . . . . 255 16.2.2 Regimes of Network Dynamics . . . . . . . . . . . . . . . . . . . . . 259 16.3 Lyapunov Exponent and Chaos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262 16.4 Excitation-Inhibition Balance Theory . . . . . . . . . . . . . . . . . . . . . . . 264 16.5 Training Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . 268 16.5.1 Force-Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268 16.5.2 Backpropagation Through Time . . . . . . . . . . . . . . . . . . . . 268 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272\n17 Statistical Mechanics of Random Matrices . . . . . . . . . . . . . . . . . . . . . . 275 17.1 Spectral Density . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 17.2 Replica Method and Semi-circle Law . . . . . . . . . . . . . . . . . . . . . . . 277 17.3 Cavity Approach and Marchenko–Pastur Law . . . . . . . . . . . . . . . . 281 17.4 Spectral Densities of Random Asymmetric Matrices . . . . . . . . . . . 285 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n18 Perspectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294\n\nAbout the Author\nDr. Haiping Huang received his Ph.D. degree in theoretical physics from the institute of Theoretical Physics, the Chinese Academy of Sciences. He works as an associate professor at the School of Physics, Sun Yat-sen University,China. His research interests include the origin of the computational hardness of the binary perceptron model, the theory of dimension reduction in deep neural networks, and inherent symmetry breaking in unsupervised learning.In 2021, he was awarded Excellent Young Scientists Fund by National Natural Science Foundation of China.\nxv\n\nAcronyms\n\nAMP AT BA BM BP BPTT CD CLT CNN DG EA EM gBP KL LIF LSTM MCMC MCS MFA MPM PS PSB RAP RBM RF RG RNN RS RSB SaS SDE\n\nApproximate message passing De Almeida-Thouless Bethe approximation Boltzmann machine Belief propagation Backpropagation through time Contrastive-divergence Central limit theorem Convolution neural network Dichotomized Gaussian Edwards–Anderson Expectation-maximization Generalized backpropagation Kullback–Leibler Leaky-integrated ﬁring Long short-term memory Markov chain Monte Carlo Monte Carlo sweep Mean-ﬁeld approximation Maximizer of the posterior marginal Permutation symmetry Permutation symmetry breaking Random active path Restricted Boltzmann machine Receptive ﬁeld Random guess Recurrent neural network Replica symmetry Replica symmetry breaking Spike and slab Saddle-point equation\n\nxvii\n\nxviii\n\nSG\n\nSpin glass\n\nSK\n\nSherrington-Kirkpatrick\n\nsMP Simpliﬁed message passing\n\nSSB Spontaneous symmetry breaking\n\nTAP Thouless–Anderson–Palmer\n\nAcronyms\n\nChapter 1\nIntroduction\n\nNeural network studies stemmed from the curiosity about how the brain works and even biological mechanisms of high-level intelligence [1]. This original curiosity has a very long history that is also a history of humans’ endeavors to understand the brain. A modern artiﬁcial neural model was proposed by McCulloch and Pitts in 1943 [2], and the neuron of complex biological processes was abstractly modeled as a non-linear transfer function of simply weighted sum of inputs. A few years later, Donald Hebb proposed the Hebbian learning rule [3], i.e., “cells that ﬁre together, wire together”. This rule forms the basics of a later development, i.e., the abstract model of associative memory, the so-called Hopﬁeld model [4, 5], where the Hebbian rule was used to construct the effective coupling between neurons in the model that can realize the retrieval of a memory item (e.g., a random pattern the Hebbian rule uses), under a less noisy neural dynamics from an initial state where the memory item is corrupted by a few bits. The Hebbian rule, despite its simplicity, plays a signiﬁcant important role in the current status of both experimental and theoretical neuroscience studies [6].\nBased on the McCulloch–Pitts model of artiﬁcial neurons, Frank Rosenblatt introduced the ﬁrst perceptron model of supervised classiﬁcation tasks [7]. At that time, this model can only be used to classify linearly separable patterns [8]. However, this abstract model plays an important role in neuroscience studies, as the perceptron model was popular in modeling the learning behavior of the cerebellar Purkinje cells [9, 10]. The perceptron model can also be easily generalized to multilayer feedforward neural networks, which are able to separate non-linearly separable patterns, due to the highly nested non-linear layer-wise computations. This nested non-linearity makes an analytic understanding of inner workings challenging in the academic community [11, 12]. However, the backpropagation of the error from top layers was shown to work in practical training of multi-layer neural networks [13], which establishes the algorithmic foundation of deep learning.\nFukushima introduced neocognitron in 1980, using the biological concept of simple and complex cells observed in visual pathways of a cat’s visual cortex [14]. When\n\n© Higher Education Press 2021\n\n1\n\nH. Huang, Statistical Mechanics of Neural Networks,\n\nhttps://doi.org/10.1007/978-981-16-7570-6_1\n\n2\n\n1 Introduction\n\nthese neural computations are organized in a hierarchical way, the position-shift invariance can be achieved. This neocognitron model inspired the further development of multi-layer neural computation, for example, the powerful architecture— convolution neural network (CNN) proposed in 1990s [15], where the computation of simple cells corresponds to convolution while computation of complex cells corresponds to pooling, showing the power of multi-layer neural networks in practice, e.g., in solving computer vision tasks [16].\nIn 1985, Hinton and Sejnowski introduced the Boltzmann machine [17], where the model parameters, e.g., coupling and ﬁelds of an Ising model, can be learned directly from the data samples, matching only the ﬁrst two moments of the data statistics [18]. This framework has a recently renewed interest in system neuroscience [19], being known as an inverse Ising problem in physics [20] with a wide application in different interdisciplinary ﬁelds ranging from neural activity modeling and protein structure prediction to ﬁnancial data analysis [21]. Paul Smolensky later introduced the original two-layer neural network with stochastic activations [22], the so-called restricted-Boltzmann machine (RBM) [22, 23], where neurons in a traditional Boltzmann machine are divided into visible and hidden groups. In 2006, Hinton and Salakhutdinov proposed efﬁcient methods to train a deep belief network composed of layer-wise stacking of RBMs [24], initializing the deep learning revolutionary in both academic and industrial neural network studies.\nAnother type of neural network architecture has salient features in its recurrent computation, incorporating temporal information. There appeared extensive research interests in algorithmic issues around 1990 [25–28]. However, training the recurrent neural network (RNN) is typically challenging, due to vanishing/exploding gradients of the objective function [29]. In the current deep learning era, many smart techniques are being proposed to tackle this challenge. In particular, Hochreiter and Schmidhuber introduced the long short-term memory (LSTM), using various kinds of informationgating mechanisms [30], to avoid the training difﬁculty of RNNs. The RNN structure is also considered as a canonical model of perception, learning, memory, action and other high-level cognition [31–33].\nIn the history of artiﬁcial neural networks, still a lot of important topics are not covered in the above retrospect. For complete reviews, we refer interested readers to several recent reviews of neural networks [34–36]. On the other side, the statistical mechanics plays a key role in understanding the emergent behavior of artiﬁcial neural networks, even real neuronal networks [37].\nThe ﬁrst statistical mechanical theory of neural networks was published in 1985; providing a complete phase diagram of the Hopﬁeld network [38, 39] and explaining low temperature and low memory load are necessary to guarantee a successful retrieval of one of the embedded random patterns (akin to memory items). The analytic techniques are rooted in studies of disordered systems, such as spin glass systems [40, 41]. One powerful technique is the replica trick, which introduces many copies of the original model, and the original complex spin-to-spin interactions are decoupled into overlaps between replicas, while the overlaps are exactly the order parameters of the statistical mechanical model. This technique was later generalized to the perceptron model, inventing the concept of the Gardner volume to determine\n\n1 Introduction\n\n3\n\nthe capacity of a perceptron system [42, 43]. The Gardner method is still popular as a powerful physics tool in the theoretical neuroscience community [10].\nIn 1988, Sompolinsky et al. developed another powerful physics method for analyzing the recurrent dynamics of RNNs with random weights [44]. This method treats the behavior of a real RNN as an effective mean-ﬁeld limit of a homogeneous system, whose ﬁrst two moments of neural dynamics statistics are recursively established, resulting in a mean-ﬁeld calculation of the Lyapunov exponent determining whether a transition-to-chaos is possible. This framework can be derived under the path-integral representation of the dynamics [45, 46] and is still popular in analyzing more complex RNNs with structured connectivity. The mean-ﬁeld study of a random RNN was later generalized to neuronal networks of excitatory and inhibitory cells [47, 48], satisfying biological Dale’s rule (a biological neuron cannot produce both excitatory and inhibitory synapses). In this study, the excitatory–inhibitory balance condition [49, 50], i.e., feedback inhibition cancels with strong excitatory recurrent inputs, can be identiﬁed in the mean-ﬁeld limit, leading to a mechanistic explanation of the irregular asynchronous neural activity observed in cortical circuits. Brunel further studied the emergent behavior of spiking activity of a sparsely connected excitatory–inhibitory neural network [51]. These theoretical paradigms still beneﬁt the computational and theoretical community even now. Therefore, the statistical physics methods, including equilibrium phase diagram analysis and non-equilibrium mean-ﬁeld theory, are very promising in exploring the black box of deep neural networks, which may further connect to other branches, e.g., random matrix theory [52], etc.\nIn this book, we will provide our personal selections of statistical mechanical techniques applied to neural networks studies, and an in-depth introduction of these statistical physics methods, especially applications in simple toy models where learning mechanisms can be revealed in a mathematically concise way, even with theoretical predictions of new emergent behavior.\n\nReferences\n1. R. Yuste, Nat. Rev. Neurosci. 16(8), 487 (2015) 2. W.S. McCulloch, W. Pitts, Bull. Math. Biophys. 5, 115 (1943) 3. D.O. Hebb, The Organization of Behavior (Wiley, New York, 1949) 4. J.J. Hopﬁeld, Proc. Natl. Acad. Sci. USA 79, 2554 (1982) 5. S.I. Amari, Biological Cybern. 26, 175 (1977) 6. W. Gerstner, M. Lehmann, V. Liakoni, D. Corneil, J. Brea, Front. Neural Circ. 12, 53 (2018) 7. F. Rosenblatt, Psychol. Rev. 65, 386 (1958) 8. S. Papert, M.L. Minsky, Perceptrons: An Introduction to Computational Geometry (MIT Press,\nCambridge, 1988) 9. C. Clopath, J.P. Nadal, N. Brunel, PLOS Comput. Biol. 8(4), e1002448 (2012) 10. N. Brunel, Nat. Neurosci. 19, 749 (2016) 11. A. Saxe, S. Nelli, C. Summerﬁeld, Nat. Rev. Neurosci. 22, 55 (2020) 12. D. Hassabis, D. Kumaran, C. Summerﬁeld, M. Botvinick, Neuron 95(2), 245 (2017) 13. D.E. Rumelhart, G.E. Hinton, R.J. Williams, Nature 323, 533 (1986) 14. K. Fukushima, Biolog. Cybern. 36(4), 193 (1980) 15. Y. Lecun, L. Bottou, Y. Bengio, P. Haffner, Proc. IEEE 86, 2278 (1998)\n\n4\n\n1 Introduction\n\n16. A. Krizhevsky, I. Sutskever, G.E. Hinton, in Advances in Neural Information Processing Systems 25, ed. by P. Bartlett, F. Pereira, C. Burges, L. Bottou, K. Weinberger (2012), pp. 1097– 1105\n17. D.H. Ackley, G.E. Hinton, T.J. Sejnowski, Cognit. Sci. 9(1), 147 (1985) 18. E.T. Jaynes, Phys. Rev. 106, 620 (1957) 19. E. Schneidman, M.J. Berry, R. Segev, W. Bialek, Nature 440, 1007 (2006) 20. Y. Roudi, E. Aurell, J. Hertz, Front. Comput. Neurosci 3, 1 (2009) 21. H.C. Nguyen, R. Zecchina, J. Berg, Adv. Phys. 66(3), 197 (2017) 22. P. Smolensky, Information processing in dynamical systems: foundations of harmony theory\n(MIT Press, Cambridge, 1986), pp. 194–281 23. Y. Freund, D. Haussler, Unsupervised learning of distributions on binary vectors using two\nlayer networks. Technical Report, Santa Cruz, CA, USA (1994) 24. G. Hinton, R. Salakhutdinov, Science (New York, N.Y.) 313, 504 (2006) 25. J.L. Elman, Cognit. Sci. 14(2), 179 (1990) 26. P.J. Werbos, Neural Netw. 1(4), 339 (1988) 27. F.J. Pineda, Phys. Rev. Lett. 59(19), 2229 (1987) 28. R.J. Williams, J. Peng, Neural Comput. 2(4), 490 (1990) 29. R. Pascanu, T. Mikolov, Y. Bengio, in Proceedings of the 30th International Conference on\nMachine Learning (2013), pp. 1310–1318 30. S. Hochreiter, J. Schmidhuber, Neural Comput. 9(8), 1735 (1997) 31. D. Sussillo, L. Abbott, Neuron 63(4), 544 (2009) 32. D.V. Buonomano, W. Maass, Nat. Rev. Neurosci. 10(2), 113 (2009) 33. S. Vyas, M.D. Golub, D. Sussillo, K.V. Shenoy, Ann. Rev. Neurosci. 43(1), 249 (2020) 34. J. Schmidhuber, Neural Netw. 61, 85 (2015) 35. Y. LeCun, Y. Bengio, G. Hinton, Nature 521(7553), 436 (2015) 36. P. Mehta, M. Bukov, C.H. Wang, A.G. Day, C. Richardson, C.K. Fisher, D.J. Schwab, Phys.\nRep. 810, 1 (2019) 37. D.J. Amit, Modeling Brain Function: The World of Attractor Neural Networks (Cambridge\nUniversity Press, Cambridge, 1989) 38. D.J. Amit, H. Gutfreund, H. Sompolinsky, Phys. Rev. A 32, 1007 (1985) 39. D.J. Amit, H. Gutfreund, H. Sompolinsky, Physical Review Letters 55(14), 1530 (1985) 40. M. Mézard, G. Parisi, M.A. Virasoro, Spin Glass Theory and Beyond (World Scientiﬁc, Sin-\ngapore, 1987) 41. P. Peretto, Biolog. Cybern. 50(1), 51 (1984) 42. E. Gardner, Europhys. Lett. (epl) 4, 481 (1987) 43. E. Gardner, J. Phys. A: Math. Gen. 21, 257 (1988) 44. H. Sompolinsky, A. Crisanti, H.J. Sommers, Physical Review Letters 61(3), 259 (1988) 45. A. Crisanti, H. Sompolinksy, Phys. Rev. E 98(6), 62120 (2018) 46. M. Helias, D. Dahmen (2019). arXiv:1901.10416 47. C. van Vreeswijk, H. Sompolinsky, Science 274(5293), 1724 (1996) 48. C. van Vreeswijk, H. Sompolinsky, Neural Comput. 10(6), 1321 (1998) 49. M.N. Shadlen, W.T. Newsome, J. Neurosci. 18(10), 3870 (1998) 50. M. Okun, I. Lampl, Nat. Neurosci. 11(5), 535 (2008) 51. N. Brunel, J. Comput. Neurosci. 8(3), 183 (2000) 52. M.L. Mehta, Random Matrices (Academic, San Diego, 2004)\n\nChapter 2\nSpin Glass Models and Cavity Method\n\nSpin glasses are magnets with two-spin interactions of random signs (Mézard et al., in Spin Glass Theory and Beyond. World Scientiﬁc, Singapore, 1987 [1]), e.g., an alloy with randomly localized magnetic moments. In spin glass models, the randomness emerges in spin interactions. For example, in the Sherrington–Kirkpatrick model (Sherrington and Kirkpatrick in Phys. Rev. Lett. 35(26):1792, 1975 [2]), all two-spin interactions follow independently a Gaussian distribution with variance N −1/2 (N is the system size); in the Edwards–Anderson model (Edwards and Anderson in J. Phys. F: Metal Phys. 5(5):965, 1975 [3]), the spins sit on a ﬁnite-dimensional lattice, and in the Bethe lattice model (Viana and Bray in J. Phys. C: Solid State Phys. 18(15):3037, 1985 [4]; Mézard and Parisi in Eur. Phys. J. B 20:217, 2001 [5]), the spins locate at a random lattice of ﬁnite connectivity for each spin. All these models belong to the category of multi-spin interaction models, originally studied in physics, later widely explored in the context of optimization problems in computer science, machine learning and computational neuroscience.\n\n2.1 Multi-spin Interaction Models\n\nBefore going into details of the underlying physics, we would like to give a few seminal applications of the spin glass models. The ﬁrst one is the random K -SAT problem. The random K -SAT problem is ﬁnding a solution, say an assignment of N Boolean variables, to satisfy a random formula composed of logical AND of M clauses. Each clause is expressed as a logical OR function of K randomly selected distinct variables (either directed or negated with equal probability) from the variable set. For example, one short formula is given by\n\nF = (x3 ∨ x7 ∨ x2) ∧ (x1 ∨ x5 ∨ x6) ∧ (x4 ∨ x7 ∨ x5).\n\n(2.1)\n\n© Higher Education Press 2021\n\n5\n\nH. Huang, Statistical Mechanics of Neural Networks,\n\nhttps://doi.org/10.1007/978-981-16-7570-6_2\n\n6\n\n2 Spin Glass Models and Cavity Method\n\nFrom a physics viewpoint, the random K -SAT can be treated as a spin glass problem with a focus on the typical case analysis.1 If xi is TRUE, then we transform it to an Ising spin with value 1 (spin up); otherwise, it is transformed to −1 (spin down). Given a conﬁguration of spins, the number of violated clauses can be deﬁned\nas an energy function in statistical physics [6],\n\nM\nE(σ ) =\n\nK\n\n1\n\n+\n\nJ\n\nm j\n\nσi\n\nm j\n\n,\n\n2\n\nm=1 j=1\n\n(2.2)\n\nwhere\n\ni\n\nm j\n\nis\n\nthe\n\nj th\n\nvariable\n\nappearing\n\nin\n\nthe\n\nmth\n\nclause.\n\nThe\n\nquenched\n\ndisorder\n\nJ\n\nm j\n\nis 1 if the Boolean variable in the formula appears negated and −1 otherwise. Hence,\n\nthe constraint satisfaction problem reduces to a physics problem of ﬁnding minima\n\nof the energy function.\n\nAnalogously, the random K -XOR SAT formula can be written as\n\n⎡⎛\n\n⎞⎤\n\nM\n\nK\n\nF=\n\n⎣⎝\n\nxi\n\nm j\n\n⎠\n\n⊕\n\nym ⎦\n\n,\n\nm=1\n\nj =1\n\n(2.3)\n\nwhere the symbol ⊕ denotes the logical XOR operation, and ym is quenched random Boolean value. This formula corresponds to a linear system, with an efﬁcient solver of the Gaussian elimination procedure. In physics, the diluted Ising p-spin model with coupling ±1 belongs to the class of random K -XOR problem. Similar to the random K -SAT Problem, one can easily write down the associated energy function [7]\n\nE(σ ) = M 1 − Jm\n\nK j =1\n\nσi\n\nm j\n\n,\n\n2\n\nm=1\n\n(2.4)\n\nwhere Jm is an Ising-mapping of the Boolean variable ym. The above two constraint satisfaction problems belong to multi-spin interaction\nmodels in physics. Physicists are interested in studying the mean-ﬁeld limit N → ∞ and M → ∞ but keeping the ratio M/N constant. One expects that rich phase transitions emerge due to complex interactions among spin variables. Next, we will illustrate how the cavity method can be used to compute the free energy function of this class of models. Cavity method was ﬁrst proposed to reproduce the replica results of the Sherrington–Kirkpatrick model [8], and further reformulated in the study of neural networks [9], and was ﬁnally proposed at the concise physics level and systematic mathematical level on the Bethe lattice, a broad class of glass models of ﬁnite connectivities [5]. We will also explore the core physics assumption behind the cavity method in detail in a multi-spin interaction model. The multi-spin interaction models like the above two cases can be analogously treated.\n\n1 The computational complexity is deﬁned in the worst-case setting.\n\n2.1 Multi-spin Interaction Models\n\n7\n\nThe multi-spin interaction model can be also deﬁned in the context of information transmission, for which we shall give a concrete example. Let us consider the case that we want to send a message to a receiver, and the message may be perturbed during transmission because of the noise in the channel. It is a highly non-trivial task for the receiver to retrieve the original message from the perturbed one. One solution is to introduce redundancy to the original message at the sender site. Then the receiver can correct some transmission errors according to the redundancy. In 1948, Claude Shannon proved that error-free transmission is possible when the code rate is below the channel capacity, which establishes a fundamental bound for designing engineering practical codes [10]. Numerous efforts have been devoted to design the codes approaching Shannon’s bound (channel capacity). Among them, the Sourlas code is the ﬁrst one in physics [11], which relates error-correcting codes to a spin glass model.\nIt is easy to ﬁgure out how to construct a Sourlas code. Supposed that we have an N -bit binary original message ξ ∈ {±1}N , and then encode them into an M-bit transmitted message J 0 = {J10, J20, . . . , JM0 }. The ath bit of J 0 is the product of a subset ∂a of randomly selected original message bits,\n\nJa0 = ξi .\ni ∈∂ a\n\n(2.5)\n\nWe then denote Ja as the ath bit of the received message, which may not be equal to the transmitted message due to the channel noise ﬂipping message bits. We further assume that each bit of transmitted messages can be independently ﬂipped with the same probability p. Hence, the conditional probability of a received message given a transmitted one reads\n\nP( Ja| Ja0) = pδ( Ja + Ja0) + (1 − p)δ( Ja − Ja0).\n\n(2.6)\n\nTo decode the sent message, we write the computational task as a statistical mechanics problem with the following Hamiltonian [12]:\n\nM\nH (σ ) = − Ja σi ,\na=1 i∈∂a\n\n(2.7)\n\nwhere σi is the dynamical binary spin variable for decoding the original message\n\n{ξi }. What we need to do is computing the posterior probability P(σ | J) which is\n\ngiven by\n\nP(σ | J) = exp(−β H (σ )) ,\n\n(2.8)\n\nZ\n\nwhere β is the inverse temperature, and Z is the partition function. This decoding process amounts now to searching for the ground state of the statistical mechanics problem. The energy of the model takes a minimal value if i∈∂a σi = Ja. According\n\n8\n\n2 Spin Glass Models and Cavity Method\n\nto the canonical ensemble theory, all emergent properties of the decoding process are included in the partition function that is mathematically formulated as follows:\n\nZ = exp(−β H (σ )).\n{σi }\n\n(2.9)\n\n2.2 Cavity Method\n\nIn this section, we apply the cavity approximation to compute the partition function. Notice that a direct calculation of Eq. (2.9) involves in summing up 2N terms, which is computationally impossible once N > 30. The cavity method can reduce the computational cost down to the order of O(N ) for a sparsely connected factor graph model. Let us explain this in detail as follows.\nThe model can be represented by a factor graph [13], illustrating how spins interact with each other (see Fig. 2.1). Because we aim at analyzing the Boltzmann distribution, i.e., the posterior [Eq. (2.8)], we use the probabilistic language, for the goal of computing the marginal probability as well. To achieve this, we should modify the original graph that allows strong correlations among variables. If we add one function node a to the original system (see Fig. 2.2), the Hamiltonian of the new system can be written as the sum of the Hamiltonian of the original system and the change caused by the newly added function node. More precisely,\n\nIt then proceeds that\n\nH new = H old − Ja σi .\ni ∈∂ a\n\n(2.10)\n\ni ... j ... k N\n\na b c d e f ... M\nFig. 2.1 Factor graph representation of a random construction of the Sourlas codes. Circles are spin variables (variable nodes) {σi }, and squares are received message (function nodes) {Ja}. In the ﬁgure, we only show three message bits, and each square is connected to them. In fact, the square can be connected to other different message bits (not shown), thereby forming a sparse random graph, where the degree of variable nodes follows a Poisson distribution\n\n2.2 Cavity Method\n\nj\n\nk\n\na\n\nl\n\n9\nm n\nb i\nc\n\nFig. 2.2 Addition of the function node a to original system (outside the shadow part). We call the shadow part a cavity, and the nodes {i, j, k, l} serve as the boundary of the cavity\n\nZ new =\n\nexp −β H old + β Ja σi\n\n{σi }iN=1\n\ni ∈∂ a\n\n= Z old\n\nexp(−β H old)\n\nZ old\n\nexp\n\nβ Ja\n\nσi .\n\n{σi }iN=1\n\ni ∈∂ a\n\n(2.11)\n\nIt\n\nis\n\neasy\n\nto\n\nsee\n\nthat\n\nexp(−β H old) Z old\n\nis\n\nexactly\n\nthe\n\njoint\n\nprobability\n\ndistribution\n\nof\n\n{σi }iN=1\n\nin the old system. One can ﬁnd that exp(β Ja i∈∂a σi ) only relates to {σi |i ∈ ∂a},\n\nand then we can divide the conﬁguration sum into two parts: one involves in only\n\nvariable nodes with direct connections to the newly added functional node a, and the\n\nother involves in the rest. We then have\n\nZ new = Z old\n\nexp(−β H old) Z old\n\nexp(β\n\nJa\n\nσi )\n\n{σi |i∈∂a} {σi |i∈/∂a}\n\ni ∈∂ a\n\n= Z old\n\nexp(β Ja σi )\n\nexp(−β H Z old\n\nold\n\n)\n\n.\n\n{σi |i∈∂a}\n\ni∈∂a {σi |i∈/∂a}\n\n(2.12)\n\nThe last summation in Eq. (2.12) is exactly the marginal probability distribution of {σi |i ∈ ∂a} in the old system. Compared with the new system, the old system has a cavity in the position where the function node a is added to the new system. Therefore, we denote the marginal probability as a cavity distribution Pcavity({σi |i ∈ ∂a}) and\n\n10\n\n2 Spin Glass Models and Cavity Method\n\ncall {σi |i ∈ ∂a} as the boundary of the cavity. It is reasonable to assume that variable nodes on the boundary of the cavity are weakly correlated, because of the weak couplings in a fully connected system or the sparsely connected topology of a sparse model. This assumption is exact if the underlying factor graph is a tree. Thus, the Pcavity({σi |i ∈ ∂a}) can be factorized as\n\nPcavity({σi |i ∈ ∂a}) ≈ qi→a(σi ),\ni ∈∂ a\n\n(2.13)\n\nwhere qi→a denotes the distribution of σi without the presence of the function node a.\n\nLet us then deﬁne a cavity magnetization mi→a ≡ qi→a(σi = +1) − qi→a(σi = −1),\n\nand\n\nthus,\n\nqi→a(σi )\n\n=\n\n. 1+σi mi→a\n2\n\nThen\n\nZ new\n\ncan\n\nbe\n\nrewritten\n\nas\n\nZ new = Z old\n\nexp(β Ja\n\nσi )\n\n1 + σi mi→a 2\n\n{σi |i∈∂a}\n\ni∈∂a i∈∂a\n\n= Z old cosh(β Ja) 1 + tanh(β Ja) mi→a .\ni ∈∂ a\n\n(2.14)\n\nAccording to the free energy deﬁnition F = −1/β ln Z , the free energy shift due to adding the function node a is given by\n\n−β\n\nFa\n\n= ln\n\nZ new Z old\n\n= ln\n\ncosh(β Ja) 1 + tanh(β Ja)\n\nmi→a .\n\ni ∈∂ a\n\n(2.15)\n\nSimilarly, if we add one variable node i and its neighboring function nodes {b|b ∈ ∂i} to the system (see Fig. 2.3),2 the partition function of the new system reads\n\n⎛\n\n⎞\n\nZ new =\n\nexp ⎝−β H old + β Jb σ j ⎠\n\nσ old σi\n⎛\n\nb∈∂i j∈∂b\n⎞\n\n=\n\nexp ⎝−β H old + β Jbσi\n\nσj⎠\n\nσ old σi\n\nb∈∂ i\n\nj ∈∂ b\\i\n\n⎛\n\n⎞\n\n= Z old\nσ old\n\nσi\n\nexp(−β H old) exp ⎝β Z old\n\nJb σi\n\nσj⎠,\n\nb∈∂ i\n\nj ∈∂ b\\i\n\n(2.16)\n\nwhere j ∈ ∂b\\i denotes the set of variable nodes with connections to the function node b, yet the node i is excluded. The subset of nodes {σ j | j ∈ ∂b\\i, b ∈ ∂i} is the boundary of the cavity (see Fig. 2.3). We can ﬁrst sum over the conﬁguration of all\n\n2 This operation will make the deﬁnition of cavity probabilities reasonable.\n\n2.2 Cavity Method\n\nj\n\nk\n\na\n\nl\n\n11\nm n\nb i\nc\n\nFig. 2.3 Adding a variable node i together with its neighboring function nodes {a, b, c} to the original system (outside the cavity). The subset { j, k, l, m, n . . .} denotes the boundary of the cavity\n\nvariable nodes that are not at the boundary of the cavity (except i), akin to what we have done in Eq. (2.12). Using Eq. (2.13), we then arrive at the following result:\n\nZ new = Z old\n\nPcavity({σ j | j ∈ ∂b\\i ; b ∈ ∂i }) exp\n\nβ Jbσi\n\nσj\n\n{σ j | j∈∂b\\i;b∈∂i} σi\n\nb∈∂ i\n\nj ∈∂ b\\i\n\n≈ Z old\n\nq j→b(σ j ) exp\n\nβ Jbσi\n\nσj\n\n{σ j | j∈∂b\\i;b∈∂i} σi b∈∂i j∈∂b\\i\n\nb∈∂ i\n\nj ∈∂ b\\i\n\n= Z old\n\nq j→b(σ j ) exp\n\nβ Jbσi\n\nσj\n\nσi {σ j | j∈∂b\\i;b∈∂i} b∈∂i j∈∂b\\i\n\nb∈∂ i\n\nj ∈∂ b\\i\n\n= Z old\n\nq j→b(σ j ) exp β Jbσi\n\nσj\n\nσi b∈∂i {σ j | j∈∂b\\i} j∈∂b\\i\n\nj ∈∂ b\\i\n\n= Z old\n\nσi\n\n1 + σ j m j→b exp 2\nb∈∂i {σ j | j∈∂b\\i} j∈∂b\\i\n\nβ Jbσi\n\nσj\n\nj ∈∂ b\\i\n\n= Z old\n\n+ b→i\n\n+\n\n− b→i\n\n,\n\nb∈∂ i\n\nb∈∂ i\n\nwhere\n\n(2.17)\n\n12\n\n2 Spin Glass Models and Cavity Method\n\n⎛\n\n⎞\n\n+ b→i\n\n=\n\n1\n\n+\n\nσjm 2\n\nj →b\n\nexp ⎝β\n\nJb\n\n×\n\n(+1)\n\n×\n\nσj⎠\n\n{σ j | j ∈∂b\\i}\n\nj ∈∂ b\\i\n⎛\n\nj ∈∂ b\\i\n⎞\n\n(2.18)\n\n= cosh(β Jb) ⎝1 + tanh(β Jb)\n\nm j→b⎠ ,\n\nj ∈∂ b\\i\n\n⎛\n\n⎞\n\n− b→i\n\n=\n\n1\n\n+\n\nσjm 2\n\nj →b\n\nexp ⎝β\n\nJb\n\n×\n\n(−1)\n\n×\n\nσj⎠\n\n{σ j | j ∈∂b\\i}\n\nj ∈∂ b\\i\n⎛\n\nj ∈∂ b\\i\n⎞\n\n= cosh(β Jb) ⎝1 − tanh(β Jb)\n\nm j→b⎠ .\n\nj ∈∂ b\\i\n\n(2.19)\n\nHence, the free energy shift due to adding the variable node i together with its neighboring function nodes {b ∈ ∂i} is given by\n\n−β\n\nFi\n\n= ln\n\nZ new Z old\n\n= ln\n\n+ b→i\n\n+\n\n− b→i\n\n.\n\nb∈∂ i\n\nb∈∂ i\n\nFinally, the total free energy is given by\n\n(2.20)\n\nF=\ni\n\nFi +\na\n\nFa − |∂a| Fa,\na\n\n(2.21)\n\nwhere |∂a| is the number of variable nodes connecting to the function node a. The last\nterm of Eq. (2.21) is to ensure that each node’s contribution to the total free energy has been counted only once. Once we have access to {m j→b}, we can calculate the free energy function. In the next section, we explain how to calculate {mi→a}.\n\n2.3 From Cavity Method to Message Passing Algorithms\n\nAccording to the cavity assumption, the cavity magnetization {mi→a} can be iteratively constructed, because the local structure of a random factor graph is statistically homogeneous. Note that mi→a is the expectation value of σi without the contribution from the function node a, which is expected from the deﬁnition of the cavity\noperation. Hence, mi→a can be rewritten as follows:\n\nmi→a =\n\nσ σi exp(−β Hi→a(σ )) , σ exp(−β Hi→a(σ ))\n\n(2.22)\n\n2.3 From Cavity Method to Message Passing Algorithms\n\n13\n\nwhere Hi→a denotes the Hamiltonian without the interaction a, which reads\n\nHi→a = Hcavity −\n\nJb σi\n\nσj,\n\nb∈∂ i \\a\n\nj ∈∂ b\\i\n\n(2.23)\n\nwhere Hcavity is the Hamiltonian of the cavity system where the variable node i together with its neighboring function nodes b ∈ ∂i (except a) are all removed from the original system.\nSimilar to what we have done in Sect. 2.2, we can sum over all possible conﬁgurations of variable nodes not on the boundary of the cavity at ﬁrst, and we, thus, get the marginal distribution of the boundary nodes in the cavity system. We then have\n\nmi→a = =\n\nσ σi exp(−β Hi→a (σ )) Z cavity\nσ exp(−β Hi→a (σ )) Z cavity\nσi B σi Pcavity(B) exp( σi B Pcavity(B) exp(\n\nb∈∂i\\a β Jbσi b∈∂i\\a β Jbσi\n\nj∈∂b\\i σ j ) , j∈∂b\\i σ j )\n\n(2.24)\n\nwhere Zcavity denotes the partition function related to Hcavity, and B ≡ {σ j | j ∈ ∂b\\i; b ∈ ∂i\\a}, which denotes the boundary of the cavity. Then we factorize the\ncavity probability according to the cavity approximation:\n\nPcavity(B) ≈\n\nq j→b(σ j ).\n\nb∈∂i\\a j∈∂b\\i\n\nUsing the same techniques as in Eq. (2.17), we ﬁnally arrive at\n\nmi→a =\n\nb∈∂ i \\a b∈∂ i \\a\n\n+ b→i\n\n−\n\n+ b→i\n\n+\n\nb∈∂ i \\a b∈∂ i \\a\n\n−\n\nb→i −\n\n.\n\nb→i\n\nIf we deﬁne the conjugate cavity magnetization as\n\n(2.25) (2.26)\n\nmˆ b→ j ≡ tanh(β Jb)\n\nm j→b,\n\nj ∈∂ b\\i\n\n(2.27)\n\nwe can then write Eq. (2.26) into the following form:\n\nmi→a =\n\nb∈∂i\\a (1 + mˆ b→i ) − b∈∂i\\a (1 + mˆ b→i ) +\n\nb∈∂i\\a (1 b∈∂i\\a (1\n\n− −\n\nmˆ b→i mˆ b→i\n\n) )\n\n.\n\n(2.28)\n\nThe above expression can be transformed into the language of cavity ﬁelds, e.g., a cavity local ﬁeld hi→a and cavity bias ua→i as also deﬁned in the seminal work [5]. We can then use these ﬁelds or biases to parameterize the cavity probability:\n\n14\n\n2 Spin Glass Models and Cavity Method\n\nqi→a(σi )\n\n≡\n\nexp(βhi→aσi ) , 2 cosh βhi→a\n\npa→i (σi )\n\n≡\n\nexp(βua→i σi ) , 2 cosh βua→i\n\nwhere\n\npa→i (σi )\n\n=\n\n1+σi mˆ a→i 2\n\n.\n\nIt\n\nthen\n\nproceeds\n\nthat\n\n(2.29)\n\nmi→a = tanh βhi→a, mˆ a→i = tanh βua→i .\n\n(2.30)\n\nTherefore, Eqs. (2.27) and (2.28) turn out to be\n\n⎛\n\n⎞\n\nh i →a\n\n=\n\n1 β\n\n⎝\n\nβub→i ⎠ ,\n\nb∈∂ i \\a\n\nua→i =\n\n1 tanh−1 β\n\ntanh(β Ja)\n\ntanh(βh j→a) .\n\nj ∈∂ a\\i\n\n(2.31)\n\nEquation (2.31) is the very message passing equation in the Sourlas-code scenario.\nIn essence, the cavity method is a probabilistic iterative method. One can iteratively solve these equations, until a ﬁxed point of messages ({mi→a}) is reached. These messages are then used to evaluate the full magnetization mi as follows:\n\nmi = tanh\n\nβub→i ,\n\nb∈∂ i\n\n(2.32)\n\nand the sent message can be decoded by the maximizer of the posterior marginal\n\n(MPM),\n\ni.e.,\n\nσi\n\n= arg maxσi\n\nPi (σi ),\n\nwhere\n\nPi (σi ) =\n\n1+σi 2\n\nmi\n\n.\n\nIn\n\naddition,\n\nthe\n\nfree\n\nenergy and other thermodynamic quantities of interest can be evaluated according to\n\nthe derived formulas. The computational complexity is clearly of the order of O(N )\n\nfor a sparsely connected factor graph and the order O(N 2) in the case that all variable\n\nnodes connect to each function node. We remark that this procedure is quite general\n\nand can be adapted to learning problems of a variety of neural networks, which we\n\nshall introduce in the remaining chapters.\n\nReferences\n1. M. Mézard, G. Parisi, M.A. Virasoro, Spin Glass Theory and Beyond (World Scientiﬁc, Singapore, 1987)\n2. D. Sherrington, S. Kirkpatrick, Phys. Rev. Lett. 35(26), 1792 (1975) 3. S.F. Edwards, P.W. Anderson, J. Phys. F: Metal Phys. 5(5), 965 (1975) 4. L. Viana, A.J. Bray, J. Phys. C: Solid State Phys. 18(15), 3037 (1985)\n\nReferences\n\n15\n\n5. M. Mézard, G. Parisi, Eur. Phys. J. B 20, 217 (2001) 6. M. Mézard, R. Zecchina, Phys. Rev. E 66(5), 056126 (2002) 7. S. Franz, M. Leone, F. Ricci-Tersenghi, R. Zecchina, Phys. Rev. Lett. 87(12), 127209 (2001) 8. M. Mézard, G. Parisi, M.A. Virasoro, Europhys. Lett. (EPL) 1(2), 77 (1986) 9. M. Mezard, J. Phys. A 22(12), 2181 (1989) 10. C.E. Shannon, Bell Syst. Tech. J. 27(3), 379 (1948) 11. N. Sourlas, Nature 339(6227), 693 (1989) 12. H. Huang, H. Zhou, Phys. Rev. E 80, 056113 (2009) 13. F. Kschischang, B. Frey, H.A. Loeliger, IEEE Trans. Inf. Theory 47(2), 498 (2001)\n\nChapter 3\nVariational Mean-Field Theory and Belief Propagation\n\nIn the previous chapter, we have introduced the cavity method and its application to computing the approximate free energy of a multi-spin interaction model, and the approximation is equivalent to the Bethe approximation, which we shall provide an in-depth introduction in this chapter. In this chapter, we apply the variational method together with the mean-ﬁeld approximation (MFA) and Bethe approximation (BA) to construct the free energy of the multi-spin interaction model. We show that the belief propagation (BP) algorithm in computer science can be derived under the variational framework, which is in fact equivalent to the cavity method in physics. Furthermore, we emphasize that BA is a more accurate approximation, which reduces to MFA when the coupling is relatively weak or when a high-temperature limit is performed. Finally, we give a brief introduction of the inverse Ising model, where model parameters (couplings and ﬁelds) can be learned by using the mean-ﬁeld methods. Besides being a useful tool in statistical physics, the BP algorithm is also an efﬁcient way to solve many important inference problems in areas of computer science, modern coding and learning in neural networks—one focus of this book.\n\n3.1 Variational Method\n\nThe variational method is an important technique for statistical inference problems. With the target function we want to optimize and some constraints the problem should satisfy, we can apply the variation of model parameters on the target function. We take a simple example of the derivation of the Boltzmann distribution in statistical physics. The entropy of a system in statistical physics can be deﬁned by\n\nS = −k Pr ln Pr ,\nr\n\n(3.1)\n\nwhere k indicates the Boltzmann constant, r is the index of a thermodynamic state and Pr is the to-be-determined distribution of the state r . According to the theory of\n\n© Higher Education Press 2021\n\n17\n\nH. Huang, Statistical Mechanics of Neural Networks,\n\nhttps://doi.org/10.1007/978-981-16-7570-6_3\n\n18\n\n3 Variational Mean-Field Theory and Belief Propagation\n\nthermodynamics, the system is in equilibrium when the entropy reaches its maximum, and the distribution must meet the following two constraints:\n\nPr = 1,\nr\nEr Pr = μ,\nr\n\n(3.2) (3.3)\n\nwhich correspond to the normalization of a probability measure and a target mean\n\nenergy level μ of the system, respectively. Hence, we can use the Lagrange multiplier\n\nmethod:\n\nL = S + λ1 Pr − 1 + λ2 Er Pr − μ ,\n\n(3.4)\n\nr\n\nr\n\nwhere λ1 and λ2 are the Lagrange multipliers for the two constraints, respectively.\n\nThen, the equilibrium requires that\n\n∂L ∂ Pr\n\n= 0, and we ﬁnally arrive at\n\ne−β Er\n\nPr =\n\n, Z\n\nZ = e−β Er ,\n\nr\n\n(3.5)\n\nwhere the inverse temperature β\n\n=\n\n1 kT\n\ncan be deduced from the second law of ther-\n\nmodynamics, and Z is the partition function, namely the normalization constant\n\nto enforce the ﬁrst constraint. In the following, we assume k = 1 for optimization\n\nproblems in a high-dimensional parameter space.\n\nIn sum, from the Lagrange multiplier method with a little knowledge from the\n\nequilibrium thermodynamics, we derive the well-known Boltzmann distribution,\n\nwhere the inverse temperature clearly tunes the energy level of the system [1].\n\n3.2 Variational Free Energy\n\nThe behavior of the free energy contributes to the emergent behavior of a thermodynamic system. However, calculating the free energy in a brute-force way is intractable due to the O(2N ) computational complexity. To overcome the barrier, the variational method provides an effective way to construct an approximate free energy. We take an example of the above-mentioned multi-spin interaction model which is captured by the following Boltzmann distribution:\n\nP(x) = e−β E(x) , Z\nZ = e−β E(x),\nx\n\n(3.6)\n\n3.2 Variational Free Energy\n\n19\n\nwhere x = {x1, x2, . . . , xN } represents the state of N spins in the system. The energy E(x) is given by\n\nE(x) = − Ja xi ,\n\n(3.7)\n\na\n\ni ∈∂ a\n\nwhere a is the index of the interaction, and i ∈ ∂a speciﬁes the set of spins that participate in the ath interaction where we use xa to represent these spins. Ja is the coupling strength of the ath interaction. The inverse temperature here can be set to\nan arbitrary value, and in an equivalent way, the temperature can be absorbed into the coupling Ja. We, thus, set β = 1 without loss of generality. We further deﬁne fa(xa) = eJa i∈∂a xi , which denotes the contribution of the ath interaction to the Boltzmann measure. Thus, we can rewrite the distribution P(x) and energy E(x)\ninto the following forms:\n\nP(x) = 1 Z\n\nfa ( x a ),\n\na\n\nE(x) = − ln fa(xa).\na\n\n(3.8) (3.9)\n\nThese expressions facilitate the following derivation of BP algorithm. The Helmholtz free energy reads\n\nFH = − ln Z .\n\n(3.10)\n\nAs we mentioned above, an exact computation of the Helmholtz free energy is impossible for a large-size system. Instead, we introduce a trial probability distribution b(x) and write the free energy, which is called the Gibbs free energy with some parameters (e.g., magnetizations) to be optimized:\n\nF(b) = U (b) − H (b),\n\n(3.11)\n\nwhere we deﬁne U (b) as the variational internal energy:\n\nU (b) = b(x)E(x),\nx\nand H (b) as the variational entropy:\n\n(3.12)\n\nH (b) = − b(x) ln b(x).\nx\n\n(3.13)\n\nIt is then necessary to compute the difference between the Gibbs free energy and the Helmholtz free energy, as given by\n\n20\n\n3 Variational Mean-Field Theory and Belief Propagation\n\nF(b) − FH = b(x)E(x) + b(x) ln b(x) + ln Z\n\nx\n\nx\n\n= b(x)(− ln Z − ln P(x)) + b(x) ln b(x) + ln Z\n\nx\n\nx\n\n=\n\nb(x)\n\nln\n\nb(x) P(x)\n\n=\n\nD(b|| P ),\n\nx\n\n(3.14)\n\nwhere D(b||P) is the Kullback–Leibler divergence between two probability distributions b(x) and P(x), which is always non-negative and is zero only if b(x) = P(x), ∀x. Therefore, F(b) ≥ FH and F(b) = FH only if b(x) = P(x), ∀x.\nThe above analysis shows that the trial probability distribution b(x) yielding a lower Gibbs free energy will have a smaller distance from the true distribution P(x). That is to say, we transform the original free energy estimation problem to a (Gibbs)\nfree energy minimization problem. To obtain a more accurate free energy, we must ﬁnd a b(x) to minimize the Gibbs free energy F(b), which is exactly what the variational method wants to do. To proceed, we have to specify the trial probability b(x) by introducing the so-called variational parameters, which can be physicsrelevant quantities. In the next sections, we introduce two kinds of approximations for b(x), which are mean-ﬁeld and the Bethe approximations.\n\n3.2.1 Mean-Field Approximation\n\nThe mean-ﬁeld approximation for b(x) is written in a factorized form:\n\nbMF(x) =\n\nbi (xi ) =\n\n1 + mi xi , 2\n\ni\n\ni\n\n(3.15)\n\nwhere m is the magnetization vector of spins x. This approximation is the naive one\nthat assumes each spin behaves independently of each other. Note that xi can only take two values ±1 (e.g., spin up and down, respectively). Given the form of bM F (x), we can compute the mean-ﬁeld internal energy and mean-ﬁeld entropy as follows:\n\nUMF = bMF(x)E(x)\n\nx\n\n⎛\n\n⎞\n\n=\n\nbi (xi ) ⎝− Ja\n\nxi ⎠\n\nxi\n\na i∈∂a\n\n= − Ja\n\nxi\n\na\n\ni ∈∂ a\n\n= − Ja mi ,\na i∈∂a\n\n(3.16)\n\nand\n\n3.2 Variational Free Energy\n\n21\n\nHM F = − bM F (x) ln bM F (x)\nx\n\n=−\n\n1 + mi xi ln 1 + mi xi\n\nxi\n\n2\n\n2\ni\n\n=−\n\n1 + m j x j ln 1 + mi xi\n\ni xj\n\n2\n\n2\n\n=−\n\n1 + m j x j 1 + mi xi ln 1 + mi xi\n\ni xi x\\xi j (=i)\n\n2\n\n2\n\n2\n\n=−\n\n1 + mi xi ln 1 + mi xi\n\ni xi\n\n2\n\n2\n\n= Si ,\ni\n\n(3.17)\n\nwhere Si is deﬁned as the entropy of spin xi , and the symbol \\ indicates the operation of exclusion. Thus, the mean-ﬁeld free energy can be derived as follows:\n\nFM F = UM F − HM F\n\n= − Ja mi +\n\na\n\ni ∈∂ a\n\ni\n\n1 + mi xi ln 1 + mi xi .\n\n2\n\n2\n\nxi\n\n(3.18)\n\nThe normalization constraint is automatically satisﬁed by the factorized form of\n\nthe naive mean-ﬁeld distribution [Eq. (3.15)]. The magnetization now becomes the\n\nvariational parameter for the trial probability bM F (x). To minimize the upper bound\n\nof the Helmholtz free energy, we have to compute\n\n∂ FM F ∂mi\n\nand set the gradient to zero:\n\n∂ FM F ∂mi\n\n=−\n\na\n\nJa\n\nmj +\n\nj ∈∂ a\\i\n\nxi\n\nxi ln 1 + mi xi + xi\n\n2\n\n2\n\n2\n\n=−\n\na\n\nJa\n\nmj\nj ∈∂ a\\i\n\n+\n\n1 2\n\nln\n\n1 + mi 1 − mi\n\n= 0,\n\n(3.19)\n\nand ﬁnally, we derive the recursive-form of mi :\n\nmi = tanh\n\nJa\n\nmj .\n\na∈∂i j∈∂a\\i\n\n(3.20)\n\nTo obtain the ﬁxed-point (equilibrium) values of m, we can run these equations until a stationary point is reached. Using these equilibrium magnetizations, we can obtain the value of the Gibbs free energy [2].\nHowever, the spin-independence assumption of the naive mean-ﬁeld method may not be accurate, especially when a low-temperature thermodynamic phase is of interest. We need to consider the correlations among the spins in a short-range region of\n\n22\n\n3 Variational Mean-Field Theory and Belief Propagation\n\n1\n\n2\n\n3\n\n4\n\nA\n\nB\n\nC\n\nVariable node Function node\n\nFig. 3.1 Regions in a factor graph. Solid circles are deﬁned as regions, while the dashed circle is not a valid region\nthe factor graph, which is precisely the concept of the Bethe approximation, which we shall explore in the next section.\n\n3.2.2 Bethe Approximation\n\nThe Bethe approximation [3] is an extension of the classic mean-ﬁeld method, taking into account correlations between nearest neighboring sites. To introduce the Bethe approximation, we ﬁrst deﬁne the concept of region in the factor graph. As Fig. 3.1 shows, the region is deﬁned by a set of function nodes and all the variable nodes connected to these functional nodes. Note that the function node set can be empty. Variable nodes and functional nodes represent spins and interactions in the multispin interaction model. In this setting, we can introduce the region energy ER(x R), the region internal energy UR(bR), the region entropy HR(bR) and the region free energy FR(bR) as follows:\n\nE R(x R) = − ln fa(xa),\na∈R\nUR(bR) = bR(x R)E R(x R),\nxR\nHR(bR) = − bR(x R) ln bR(x R),\nxR\nFR(bR) = UR(bR) − HR(bR),\n\n(3.21) (3.22) (3.23) (3.24)\n\nwhere x R are the variable nodes in the region R, and bR(x R) is the joint distribution of x R. The basic idea of a region-based free energy approximation is to break up the\n\n3.2 Variational Free Energy\n\n23\n\nfactor graph into regions and then sum up their contributions to approximate the true free energy, where all the variable nodes and function nodes should be summed up only once. Because overlaps between different regions cannot be avoided in a nonnaive approximation, counting numbers CR (an integer that may be zero or negative) must be introduced to avoid double calculation. Given a region set R, the total internal energy UR and entropy HR can be written as\n\nUR = CRUR(bR),\nR∈R\nHR = CR HR(bR),\nR∈R\nwith the following two constraints for counting numbers:\n\n(3.25) (3.26)\n\nI[a ∈ R]CR = 1,\nR∈R\nI[i ∈ R]CR = 1,\nR∈R\n\n(3.27) (3.28)\n\nwhere I[a ∈ R] = 1 when the function node a is in the region R, and takes zero otherwise. I[i ∈ R] has a similar meaning for variable nodes.\nIn the Bethe approximation, the factor graph is broken into two kinds of regions\n(see Fig. 3.2), which are a large region RL with one functional node and the variable nodes connected to it, and a small region RS with only one variable node. Under this division, counting numbers can be derived as CRL = 1 and CRS = 1 − di , where di is the number of the function nodes connected to the variable node i in the small region. These counting numbers can also be derived from the identity CR = 1 − S∈S(R) CS, where S(R) denotes the region set that is the set of super-regions of R. If the set of variable and function nodes in R1 are a subset of nodes in R2, then R2 is the super-region of R1 [4]. Thus, we can compute the Bethe internal energy UBethe and the Bethe entropy HBethe as follows:\n\nLarge Region Small Region\n\nVariable node Function node\n\nFig. 3.2 Region division in the Bethe approximation\n\n24\n\n3 Variational Mean-Field Theory and Belief Propagation\n\nUBethe = −\na\nHBethe = −\na\n\nba(xa) ln fa(xa),\nxa\n\nba(xa) ln ba(xa) + (di − 1) bi (xi ) ln bi (xi ),\n\nxa\n\ni\n\nxi\n\n(3.29) (3.30)\n\nwhere we replace bRL (x RL ) and bRS (x RS ) with ba(xa) and bi (xi ), respectively. The Bethe free energy is then given by\n\nFBethe = −\n\nba(xa) ln fa(xa) +\n\nba(xa) ln ba(xa)\n\na xa\n\na xa\n\n− (di − 1) bi (xi ) ln bi (xi ),\n\ni\n\nxi\n\n(3.31)\n\nwhich is the target function to minimize later. By taking into account the nearest-\n\nneighbor correlations, the trial probability distribution can also be written in a com-\n\npact form [4, 5]:\n\nbBA(x) =\n\ni\n\na ba(xa) bi (xi )di −1\n\n,\n\n(3.32)\n\nwhich is automatically normalized and exact when the factor graph is a tree, but still a good approximation when the factor graph is not tree-like. A rigorous proof is hard, but the approximation should be compared with simulations in practice. Inserting the form of bB A(x) into the Gibbs free energy, one can derive the same form as that in Eq. (3.31).\nBefore using the Lagrange multiplier method, we ﬁrst formulate the probability constraints as follows:\n\nbi (xi ) = 1, ∀i ;\nxi\nba(xa) = 1, ∀a;\nxa\nba(xa) = bi (xi ), ∀(i, a).\nxa \\xi\nFinally, the Lagrange objective function reads\n\n(3.33) (3.34) (3.35)\n\nL =−\n\nba(xa) ln fa(xa) +\n\nba(xa) ln ba(xa)\n\na xa\n\na xa\n\n− (di − 1) bi (xi ) ln bi (xi ) + λi bi (xi ) − 1\n\ni\n\nxi\n\ni\n\nxi\n\n(3.36)\n\n+ λa ba(xa) − 1 +\n\nρi,a(xi )\n\nba(xa) − bi (xi ) .\n\na\n\nxa\n\ni,a xi\n\nxa \\xi\n\n3.2 Variational Free Energy\n\n25\n\nPaoi (xi )\nai\n\nbi (xi )\n\nba (xa )\nai\n\nPioa (xi )\n\nVariable node Function node\n\nFig. 3.3 Message passing process in the BP algorithm. (Left Panel) cavity probabilities converge to a variable node; (Right panel) cavity probabilities converge to a function node\n\nAfter performing the variation on L, we can obtain the form of the spin distribution bi (xi ) and joint distribution ba(xa) [4]:\n\nbi (xi ) =\n\n1 Zi\n\na ∈∂ i\n\nPa→i (xi ),\n\nba(xa) =\n\n1 Za\n\nfa(xa)\n\nPb→i (xi ),\n\ni∈∂a b∈∂i\\a\n\n(3.37a) (3.37b)\n\nwhere we deﬁne Pa→i (xi ) and Pi→a(xi ) as the messages passing between the functional nodes and variable nodes in two directions as illustrated in Fig. 3.3. These two messages obey the following iterative equations:\n\nPa→i (xi ) =\n\nfa(xa)\n\nPj→a(x j ),\n\nx j : j ∈∂a\\i\n\nj ∈∂ a\\i\n\nPi→a(xi ) =\n\n1 Z i →a\n\nPb→i (xi ).\nb∈∂ i \\a\n\n(3.38a) (3.38b)\n\nNote that Eq. (3.38a) is compatible with the marginal probability constraint used to\nwrite the constrained Bethe free energy, while Eq. (3.38b) follows directly from the result of bi (xi ) by just excluding the function node a. Finally, the (joint) marginal probabilities bi (xi ), ba(xa) can be interpreted as beliefs and written in an explicit form as\n\nbi (xi ) ∝ Pa→i (xi ),\na ∈∂ i\nba(xa) ∝ fa(xa) Pi→a(xi ),\ni ∈∂ a\n\n(3.39a) (3.39b)\n\n26\n\n3 Variational Mean-Field Theory and Belief Propagation\n\nwhich is consistent with Eq. (3.37). Equation (3.38) is also called the belief propagation (BP) algorithm in computer\nscience, where we can perform the iteration of the messages {Pa→i (xi ), Pi→a(xi )} to their ﬁxed point and calculate the beliefs bi (xi ) and ba(xa). The ﬁxed points of the BP algorithm correspond to stationary points of the constrained Bethe free energy [6]. Depending on speciﬁc settings, the number of stationary points may be different, being ﬁnite or exponentially large. For example, if the factor graph is loopy, or the model has a complex low-temperature phase, the BP iteration may not converge, or oscillate among several solutions. Note that the cavity method allows an extension to handling the case of exponentially many states (in physics, corresponding to onestep replica symmetry breaking, see Chap. 9). The probability distributions of cavity ﬁelds over the states are then required to be introduced. We will provide an in-depth discussion about this point in Chap. 9.\nThe messages Pi→a(xi ) here can be interpreted as the probability distribution of the variable node i with the removal of function node a, which is similar to the deﬁnition in the cavity method. Actually, we can prove that the BP equations are equivalent to the cavity equations as follows. First, we substitute the expression of Pb→i (xi ) into Pi→a(xi ) in the BP equation, and we obtain\n\n1\n\nPi→a(xi ) =\n\nZ i →a\n\nb∈∂i\\a x j : j∈∂b\\i\n\nfb(xb)\nj ∈∂ b\\i\n\nPj→b(x j )\n\n=1\n\ne Jb\n\nZi→a b∈∂i\\a x j : j∈∂b\\i\n\nj∈∂b x j\n\n1 + m j→b x j .\n\n2\n\nj ∈∂ b\\i\n\n(3.40)\n\nWe then deﬁne A+b =\n\ne x j : j ∈∂b\\i Jb j∈∂b\\i x j\n\nj ∈∂ b\\i\n\n, 1+m j→b x j\n2\n\nwhere\n\nwe\n\ntake\n\nxi\n\n=\n\n+1. A−b follows the similar deﬁnition with xi = −1. Thus, Zi→a = b∈∂i\\a A+b +\n\nb∈∂i\\a A−b . After a few algebra operations, A+b and A−b can be written, respectively,\n\nas\n\n⎛\n\n⎞\n\nA+b = cosh Jb ⎝1 + tanh Jb\n\nm j→b⎠ ,\n\nj ∈∂ b\\i\n\n⎛\n\n⎞\n\n(3.41)\n\nA−b = cosh Jb ⎝1 − tanh Jb\n\nm j→b⎠ ,\n\nj ∈∂ b\\i\n\n(3.42)\n\nwhich is exactly the same as that derived by the cavity method in the previous chapter. According to the deﬁnition, we can then derive mi→a:\n\n3.2 Variational Free Energy\n\n27\n\nmi→a = Pi→a (xi )\n\nxi\n\n=\n\nb∈∂i\\a A+b − b∈∂i\\a A+b +\n\nb∈∂i\\a A−b b∈∂i\\a A−b\n\n= b∈∂i\\a (1 + tanh Jb j∈∂b\\i m j→b) − b∈∂i\\a (1 + tanh Jb j∈∂b\\i m j→b) +\n\nb∈∂i\\a (1 − tanh Jb b∈∂i\\a (1 − tanh Jb\n\nj∈∂b\\i m j→b) . j∈∂b\\i m j→b)\n(3.43)\n\nAfter introducing an auxiliary variable tanh Jb j∈∂b\\i m j→b, we ﬁnally obtain\n⎛\n\nub→i through ⎞\n\nmi→a = tanh ⎝\n\nub→i ⎠ ,\n\nb∈∂ i \\a\n\ntanh ub→i = tanh Jb\n\nm j→b,\n\nj ∈∂ b\\i\n\ntanh ub→i =\n(3.44a) (3.44b)\n\nwhich is the standard cavity equation when β = 1. The Bethe approximation is merely a pair approximation of a more general\nmethod—cluster variational method [5]. The cluster variational method is able to treat arbitrary large clusters of correlated sites, and yet, the computational complexity increases. Recent developments also include loop corrections for probabilistic inference on factor graphs [7, 8].\n\n3.2.3 From the Bethe to Naive Mean-Field Approximation\n\nIn the naive mean-ﬁeld approximation, we use a factorized form of the trial probability distribution that neglects the correlation among spins. In contrast, the Bethe\n\napproximation considers a short-range correlation among spins, where it is expected\n\nthat in a high temperature, even these short-range correlations become unimportant,\n\nand thus, the naive mean-ﬁeld approximation will be recovered. More precisely, we\n\ntake an example of a two-body interaction model. Suppose our model is a two-body interaction model with inverse temperature β. In this setting, the mean-ﬁeld iteration\n\nequations are given by\n\nmi = tanh β Ji j m j .\nj ∈∂ i\n\n(3.45)\n\nNext, we derive this equation from the Bethe approximation. In the Bethe approximation, the cavity iteration equations are given by\n\n28\n\n3 Variational Mean-Field Theory and Belief Propagation\n\nmi→a = tanh\n\nub→i ,\n\nb∈∂ i \\a\n\ntanh ub→i = tanh β Jb\n\nm j→b,\n\nj ∈∂ b\\i\n\nwhere mi→a can be derived as\n\n⎛\n\n⎞\n\nmi→a = tanh ⎝\n\nub→i ⎠\n\nb∈∂ i \\a\n\n(3.46a) (3.46b)\n\n= tanh\n\nub→i − ua→i\n\nb∈∂ i\n= tanh(tanh−1(mi ) − ua→i )\n\n= mi − tanh β Ja j∈∂a\\i m j→a , 1 − mi tanh β Ja j∈∂a\\i m j→a\n\n(3.47)\n\nwhere\n\nwe\n\nhave\n\nused\n\nthe\n\nidentity\n\ntanh(x\n\n+\n\ny)\n\n=\n\ntanh x+tanh y 1+tanh x tanh y\n\n.\n\nConsidering\n\nthe\n\ntwo-\n\nbody interaction, we have\n\nmi→ j\n\n=\n\nmi − tanh β Ji j m j→i 1 − mi tanh β Ji j m j→i\n\n,\n\nm j→i\n\n=\n\nmj 1−\n\n− tanh β Ji j mi→ j m j tanh β Ji j mi→ j\n\n.\n\n(3.48a) (3.48b)\n\nWe can then eliminate mi→ j and m j→i in the cavity equation, by obtaining the noncavity functions of m j→i and mi→ j as a function of single magnetizations. We ﬁrst have the following expressions based on Eq. (3.48) [9]:\n\nmi→ j = f (mi , m j , tanh β Ji j ),\n\nm j→i = f (m j , mi , tanh β Ji j ),\n\nf (a, b, t) = 1 − t2 −\n\n(1\n\n− t 2)2 2t (b\n\n− −\n\n4t (a at)\n\n−\n\nbt\n\n)(b\n\n−\n\nat)\n\n.\n\nThus, we can write a non-cavity version of mi as follows:\n\n(3.49) (3.50)\n(3.51)\n\nmi = tanh\n\ntanh−1( f (m j , mi , tanh β Ji j ) tanh β Ji j ) .\n\nj ∈∂ i\n\n(3.52)\n\nSince we assume β Ji j is weak (e.g., in a high-temperature phase), we can perform\n\nthe\n\nTaylor\n\nexpansions\n\nlike\n\ntanh−1\n\nx\n\n≈\n\nx,\n\ntanh\n\nx\n\n≈\n\nx,\n\n(1\n\n+\n\nx )a\n\n≈\n\n1\n\n+\n\nax\n\n+\n\n1 2\n\na(a\n\n−\n\n1)x2, when x is a small quantity, and we ﬁnally get\n\n3.2 Variational Free Energy\n\n29\n\nmi = tanh\n\n(β Ji j m j\n\n−\n\nβ\n\n2\n\nJi2j\n\n(1\n\n−\n\nm\n\n2 j\n\n)mi\n\n)\n\n,\n\nj ∈∂ i\n\n(3.53)\n\nwhere the second term in the summation is called the Onsager reaction term, a characteristic of a high-temperature expansion solution of a spin glass model [10, 11], which we shall introduce in more details later. Neglecting the second-order term of couplings, one recovers the naive mean-ﬁeld equation.\n\n3.3 Mean-Field Inverse Ising Problem\n\nIn the previous sections, we describe how to ﬁnd the statistical physics solutions of an equilibrium thermodynamic problem under some approximations, which is exactly a direct problem. However, if we acquire data samples from an unknown model, we can predict the model parameters, e.g., couplings and ﬁelds, from these raw data samples, which is called the inverse problem. The direct problem can provide insights into the inverse problem. Let us explain this in more details.\nAn Ising model considering only up to pairwise interactions is described by\n\nH (σ ) = − Ji j σi σ j − hi σi ,\n\ni<j\n\ni\n\nP(σ ) =\n\n1 e\n\ni< j Ji j σi σ j +\n\n. i hi σi\n\nZ\n\n(3.54a) (3.54b)\n\nNote that β has been absorbed into the model parameters in the current setting. Given measured magnetizations mi = σi data and correlation functions Ci j = σi σ j data − mi m j , what we want to estimate is the coupling constants and external ﬁelds {Ji j , hi }, which is a typical unsupervised learning problem. This is exactly the Boltzmann machine learning [12]. It starts from a set of initial parameters {Ji j , hi } and then updates the parameters by an increment:\n\nJi j =η( σi σ j data − σi σ j Ising), hi =η( σi data − σi Ising),\n\n(3.55a) (3.55b)\n\nwhere η is a predeﬁned learning rate. The iteration runs until the model average and data average match with each other within a certain accuracy. The model average can be estimated by the Monte Carlo algorithms, which we shall introduce in the following chapter. However, when the system size is large, the mean-ﬁeld method is relatively fast.\nTo carry out the inference, we ﬁrst compute the magnetization:\n\nmi\n\n=\n\n∂ log Z ( Ji∗j , hi∗) ∂hi\n\n=\n\n{σ }\n\nσi e\n\ni< j Ji∗j σi σ j +\nZ\n\ni hi∗σi\n,\n\n(3.56)\n\n30\n\n3 Variational Mean-Field Theory and Belief Propagation\n\nand then we apply the ﬂuctuation-response theorem [13]:\n\n∂mi ∂h j\n\n=\n\n{σ }\n\nσi σ j e\n\ne − σi\n{σ }\n\ni< j Ji∗j σi σ j +\nZ\ni< j Ji∗j σi σ j +\nZ\n\ni hi∗σi i hi∗σi\n\ne σj\n{σ }\n\ni< j Ji∗j σi σ j +\nZ\n\ni hi∗σi\n\n=Ci j = σi σ j data − mi m j .\n\n(3.57)\n\nThe symbol with the superscript ∗ indicates the current estimates of the model\n\nparameters. These steps amount to the expectation step of a standard Expectation-\n\nMaximization procedure [14]. The updating procedure in Eq. (3.55) corresponds to\n\nthe M step.\n\nUsing the above relationship Ci j\n\n=\n\n∂mi ∂h j\n\nand the naive mean-ﬁeld equation mi\n\n=\n\ntanh(hi + k=i Jik mk ), we get\n\nCi j = (1 − mi2) δi j + Jik Ck j ,\nk =i\nC = P + PJC,\n\n(3.58)\n\nwhere\n\nP\n\nis\n\na\n\ndiagonal\n\nmatrix\n\nwith\n\nPi j\n\n=\n\n(1 −\n\nm\n\n2 i\n\n)δi\n\nj\n\n.\n\nFinally,\n\nwe\n\nobtain\n\nthe\n\nnaive\n\nmean-ﬁeld (nMF) solution of the inverse Ising problem:\n\nJinjMF = (P)i−j1 − (C)i−j1.\n\n(3.59)\n\nThe external ﬁelds can then be reconstructed based on the predicted couplings. The naive mean-ﬁeld solution is the simplest one among other mean-ﬁeld methods, including high-temperature expansion, small-correlation expansion and the Bethe approximation [2, 15].\n\nReferences\n1. J.M. Yeomans, Statistical Mechanics of Phase Transitions (Oxford University Press, Oxford, 1992)\n2. H. Huang, Y. Kabashima, Phys. Rev. E 87, 062129 (2013) 3. H.A. Bethe, Proc. R. Soc. Lon. Ser. A-Math. Phys. Sci. 150(871), 552 (1935) 4. J. Yedidia, W. Freeman, Y. Weiss, IEEE Trans. Inf. Theory 51(7), 2282 (2005) 5. A. Pelizzola, J. Phys. A 38(33), R309 (2005) 6. T. Heskes, Neural Comput. 16(11), 2379 (2004) 7. J.M. Mooij, H.J. Kappen, J. Mach. Learn. Res. 8(40), 1113 (2007) 8. J.Q. Xiao, H. Zhou, J. Phys. A: Math. Theor. 44(42), 425001 (2011) 9. F. Ricci-Tersenghi, J. Stat. Mech.: Theory Exper. 2012(8), 8015 (2012) 10. D.J. Thouless, P.W. Anderson, R.G. Palmer, Philos. Mag. 35(3), 593 (1977)\n\nReferences\n\n31\n\n11. T. Plefka, J. Phys. A 15(6), 1971 (1982) 12. D.H. Ackley, G.E. Hinton, T.J. Sejnowski, Cognit. Sci. 9(1), 147 (1985) 13. H.J. Kappen, F.B. Rodríguez, in Advances in Neural Information Processing Systems (1998),\npp. 280–286 14. A.P. Dempster, N.M. Laird, D.B. Rubin, J. R. Stat. Soc. Ser. B 39, 1 (1977) 15. H.C. Nguyen, R. Zecchina, J. Berg, Adv. Phys. 66(3), 197 (2017)\n\nChapter 4\nMonte Carlo Simulation Methods\n\nA few systems in equilibrium physics can be analytically solved. It is, therefore, necessary to develop numerical techniques to estimate the equilibrium properties of a physics system. For example, given the Hamiltonian of the Ising model, it still requires O(2N ) time complexity to directly compute expected energy, where N is the number of spins. To either check how accurate a crude approximation is, e.g., meanﬁeld approximation or the Bethe approximation, or estimate the typical energy level of a statistical mechanics model that cannot be analytically solved, we rely on the Monte Carlo simulation techniques, including their variants, which are widely used not only in the physics ﬁeld itself but also in the machine learning community. For example, the Gibbs sampling is performed with the classical Monte Carlo methods or its variants with the help of importance sampling. In this chapter, we will introduce the basic knowledge about the sampling method and its applications to standard physics models.\n\n4.1 Monte Carlo Method\nThe main idea of the Monte Carlo method is simple. For example, calculating a multi-dimensional integral can be carried out by drawing a set of samples according to a predeﬁned distribution. We ﬁrst introduce the standard steps to implement the Monte Carlo method:\n• Transforming the original problem of interest to a statistical problem, like calculating the expectation of some random variables under a speciﬁc distribution.\n• Sampling random variables from the speciﬁc distribution. • Using the samples from the second step to compute any quantity of interest and\nobtaining the result of the problem.\nWe give here a representative example of estimating an integral or a sum:\n\n© Higher Education Press 2021\n\n33\n\nH. Huang, Statistical Mechanics of Neural Networks,\n\nhttps://doi.org/10.1007/978-981-16-7570-6_4\n\n34\n\n4 Monte Carlo Simulation Methods\n\nA = A(x) f (x)dx, (4.1)\nA = A(x) p(x).\nx\nTo calculate the above expectations, one can sample random variables from the distribution f (x) or p(x) and then obtain a sample collection {x1, x2, x3, . . . , xM } of the size M. Finally, { A(x1), A(x2), A(x3), . . . , A(xM )} can be obtained. As A(xi ) is independently estimated, the law of large number implies that\n\nlim P\nM →∞\n\n1 M\n\nM\nA(xi ) −\ni =1\n\nA\n\n<\n\nThus, the expectation can be estimated as\n\n= 1, ∀ > 0.\n\n(4.2)\n\n1 A M A(xi ).\nxi\n\n(4.3)\n\nAs the collected samples {x1, x2, x3, . . . , xM } are independent and identically dis-\n\ntributed, the statistical error due to the sampling is related to the variance of A and\n\ncan\n\nbe\n\nestimated\n\nto\n\nbe\n\nof\n\nthe\n\norder\n\nof\n\nO(\n\nM\n\n−\n\n1 2\n\n)\n\n[1].\n\nMoreover,\n\nthe\n\nMonte\n\nCarlo\n\nestimator is unbiased. Given a large number of the Monte Carlo samples, the empir-\n\nical estimation converges to the true expectation we want to compute [2]. Interested\n\nreaders can ﬁgure out the procedure as the above description to estimate the integral\n\n∞ −∞\n\nx\n\n2\n\nDx,\n\nwhere\n\nDx\n\nindicates\n\nthe\n\nrandom\n\nvariable\n\nx\n\nis\n\na\n\nstandard\n\nGaussian\n\nvariable.\n\nThe Monte Carlo estimation can be compared with the analytic result of 1. As the\n\nnumber of random samples increases, the estimation will approach the exact result.\n\nIn the remaining chapters, we will also show this kind of method is also very effec-\n\ntive and popular to solve the saddle-point equations of the replica method applied to\n\nsolve a variety of neural network models.\n\n4.2 Importance Sampling\nIn the Monte Carlo simulation, sampling a distribution is usually required, e.g., the Gaussian distribution as mentioned in the previous section. Unfortunately, most distributions are very challenging to sample, e.g., the Boltzmann distribution in statistical physics. Here, we shall introduce some basic strategies to generate random samples from distributions that are more complicated than the commonly used ones, such as uniform, Poisson and Gaussian distributions.\nBy introducing a simple trial distribution, say q(x) that is easy to sample, we can recast Eq. (4.1) as\n\n4.2 Importance Sampling\n\n35\n\nA(x) f (x)\n\nA=\n\nq (x)d x,\n\nq (x)\n\nA(x) p(x)\n\n(4.4)\n\nA=\n\nq (x).\n\nq (x)\n\nx\n\nThe expectations\n\nA f and\n\nA p are then transformed to\n\nA(x) f (x) q (x)\n\nq , and\n\nA(x) p(x) q (x)\n\nq.\n\nTherefore, we can ﬁrst sample the distribution q(x) to get samples {x1, x2, x3, . . . ,\n\nxM } and then calculate the expectations:\n\nA\n\nM i =1\n\nA(xi ) p(xi\n\n)/q(xi ) ,\n\nM\n\n(4.5)\n\nwhere\n\nthe\n\nfactor\n\np(xi ) q(xi )\n\ncan\n\nbe\n\nthought\n\nof\n\nas\n\nan\n\nimportance\n\nweight\n\nof\n\nthe\n\nsample\n\nxi in computing the expectation. This estimation is, thus, called the importance\n\nsampling [2]. When q(xi ) = p(xi ), the importance sampling turns out to be Eq. (4.3).\n\nChoosing a trial distribution is important; otherwise, the Monte Carlo estimation\n\nwill become noisy with a large variance, being very slow to converge to a quantity\n\nof satisﬁed accuracy. An annealed importance sampling is introduced to build a\n\nsuitable q(x) starting from a trivial one [ p0(x)]. A common scheme specifying the intermediate distribution is given by\n\np j (x) = p0(x)1−βj pn(x)βj ,\n\n(4.6)\n\nwhere 0 = β0 < β1 < · · · < βn = 1. In other words, p j (x) interpolates between p0(x) and pn(x) = p(x). The samples can then be sequentially generated by designing an appropriate transition probability of two states. Interested readers can ﬁnd the\noriginal paper [3] for implementation details.\n\n4.3 Markov Chain Sampling\n\nTo realize a sampling where a sequence of samples are generated, one can construct\n\na Markov chain during sampling. The Markov property implies that the next state of\n\na dynamics is only related to the current state, and the conditional probability can be\n\nwritten as\n\nP St+1|S1, . . . , St = P St+1|St ,\n\n(4.7)\n\nwhere St is the state at time t. A Markov chain obeys the Markov property for its dynamics. One can construct a time-homogeneous Markov chain by setting up an initial distribution π(S0) together with the transition probability W (S → S ). A stationary distribution π(S ) can, thus, be identiﬁed by satisfying the following\ncondition:\n\n36\n\n4 Monte Carlo Simulation Methods\n\nπ(S ) = W (S → S )π(S).\n\n(4.8)\n\nS\n\nThe task of designing a Markov chain becomes simple if the detailed balance criterion\n\nis obeyed [1], i.e.,\n\nW (S → S )π(S) = W (S → S)π(S ).\n\n(4.9)\n\nThe detailed balance criterion guarantees that the designed Markov chain converges to the target distribution [Eq. (4.8)] [1].\n\n4.4 Monte Carlo Simulations in Statistical Physics\n\nIn statistical physics, an equilibrium system is described by the Boltzmann distribu-\n\ntion:\n\nPeq(s) =\n\n1 e−βH(s), Z\n\n(4.10)\n\nwhere the partition function Z = s e−βH(s), and H(s) is the system’s Hamiltonian. Then the expectation or thermal average of an observable O(s) is given by\n\nO =1\n\nO( s)e−β H (s) .\n\nZs\n\n(4.11)\n\nThe partition function is usually intractable, making an analytic estimation of thermodynamic quantities impossible. The Markov Chain Monte Carlo (MCMC) is then useful for estimating the quantities of interest. To illustrate the MCMC method, we simulate the SK model as an example. The SK model is a fully connected mean-ﬁeld glass model, and the statistical mechanics properties were ﬁrst studied analytically in the seminal work [4]. The Hamiltonian is given by\n\n1\n\nH\n\n=\n\n− 2\n\ni=j\n\nJi j σi σ j ,\n\n(4.12)\n\nwhere the spin σi = ±1, and the couplings follow independently a Gaussian distribution of zero mean and variance 1/N . The model has a paramagnetic-to-spin glass transition at the critical temperature T = 1. By using the MCMC method, we can acquire the equilibrium properties of the SK model, which can be compared with the theoretical analysis.\nNext, we introduce two Monte Carlo techniques to numerically evaluate the model. But we emphasize that both methods are generally applicable to other similar models, for which an exact computation of relevant thermodynamic quantities may be impossible.\n\n4.4 Monte Carlo Simulations in Statistical Physics\n\n37\n\n4.4.1 Metropolis Algorithm\n\nThe detailed balance condition of the Boltzmann distribution can be written as fol-\n\nlows:\n\nPeq(si )W (si → s j ) = Peq(s j )W (s j → si ),\n\n(4.13)\n\nwhere W (si → s j ) is the transition probability from state si to state s j . The ratio between two transition probabilities can be rewritten as\n\nW (si → s j ) = e−β H(si ,s j ), W (s j → si )\n\n(4.14)\n\nwhere H(si , s j ) = H(s j ) − H(si ), and the Boltzmann distribution is used. Our purpose is to ﬁnd a transition probability matrix satisfying the detailed balance condition. In fact, choosing the transition probability form is not unique, and there are two frequently used forms. One is the Metropolis algorithm:\n\nW (si → s j ) =\n\n1, e−β H(si ,s j ),\n\nH(si , s j ) < 0; H(si , s j ) ≥ 0,\n\n(4.15)\n\nwhich can be also recast into the form W (si → s j ) = min(e−β H(si ,sj ), 1). A pseudocode is given in Algorithm 4.1. Another popular choice is the heat-bath algorithm:\n\ne−β H(si ,s j ) W (si → s j ) = 1 + e−β H(si ,sj ) .\n\n(4.16)\n\nIt can be veriﬁed that the Metropolis dynamics is always more likely to accept an\n\nattempt of spin changes that leads to a small change of energy. In addition, if we\n\ndeﬁne the transition probability as a function F(e−β H ), it can be also veriﬁed that\n\nthe\n\nabove\n\ntwo\n\nchoices\n\nsatisfy\n\nF(x) F (1/x )\n\n=x\n\nfor\n\nall\n\nx,\n\ncompatible\n\nwith\n\nthe\n\ndetailed\n\nbalance criterion.\n\nFor a fast sampling, we can ﬂip just one single spin (rather than a small group of\n\nspins) at each step of the Metropolis dynamics, and then, we can obtain the following\n\ntransition rule:\n\n1\n\nW (σi\n\n→\n\n−σi )\n\n=\n\n[1 2\n\n−\n\nσi\n\ntanh βhi ],\n\n(4.17)\n\nwhere hi = j=i Ji j σ j is the local ﬁeld acting on the spin σi . This rule is derived from the heat-bath choice.\nA random initial state is far from equilibrium with a high probability, and thus, a Markov chain dynamics requires a relaxation time for the system to reach the equilibrium state. This time scale is called the equilibration time τeq . In practice, τeq is measured in the unit of the Monte Carlo sweep (MCS), in which one MCS equals to N proposed single-spin-updates. To verify whether the system arrives at equilibrium, it is necessary in practice to check the evolution of some observables, e.g., energy.\n\n38\n\n4 Monte Carlo Simulation Methods\n\nFig. 4.1 Evolutions of the energy density of the SK model with N = 500 and Ji j ∼ N(0, 1/N ). a Metropolis Monte Carlo simulation. b Parallel Tempering Monte Carlo. The dashed lines are the corresponding predictions of replica theory. The time step is a measure in the unit of the Monte Carlo step (MCS). Each step means a sweep of all spins for the proposed update. β deﬁnes the inverse temperature\n\nAlgorithm 4.1 Metropolis Algorithm\n\nInput: The number of samples M, temperature T , τeq , δt Output: A collection of samples\n\n1: Initialize conﬁguration S randomly;\n\n2: Initialize i = 0;\n\n3: Initialize counter = 0; 4: while (i< M) do\n\n5: generate a trial state S ;\n\n6: compute W (S → S|T );\n\n7: if W > rand(0, 1) then\n\n8:\n\nS=S\n\n9: if [(counter > τeq ) and (counter % δt==0))] then\n\n10:\n\nAppend S to the sample collection.\n\n11:\n\ni = i+1\n\n12: counter = counter + 1 13: return the sample collection.\n\nAs shown in Fig. 4.1a, the energy of the SK model arrives at equilibrium at about τeq MCSs, which depends on the temperature. After the relaxation, the energy ﬂuctuates around a typical value, which could be predicted by theory. Therefore, samples can be collected after τeq MCSs to estimate equilibrium values of thermodynamic quantities of interest.\nEven if the dynamics reaches a steady state, an independent sampling of the equilibrium state requires a certain number of MCSs separating two consecutive samplings. Therefore, we need to compute a time-dependent autocorrelation function of any observable O:\n\nCO(t) =\n\nO (t0) O (t0 + t) O2(t0)\n\n− −\n\nO(t0) O(t0)\n\nO(t0 + t)\n2\n\n,\n\n(4.18)\n\n4.4 Monte Carlo Simulations in Statistical Physics\n\n39\n\nFig. 4.2 The relaxation dynamics of the autocorrelation function for the same SK model deﬁned in Fig. 4.1\n\nwhere · indicates a thermal average, and t0 denotes the starting time. In general, CO(t) ∼ exp (−t/τauto), and τauto is the corresponding time scale. The correlation\n\nlength diverges at a continuous phase transition, while the autocorrelation time also\n\ndiverges at the transition, which is also called the critical slowing down phenomenon.\n\nIn\n\nglass\n\nphysics,\n\nthe\n\nEdwards–Anderson\n\norder\n\nparameter\n\nqEA\n\n=\n\n1 N\n\ni σi 2 [5],\n\nwhich can be treated as the long-time limit of the time-dependent autocorrela-\n\ntion function\n\nqEA\n\n=\n\nlimt→∞ C(t), where C(t)\n\n=\n\n1 N\n\ni σi (0)σi (t) . The Edwards–\n\nAnderson order parameter can also be used to detect ergodicity breaking. A typical\n\nexample of the autocorrelation proﬁle is shown in Fig. 4.2 for the SK model at\n\ndifferent temperatures.\n\n4.4.2 Parallel Tempering Monte Carlo\nWhen we are interested in a low-temperature phase for a spin glass model (e.g., the Sherrington–Kirkpatrick model, the Hopﬁeld model, etc.), the Metropolis algorithm is easy to get trapped in a local minimum, once the Gibbs measure is decomposed into an exponential (in the number of degrees of freedom) number of metastable states. In general, there does not exist one efﬁcient local dynamics method overcoming this challenging fair-sampling problem. However, there do exist a variety of sampling heuristics. One well-known example is the simulated annealing [6], where the starting temperature for the Metropolis sampling is much higher than the target low temperature, and the dynamics is run at each intermediate decreasing temper-\n\n40\n\n4 Monte Carlo Simulation Methods\n\nature for a certain number of MCSs, and ﬁnally, a ground state of lower energy is\nexpected to be reached by the annealing process.\nThe other more efﬁcient one is the parallel tempering method [7], focusing on\novercoming energy barriers by simulating several copies of the original system at\ndifferent temperatures. In this method, M replicas without interaction, which means replicas are independent, are used to construct an ensemble. The mth replica has the original Hamiltonian H(Xm) and obeys the Boltzmann distribution with an inverse temperature βm. The corresponding inverse temperatures satisfy βm < βm+1 for convenience. Then the state of the ensemble can be described by an extended state {X } = {X1, X2, . . . , X M }, and the partition function of the ensemble is given by\n\nM\n\nM\n\nZ = exp − βmH (Xm) = Z (βm) ,\n\n{X}\n\nm=1\n\nm=1\n\n(4.19)\n\nwhere Z (βm) is the partition function of the original system with βm. The probability of the extended state with a temperature set can be written as\n\nP({X, β}) =\n\nM\n\nPeq\n\n(Xm , βm )\n\n=\n\n1 Z\n\nexp\n\nM\n− βmH (Xm)\n\n.\n\nm=1\n\nm=1\n\n(4.20)\n\nTo construct the detailed balance condition, we only consider exchanging conﬁgurations between two replicas. For example, the extended state {. . . ; X, βm; . . . ; X , βn; . . .} changes to {. . . ; X , βm; . . . ; X, βn; . . .} with a transition probability W (X , βm; X, βn|X, βm; X , βn). The detailed balance condition can, thus, be written as\n\nP({. . . ; X, βm; . . . ; X , βn; . . .})W (X , βm; X, βn|X, βm; X , βn)\n\n(4.21)\n\n= P({. . . ; X , βm; . . . ; X, βn; . . .})W (X, βm; X , βn|X , βm; X, βn).\n\nIt is then easy to derive the ratio between the two transition probabilities:\n\nwhere\n\nW (X , βm; X, βn|X, βm; X , βn) = exp(− ), W (X, βm; X , βn|X , βm; X, βn)\n= (βn − βm) H(X ) − H X .\n\n(4.22) (4.23)\n\nA reasonable choice of the transition probability can then be expressed as follows:\n\nW (X , βm; X, βn|X, βm; X , βn) =\n\n1,\n\nfor\n\nexp(− ), for\n\n< 0, > 0.\n\n(4.24)\n\nIn sum, the parallel tempering Monte Carlo can be implemented by the following procedure. First, using the conventional MCMC method to simulate each replica in the\n\n4.4 Monte Carlo Simulations in Statistical Physics\n\n41\n\nensemble for a certain number of MCSs. Then conﬁgurations of two neighboring temperatures are exchanged with the transition probability W (X , βm; X, βm+1|X, βm; X , βm+1). In general, arbitrary pairs of replicas (say, at two different temperatures Tn and Tm) with associated microscopic conﬁgurations can undergo temperature switching [8]. We remark that the probability for the temperature exchange between\nnonadjacent replicas decreases exponentially, yet essential to speed up crossing the\nhigh energy barriers [8]. Finally, an expectation of any observable O can be obtained:\n\n1M O βm = M O (Xm(t)) .\nt =1\n\n(4.25)\n\nA pseudo-code for the parallel tempering method is shown in Algorithm 4.2.\n\nAlgorithm 4.2 Parallel tempering Monte Carlo\n\nInput: The number of samples L, βmax , βmin, and the number of temperatures M.\n\nOutput: Sample collection.\n\n1: Initialize β1 = βmin, βM = βmax ;\n\n2:\n\nLinear\n\ninitialization\n\nof\n\nβ:\n\nβm\n\n=\n\nβ1\n\n+\n\n(βM\n\n−\n\nβ1)\n\nm−1 M −1\n\n;\n\n3: Initialize the extended state randomly: {X } = {X1, X2, . . . , X M };\n\n4: Initialize i = 0;\n\n5: Initialize counter = 0; 6: while (i< L) do\n\n7: Applying the MCMC (e.g., the Metropolis method) for each replica for a few MCSs\n\n8: for βm in {β1, β2, . . . , βM−1} do\n\n9:\n\ncompute = (βm+1 − βm ) (H(Xm ) − H (Xm+1)).\n\n10:\n\nif exp(− ) > rand(0, 1) then\n\n11:\n\nSwap Xm and Xm+1.\n\n12: Append {X } to the sample collection.\n\n13: i = i+1\n\n14: return the sample collection.\n\nA high-temperature phase has a fast dynamics, while a low-temperature phase\nhas a very slow dynamics, due to the potential rugged energy landscape. To ensure a proper acceptance ratio, the acceptance probability e− should be of order of one.\nAccording to Eq. (4.23), one has\n\n−\n\n=\n\nδ\n\n(H\n\n( X n+1 )\n\n−\n\nH\n\n( X n ))\n\n∼\n\nδ2\n\nd dβ\n\nE,\n\n(4.26)\n\nwhere δ indicates the small inverse-temperature difference, and E = H is the mean thermal energy and is an extensive quantity. To ensure the acceptance probability is of order one, the difference between √neighboring temperatures δ should be of order √1 , implying that a number of order N of replicas are required [7]. In essence, the\nN\n\n42\n\n4 Monte Carlo Simulation Methods\n\nnew conﬁguration from the fast mixing chain allows the chains at a low temperature to sample the state space more efﬁciently, compared with a pure Metropolis local dynamics.\n\nReferences\n1. U. von Toussaint, Rev. Mod. Phys. 83(3), 943 (2011) 2. H.G. Katzgraber (2009). arXiv:0905.1629 3. R.M. Neal, Stat. Comput. 11(2), 125 (2001) 4. D. Sherrington, S. Kirkpatrick, Phys. Rev. Lett. 35(26), 1792 (1975) 5. S.F. Edwards, P.W. Anderson, J. Phys. F: Met. Phys. 5(5), 965 (1975) 6. S. Kirkpatrick, C.D. Gelatt, M.P. Vecchi, Science 220(4598), 671 (1983) 7. K. Hukushima, K. Nemoto, J. Phys. Soc. Jpn. 65(6), 1604 (1996) 8. C.E. Fiore, M.G.E. da Luz, Phys. Rev. E 82, 031104 (2010)\n\nChapter 5\nHigh-Temperature Expansion\n\nIn this chapter, we introduce one important theoretical technique—high-temperature expansion, to derive the Thouless–Anderson–Palmer (TAP) equation, a seminal equation in standard spin glass theory (Thouless et al. in Phil. Mag. 35(3):593, 1977 [1]; Plefka in J. Phys. A 15(6):1971, 1982 [2]; Georges and Yedidia in J. Phys. A: Math. Gen. 24:2173, 1991 [3]). This technique is quite popular and useful even in machine learning community, acting as a perturbation analysis to derive efﬁcient algorithms for inference and learning (Maillard et al. in J. Stat. Mech.: Theory Exper. 2019(11):113301, 2019 [4]).\n\n5.1 Statistical Physics Setting\n\nIn statistical physics, given a Hamiltonian H , the corresponding partition function\n\nis deﬁned as\n\nZ = e−β H(σ ),\n\n(5.1)\n\nσ\n\nwhich is the normalization constant of the Boltzmann distribution. The inverse temperature β = 1/T , and σ denotes the conﬁguration vector. The average of any thermodynamic quantity A(σ ) with respect to the Boltzmann distribution is given by\n\nA = A(σ )P(σ ),\n\n(5.2)\n\nσ\n\nwhere\n\nthe Boltzmann\n\ndistribution\n\nP(σ )\n\n=\n\n. e−β H(σ )\nZ\n\nThe internal\n\nenergy\n\nE(β) is, thus,\n\ndeﬁned as\n\nE(β) = H = H (σ )P(σ ).\n\n(5.3)\n\nσ\n\nAccording to the probabilistic interpretation, the entropy is deﬁned as\n\n© Higher Education Press 2021\n\n43\n\nH. Huang, Statistical Mechanics of Neural Networks,\n\nhttps://doi.org/10.1007/978-981-16-7570-6_5\n\n44\n\n5 High-Temperature Expansion\n\nS(β) = − P(σ ) ln P(σ ).\n\n(5.4)\n\nσ\n\nThe Helmholtz free energy is, thus, deﬁned by\n\nF (β )\n\n=\n\nE (β )\n\n−\n\nT\n\nS(β)\n\n=\n\n−\n\n1 β\n\nln\n\nZ (β).\n\n(5.5)\n\nIn a complex system, like a neural network, the Boltzmann distribution is com-\n\nmonly hard to compute (including uniform sampling). However, the variational\n\nmethod approximates the intractable distribution P(σ ) by Q(σ ) which belongs to\n\na family M of tractable distributions. The distribution Q is chosen such that it\n\nminimizes a certain distance measure D(Q, P) within the family M. For example,\n\nD(Q, P) can be chosen as the Kullback–Leibler (KL) divergence between Q and\n\nP:\n\nQ(σ )\n\nQ\n\nKL(Q||P) = Q(σ ) ln\n\n= ln ,\n\n(5.6)\n\nσ\n\nP(σ )\n\nPQ\n\nwhere\n\n···\n\nQ\n\ndenotes\n\nan\n\nexpectation\n\nwith\n\nrespect\n\nto\n\nQ.\n\nInserting\n\nP(σ )\n\n=\n\ne−β H (σ ) Z\n\ninto Eq. (5.6), we get\n\nKL(Q||P) = ln Z + β E[Q] − S[Q] = ln Z + β F[Q],\n\n(5.7)\n\nwhere the variational energy is then deﬁned by\n\nE[Q] = Q(σ )H (σ ),\n\n(5.8)\n\nσ\n\nand the entropy of the trial distribution Q is given by\n\nS[Q] = − Q(σ ) ln Q(σ ).\n\n(5.9)\n\nσ\n\nThe variational free energy is, thus, given by\n\nF[Q] = E[Q] − T S[Q].\n\n(5.10)\n\nWe remark that F[Q] constructs an upper bound to the Helmholtz free energy, due\n\nto the non-negativity of the KL divergence. The bound is tight once Q = P.\n\nTo proceed, we introduce the Gibbs free energy Gβ(m) under the distribution Q\n\nas follows:\n\nGβ (m) = min{F[Q]| σ Q = m}.\nQ\n\n(5.11)\n\n5.1 Statistical Physics Setting\n\n45\n\nThe Helmholtz free energy is just a thermodynamic value equal to E − T S at equilibrium, but the Gibbs free energy is a function that gives the value of E − T S when some constraints (e.g., magnetizations) are applied. The advantage of working with a Gibbs free energy instead of a direct computation of the Helmholtz free energy is that it is much easier to apply intuitive approximations, as we explain below.\nWe then minimize the Gibbs free energy in the following steps. First, we constrain the minimization in the family of distributions satisfying σ Q = m for ﬁxed m. By adding a Lagrange multiplier λ, we obtain\n\nGβ (m,\n\nλ)\n\n=\n\nE[Q]\n\n−\n\nT\n\nS[Q]\n\n−\n\n1 β\n\nλi ( σi Q − mi )\n\ni\n\n=\n\nσ\n\nQ(σ )H (σ )\n\n−\n\nT\n\nS[Q]\n\n−\n\n1 β\n\nσ\n\nλi σi\n\nQ(σ )\n\n+\n\n1 β\n\nλi mi\n\ni\n\ni\n\n=\n\nσ\n\nQ(σ )[H (σ ) − 1 β\n\ni\n\nλi σi ]\n\n−\n\nT\n\nS[Q]\n\n+\n\n1 β\n\ni\n\nλi mi .\n\n(5.12)\n\nEquation (5.12) is of the form of the variational free energy [Eq. (5.10)], where H (σ )\n\nis replaced by H (σ ) −\n\ni\n\nλi β\n\nσi\n\n.\n\nHence,\n\nthe\n\nvalid\n\ndistribution\n\nis\n\ngiven\n\nby\n\ne−β H (σ )+ i λi σi\n\nQλ(σ ) =\n\n, Zλ\n\n(5.13)\n\nwhere Zλ = σ e−β H(σ)+ i λi σi . This equation comes from the fact that the variational free energy takes a minimum when Q is the Boltzmann distribution P(σ ). Inserting this distribution back into Eq. (5.12) yields\n\nGβ (m, λ)\n\n=\n\n−\n\n1 β\n\nln\n\nσ\n\ne−β H (σ )+\n\ni λi σi + 1 β\n\ni\n\nλi mi\n\n= − 1 ln\n\ne−β H (σ )+ i λi σi − . i λi mi\n\nβσ\n\n(5.14)\n\nThe constraint σ Q = m has been enforced by the Lagrange multiplier λ that is determined by\n\nβ\n\nGβ\n\n(m)\n\n=\n\nmax\nλ\n\n− ln\n\ne−β H (σ )+ + i λi σi\n\nλi mi .\n\nσ\n\ni\n\n(5.15)\n\nThe max operation is related to the property of the Hessian matrix. By using the Lagrangian multiplier method, we carry out the derivatives:\n\n∂\n\n−βGβ (m, λ) ∂ λi\n\n=\n\nσi Q − mi = 0\n\n⇒ mi = σi Q,\n\n(5.16)\n\n46\n\n5 High-Temperature Expansion\n\n∂ −βGβ (m, λ) ∂mi\n\n=\n\nj\n\n∂ ∂λj\n\n−β Fβ (λ)\n\n∂λj ∂mi\n\n− λi\n\n−\n\nj\n\n∂λj ∂mi\n\nm\n\nj\n\n=\n\nj\n\n∂λj ∂mi\n\n(\n\nσj\n\nλ − m j ) − λi\n\n= −λi\n\n= 0,\n\n(5.17)\n\nwhere −β Fβ (λ) d=ef ln Zλ. Finally, we obtain\n\nminmGβ (m)\n\n=\n\nF[P]\n\n=\n\n−\n\n1 β\n\nln\n\nZ.\n\n(5.18)\n\nNote that Gβ(m) is a convex function with a unique minimum at meq. In sum, the approximate computation of Gβ(m) can be used to get an approximation for the true free energy F[P] as well.\n\n5.2 High-Temperature Expansion\n\nIn this section, we apply the high-temperature expansion to approximate the true Helmholtz free energy. We ﬁrst introduce the seminal Sherrington–Kirkpatrick (SK) model. This model was introduced in 1975 as a simple model of spin glass [5]. It is actually an Ising model with disordered couplings. For simplicity, we ignore external ﬁelds here. The Hamiltonian of the model is given by\n\nN\nH (σ ) = − Ji j σi σ j ,\ni<j\n\n(5.19)\n\nwhere couplings Ji j are independent and are Gaussian random variables for i < j with mean J0 (here we just assume J0 to be 0) and variance J 2/N . Ji j acts as quenched disorder for the model.\nThe Gibbs free energy is unfortunately intractable, making an optimization in the magnetization space challenging as well. Therefore, we need to consider a perturbation analysis of the free energy, e.g., in terms of high temperatures. The approximation accuracy can be controlled by including higher orders of expansion.\nWe deﬁne a new partition function associated with the Hamiltonian as follows:\n\nZ˜ β =\n\ne−β H˜ (σ ),\n\nσ\n\n(5.20)\n\nwhere the modiﬁed Hamiltonian is given by\n\n5.2 High-Temperature Expansion\n\n47\n\nH˜ (σ ) = H (σ ) −\n\ni\n\nλi (β) β\n\n(σi\n\n− mi)\n\n=\n\n−1 2\n\ni=j\n\nJi j σi σ j\n\n−\n\ni\n\nλi (β) β\n\n(σi\n\n−\n\nmi)\n\n,\n\n(5.21)\n\nwhere we write λi (β) as an explicit function of the temperature, because λi is used\n\nto enforce the magnetization that depends on the temperature. The relation between\n\nthe Gibbs free energy and the new partition function is\n\n− βGβ (m, λ) = ln Z˜ β .\n\n(5.22)\n\nWe then carry out the Taylor expansion at β = 0:\n\n− βGβ (m)\n\n=\n\nln Z˜ β\n\nβ=0\n\n+\n\n∂ ∂β\n\nln Z˜ β\n\nβ\nβ=0\n\n+\n\n∂2 ∂β2\n\nln Z˜ β\n\nβ=0\n\nβ2 2\n\n+···\n\n.\n\n(5.23)\n\nAt β = 0, we obtain\n\nZ˜ β\n\n=\n\ne i λi (σi −mi )\n\nβ=0\n\nσ\n\n=\n\neλi (σi −mi )\n\ni σi\n\n= e−λi mi (2 cosh λi ) .\n\ni\n\ni\n\n(5.24)\n\nBecause\n\n∂ ln Z˜ β=0 ∂ λi\n\n= −mi\n\n+ tanh (λi ) = 0\n\n⇒\n\nmi = tanh (λi ) λi = atanh (mi )\n\n,\n\n(5.25)\n\nthen we can calculate the ﬁrst term:\n\nln Z˜ β |β=0 = − atanh (mi ) mi + ln (2 cosh (atanh mi ))\n\ni\n\ni\n\n=−\ni\n\n1 2 mi\n\nln\n\n1 + mi 1 − mi\n\n+\n\ni\n\nln e + e −\n\n1 2\n\nln\n\n1+mi 1−mi\n\n1 2\n\nln\n\n1+mi 1−mi\n\n=−\ni\n\n1 2 mi\n\nln\n\n1 + mi 1 − mi\n\n+\n\ni\n\nln\n\n1 + mi 1 − mi\n\n+1\n\n1 − mi 1 + mi\n\n=\n\n− mi ln 1 + mi + mi ln 1 − mi + ln √\n\n2\n\n2\n\n2\n\n2\n\n2\n\ni\n\ni\n\n(1 − mi ) (1 + mi )\n\n=−\ni\n\n1 + mi ln 1 + mi + 1 − mi ln 1 − mi .\n\n2\n\n2\n\n2\n\n2\n\n(5.26)\n\n48\n\n5 High-Temperature Expansion\n\nHere,\n\nwe\n\nhave\n\nused\n\nthe\n\nmathematical\n\nidentity:\n\natanh mi\n\n=\n\n1 2\n\nln\n\n1+mi 1−mi\n\n.\n\nThe\n\nsecond\n\nterm is given by\n\n∂ ln Z˜ β ∂β\n\nβ=0\n\n=\n\n1 Z˜ β\n\nσ\n\n(−H )e−β H˜ +\n\n∂ λi ∂β\n\n(σi\n\n−\n\nmi ) e−β H˜\n\ni\n\n= − H |β=0\n\n=1 2\n\nJi j mi m j .\n\ni=j\n\n(5.27)\n\nNote that the thermal average is carried out under the Boltzmann measure of H˜ (σ ), and the correlation between two spins is negligible in the high-temperature limit. The third term is given by\n\n∂2 ∂β2\n\nln\n\nZ˜ β\n\n=\n\n−\n\n∂H ∂β\n\n=\n\n−\n\n∂ ∂β\n\nH e−β H˜ σ Z˜ β\n\n=−\n\nσ\n\ne−β H˜ Z˜ β H\n\n−H +\n\ni\n\n∂ λi ∂β\n\n(σi\n\n− mi)\n\n−\nσ\n\nH e−β H˜ Z˜ β\n\n·\n\n∂ ln Z˜ β ∂β\n\n= − H −H +\n\n∂ λi ∂β\n\n(σi\n\n− mi)\n\n− H (− H )\n\ni\n\n= H H− H −\n\n∂ λi ∂β\n\n(σi\n\n− mi)\n\ni\n\n:= u H .\n\nHere, we have introduced a very useful operator u as follows [3]:\n\n(5.28)\n\nu := H − H −\n\n∂ λi ∂β\n\n(σi\n\n−\n\nmi)\n\n=\n\nH\n\n−\n\nH\n\n−k\n\ni\n\nwhere k :=\n\ni\n\n∂ λi ∂β\n\n(σi\n\n−\n\nmi ).\n\nBecause\n\n∂ ln Z˜ β ∂mi\n\n= −λi\n\nZ˜ β Z˜ β\n\n= −λi ,\n\nwe have the following result:\n\n(5.29) (5.30)\n\n5.2 High-Temperature Expansion\n\n49\n\n∂ λi\n\n= − ∂ ∂ ln Z˜ β = − ∂ ∂ ln Z˜ β\n\n∂β β=0\n\n∂β ∂mi\n\n∂mi ∂β\n\n= −1 ∂ 2 ∂mi\n\ni=j\n\nJi j mi m j\n\n= − Ji j m j .\nj (=i)\n\n(5.31)\n\nWe, thus, conclude that\n\nu|β=0\n\n=\n\n−1 2\n\nJi j σi σ j\n\n+\n\n1 2\n\nJi j mi m j +\n\nJi j m j (σi − mi )\n\ni=j\n\ni=j\n\ni j ( j =i)\n\n=\n\n−1 2\n\ni=j\n\nJi j\n\n(σi\n\n− mi)\n\nσj − mj\n\n.\n\n(5.32)\n\nTo proceed, we should ﬁrst calculate the mean and variance of u:\n\nu = 0,\n\n(5.33)\n\nand u2 = u (H − H − k) = uH − u H − ku = uH .\n\n(5.34)\n\nWe can prove above Eqs. (5.33) and (5.34) by using the following identity:\n\nd\n\n1\n\ndβ O = Z˜ β\n\nσ\n\nOe−β H˜\n\n−H +\n\ni\n\n∂\n\nλi (β ∂β\n\n)\n\n(σi\n\n−\n\nmi)\n\n+\n\nσ Oe−β H˜ Z˜ β\n\n− ∂ ln Z˜ β ∂β\n\n+\n\nσ\n\n∂O ∂β\n\ne−β\n\nH˜\n\nZ˜ β\n\n= ∂O − Ou , ∂β\n\n(5.35)\n\nwhere O is any observable, and\n\nd dβ\n\nσi\n\n=0=\n\n∂ σi ∂β\n\n(σi − mi ) u = 0,\n\nku = 0.\n\n− σi u = − σi u ,\n\n(5.36)\n\nNote that the full derivative vanishes due to the constrained magnetization [i.e., as a\n\nconstant, see also Eq. (5.11)]. Therefore, we can obtain\n\n∂2 ∂β2\n\nln Z˜ β\n\nβ=0\n\nby calculating\n\nu2 |β=0:\n\n50\n\n5 High-Temperature Expansion\n\n∂2 ∂β2\n\nln\n\nZ˜ β\n\n=\nβ=0\n\nu2\n\n|β=0\n\n=1 4\n\nJi j Jkl (σi − mi ) σ j − m j (σk − mk ) (σl − ml )\n\ni = j,k=l\n\n1 2\n\nJi2j (σi − mi )2 σ j − m j 2\n\ni=j\n\n=\n\n1 2\n\ni=j\n\nJi2j\n\n1 − mi2\n\n1 − m2j ,\n\n(5.37)\n\nwhere we have used the formula:\n\n(σi − mi )2\n\n=\n\n1\n\n−\n\n2m\n\n2 i\n\n+\n\nm\n\n2 i\n\n=\n\n1\n\n−\n\nmi2.\n\nFinally,\n\nwe obtain\n\n−βGβ (m) = −\ni\n\n1 + mi ln 1 + mi + 1 − mi ln 1 − mi\n\n2\n\n2\n\n2\n\n2\n\n+\n\n1β 2\n\ni=j\n\nJi j mi m j\n\n+\n\nβ2 4\n\ni=j\n\nJi2j\n\n1\n\n−\n\nm\n\n2 i\n\n1\n\n−\n\nm\n\n2 j\n\n+ O(β3).\n\n(5.38)\n\nThe ﬁrst term on the right side of the above equation is called the mean-ﬁeld varia-\n\ntional entropy. The second term is the mean-ﬁeld variational energy. The third term\n\ncorresponds to the Onsager reaction correction. All three terms construct the TAP\n\nfree energy for the SK model.\n\nTo minimize the free energy, we carry out the differentiation with respect to {mi },\n\n∂\n\n−βGβ (m) ∂mi\n\n= − atanh(mi ) + β\n\nJi j m j\n\n+\n\nβ2 2\n\nJi2j\n\nj (=i)\n\nj (=i)\n\n1\n\n−\n\nm\n\n2 j\n\n(−2mi ) = 0,\n\n(5.39)\n\nand ﬁnally obtain the self-consistent equation (the so-called TAP equation):\n\n⎛\n\n⎞\n\nmi = tanh ⎝β\n\nJi j m j − β2\n\nJi2j\n\n1\n\n−\n\nm\n\n2 j\n\nmi⎠ .\n\nj (=i)\n\nj (=i)\n\n(5.40)\n\nThe ﬁrst term on the right-hand side represents the standard mean-ﬁeld approximation of local ﬁelds. The second term is called the Onsager reaction ﬁeld added to remove the effects of self-response [6]. If we consider the external ﬁelds {hi }, the TAP equation becomes\n\n⎛\n\n⎞\n\nm\n\nt i\n\n+1\n\n=\n\ntanh\n\n⎝β h i\n\n+\n\nβ\n\nJi j mtj − β2\n\nJi2j\n\n1\n\n−\n\n(m\n\nt j\n\n)2\n\nm\n\nt i\n\n−1⎠\n\n,\n\nj (=i)\n\nj (=i)\n\n(5.41)\n\n5.2 High-Temperature Expansion\n\n51\n\nwhere we have put the correct time indexes for iteration [7]. In the thermodynamic limit, the TAP approximation becomes exact for the SK model, as the terms O(β3) vanish. The ﬁxed points of TAP are the stationary points of the TAP free energy. At low temperatures, the TAP equation have many solutions with mi = 0, which can be interpreted as stable or metastable thermodynamic states [8, 9].\n\n5.3 Properties of the TAP Equation\n\nIn this section, we study the behavior of the solution of the TAP equation [Eq. (5.40)\nwhere external ﬁelds are added] around the spin glass transition point [6]. Because Ji j are assumed to be independent random variables (for i < j) with zero mean and the variance J 2/N . The Onsager term of the TAP equation becomes\n\nN\n\nN\n\nβ2\n\nJi2j 1 − m2j mi = β2 J 2mi − β2\n\nJi2j\n\nm\n\n2 j\n\nmi\n\n,\n\nj (=i)\n\nj (=i)\n\n(5.42)\n\nwhen N → ∞ (the law of large numbers applies). Around the spin glass transition point, we assume that the magnetizations {mi } are small, expand the right-hand side of the TAP equation to the ﬁrst order in m and ﬁnally arrive at\n\nmi = β Ji j m j + βhi − β2 J 2mi .\nj\n\n(5.43)\n\nWe also assume that hi is not dominant. For the symmetric matrix J, we have J = Q QT, where Q is the orthogonal matrix, and = diag(λ1, λ2, . . . , λN ) in which\n{λi } are eigenvalues of the interaction matrix J. Let us write Ji j in the following\n\nform:\n\nJi j = Qin Q jnλn.\n\n(5.44)\n\nn\n\nTo proceed, we deﬁne the λ-magnetization and λ-ﬁeld by [6]\n\nmλn = Qinmi , hλn = Qin hi .\n\ni\n\ni\n\nThen we have the following result:\n\n(5.45)\n\n52\n\n5 High-Temperature Expansion\n\nβ Qin Ji j m j = β Qin\n\nQim Q jm λm m j\n\ni\n\nj\n\ni\n\njm\n\n= β λm Qin Qim Q jm m j\n\nm\n\ni\n\nj\n\n= βλn Q jnm j\nj\n= βλnmλn ,\n\n(5.46)\n\nwhere we have used the orthogonal condition: i Qin Qim = δnm. Then we can\n\nrewrite Eq. (5.43) as\n\nmλ = βmλλ + βhλ − β2 J2mλ.\n\n(5.47)\n\nWe can, thus, conclude that the λ-susceptibility can be expressed as [10]\n\nχλ\n\n=\n\n∂mλ ∂hλ\n\n=\n\n1\n\n−\n\nβ\n\nλ\n\nβ +\n\n(β\n\nJ\n\n)2\n\n.\n\n(5.48)\n\nIn addition, the eigenvalues of the random matrix J follow the well-known semi-circle\n\nlaw1 [11]:\n\n√ ρ (λ) = 4 J 2 − λ2 .\n2π J 2\n\n(5.49)\n\nIt is easy to derive from Eq. (5.48) that the susceptibility corresponding to the largest eigenvalue λ = 2J diverges at Tg = J , suggesting a continuous phase transition. The location of this transition agrees exactly with that obtained from the replica result [5].\nAn alternative way to see the stability condition of the paramagnetic phase is to compute the Hessian matrix:\n\nHi j\n\n=\n\n∂2(βGβ (m)) ∂mi∂m j\n\nm=0\n\n= −β Ji j\n\n+ (β2 J 2 + 1)δi j .\n\n(5.50)\n\nThe stability condition is that all the eigenvalues of the Hessian matrix should be\n\npositive,\n\nleading\n\nto\n\nthe\n\nsame\n\nresult\n\nas\n\nabove.\n\nThe\n\nsusceptibility\n\nmatrix\n\nχi j\n\n=\n\n∂mi ∂h j\n\nis related to the Hessian matrix as (H−1)i j = β−1χi j = σi σ j c followed from the\n\nlinear response theory. The subscript c denotes the connected two-point correlation.\n\nReferences\n1. D.J. Thouless, P.W. Anderson, R.G. Palmer, Phil. Mag. 35(3), 593 (1977) 2. T. Plefka, J. Phys. A 15(6), 1971 (1982)\n1 We will derive this law in Chap. 17.\n\nReferences\n\n53\n\n3. A. Georges, J. Yedidia, J. Phys. A: Math. Gen. 24, 2173 (1991) 4. A. Maillard, L. Foini, A.L. Castellanos, F. Krzakala, M. Mézard, L. Zdeborova, J. Stat. Mech.:\nTheory Exper. 2019(11), 113301 (2019) 5. D. Sherrington, S. Kirkpatrick, Phys. Rev. Lett. 35(26), 1792 (1975) 6. H. Nishimori, Statistical Physics of Spin Glasses and Information Processing: An Introduction\n(Oxford University Press, Oxford, 2001) 7. E. Bolthausen, Commun. Math. Phys. 325(1), 333 (2014) 8. A. Crisanti, L. Leuzzi, G. Parisi, T. Rizzo, Phys. Rev. Lett. 92(12), 127203 (2004) 9. T. Aspelmeier, A.J. Bray, M.A. Moore, Phys. Rev. Lett. 92(8), 87203 (2004) 10. A.J. Bray, M.A. Moore, J. Phys. C: Solid State Phys. 12(11), L441 (1979) 11. M.L. Mehta, Random Matrices (Academic, San Diego, 2004)\n\nChapter 6\nNishimori Line\n\nIn this chapter, we introduce the Nishimori line as an important concept, i.e., Nishimori temperature or constraint, on spin glass models of broad contexts. This concept was ﬁrst discovered in the traditional two-body interaction spin glass model [1, 2], which demonstrated that on a special temperature, the model energy of a complex glass model is analytic, and the replica symmetry breaking (RSB) phase (introduced in Chap. 9) is not dominant for ground states, and thus, the underlying physics is greatly simpliﬁed. The concept was later connected to the Bayes optimal setting of statistical inference problems [3–6]. Thus, this concept is an important theoretical perspective to understand the Bayesian learning process, one of the most popular paradigms in the deep learning era. Here, we introduce the basic knowledge about this concept ﬁrst, and we leave more applications to later chapters of learning theory.\n\n6.1 Model Setting\n\nThe original model Hidetoshi Nishimori used to derive the special temperature is\n\ndeﬁned as follows:\n\nH = − Ji j σi σ j ,\n\n(6.1)\n\ni<j\n\nwhere Ji j acts as a quenched disorder. The coupling distribution function is speciﬁed\n\nas follows:\n\nP(Ji j ) = pδ(Ji j − J ) + (1 − p)δ(Ji j + J ),\n\n(6.2)\n\nwhere p denotes a ferromagnetic bias for the coupling, and J is a positive constant.\nEach coupling is generated independently from this binomial distribution. Let Ji j = J τi j , where τi j = ±1. For the sake of convenience, we then introduce\nan auxiliary temperature βp to parameterize the original distribution P(Ji j ):\n\n© Higher Education Press 2021\n\n55\n\nH. Huang, Statistical Mechanics of Neural Networks,\n\nhttps://doi.org/10.1007/978-981-16-7570-6_6\n\n56\n\n6 Nishimori Line\n\neβp τi j\n\nP( Ji j )\n\n=\n\nP(τi j )\n\n=\n\n, 2 cosh βp\n\n1 1− p\n\nβp = 2 ln\n\n. p\n\n(6.3a) (6.3b)\n\nThe form of βp ensures that the two forms of the coupling distribution are equivalent. Readers can easily verify this point by considering both possible values of the coupling.\n\n6.2 Exact Result for Internal Energy\n\nAccording to the model deﬁnition, the internal energy can be written as follows:\n\n⎛\n\n⎞\n\nH τ,σ = P(τ ) P(σ ) ⎝− J τi j σi σ j ⎠\n\nτ\n\nσ\n\ni<j\n\n⎛\n\n⎞\n\neβp i< j τi j =\nτ (2 cosh βp)NB σ\n\neβ J i< j τi j σi σ j\n\neβ J\nσ\n\ni< j τi j σi σ j\n\n⎝− J\n\ni<j\n\nτi j σi σ j ⎠ ,\n\n(6.4)\n\nwhere NB is the number of interactions (also called bonds in a lattice model). Note that P(τ ) is factorized, as the {τi j } are independent. We further remark that the Hamiltonian of the model is invariant under the following gauge transformation:\n\nτi j → τi j si s j , σi → σi si .\n\n(6.5) (6.6)\n\nNote that {si } is also an Ising-valued conﬁguration. Therefore, we apply this transformation to the model internal energy as follows:\n\nH τ,σ = −\nτ\n=− 1 2N\n=− 1 2N\n\neβp i< j τi j si s j\n\ne J β J\nσ\n\ni< j τi j σi σ j\n\ni< j τi j σi σ j\n\n(2 cosh βp)NB\n\neβ J\nσ\n\ni< j τi j σi σ j\n\neβp i< j τi j si s j\ns\n\ne J β J\nσ\n\ni< j τi j σi σ j\n\ni< j τi j σi σ j\n\nτ (2 cosh βp)NB\n\neβ J\nσ\n\ni< j τi j σi σ j\n\nZs τ (2 cosh βp)NB\n\nσ eβ J\n\nJ i< j τi j σi σ j Zσ\n\ni< j τi j σi σ j ,\n\n(6.7)\n\nwhere 2N is introduced to cancel the sum operation s •. Clearly, when β J = βp, the partition functions Zs and Zσ cancel with each other. Then, we have\n\n6.2 Exact Result for Internal Energy\n\n57\n\n1\n\n1\n\nH\n\nτ ,σ\n\n=− 2N\n\n(2 cosh βp)NB\n\nτ\n\n1\n\n1\n\n∂\n\n=−\n\n2N (2 cosh βp)NB ∂β\n\n=− 1\n\n1\n\n∂\n\n2N (2 cosh βp)NB ∂β\n\n1\n\n1\n\n∂\n\n=−\n\n2N (2 cosh βp)NB ∂β\n\n= −NB J tanh βp.\n\ne J β J i< j τi j σi σ j\n\nτi j σi σ j\n\nσ\n\ni<j\n\neβ J i< j τi j σi σ j\nτσ\n\neβ J τi j σi σ j\nσ i < j τi j\n\n(2 cosh βp)NB\nσ\n\n(6.8)\n\nTherefore, under the Nishimori temperature β J = βp, the internal energy for the model has an analytical expression. In general, the internal energy does not have a closed-form expression.\n\n6.3 Proof of No RSB Effects on the Nishimori Line\n\nIn this section, we will prove that, using the gauge transformation, the distribution of spin glass order parameters does not have a complex structure on the Nishimori line (βp) and coincides exactly with the distribution of magnetizations.\nThe magnetization distribution is deﬁned as follows:\n\nek p i< j τi j Pm (x; k) = τ (2 cosh k p)NB σ\n\nek i< j τi j σi σ j e δ σ k i< j τi j σi σ j\n\nx− 1 N\n\nσi\ni\n\n,\n\n(6.9)\n\nwhere we have deﬁned k = β J and k p = βp. Double averages are performed in the deﬁnition of the magnetization distribution: the one over σ is the thermal average, and the other over τ is the disorder average. Both averages are standard thermodynamic operations in the spin glass theory. The disorder average is usually challenging.\nNext, we apply the following gauge transformation:\n\nτi j → τi j si s j , σi → σi si .\n\n(6.10) (6.11)\n\nThen, Pm(x; k) changes to\n\n1 Pm(x; k) = 2N τ\n\nek p i< j τi j si s j s (2 cosh k p)NB σ\n\nek i< j τi j σi σ j\n\nδ k e σ\n\ni< j τi j σi σ j\n\nx− 1 N\n\nσi si .\ni\n(6.12)\n\n58\n\n6 Nishimori Line\n\nThis form of Pm(x; k) can be further simpliﬁed to make the underlying physics more transparent. A simple algebraic manipulation leads to\n\nPm\n\n(x;\n\nk)\n\n=\n\n1 2N\n\nτ\n\nek p i< j τi j si s j s (2 cosh k p)NB σ\n\n×\n\ns ekp s ekp\n\ni< j τi j si s j\n. i< j τi j si s j\n\nek i< j τi j σi σ j\n\nδ k e σ\n\ni< j τi j σi σ j\n\nx− 1 N\n\nσi si\ni\n\nWe then perform the second gauge transformation: τi j → τi j si s j , σi → σi si , and si → si si , resulting in\n\nek p i< j τi j\n\nPm(x; k) =\nτ\n\n(2 cosh k p)NB\n\ns\n\nek p i< j τi j si s j ek p i< j τi j si s j\ns\n\n×\nσ\n\nek i< j τi j σi σ j e δ σ k i< j τi j σi σ j\n\nx− 1 N\n\nσi si\ni\n\n(6.13)\n\n1\n\n=\nτ\n\nP(τ )\nσ\n\nP(σ )\ns\n\nP (s)δ\n\nx− N\n\ni\n\nσi si\n\n=Pq (x; k, k p),\n\nwhere P(σ ) and P(s) are the Boltzmann measures with (rescaled) inverse tempera-\n\nture k and k p, respectively.\n\nUnder the Nishimori temperature, Pm(x; k p) = Pq (x; k p, k p) = Pq (x; k p). We,\n\nthus, conclude that the distribution of spin glass order parameter (overlap q =\n\n1 N\n\ni σi si ) shares the same form as the magnetization distribution. It is well known\n\nthat the magnetization distribution in statistical physics is simple, while the over-\n\nlap distribution can be very complex (e.g., when replica symmetry breaking effects\n\ndominate the phase space, like in the SK model). The two equivalent distributions\n\non the Nishimori line suggest an absence of spin glass phase for the ground states.\n\nHowever, RSB may be needed to describe the metastable (out of equilibrium) states\n\nof the system (e.g., in the study [7]). Altogether, on the Nishimori line, the system\n\nnever enters the glassy phase and the dominant thermodynamic phase is always a RS\n\ntype.\n\nReferences\n1. H. Nishimori, J. Phys. C: Solid State Phys. 13(21), 4071 (1980) 2. H. Nishimori, Prog. Theor. Phys. 66(4), 1169 (1981) 3. Y. Iba, J. Phys. A: Math. Gen. 32, 3875 (1999) 4. L. Zdeborova, F. Krzakala, Adv. Phys. 65(5), 453 (2016) 5. H. Huang, J. Stat. Mech.: Theory Exper. 2017(5), 053302 (2017) 6. T. Hou, H. Huang, Phys. Rev. Lett. 124, 248302 (2020) 7. M. Yoshida, T. Uezu, T. Tanaka, M. Okada, J. Phys. Soc. Jpn. 76(5), 54003 (2007)\n\nChapter 7\nRandom Energy Model\n\nIn this chapter, we brieﬂy introduce the well-known random energy model (Derrida in Phys. Rev. Lett. 45:79, 1980 [1]; Derrida in Phys. Rev. B 24(5):2613, 1981 [2]), which is the inﬁnite-body interaction limit of p-spin interaction models, but still captures characteristics of spin glasses (Gross and Mezard in Nuclear Phys. 240(4):431, 1984 [3]). Here, we focus on basic concepts and their connections to frozen phases commonly observed in other constraint satisfaction problems, e.g., binary Perceptron (introduced in Chap. 13).\n\n7.1 Model Setting\n\nWe consider Ising-type spins, whose interaction follows the Hamiltonian:\n\nH(σ ) = −\n\nJi1...i p σi1 · · · σi p ,\n\n1≤i1...i p≤N\n\nwhere the coupling follows the Gaussian distribution deﬁned by\n\nP ( Ji1...i p ) =\n\nN p−1 exp\n\n− Ji21...i p N p−1\n\n,\n\nπ J2 p!\n\nJ2 p!\n\n(7.1) (7.2)\n\nwhere J is positive, and the scaling of variance ensures that extensive energy is well-deﬁned. A generalized Hopﬁed model with multi-body interactions can also be included in this class of models [4]. In this scaling, it is easy to verify that p = 2 corresponds to the standard Sherrington–Kirkpatrick model.\nWe are interested in the distribution of the energy level E, to see if this distribution becomes simple in the limit p → ∞. In general, the distribution can be very complex. According to the deﬁnition, we have\n\n© Higher Education Press 2021\n\n59\n\nH. Huang, Statistical Mechanics of Neural Networks,\n\nhttps://doi.org/10.1007/978-981-16-7570-6_7\n\n60\n\n7 Random Energy Model\n\nP(E) = δ(E − H(σ ))\n\n⎡\n\n⎤\n\n=\n\nd Eˆ 2π\n\nexp\n\n⎣i\n\nEˆ\n\nE\n\n+\n\ni\n\nEˆ\n\nJi1...i p σi1 · · · σi p ⎦\n\n1≤i1...i p≤N\n\n= d Eˆ ei Eˆ E\n\ne , i Eˆ Ji1...i p σi1 ···σi p\n\n2π\n\n1≤i1...i p≤N\n\n(7.3)\n\nwhere the quenched-disorder average (indicated by the over-bar) can be explicitly calculated out as follows:\n\ne = i Eˆ Ji1...i p σi1 ···σi p\n\nP ( Ji1...i p )d Ji1...i p ei Eˆ Ji1...i p σi1 ···σi p\n\n= exp\n\n(i Eˆ σi1 · · · σip )2 J 2 p! 4N p−1\n\n.\n\n(7.4)\n\nNote\n\nthat\n\nthe\n\ntotal\n\nnumber\n\nof\n\nthe\n\nproducts\n\nin\n\nEq.\n\n(7.3)\n\ncan\n\nbe\n\napproximated\n\nby\n\nNp p!\n\nwhen N → ∞. Therefore, we ﬁnally arrive at\n\nP(E) =\n\nd Eˆ ei\n\nEˆ\n\nE\n\n+\n\n(i\n\nEˆ\n\n)2 N 4\n\nJ\n\n2\n\n2π\n\n=√ 1\n\ne , −\n\nE2 N J2\n\nNπ J2\n\n(7.5)\n\nwh√ich is exactly a Gaussian distribution with zero mean and a ﬂuctuation of the order O( N ).\nThe Gaussian distribution of energy levels in p-spin interaction models does not imply any information about whether the energy levels are correlated or not. To address this question, we derive the joint distribution of two energy levels, say E1 and E2, as follows:\n\nP(E1, E2, q) = δ(E1 − H(σ 1))δ(E2 − H(σ 2))\n\n⎡⎛\n\n⎞⎤\n\n=\n\nd Eˆ1 Eˆ2 4π 2\n\nei(Eˆ1 E1+Eˆ2 E2)exp ⎣i ⎝Eˆ1\n\ni1<···<i p\n\nJi1...i p σi11\n\n· · · σi1p\n\n+\n\nEˆ 2\n\ni1<···<i p\n\nJi1...i p σi21\n\n· · · σi2p ⎠⎦\n\n=\n\nd\n\nEˆ 1 4π\n\nEˆ 2 2\n\nei( Eˆ 1\n\nE1\n\n+\n\nEˆ 2\n\nE2\n\n)\n\ni1 <···<i\n\np\n\nexp\n\ni Eˆ1 Ji1...i p σi11 · · · σi1p + i Eˆ2 Ji1...i p σi21 · · · σi2p\n\n,\n\n(7.6)\n\nwhere we\n\nhave\n\ndeﬁned the overlap between two conﬁgurations as q\n\n=\n\n1 N\n\ni σi1σi2.\n\nTo proceed, we must calculate the disorder average in the above expression of\n\nP(E1, E2, q). The disorder average is carried out as follows:\n\n7.1 Model Setting\n\n61\n\nexp i Eˆ1 Ji1...i p σi11 · · · σi1p + i Eˆ2 Ji1...i p σi21 · · · σi2p\n\n= d Ji1...i p P ( Ji1...i p ) exp i Eˆ 1 Ji1...i p σi11 · · · σi1p + i Eˆ 2 Ji1...i p σi21 · · · σi2p\n\n= exp\n\n(i\n\nEˆ 1 )2\n\n+\n\n(i\n\nEˆ 2 )2\n\n+\n\n2J2\n\np!(i Eˆ1)(i 4N p−1\n\nEˆ2) (σi11\n\n·\n\n·\n\n·\n\nσi1p σi21\n\n·\n\n·\n\n· σi2p )\n\n,\n\n(7.7)\n\nwhere we have used the fact that spin takes a binary value ±1. Inserting the disorder average into Eq. (7.6), we obtain\n\nP(E1, E2, q) =\n\nd\n\nEˆ 1 4π\n\nEˆ 2\n2\n\nei( Eˆ 1\n\nE1\n\n+\n\nEˆ 2\n\nE2\n\n)\n\n×\n\nexp\n\ni1<···<i p\n\n(i\n\nEˆ 1 )2\n\n+\n\n(i\n\nEˆ 2 )2\n\n+\n\n2J2\n\np!(i Eˆ1)(i 4N p−1\n\nEˆ2) (σi11\n\n· · · σi1p σi21\n\n· · · σi2p )\n\n=\n\nd\n\nEˆ 1 4π\n\nEˆ 2\n2\n\nei( Eˆ 1\n\nE1\n\n+\n\nEˆ 2\n\nE2\n\n)\n\nexp\n\nJ2N 4\n\n(i Eˆ1)2 + (i Eˆ2)2 + 2q p(i Eˆ1)(i Eˆ2)\n\n.\n\n(7.8)\n\nTo arrive at the last equality, we have used the relationship p!\n\nN i1<i2<···<i p\n\n•\n\ni1,i2,...,ip • for large N , together with the deﬁnition of the overlap q. Finally, calcu-\n\nlating the Gaussian integral out in Eq. (7.8), we conclude that the joint distribution\n\nparameterized by q and J is given by\n\nP(E1, E2, q) =\n\nπ J2N\n\n1 exp\n1 − q2p\n\n2E1 E2q p − E12 − E22 J 2 N (1 − q2 p)\n\n=\n\nN π J 2(1 + q p)N π J 2(1 − q p)\n\n−1/2\nexp\n\n−\n\n2\n\n(E1 + J 2 N (1\n\nE 2 )2 +qp\n\n)\n\n−\n\n(E1 − E2)2 2 J 2 N (1 − q p)\n\n.\n\n(7.9)\n\nSupposed that |q| < 1, we immediately have P(E1, E2, q) −p−→−→ ∞ P(E1)P(E2), where P(E1) and P(E2) are the Gaussian distributions derived before. This implies that the energy levels are uncorrelated, and each of them follows exactly the Gaussian\ndistribution.\n\n7.2 Phase Diagram\n\nThe above mathematical results draw concise physics pictures of the inﬁnite-body interaction model. We can then easily compute the typical number of conﬁgurations with predeﬁned energy level E,\n\n2\n\nn(E) = 2N P(E) = √ 1\n\nN ln 2−\n\nE NJ\n\ne\n\n.\n\nπ N J2\n\n(7.10)\n\n62\n\n7 Random Energy Model\n\n√ One can then derive a critical energy level E0 = N J ln 2, above which (in the absolute value) no conﬁgurations exist. However, for |E| < E0, there are expo-\n\nnentially many conﬁgurations at the corresponding energy level. In the thermo-\n\ndynamic limit, the entropy density (per spin) below the critical energy level is\n\ngiven\n\nby\n\ns(E)\n\n=\n\nlim N →∞\n\nln\n\nn(E) N\n\n= ln 2 −\n\n2\nJ , where\n\ndenotes the energy den-\n\nsity. According\n\nto the thermodynamic\n\nrelationship\n\ndS dE\n\n=\n\n1 T\n\n,\n\none\n\ncan\n\nalso obtain the\n\nexpression for the energy level\n\n=\n\n−\n\nJ2 2T\n\n,\n\nwhich\n\nalso\n\ndetermines\n\nthe\n\ncritical\n\ntemper-\n\nature\n\nTc\n\n=\n\n√J 2 ln 2\n\nwhere\n\nthe\n\nentropy\n\nvanishes.\n\nFinally, the equilibrium property of the random energy model is summarized by\n\nthe free energy proﬁle (F = E − T S):\n\nF/N =\n\n−T√ln 2\n\n−\n\nJ2 4T\n\nT > Tc .\n\n−J ln 2\n\nT < Tc\n\n(7.11)\n\nThis implies that below the critical temperature, the free energy of the system does not depend on the temperature, due to the vanishing entropy for a system of discrete degrees of freedom. The vanishing entropy suggests that the system enter a frozen glassy phase—the transition is continuous in the thermodynamic sense (no latent heat). This frozen glassy phase is also discovered in the Gallager codes [5, 6] and binary Perceptron [7–9]. We ﬁnally remark that the one-step replica symmetry breaking (see Chap. 9) was conﬁrmed to be exact for the random energy model [3].\n\nReferences\n1. B. Derrida, Phys. Rev. Lett. 45, 79 (1980) 2. B. Derrida, Phys. Rev. B 24(5), 2613 (1981) 3. D. Gross, M. Mezard, Nuclear Phys. 240(4), 431 (1984) 4. E. Gardner, J. Phys. A 20(11), 3453 (1987) 5. A. Montanari, Eur. Phys. J. B 23(1), 121 (2001) 6. H. Huang, Commun. Theor. Phys. 63(1), 115 (2015) 7. W. Krauth, M. Mezard, J. De Phys. 50(20), 3057 (1989) 8. H. Huang, Y. Kabashima, Phys. Rev. E 90, 052813 (2014) 9. H. Huang, K.Y.M. Wong, Y. Kabashima, J. Phys. A: Math. Theor. 46, 375002 (2013)\n\nChapter 8\nStatistical Mechanical Theory of Hopﬁeld Model\n\nHopﬁeld model is a well-known abstract model of associative memory in the brain (Amari in Biolog. cybern. 26:175, 1977 [1]; Hopﬁeld in Proc. Natl. Acad. Sci. USA 79:2554, 1982 [2]). Its equilibrium properties were ﬁrst analyzed in the seminal paper (Amit et al. in Phys. Rev. Lett. 55(14):1530, 1985 [3]) by Amit, Gutfreund and Sompolinsky. To obtain the phase diagram, the replica method developed originally in spin glass theory was used and then became popular in neural network research. This work also opened a new discipline—computational/theoretical neuroscience, being an important branch of worldwide brain projects in this new century. In this chapter, we will introduce in detail physics of this model, including phase transitions in associative memory, by an in-depth application of the replica trick (Mézard et al. in Spin Glass Theory and Beyond. World Scientiﬁc, Singapore, 1987 [4]).\n\n8.1 Hopﬁeld Model\n\nIn the Hopﬁeld network, all neurons are connected with each other by real-valued weights (see Fig. 8.1). Randomly generated patterns can be stored in this network by assigning the weights wi j in a Hebbian way (i.e., cells that ﬁre together, wire together). After assigning all the weights, if one feeds a distorted pattern to the network, the network dynamics can converge to the correct undistorted pattern by locally updating the neural state.\nIn the Hopﬁeld model, the state of neuron i at time step t takes binary values (±1)\n\nSi (t) =\n\n−1 inactive . 1 active\n\n(8.1)\n\nThe update rule takes the form\n\n© Higher Education Press 2021\n\n63\n\nH. Huang, Statistical Mechanics of Neural Networks,\n\nhttps://doi.org/10.1007/978-981-16-7570-6_8\n\n64\nFig. 8.1 Typical structure of a Hopﬁeld network. The circles represent neurons, and the lines with arrows represent symmetric weights between two neurons (wi j = w ji ). Every neuron is connected to all other neurons\n\n8 Statistical Mechanical Theory of Hopﬁeld Model\n\n⎛\n\n⎞\n\nSi (t + 1) ← sgn ⎝ wi j S j (t) − θi ⎠ ,\nj\n\n(8.2)\n\n⎧ ⎪⎨1 x > 0\n\nwhere sgn(x) = ⎪⎩0−1\n\nx = 0 , and θi is the ﬁring bias of the neuron Si . In fact, this x <0\n\nrule is a zero-temperature Monte Carlo dynamics of the model.\n\nNow we need to choose the right weights {wi j } to ensure that the binary patterns {ξ (μ)} are attractors. If one feeds an input S(t = 0) close to one of stored patterns (say ξ (ν)) to the network, the network is expected to converge to ξ (ν).\n\nWe consider a simple setting for the network, namely storing just one pattern, say ξ 1. We can choose the weights according to the following Hebbian rule:\n\nwi j\n\n=\n\n1 N\n\nξi(1)ξ\n\n(1) j\n\n,\n\n(8.3)\n\nfor i = j, and θi = 0. Usually we set wii = 0 for all i. To check this rule, we feed the pattern ξ (1) to the network\n\nN\n\nwi\n\nj\n\nξ\n\n(1) j\n\n=\n\n1 N\n\nN\n\nξi(1)ξ\n\n(j 1) ξ\n\n(1) j\n\n=\n\n1 N\n\nN\nξi(1) = ξi(1).\n\nj =1\n\nj =1\n\nj =1\n\n(8.4)\n\nTherefore\n\n8.1 Hopﬁeld Model\n\n65\n\nFig. 8.2 The energy landscape of the Hopﬁeld network. Minima in the energy function are attractors in the state space. But not every attractor corresponds to a stored pattern. These metastable states are referred to as spurious memories (e.g., a linear combination of several stored patterns [5])\n\n⎛\n\n⎞\n\nN\n\nsgn ⎝\n\nwi j\n\nξ\n\n(1)⎠\nj\n\n=\n\nξi(1)\n\n⇒\n\nS(t > 0) = ξ (1).\n\nj =1\n\n(8.5)\n\nIf we feed the reversed pattern −ξ (1) to the network\n\n⎛\n\n⎞\n\nN\n\nsgn ⎝−\n\nwi\n\nj\n\nξ\n\n(1)⎠\nj\n\n=\n\n−ξi(1)\n\n⇒\n\nS(t > 0) = −ξ (1) .\n\nj =1\n\n(8.6)\n\nTherefore, if ξ (1) is an attractor, then −ξ (1) is an attractor as well. This is a general\n\nproperty of the Hopﬁeld model, as we shall show by writing down the Hamiltonian.\n\nIn equilibrium statistical physics, the Hamiltonian (the energy function) is deﬁned\n\nas\n\n1N\n\nH =− 2\n\ni, j\n\nwi j Si S j ,\n\n(8.7)\n\nwhere wi j = 1/N\n\nP μ=1\n\nξiμξ\n\nμ j\n\n,\n\nwhich\n\nis\n\nsymmetric,\n\nensuring\n\nthat\n\nan\n\nequilibrium\n\nstate exists. Note that the pattern entries are independently selected as P(ξiμ = ±1) =\n\n1/2. Under the zero-temperature dynamics of the model, the Hamiltonian H remains\n\nunchanged or decrease. To show this, neglecting the ﬁring bias, we consider the\n\n66\n\n8 Statistical Mechanical Theory of Hopﬁeld Model\n\nupdate\n\n⎛\n\n⎞\n\nSk = sgn ⎝ wkj S j ⎠ ,\nj\n\n(8.8)\n\nand thus either Sk = Sk or Sk = −Sk. In the ﬁrst case, H remains unchanged. In the other case,\n\nH −H=\n\nwk j Sk S j +\n\nwik Si Sk = 2\n\nwk j Sk S j .\n\nj (=k)\n\ni (=k )\n\nj (=k)\n\n(8.9)\n\nBecause the sign of j wkj S j is the same as Sk and Sk = −Sk, it then follows that\n\nH − H < 0.\n\n(8.10)\n\nHence, either H remains unchanged or its value decreases in one update step. After a sufﬁcient number of updates, the energy function falls into a certain minimum, which is expected to correspond to a stored pattern (Fig. 8.2). This derivation can be cross-checked by implementing a zero-temperature Monte Carlo sampling on the Hamiltonian of Hopﬁeld model.\n\n8.2 Replica Method\n\nIn the thermodynamic limit, the free energy has the self-averaging property, i.e., −β f = ln Z , where Z is the partition function. As the number of degrees of freedom grows, the single-sample value of the free energy will converge sharply to the quenched average value. However, the expression ln Z , namely the quenched average, is difﬁcult to calculate in a direct way, whereas Z , namely the annealed average, is much easier to calculate. However, in most contexts of interest, ln Z = ln Z . In fact, the annealed average provides an upper bound to the quenched average, due to the Jensen’s inequality. The replica trick can be used to make a transformation of this calculation by introducing many copies of the original systems. Then the original interaction system can be decoupled to an equivalent system where correlations among replicas are considered, which greatly simpliﬁes the original challenging computation.\nIn mathematics, we have\n\nln Z = lim Z n − 1 . n→0 n\n\n(8.11)\n\nThen we calculate the expectation\n\n8.2 Replica Method\n\n67\n\nln Z = lim Z n − 1 = lim ln Z n ,\n\nn→0 n\n\nn→0 n\n\n(8.12)\n\nwhere · is the disorder average over ξ . Since Z n 1 + n ln Z + · · · , we have Z n 1 + n ln Z · · · . Therefore\n\nln Z n lim\n\n= lim ln(1 + n ln Z ) = lim n ln Z\n\n= ln Z\n\n,\n\nn→0 n\n\nn→0\n\nn\n\nn→0 n\n\n(8.13)\n\nwhere when n is small enough, we can take the expansion like Z n = en ln Z = 1 + n ln Z + · · · . The averaged free energy per spin can thus be calculated by\n\nf\n\n=\n\nlim\nn→0\n\nlim\nN →∞\n\n− ln βn\n\nZ N\n\nn\n\n.\n\n(8.14)\n\nWe ﬁrst assume that n is an integer (for the power), and after the calculation of Z n , we carry out the limit of ln Z as n approaches 0. This seems hard to understand\n\nin physics; whereas the results must be compared with physics simulations of the\n\nmodel. In this sense, the cavity approximation is more physically transparent than\n\nthe replica trick, although in most (we are not sure if all is suitable) cases, both methods yields the same result. We remark that the order of the two limits (n → 0\n\nand N → ∞) has been exchanged for the purpose of applying the Laplace method in\n\nthe thermodynamic limit. This operation is also not mathematically rigorous. But the\n\nﬁnal result is usually in consistent with physics intuition and numerical simulations.\n\nNext, we suppose that the network is able to store P random patterns (P =\n\nαN,\n\nand\n\nα\n\ndenotes\n\nthe\n\nmemory\n\nload).\n\nNote\n\nthat\n\nH\n\n=\n\n−\n\n1 2\n\nN i, j\n\nwi j Si S j\n\nand\n\nwi j\n\n=\n\n1 N\n\nP μ=1\n\nξiμ\n\nξ\n\nμj .\n\nTherefore\n\n⎡\n\n⎤\n\nZn =\n\nTr exp ⎣ β 2N\n\nNPn\n\nξiμξ\n\nμ j\n\nSiρ\n\nSρj\n\n⎦\n\ni, j μ=1 ρ=1\n\n⎡\n\n⎛\n\n⎞⎤\n\n= Tr exp ⎣ β 2N ρ,μ\n\nξiμ Siρ ⎝\n\nξ\n\nμ j\n\nS\n\nρ j\n\n⎠⎦\n\ni\n\nj\n\n⎡\n\n⎤\n\n2\n\n= Tr exp ⎣ β N 2 ρ,μ\n\n1 N\n\nξiμ Siρ\n\ni\n\n⎦\n\n⎡\n\n⎤\n\n2\n\n= Tr exp ⎣ β N\n\nρ ,μ\n\n2\n\n1 N\n\nξiμ Siρ\n\ni\n\n⎦,\n\n(8.15)\n\nwhere Tr means the summation over all conﬁgurations {S}, and · means the quenched disorder average over the random patterns.\n\n68\n\n8 Statistical Mechanical Theory of Hopﬁeld Model\n\nTo linearize the quadratic term, we apply the following Gaussian integral:\n\neab2 = a π\n\ne−ax2+2abx d x ,\n\nby carrying out the following substitutions:\n\n⎧\n\n⎪⎨b\n\n→\n\n1 N\n\ni ξiμ Siρ\n\n⎪⎩ax\n\n→ →\n\nm μρ\nβN\n2\n\n.\n\nIt\n\nis\n\nthen\n\nnatural\n\nto\n\nintroduce\n\nintegrals\n\nover\n\nm\n\nμ ρ\n\n(8.16) (8.17)\n\nZ n = Tr\n\nρ ,μ\n\nβN 2π\n\nd\n\nm\n\nμ ρ\n\nexp\n\n−βN 2\n\nm μρ\n\n2\n\n+\n\nβ\n\nm\n\nμ ρ\n\nξiμ Siρ\n\ni\n\n= Tr = Tr\n\nρ ,μ\n\nβN 2π\n\nd\n\nm\n\nμ ρ\n\nexp\n\n−βN 2\n\n⎡\n\nρ ,μ\n\nm\n\nμ ρ\n\n2+β\n\nm\n\nμ ρ\n\nρ ,μ\n\ni\n\nξiμ Siρ\n\nρ ,μ\n\nβN 2π\n\nd\n\nm\n\nμ ρ\n\nexp\n\n⎣−\n\nβN 2\n\nμ≥2\n\nρ\n\nm\n\nμ ρ\n\n2+\n\nβ\n\nm μρ\n\nμ≥2 ρ\n\ni\n\nξiμ Siρ\n\n−βN 2\n\nρ\n\nm\n\n1 ρ\n\n2+β\n\nm\n\n1 ρ\n\nξi1 Siρ\n\nρ\n\ni\n\n. (8.18)\n\nIn the above equation, we have separated the ﬁrst pattern from other patterns. We\n\nfurther assume that only the ﬁrst pattern (μ = 1) is retrieved, and thus the overlap\n\nmμρ ∼ O(1) (the deﬁnition of the overlap will become clear in the following analysis). We next consider those non-retrieved patterns (μ ≥ 2). Because i ξiμ Siρ ξ = 0 and\n\ni ξiμ Siρ 2 ξ = N +\n\ni=j\n\nξiμ\n\nξ\n\nμ j\n\nSiρ\n\nS\n\nρ j\n\nξ\n\n=\n\nN,\n\nthe\n\norder\n\nof\n\nm\n\nμ ρ\n\n(μ\n\n≥\n\n2)\n\nis\n\ngiven\n\nby\n\nm μρ\n\n=\n\n1 N\n\nξiμ Siρ ≈ O\ni\n\n√1 N\n\n.\n\n(8.19)\n\nTo\n\nuse\n\nan\n\nm μρ\n\nof\n\nO(1),\n\nwe\n\nrescale\n\nthe\n\noriginal\n\nm\n\nμ ρ\n\n→\n\n√m\n\nμ ρ\n\nβN\n\n.\n\nThen\n\nwe\n\nget\n\n8.2 Replica Method\n\nZ n = √1\n\nn ( P −1)\nTr\n\n2π\n\nd\n\nm\n\nμ ρ\n\nρ ,μ>1\n\nρ\n\n⎡\n\nβ\n\nN\n\nd m 1ρ\n\nexp\n\n⎣−\n\n1 2\n\nμ≥2\n\nρ\n\n69\n\nm\n\nμ ρ\n\n2+\n\nβ N\nμ≥2\n\nρ\n\nm μρ\n\ni\n\nξiμ Siρ\n\n−\n\nβN 2\n\nρ\n\nm1ρ 2 + β\n\nm\n\n1 ρ\n\nξi1 Siρ\n\nρ\n\ni\n\nFor the part of μ ≥ 2 involving in non-condensed patterns, we have\n\n⎡\n\n⎤\n\nexp ⎣ β N\nμ≥2\n⎡\n\nρ\n\nm μρ\n\ni\n\nξiμ Siρ ⎦ ξiμ :μ⎤>1\n\n∝ exp ⎣ ln cosh\n\nμ≥2,i\n⎡\n\n∼= exp ⎣\n\nβ\n\nμ≥2 i 2N\n\nβ N\n\nm\n\nμ ρ\n\nSiρ\n\nρ\n\n⎦\n\n⎤\n\n2\n\nm\n\nμ ρ\n\nSiρ\n\n⎦,\n\nρ\n\n. (8.20)\n(8.21)\n\nwhere we have used the formula exp( Aξ ) {ξ=±1} = exp(− A) + exp( A) =\n\n2 cosh(A)\n\n∝\n\nexp(ln cosh(A)),\n\nand\n\ntaken\n\nthe\n\napproximation\n\nln cosh x\n\n=\n\nx2 2\n\n+\n\n···\n\nas\n\nx → 0.\n\nWe can then write down the following expressions:\n\nm\n\nμ ρ\n\n2\n\n=\n\nmμρ δρσ\n\nm\n\nμ σ\n\n,\n\nρ\n\nρ ,σ\n\n(8.22)\n\nand\n\n2\n\n1 N\ni\n\nm\n\nμ ρ\n\nSiρ\n\nρ\n\n=1 N\ni\n\nm\n\nμ ρ\n\nSiρ\n\nm\n\nμ σ\n\nSiσ\n\nρ\n\nσ\n\n=\n\nρ ,σ\n\nm\n\nμ ρ\n\n1 N\n\ni\n\nSiρ\n\nSiσ\n\nm\n\nμ σ\n\n(8.23)\n\n:=\n\nm\n\nμ ρ\n\nqρσ\n\nm\n\nμ σ\n\n.\n\nρ ,σ\n\nTo further simplify the formulas, we deﬁne\n\nβ κρσ = δρσ − N\n\nSiρ Siσ := δρσ − βqρσ ,\n\ni\n\nand in the matrix form\n\n(8.24)\n\n70\n\n8 Statistical Mechanical Theory of Hopﬁeld Model\n\nK = I − βQ,\n\n(8.25)\n\nwhere\n\n1\n\nqρσ =\n\nN\n1\n\ni Siρ Siσ ρ = σ , ρ=σ\n\n(8.26)\n\nand K, Q are symmetric n × n matrices with elements κρσ and qρσ , respectively. I is an identity matrix.\nThus, we need to introduce qρσ by an integral of a Dirac delta function, and obtain\n\nZ n ∝ Tr\n\ndqρσ δ\n\nρ ,σ\n\n⎡\n\nqρσ\n\n−\n\n1 N\n\ni\n\nSiρ Siσ ⎤\n\n×\n\nμ≥2,ρ\n\nd\n\nm\n\nμ ρ\n\nexp\n\n⎣−\n\n1 2\n\nμ≥2\n\nρ ,σ\n\nmμρ κρσ mμσ ⎦\n\n×\n\nd\n\nm\n\n1 ρ\n\nexp\n\nρ\n\n−βN 2\n\nρ\n\nm\n\n1 ρ\n\n2+\n\nβN N\n\nm\n\n1 ρ\n\nρ\n\ni\n\nξi1 Siρ\n\n(8.27)\n,\nξ1\n\nwhere we have neglected irrelevant prefactors. By using the multivariate Gaussian\n\nintegral\n\ndme−MTKM =\n\nπn ,\n\nRn\n\ndet(K)\n\n(8.28)\n\nwe get\n\n⎡\n\n⎤\n\nμ≥2,ρ\n\nd\n\nm\n\nμ ρ\n\nexp ⎣− 1 2\n\nμ≥2\n\nρ ,σ\n\nmμρ κρσ mμσ ⎦ =\n\nC\n\n(det\n\nK)\n\nP\n\n−1 2\n\n,\n\n(8.29)\n\nwhere C is a constant. Because det(eK) = eTr K, and det K = eTr ln K, we have\n\n(det\n\nK)−\n\nP −1 2\n\n=\n\ne−\n\nP −1 2\n\nTr ln K\n\n=\n\ne−\n\nP −1 2\n\nTr ln[I−βQ]\n\n.\n\n(8.30)\n\nBy using the Fourier representation of the Dirac delta function\n\nδ(x)\n\n=\n\n1 2π\n\n+∞\ne−ikx dk ,\n−∞\n\nwe obtain\n\n(8.31)\n\n8.2 Replica Method\n\n71\n\nTr\n\ndm1ρ dqρσ δ\n\nρ\n\nρ ,σ\n\nqρσ\n\n−\n\n1 N\n\nSiρ Siσ\n\n⎡i\n\n·I\n\n⎤ (8.32)\n\n∝ Tr\n\nρ\n\nd m 1ρ\n\ndqρσ drρσ\nρ ,σ\n\nexp ⎣− N αβ2 2\n\nρ ,σ\n\nrρσ qρσ\n\n+\n\nαβ 2 2\n\nrρσ Siρ Siσ ⎦ · I ,\ni,ρ ,σ\n\nwhere the symbol I represents the other non-shown parts in Eq. (8.27), and we\n\nhave\n\nrescaled\n\nrρσ\n\n→\n\n−\n\niN αβ2 2\n\nrρσ\n\n(after\n\nthe\n\ntransformation,\n\nrρσ\n\n∼ O(1)),\n\nand\n\nused\n\nα = P/N.\n\nThen we deﬁne the Si -dependent part as\n\n⎡\n\n⎤\n\nTr exp ⎣β\n\nρ\n\nm\n\n1 ρ\n\ni\n\nξi1 Siρ\n\n+\n\nαβ 2 2\n\nrρσ Siρ Siσ ⎦\n\ni,ρ ,σ\n\nξ1\n\n= exp\n\ni\n\nln Tr exp\n\nβ\n\nρ\n\nm\n\n1 ρ\n\nξi1\n\nSρ\n\n+\n\nαβ 2 2\n\nρ ,σ\n\nrρσ Sρ Sσ\n\n⎧\n\n⎨ = exp ⎩N\n\nln Tr exp\n\nβ\n\nρ\n\nm\n\n1 ρ\n\nξ\n\n1\n\nSρ\n\n+\n\nαβ 2 2\n\nρ ,σ\n\nrρσ Sρ Sσ\n\n⎫ξ 1 ⎬ ⎭\nξ1\n\n(8.33)\n\n:= exp N ln Tr exp(β Hξ1 ) ξ1 ,\n\nwhere we have used the fact that the sum over i is equivalent to taking the average over the pattern conﬁguration because of i.i.d properties of the random pattern, and we have deﬁned\n\nβ Hξ1\n\n=\n\n1 αβ2 2\n\nρ ,σ\n\nrρσ Sρ Sσ\n\n+β\n\nρ\n\nm\n\n1 ρ\n\nξ\n\n1\n\nSρ\n\n,\n\n(8.34)\n\nwhere ξ 1 is just a typical entry of the random pattern vector. Finally, we obtain\n\nZn ∝\n\ndm1ρ dqρσ drρσ exp\n\nρ\n\nρ ,σ\n\n−N 2\n\nαβ 2\n\nρ ,σ\n\nrρσ qρσ\n\nP −1\n\nβN\n\n× exp −\n\nTr ln[I − βQ] exp −\n\n2\n\n2ρ\n\nm\n\n1 ρ\n\n2+N\n\nln Tr eβ Hξ1\n\nξ1\n\n.\n\n(8.35)\n\nBecause we assume that N is large enough, we can use the Laplace’s method, which\n\nis\n\nb\neN f (z)dz ≈\n\n2π\n\neN f (z0).\n\na\n\n−N f (z0)\n\n(8.36)\n\n72\n\n8 Statistical Mechanical Theory of Hopﬁeld Model\n\nwhere z0 is the maximum point. Thus, we can perform the approximation Z n ∼ eN F(θ∗), where F(θ ∗) = maxθ F(θ ). Here, we use θ to indicate the order parameter set of the model.\nThen the quenched disorder averaged free energy becomes\n\nln Z = lim ln Z n = lim ln eN F(θ∗) = N lim F(θ ∗) ,\n\nn→0 n\n\nn→0 n\n\nn→0 n\n\n(8.37)\n\nwhere\n\nF\n\nrρσ\n\n,\n\nqρ\n\nσ\n\n,\n\nm\n\n1 ρ\n\n= − αβ2 2\n\nρ ,σ\n\nrρσ qρσ\n\n−\n\nα 2\n\nTr ln[I − βQ]\n\nβ −\n2ρ\n\nm\n\n1 ρ\n\n2+\n\nln Tr eβ Hξ1\n\nξ1\n\n.\n\n(8.38)\n\nWe have taken the approximation P − 1 P = α N as N is large enough. Note that\n\n(rρσ\n\n,\n\nqρσ\n\n,\n\nm\n\n1 ρ\n\n)\n\nis\n\nthe\n\norder\n\nparameter\n\nset\n\nof\n\nthe\n\nmodel.\n\nTheir\n\nphysical\n\nmeanings\n\nwill\n\nbe clear in the following analysis.\n\nTo calculate the maximum of F\n\nrρ\n\nσ\n\n,\n\nqρσ\n\n,\n\nm\n\n1 ρ\n\n, we ﬁrst calculate the derivatives\n\nof F\n\nrρ\n\nσ\n\n,\n\nqρσ\n\n,\n\nm\n\n1 ρ\n\nwith respect to the order parameters.\n\nFirst, we take a derivative with respect to qρσ ,\n\n⎡\n\n∂F ∂ qρ σ\n\n=0⇒\n\n∂ ∂ qρ σ\n\n⎣− N αβ2 2\n\nρ ,σ\n\nrρσ qρσ\n\n+\n\nβ 2\n\n(\n\n⎤\n\nβ N )2\n\nmμρ qρσ\n\nm\n\nμ σ\n\n⎦\n\n=\n\n0\n\n,\n\nμ≥2 ρ,σ\n\n(8.39)\n\nwhere the second term inside the bracket comes from the original formula [Eq. (8.27)] in which the integral over {mμρ } is kept. Note that the magnetization is rescaled back. We then obtain the conjugated order parameter\n\nrρσ\n\n=\n\n1 α\n\nm μρ\n\nm\n\nμ σ\n\n,\n\nμ≥2\n\n(8.40)\n\nwhere\n\nwe\n\nneed\n\nto\n\nuse\n\nthe\n\nrescaling\n\nm\n\nμ ρ\n\n→\n\n√m\n\nμ ρ\n\nβN\n\nthat is done before. rρσ\n\nis thus\n\nunderstood as the sum of effects of non-condensed patterns (only one retrieved pattern\n\nhere).\n\nSecond,\n\nwe\n\ntake\n\na\n\nderivative\n\nwith\n\nrespect\n\nto\n\nm\n\nμ ρ\n\n[see\n\nthe\n\noriginal\n\nformula\n\nEq. (8.20)]\n\n8.2 Replica Method\n\n73\n\n∂F\n\n∂\n\nm\n\nμ ρ\n\n=\n\n0\n\n⇒\n\n∂\n\n∂\n\nm\n\nμ ρ\n\n−βN 2\n\nm\n\nμ ρ\n\n2\n\n+\n\nβ\n\nm\n\nμ ρ\n\ni\n\nξiμ Siρ\n\n=0,\n\n(8.41)\n\nand obtain\n\nm\n\nμ ρ\n\n=\n\n1 N\n\nξiμ Siρ .\n\ni\n\n(8.42)\n\nThe\n\nparameter\n\nm\n\nμ ρ\n\nis\n\nexactly\n\nthe\n\noverlap\n\nbetween\n\nthe\n\nstate\n\nof\n\nthe\n\nsystem\n\nand\n\nthe\n\nμth\n\npattern, characterizing the quality of memory retrieval.\n\nFinally, from the requirement of a stationary free energy [see Eq. (8.32)]\n\n⎡\n\n⎤\n\n∂F ∂rρσ\n\n=0⇒\n\n∂ ∂rρσ\n\n⎣− N αβ2 2\n\nρ ,σ\n\nrρσ qρσ\n\n+\n\nαβ 2 2\n\ni,ρ ,σ\n\nrρσ Siρ Siσ ⎦\n\n=\n\n0\n\n,\n\n(8.43)\n\nwe obtain the Edwards–Anderson order parameter\n\n1 qρσ = N\n\nSiρ Siσ .\n\ni\n\n(8.44)\n\nqρσ is understood as the mutual overlap of two pure states in general. If a single state dominates the phase space, the Edwards–Anderson order parameter characterizes the size of that state.\n\n8.2.1 Replica-Symmetric Ansätz\n\nTo proceed, we need to make an approximation about the overlap matrix, i.e., considering the simplest form—the overlap is invariant under permutation of replica indexes. This is called the replica symmetry (RS) ansätz\n\n⎧\n\n⎨ rρσ = r, ∀ρ, σ\n\n⎩\n\nm 1ρ qρσ\n\n= m, = q,\n\n∀ρ ∀ρ\n\n=\n\nσ\n\n.\n\n(8.45)\n\nThen we have\n\nF(r, q, m) = − αβ2 rq n2 − n − αβ2 nr − α Tr ln[I − βQ]\n\n2\n\n2\n\n2\n\n− β nm2 + ln Tr eβ Hξ1 , 2\n\n(8.46)\n\nand\n\n74\n\n8 Statistical Mechanical Theory of Hopﬁeld Model\n\nN αβ2rq N αβ2r α N Tr ln[I − βQ] β N m2\n\nln Z =\n\n−\n\n− lim\n\n−\n\n2\n\n2\n\n2 n→0\n\nn\n\n2\n\nln Tr eβ Hξ1\n\n+ N lim\n\n,\n\nn→0\n\nn\n\n(8.47)\n\nwhere\n\nβ Hξ1 = βmξ 1\n\nρ\n\nSρ + 1 αβ2r Sρ Sσ .\n\n2\n\nρ ,σ\n\n(8.48)\n\nFirst, we calculate the last term of ln Z .\n\nTr eβ Hξ1 = Tr eβmξ1\n\n( ρ\n\nSρ\n\n+\n\n1 2\n\nαβ\n\n2\n\nr\n\n) ρ Sρ 2\n\n:= Tr e A( ) ρ Sρ 2+B ρ Sρ\n\n= Tr\n\nA π\n\nd z e−Az2+2 Az ρ Sρ +B ρ Sρ\n\n=\n\nA π\n\nd z e−Az2 Tr e(2Az+B)Sρ\nρ\n\n= αβ2r 2π\n\ndz\n\ne−\n\n1 2\n\nαβ2r z2\n\n2 cosh\n\nαβ2r z + βmξ 1\n\nn\n\n= αβ2r 2π\n\ndz\n\ne−\n\n1 2\n\nαβ2r z2+n\n\nln[2\n\ncosh(αβ2r z+βmξ 1)]\n\n=1 2π\n\ndz\n\ne−\n\n1 2\n\nz\n\n2\n\n+n\n\nln[2\n\ncosh(β\n\n√αr\n\nz+β\n\nm\n\nξ\n\n1\n\n)]\n\n.\n\n(8.49)\n\nNote that A and B are auxiliary variables in intermediate computations. The limit of the above term is clearly given by\n\nlim Tr eβ Hξ1 = 1\n\nn→0\n\n2π\n\ndz\n\ne−\n\n1 2\n\nz2\n\n=\n\n1\n\n.\n\nThus, we can obtain the limit by the derivative with respect to n\n\n(8.50)\n\n8.2 Replica Method\n\nln Tr eβ Hξ1\n\nlim\n\nn→0\n\nn\n\n=\n\nlim\nn→0\n\nd dn\n\nTr\n\neβ Hξ1\n\nTr eβ Hξ1\n\n=\n\n1\n\nd\n\nlim\n\nd\n\nz\n\ne−\n\n1 2\n\nz 2 +n\n\nln[2\n\ncosh(β\n\n√ ar\n\nz+βmξ\n\n1)]\n\n2π n→0 dn\n\n=\n\n1 lim\n\nd\n\nz\n\ne−\n\n1 2\n\nz2\n\nd\n\n2 cosh\n\nβ\n\n√ αr\n\nz\n\n+\n\nβ\n\nmξ\n\n1\n\nn\n\n2π n→0\n\ndn\n\n=\n\n1 lim\n\nd\n\nz\n\ne−\n\n1 2\n\nz2\n\n2 cosh\n\n√ β αr\n\nz\n\n+\n\nβmξ\n\n1\n\nn\n\n2π n→0\n\nln\n\n2 cosh\n\n√ β αr\n\nz\n\n+\n\nβmξ\n\n1\n\n=\n\n1\n\ndz\n\ne−\n\n1 2\n\nz2\n\nlim\n\n2 cosh\n\n√ β αr\n\nz\n\n+\n\nβmξ\n\n1\n\nn\n\n2π\n\nn→0\n\nln\n\n2 cosh\n\n√ β αr\n\nz\n\n+\n\nβmξ\n\n1\n\n=\n\n1\n\ndz\n\ne−\n\n1 2\n\nz2\n\nln\n\n2 cosh\n\n√ β αr\n\nz\n\n+\n\nβmξ\n\n1\n\n2π\n\n=\n\nDz\n\nln\n\n2 cosh\n\n√ β αr\n\nz\n\n+\n\nβmξ\n\n1\n\n.\n\n75\n(8.51)\n\nThen we calculate the third term of ln Z . Since Q is a symmetric matrix, we can diagonalize this matrix and get\n\nAQA−1 = = diag(λ1, λ2, . . . , λn) .\n\n(8.52)\n\nWe can thus expand ln[I − βQ] to a power series with respect to Q (here we take\n\nthe formula ln(1 − x) = −\n\n∞ n=1\n\nxn n\n\n)\n\nand\n\nobtain\n\nTr ln[I − βQ] = Tr A · ln[I − βQ] · A−1\n\n∞ βl AQA−1 l = − Tr\nl\nl =1\n\n= − Tr ∞ βl ( )l l\nl =1\n\n= − ∞ βl l\n\nn\n\nλli =\n\nn\n\nln [1 − βλi ] .\n\nl=1 i=1\n\ni =1\n\n(8.53)\n\n76\n\n8 Statistical Mechanical Theory of Hopﬁeld Model\n\nThis result is equivalent to the matrix identity: Tr ln K = ln det K for a positive deﬁnite matrix.\nThen we calculate the eigenvalues of Q by\n\n1−λ q ··· q\n\nq 1−λ ··· q\n\n...\n\n...\n\n...\n\nq q ··· 1−λ\n\n1 − λ + (n − 1)q 1 − λ + (n − 1)q · · · 1 − λ + (n − 1)q\n\nq\n\n=\n\n...\n\n1−λ\n\n···\n\nq\n\n...\n\n...\n\nq\n\nq\n\n···\n\n1−λ\n\n1 1 ··· 1\n\nq 1−λ ··· q\n\n= [1 − λ + (n − 1)q] ... ...\n\n...\n\nq q ··· 1−λ\n\n1 1 ··· 1\n\n0 1−λ−q ··· 0\n\n= [1 − λ + (n − 1)q] ...\n\n...\n\n...\n\n0 0 ··· 1−λ−q\n\n= [1 − λ + (n − 1)q](1 − q − λ)n−1 = 0 .\n\n(8.54)\n\nThus, Q have one eigenvalue with the value (1 + (n − 1)q) and (n − 1) eigenvalues with values (1 − q). Then the trace turns out to be\n\nTr ln[I − βQ] = ln(1 − β + βq − nβq) + (n − 1) ln(1 − β + βq) , (8.55)\n\nand\n\nlim Tr ln[I − βQ] = lim\n\nln(\n\n1−β +β q −nβ q 1−β+βq\n\n)\n\n+\n\nln(1\n\n−\n\nβ\n\n+\n\nβq)\n\nn→0\n\nn\n\nn→0\n\nn\n\n=\n\n− 1\n\n−\n\nβq β+\n\nβq\n\n+\n\nln(1\n\n−\n\nβ\n\n+\n\nβq)\n\n,\n\n(8.56)\n\nwhere we calculate the limit by the L Hospital’s rule. Taken all together, the free energy of the Hopﬁeld model can be written as\n\n8.2 Replica Method\n\n77\n\n−β f = 1 ln Z N\n\nαβ 2\n\nα\n\nβq\n\n=\n\nr (q − 1) −\n\n2\n\n2\n\nln(1 − β + βq) − 1 − β + βq\n\n+\n\nDz\n\nln\n\n2 cosh\n\nβ\n\n√ αr\n\nz\n\n+\n\nβ\n\nm\n\nξ\n\n1\n\n.\n\n− β m2 2\n\n(8.57)\n\nTo complete the Laplace method, we ﬁnally derive the saddle-point equations for all order parameters in the RS ansätz. More precisely, we take derivatives of the free energy with respect to all the order parameters\n\n⎧\n\n⎪⎨ ⎪⎩\n\n∂(−β f ) ∂(−∂rβ f ) ∂(−∂mβ f )\n∂q\n\n= = =\n\n0 0 0\n\n,\n\n(8.58)\n\nand get\n\nq = − √1 β 2παr\n\nd\n\nz\n\ne−\n\n1 2\n\nz2\n\nz\n\ntanh\n\n√ β αr\n\nz\n\n+\n\nβmξ\n\n1\n\n+1\n\n= √1 β 2παr\n\nd\n\ne−\n\n1 2\n\nz2\n\ndz\n\ntanh\n\n√ β αr\n\nz\n\n+\n\nβmξ\n\n1\n\n+1\n\ndz\n\n= √1\n\ne−\n\n1 2\n\nz2\n\ntanh\n\n√ β αr\n\nz\n\n+\n\nβmξ\n\n1\n\n+∞\n\nβ 2παr\n\n−∞\n\n−\n\nDz\n\n1 − tanh2\n\n√ β αr\n\nz\n\n+\n\nβmξ\n\n1\n\n+1\n\n=\n\nDz\n\ntanh2\n\n√ β αr\n\nz\n\n+\n\nβmξ\n\n1\n\n=\n\nD\n\nz\n\ntanh2\n\nβ\n\n√ ( αr\n\nz\n\n+\n\nm)\n\n.\n\n(8.59)\n\nMoreover, r and m can be analogously computed, which leads to the following saddle-point equations for the associative memory model.\n\nq=\n\nDz\n\ntanh2\n\n√ β( αr\n\nz\n\n+\n\nm)\n\n,\n\n√ m = Dz ξ tanh β( αr z + mξ ) =\n\nr=\n\nq\n\n.\n\n(1 − β + βq)2\n\n√ Dz tanh β( αr z + m) ,\n\n(8.60) (8.61) (8.62)\n\nPhase transitions can be deduced from an analysis of the behavior of these equations and the corresponding free energy function.\n\n78\n\n8 Statistical Mechanical Theory of Hopﬁeld Model\n\n8.2.2 Zero-Temperature Limit\n\nUnder the replica-symmetric assumption, as T → 0 (β → ∞), we have\n\n⎧ ⎪⎨1 x > 0\n\ntanh(βx) → sign(x) = ⎪⎩0−1\n\nx=0 , x <0\n\n(8.63)\n\nEquation (8.61) becomes\n\n√ m = Dz sign( αr z + m) + O(T )\n\n= erf √m + O(T ) . 2αr\n\nOn the other hand, as β → ∞\n\n1−q =\n\n√d z\n\ne−\n\nz2 2\n\n1\n\n−\n\ntanh2\n\n√ β( αr\n\nz\n\n+\n\nm)\n\n2π\n\n√1\n\ne−\n\nz2 2\n\n2π\n\ntanh2\n\nβ\n\n√ ( αr\n\nz+m\n\n)=0\n\ndz\n\n1\n\n−\n\ntanh2\n\n√ β( αr\n\nz\n\n+\n\nm)\n\n= √1\n\ne−\n\nm2 2αr\n\n√1\n\n2π β αr\n\nd\n\nz\n\n∂ ∂z\n\ntanh\n\n√ β( αr\n\nz\n\n+\n\nm)\n\n= √2\n\n√1\n\ne . −\n\nm2 2αr\n\n2π β αr\n\nEquation (8.60) thus yields q = 1 − C T , where\n\n(8.64) (8.65)\n\nC d=ef\n\n2\n\ne . −\n\nm2 2αr\n\nπrα\n\n(8.66)\n\nUsing these intermediate results, Eq. (8.62) becomes r = (1 − C)−2. The equations√of m and r can be reduced to one equation, by deﬁning an auxiliary\nvariable y = m/ 2αr . We then have\n\nerf(y) = y\n\n√ 2α\n\n+\n\n√2\n\ne−y2\n\n.\n\nπ\n\n(8.67)\n\nOne solution is given by y = m = 0, which is a spin glass (SG) solution. For α ≥ αc = 0.138, this is the unique solution. For a < αc, Ferromagnetic solutions m = 0 appear (2P such solutions, due to the model symmetry). At α = αc, the overlap m takes the value m = 0.967 [6].\n\n8.2 Replica Method\n\n79\n\nFig. 8.3 The error probability as a function of α at T = 0\nEquation (8.67) can be solved numerically. By using the relation m = erf(y), we can obtain the values of m. The error probability is given by Perror = (1 − m)/2, which is shown in Fig. 8.3. From the plot, we can see that there is a critical value αc = 0.138 where the error probability jumps to 1/2, indicating a discontinuous transition to a spin glass phase. When α < αc, the error probability is quite low, which means that the network can reliably retrieve one of the stored patterns. When α > αc, the error probability is 1/2, suggesting the network could not have a signiﬁcant memory.\n8.3 Phase Diagram\nBy solving Eqs. (8.61), (8.60) and (8.62) numerically, we can obtain the phase diagram of the Hopﬁeld network (Fig. 8.4) [3, 6]. At a very high temperature, the thermal noise impairs the retrieval process, therefore m = 0, q = 0 and r = 0. Interesting, from an inverse Ising perspective, given the conﬁgurations from this phase, the couplings of the model can be easily inferred by a reverse engineering process [7, 8]. As the temperature is lowered down, the paramagnetic phase becomes unsta-\n\n80\n\n8 Statistical Mechanical Theory of Hopﬁeld Model\n\nble at a critical temperature-load line (Tg(α)), which can be ob√tained analytically through a linear stability analysis of Eq. (8.60), i.e., Tg = 1 + α, where α is the\n\nmemory load.\n\nOn the other hand, with decreasing memory load, the spin glass phase becomes\n\nmetastable at a critical line TM (α), where the retrieval phase becomes locally stable. This transition is thus a ﬁrst-order phase transition. In this phase, spurious states (i.e.,\n\na linear combination of several stored patterns) also emerge as metastable states. Once\n\nα < 0.051, the retrieval phase becomes globally stable when a critical temperature\n\nline Tc is crossed. The discontinuous transition point can be obtained by analyzing\n\nthe saddle-poin√t equation, and equalin√g the free energies of two competing phases. TM 1 − 1.95 α, and Tc 1 − 2.6 α [6].\n\nAt T\n\n= 0, the entropy per spin S = −\n\n∂f ∂T\n\nT →0\n\n=\n\n−\n\n1 2\n\nα[ln(1\n\n−\n\nC)\n\n+\n\nC /(1\n\n−\n\nC )]\n\nwith C = β(1 − q) is negative for all replica-symmetric solutions, which is unphys-\n\nical. Below the dashed line (so-called AT line in spin glass theory; see Chap. 9) in\n\nFig. 8.4, the retrieval states become unstable, the replica symmetry breaking (RSB)\n\neffects should be considered (a general introduction of RSB will be presented in\n\nP\n\n1\n\nSG\n\nRetrieval\n\nstable\n\nmetastable\n\n0\n\n0.05\n\n0.138\n\nFig. 8.4 The phase diagram of Hopﬁeld model (adapted from Ref. [3]). Three phases (paramagnetic, spin glass and retrieval) exist. The paramagnetic phase is separated by a continuous transition to the spin glass phase (Tg line). The phase transition from retrieval phase to spin glass phase on the TM is discontinuous. Below Tc line, the retrieval phase becomes globally stable. Below the dash line (TR), the replica-symmetric solution becomes unstable\n\n8.3 Phase Diagram\n\n81\n\nChap. 9). In physics, this implies that the permutation symmetry of replica indexes in the overlap matrix does not hold, requiring that a higher level of approximation should be taken. However, as shown in the Fig. 8.4, the RSB effect in the retrieval phase is very weak. As α → ∞, the Hopﬁeld model reduces to the well-known SK model.\n\n8.4 Hopﬁeld Model with Arbitrary Hebbian Length\n\nIn this section, we generalize the standard Hopﬁeld model to the case of arbitrary Hebbian length. This is inspired by the Monkey experiments where the monkey is trained to recognize and match visual stimuli, the temporal order of the stimulus presentations is maintained during training. The experiments revealed that the monkey’s temporal cortex is able to convert the temporal association of stimuli into a spatial correlation in the patterns of sustained activities [9, 10]. This experimental result was ﬁrst modeled by Griniasty et.al. [11], who takes one Hebbian length into the construction of the coupling matrix, i.e., the neighboring patterns in the sequence of presentation contribute to Hebbian learning. In this model, a novel phase of correlated- attractors emerges due to this revised Hebbian rule. The correlated attractor triggered by one stimulus pattern becomes correlated with neighboring patterns around the stimulus, although the patterns themselves are all independent.\nMotivated by the observation that Hebbian learning can occur in a wider learning window [12, 13], we propose to extend the Hebbian length to an arbitrary value [14], and thus deﬁne the following coupling matrix of neurons:\n\nJi j\n\n=\n\n1 N\n\nP\n\nd\n\ncξiμξ\n\nμ j\n\n+\n\nγ\n\nξiμξ\n\nμ+r j\n\n+\n\nξiμ+r\n\nξ\n\nμ j\n\n,\n\nμ=1\n\nr =1\n\n(8.68)\n\nwhere c speciﬁes the standard Hebbian strength, γ speciﬁes the coupling strength\n\nbetween r -separated patterns, and d is thus the Hebbian length of our model. The\n\ncase of d = 1 has been studied by previous works [11, 15], while d = 0 recovers the\n\nstandard Hopﬁeld model [1–3].\n\ni.e.,\n\np(ξiμ\n\n=\n\n±1)\n\n=\n\n1 2\n\nδ(ξiμ\n\n+ 1)\n\nξiμ\n\n+\n\n1 2\n\nfollows δ(ξiμ −\n\nindependently a binomial 1). We are interested in the\n\ndistribution, limit of large\n\nvalues of\n\nP\n\nand\n\nN , thereby deﬁning α\n\n=\n\nP N\n\n.\n\nα\n\nis also called the memory load of the\n\nassociative memory model.\n\n8.4.1 Computation of the Disorder-Averaged Free Energy\nThe matrix J can be recast into the form\n\n82\n\n8 Statistical Mechanical Theory of Hopﬁeld Model\n\nJ = 1 ξ TXξ , N\n\n(8.69)\n\nwhere X is a P × P circulant matrix, a special form of Toeplitz matrix with elements\n\nd\n\nXμη = cδμη + γ\n\nδμ,(η+r) mod P + δμ,(η−r) mod P\n\nr =1\n\nd\n\n= (c − γ )δμη + γ\n\nδμ,(η+r) mod P .\n\nr =−d\n\n(8.70)\n\nThe mth eigenvalue of X is given by [16]\n\nP −1\n\nλm =\n\nX 1(k +1) e−2π im k / P\n\nk=0\n\nP −1\n= X1(k+1) cos\n\n2π mk P\n\nk=0\n\nP −1\n=\n\ncδ0k + γ\n\nd\n(δ0,(k+r) mod P + δ0,(k−r) mod P )\n\ncos\n\n2π mk P\n\nk=0\n\nr =1\n\nd\n=c+γ\n\ncos −2π mr + cos 2π mr\n\nr =1\n\nP\n\nP\n\n= c + 2γ d cos 2π mr , P\nr =1\n\nfor m = 0, 1, . . . , P − 1. The Hamiltonian of the model is deﬁned by\n\n(8.71)\n\nH(s) = − 1 2\n\nJi j si s j .\n\ni=j\n\n(8.72)\n\nThe partition function is thus given by\n\nZ = Tr exp β sTξ TXξ s , 2N\n\n(8.73)\n\nwhere Tr indicates the summation over all discrete states s. In general, to compute a disorder averaged free energy ( −T ln Z ) is a computationally hard task. However, the well-known replica trick developed in spin glass theory [4] can be used to get around the difﬁculty, but assumptions on the replica matrix are required (detailed below). The replica method uses the mathematical identity\n\n8.4 Hopﬁeld Model with Arbitrary Hebbian Length\n\n83\n\nln Z = lim ln Z n , n→0 n\n\n(8.74)\n\nwhere · denotes the expectation over the distribution of ξ . To proceed, we have to compute an integer-power of the partition function\n\nZ n = Tr exp β n sa T ξ TXξ sa . 2N\na=1\n\n(8.75)\n\nWe consider the situation where there are S condensed (or foreground) patterns\n\nand P − S non-condensed (or background) patterns, which is reasonable in our\n\ncurrent setting. The choice of S can be justiﬁed a posterior, e.g., through solving the\n\nmean-ﬁeld dynamics or saddle-point equations. Thus, we can reorganize the matrix\n\nX as a block matrix, i.e.\n\nX=\n\nXFF XFB XBF XBB\n\n,\n\n(8.76)\n\nwhere XF F ∈ RS×S , XTB F = XF B ∈ RS×(P−S) and XB B ∈ R(P−S)×(P−S). It then follows that\n\n⎡\n\nZn\n\n=\n\nTr exp ⎣ β 2N\n\nsia\n\nξiμ\n\nX\n\nμν\n\nξ\n\nν j\n\ns aj\n\na,i, j,μ∈B,ν∈B\n\n+\n\nβ N\n\nsia\n\nξiμ\n\nX\n\nμν\n\nξ\n\nν j\n\ns\n\na j\n\na,i, j,μ∈B,ν∈F\n\n⎤\n\n+β 2N\n\nsia\n\nξiμ\n\nX\n\nμν\n\nξ\n\nν j\n\ns aj\n\n⎦\n\n.\n\na,i, j,μ∈F,ν∈F\n\n(8.77) We then diagonalize the submatrix XB B as XμBνB = σ λσ ημσ ηνσ , where λσ and ημσ are denoted as its eigenvalues and eigenvectors, respectively. We thus obtain\n\n⎡\n\n⎛\n\n⎞2\n\n⎤\n\nZ n = Tr exp ⎢⎣ β 2N\n\nλσ ⎝\n\nsia ξiμημσ ⎠\n\na,σ\n\ni ,μ∈ B\n\n+\n\nβ N\n\na,i, j,μ∈B,ν∈F\n\nsia\n\nξiμ\n\nX\n\nμν\n\nξ\n\nν j\n\nsaj\n\n⎥⎦\n\n⎤\n\n= Tr\na,σ\n\n+β 2N\n\nsia\n\nξiμ\n\nX\n\nμν\n\nξ\n\nν j\n\nsaj\n\n⎦\n\na,i, j,μ∈F,ν∈F\n\n⎡\n\n⎛\n\n⎞\n\nD xσa\n\nexp ⎣\ni ,μ∈ B\n\n√ξiμ N\n\n⎝\na,σ\n\nsia ημσ\n\nβλσ xσa\n\n+\n\n√β N\n\nsia\n\nX\n\nμν\n\nξ\n\nν j\n\nsaj\n\n⎠\n\na, j,ν∈F\n\n⎤\n\n+β 2N\n\nsia\n\nξiμ\n\nX μν\n\nξ\n\nν j\n\nsaj\n\n⎦\n\na,i, j,μ∈F,ν∈F\n\n,\n\n(8.78)\n\n84\n\n8 Statistical Mechanical Theory of Hopﬁeld Model\n\nwhere we have used the Hubbard–Stratonovich transformation, i.e., exp\n\n1 2\n\nb2\n\n=\n\nDx\n\nexp [±bx],\n\nwhere\n\nDx\n\n=\n\n√1 2π\n\nexp\n\n−\n\nx2 2\n\ndx.\n\nWe then deﬁne\n\n⎡\n\n⎛\n\nB\n\n=\n\nexp ⎣\ni ,μ∈ B\n\n√ξiμ N\n\n⎝\na,σ\n\nsia ημσ\n\n⎞⎤\n\nβ λσ\n\nxσa\n\n+\n\n√β N\n\nsia\n\nX\n\nμν\n\nξ\n\nν j\n\ns\n\na j\n\n⎠⎦\n\na, j,ν∈F\n\n,\n\n(8.79)\n\nand\n\n⎡\n\n⎤\n\nF\n\n=\n\nexp ⎣ β 2N\n\nsia\n\nξiμ\n\nX\n\nμν\n\nξ\n\nν j\n\ns\n\na j\n\n⎦\n\n.\n\na,i, j,μ∈F,ν∈F\n\nTaking the disorder average over {ξiμ}, we write the result as\n\n(8.80)\n\nZ n = Tr\na,σ\n\nDxσa B F .\n\n(8.81)\n\nWe ﬁrst carry out the average over the distribution of background patterns, which yields\n\nB\n\n⎧\n\n⎡\n\n⎛\n\n=\n\nexp\n\n⎨ ⎩\n\n1 2N\n\n⎣\n\ni,μ∈B a\n\nsia ⎝\nσ\n\nημσ\n\nβ λσ\n\nxσa\n\n+\n\n√β N\n\n⎞⎤2⎫⎬\n\nX\n\nμν\n\nξ\n\nν j\n\ns\n\na j\n\n⎠⎦\n\n⎭\n\nj,ν∈F\n\n.\n\n(8.82)\n\nIntroducing the state overlap as one order parameter:\n\nand\n\nm\n\na μ\n\n=\n\n1 N\n\ni ξiμsia as another order parameter, we\n\nqab = have\n\n1 N\n\nN i\n\nsia sib\n\nfor\n\na\n\n=\n\nb,\n\nB=\n\nd qab d qˆab\n\nd\n\nm\n\na μ\n\nd\n\nmˆ\n\na μ\n\na=b 2π/N a,μ∈F 2π/N\n\n⎡\n\n⎤\n\n×\n\nexp ⎣− 1 N 2\n\nqˆa b qa b\na =b\n\n+\n\n1 2\n\nqˆab\na =b\n\ni\n\nsia sib − N\n\nmaμmˆ aμ +\n\nmˆ\n\na μ\n\na,μ∈F\n\na,μ∈F\n\ni\n\nξiμsia ⎦\n\n⎡\n\n⎤\n\n× exp ⎣ 1 2 μ∈B a\n\nημσ\n\n√ βλσ xσa + β N\n\n2\n\nX\n\nμν\n\nm\n\na ν\n\n⎦\n\nσ\n\nν∈F\n\n⎡\n\n×\n\nexp\n\n⎣\n\n1 2\n\nμ∈ B\n\na =b\n\nqab\n\nημσ\n\n√ βλσ xσa + β N\n\nXμν maν\n\nσ\n\nν∈F\n\n×\n\nημσ\n\n√ βλσ xσb + β N\n\nXμν mbν .\n\nσ\n\nν∈F\n\n(8.83)\n\n8.4 Hopﬁeld Model with Arbitrary Hebbian Length\n\n85\n\nIn the above derivations, we have inserted Dirac delta functions for deﬁning those\n\norder parameters, and then applied the integral representations of these delta func-\n\ntions. The hatted order parameters are the byproducts of conjugated counterparts.\n\nUnder\n\nthe\n\nreplica\n\nsymmetric\n\nansätz\n\nwith\n\nqab\n\n=\n\nq\n\nand\n\nqˆab\n\n=\n\nqˆ\n\nfor\n\na\n\n=\n\nb,\n\nm\n\na μ\n\n=\n\nmμ and mˆ aμ = mˆ μ, we arrive at\n\nB=\n\nd q d qˆ\n\nd m d mˆ\n\n(2π/N )n(n−1)\n\n(2π/N )nS\n\n−\n\nN n mμmˆ μ exp\nμ∈F\n\n− 1 N n(n − 1)qˆq 2\n\n⎤\n\n⎡\n\n+ 1 qˆ 2 a=b\n\ni\n\nsia sib +\n\nmˆ μ\n\na,μ∈F\n\ni\n\nξiμ sia\n\n⎦\n\n×\n\nexp\n\n⎣\n\n1 2\n\nμ∈ B\n\na\n\nημσ βλσ xσa\nσ\n\n⎤\n\n⎡\n\n√ +β N\n\n2\n\nXμν mν\n\n⎦ × exp ⎣ q 2\n\nν∈F\n\nμ∈B a=b\n\nημσ\n\n√ βλσ xσa + β N\n\nXμν mν\n\nσ\n\nν∈F\n\n×\n\nημσ\n\n√ βλσ xσb + β N\n\nXμν mν\n\nσ\n\nν∈F ⎡\n\n=\n\nd q d qˆ (2π/N )n(n−1)\n\nd m d mˆ (2π/N )nS\n\nexp ⎣− 1 2\n\nN n(n\n\n−\n\n1)qˆ q\n\n+\n\n1 qˆ 2\n\na =b\n\ni\n\nsia sib − N n mμmˆ μ\nμ∈F\n\n⎤\n\n⎡\n\n⎤\n\n+\n\nmˆ μ\n\na,μ∈F\n\ni\n\nξiμsia ⎦\n\n×\n\nexp ⎣ 1\n\n−q 2\n\nμ∈ B\n\na\n\nημσ\n\n√ βλσ xσa + β N\n\n2\nXμν mν ⎦\n\nσ\n\nν∈F\n\n⎡\n\n⎤\n\n× exp ⎣ q 2 μ∈B\n\nημσ\na,σ\n\n√ βλσ xσa + βn N\n\n2\nXμν mν ⎦ .\n\nν∈F\n\n(8.84)\n\nWe apply the Hubbard–Stratonovich transformation once again, and obtain\n\nB=\n\nd q d qˆ\n\nd m d mˆ\n\n(2π/N )n(n−1) (2π/N )nS\n\nD yμa\nμ,a\n\nμ\n\nDzμ\n\n⎡\n\n⎤\n\n× exp ⎣− 1 N n(n − 1)qˆq + 1 qˆ\n\n2\n\n2 a=b\n\ni\n\nsia sib − N n mμmˆ μ +\n\nmˆ μ\n\nμ∈F\n\na,μ∈F\n\ni\n\nξiμsia ⎦\n\n⎡\n\n⎤\n\n× exp ⎣ 1 − q\n\nημσ\n\n√ βλσ xσa + β N\n\nXμν mν yμa ⎦\n\nμ∈B a σ\n⎡\n\nν∈F\n⎤\n\n× exp ⎣√q\n\nημσ\n\n√ βλσ xσa + βn N\n\nXμν mν zμ⎦ .\n\nμ∈B a,σ\n\nν∈F\n\nBy collecting terms containing xσa , we have\n\n(8.85)\n\n86\n\n8 Statistical Mechanical Theory of Hopﬁeld Model\n\nB=\n\nd q d qˆ\n\nd m d mˆ\n\n(2π/N )n(n−1) (2π/N )nS\n\nD yμa\nμ,a\n\nμ\n\nDzμ\n\n⎡\n\n⎤\n\n× exp ⎣− 1 N n(n − 1)qˆq + 1 qˆ\n\n2\n\n2 a=b\n\ni\n\nsia sib − N n mμmˆ μ +\n\nmˆ μ\n\nμ∈F\n\na,μ∈F\n\ni\n\nξiμsia ⎦\n\n⎡\n\n⎤\n\n× exp ⎣ xσa βλσ\n\nημσ\n\n1\n\n−\n\nq\n\nyμa\n\n+\n\n√ qzμ\n\n⎦\n\na,σ\n\nμ∈ B\n\n⎡\n\n⎤\n\n× exp ⎣β√N\n\nXμν mν\n\n1\n\n−\n\nq\n\nyμa\n\n+\n\n√ qzμ\n\n⎦.\n\na,μ∈B ν∈F\n\n(8.86)\n\nAccording to the deﬁnition of the overlap, F can be written as\n\n⎡\n\n⎤\n\nF\n\n=\n\nexp ⎣ βn N 2\n\nmμ Xμν mν ⎦ .\n\nμ∈F,ν∈F\n\n(8.87)\n\nCollecting all the results derived above, we have\n\nZ n = Tr\n\na,σ\n\nD xσa\n\nd q d qˆ (2π/N )n(n−1)\n\nd m d mˆ (2π/N )nS\n\nD yμa\nμ,a\n\nμ\n\nDzμ\n\n⎡\n\n⎤\n\n× exp ⎣− 1 N n(n − 1)qˆq + 1 qˆ\n\n2\n\n2\n\nsia sib − N n mμmˆ μ⎦\n\na=b i\n\nμ∈F\n\n⎡\n\n⎤\n\n⎡\n\n⎤\n\n× exp ⎣\n\nmˆ μ ξiμsia ⎦ × exp ⎣ xσa βλσ\n\nημσ\n\na,μ∈F\n\ni\n\na,σ\n\nμ∈ B\n\n⎡\n\n⎤\n\n× exp ⎣β√N\n\nXμν mν\n\n1\n\n−\n\nq\n\nyμa\n\n+\n\n√ qzμ\n\n⎦\n\na,μ∈B ν∈F\n\n⎡\n\n⎤\n\n1\n\n−\n\nq\n\nyμa\n\n+\n\n√ qzμ\n\n⎦\n\n× exp ⎣ βn N 2\n\nmμ Xμν mν ⎦\nμ∈F,ν∈F\n\n.\n\nWe deﬁne the term summing over {sia} as\n\n(8.88)\n\n8.4 Hopﬁeld Model with Arbitrary Hebbian Length\n\n87\n\n⎡\n\n⎤\n\nS\n\n=\n\nTr exp ⎣ 1 qˆ 2 a=b\n\ni\n\nsia sib +\n\nmˆ μ\n\na,μ∈F\n\ni\n\nξiμsia ⎦\n\n⎡\n\n⎤\n\n2\n\n= exp − n N qˆ Tr exp ⎣ 1 qˆ\n\n2\n\n2\n\ni\n\nsia\na\n\n+\n\nmˆ μξiμsia⎦\n\na,μ∈F\n\n⎧\n\n⎡\n\n= exp\n\n− n N qˆ 2\n\n⎨ ⎩\n\nTr exp ⎣ 1 qˆ 2\n\n2\n\n⎤ ⎫⎬N\n\nsa\na\n\n+\n\nmˆ μξ μsa⎦ ⎭ .\n\na,μ∈F\n\n(8.89)\n\nApplying the Hubbard–Stratonovich transformation, we obtain\n\nnN S = exp − 2 qˆ\n= exp − n N qˆ 2\n= exp − n N qˆ 2\nIn the limit n → 0,\n\n⎧\n\n⎡\n\n⎨\n\n⎤ ⎫⎬N\n\n⎩ Dz Tr exp ⎣ qˆsa z + mˆ μξ μsa⎦ ⎭\n\na\n\nμ∈F\n\n⎧ ⎨\n\n⎡\n\n⎤\n\n⎫ ⎬\n\nN\n\n⎩ Dz 2 cosh ⎣ qˆ z + mˆ μξ μ⎦ ⎭\n\na\n\nμ∈F\n\n⎧⎡\n\n⎛\n\n⎞ ⎤⎫\n\n⎨\n\n⎬\n\nexp ⎩N ln ⎣\n\nDz 2n coshn ⎝ qˆ z +\n\nmˆ μξ μ⎠\n\n⎦ ⎭\n\n.\n\nμ∈F\n\n(8.90)\n\n⎧\n\nS\n\n= exp\n\n− n N qˆ 2\n\n⎨ exp ⎩n N\n\nTaken together, we have\n\n⎡\n\n⎛\n\nDz ln ⎣2 cosh ⎝\n\n⎞⎤ ⎫ ⎬\nqˆ z + mˆ μξ μ⎠⎦ ⎭ .\nμ∈F\n(8.91)\n\n88\n\n8 Statistical Mechanical Theory of Hopﬁeld Model\n\nZn =\n\na,σ\n\nD xσa\n\nd q d qˆ (2π/N )n(n−1)\n\nd m d mˆ (2π/N )nS\n\nμ,a\n\nD yμa\n\nμ\n\nDzμ\n\n⎡\n\n⎤\n\n× exp ⎣− 1 N n(n 2\n\n− 1)qˆq\n\n−\n\nn N qˆ 2\n\n−\n\nN n mμmˆ μ\nμ∈F\n\n+\n\nβnN 2\n\nmμ Xμν mν ⎦\nμ∈F,ν∈F\n\n⎡\n\n⎤\n\n× exp ⎣ xσa βλσ\n\nημσ\n\n1\n\n−\n\nq\n\nyμa\n\n+\n\n√ qzμ\n\n⎦\n\na,σ\n\nμ∈ B\n\n⎡\n\n⎤\n\n× exp ⎣β√N\n\nXμν mν\n\n1\n\n−\n\nq\n\nyμa\n\n+\n\n√ q\n\nzμ\n\n⎦\n\n⎧ ⎨ × exp ⎩n N\n\na,μ∈B ν∈F\n\n⎡\n\n⎛\n\nDz ln ⎣2 cosh ⎝\n\n⎞⎤ ⎫ ⎬\nqˆ z + mˆ μξ μ⎠⎦ ⎭ .\nμ∈F\n\n(8.92)\n\nTo proceed, we ﬁrst denote the vectors ya = [yμa ; μ ∈ B]T, z = [zμ; μ ∈ B]T,\n\nm = [mμ; μ ∈ F]T, mˆ = [mˆ μ; μ ∈ F]T and ξ F = [ξ μ; μ ∈ F]T. Integrating out\n\n{xσa }, we get\n\n⎡\n\n⎤\n\nDxσa exp ⎣ xσa βλσ\n\nημσ\n\n1\n\n−\n\nq\n\nyμa\n\n+\n\n√ qzμ\n\n⎦\n\na,σ\n\na,σ\n\nμ∈ B\n\n⎡\n\n⎤\n\n=\n\nexp\n\n⎣\n\n1 2\n\nβ\n\na ,σ,μ∈ B ,ν ∈ B\n\nλσ\n\nημσ ηνσ\n\n(\n\n1\n\n−\n\nq\n\nyμa\n\n+\n\n√ q\n\nz\n\nμ\n\n)(\n\n1\n\n−\n\nq yνa\n\n+\n\n√ q\n\nzν\n\n)⎦\n\n⎡\n\n⎤\n\n=\n\nexp\n\n⎣\n\n1 2\n\nβ\n\na ,μ∈ B ,ν ∈ B\n\nXμν (\n\n1\n\n−\n\nq\n\nyμa\n\n+\n\n√ q\n\nzμ\n\n)(\n\n1\n\n−\n\nq yνa\n\n+\n\n√ q\n\nzν\n\n)⎦\n\n⎡\n\n(8.93)\n\n=\n\nexp\n\n⎣\n\n1 2\n\nβ (1\n\n−\n\nq)\n\na ,μ∈ B ,ν ∈ B\n\nyμa Xμν\n\nyνa\n\n+\n\nβ\n\n(1 − q)q\n\n⎤\n\n×\na ,μ∈ B ,ν ∈ B\n\nzμ Xμν yνa\n\n+\n\n1 nβq 2\n\nμ∈ B ,ν ∈ B\n\nzμ Xμν zν ⎦\n\n= exp\n\n1 β(1 − q) 2\n\na\n\n(ya )TXB B ya + β\n\n(1 − q)q\n\na\n\nzTXB B ya\n\n+\n\n1 2\n\nnβq\n\nzTX\n\nB\n\nB\n\nz\n\n.\n\nCollecting all terms containing {yμa }, we get\n\n"}