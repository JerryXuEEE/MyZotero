{"indexedPages":17,"totalPages":17,"version":"260","text":"nature computational science\n\nArticle\n\nhttps://doi.org/10.1038/s43588-023-00439-w\n\nDecoding reward‚Äìcuriosity conflict in\n\ndecision-making from irrational behaviors\n\nReceived: 21 June 2022 Accepted: 29 March 2023 Published online: 15 May 2023\nCheck for updates\n\nYuki Konaka‚Äâ ‚Ää‚Äâ1 & Honda Naoki‚Äâ ‚Ää‚Äâ1,2,3,4‚Äâ\nHumans and animals are not always rational. They not only rationally exploit rewards but also explore an environment owing to their curiosity. However, the mechanism of such curiosity-driven irrational behavior is largely unknown. Here, we developed a decision-making model for a twochoice task based on the free energy principle, which is a theory integrating recognition and action selection. The model describes irrational behaviors depending on the curiosity level. We also proposed a machine learning method to decode temporal curiosity from behavioral data. By applying it to rat behavioral data, we found that the rat had negative curiosity, reflecting conservative selection sticking to more certain options and that the level of curiosity was upregulated by the expected future information obtained from an uncertain environment. Our decoding approach can be a fundamental tool for identifying the neural basis for reward‚Äìcuriosity conflicts. Furthermore, it could be effective in diagnosing mental disorders.\n\nAnimals and humans perceive the external world through their sensory systems and make decisions accordingly1,2. Generally, they cannot make optimal decisions because of the uncertainty of the environment as well as the limited computational capacity of the brain and time constraints associated with decision-making3. In fact, they perform irrational actions. For example, people play lotteries and gamble despite low reward expectations. In this case, they face a dilemma between low expected reward and curiosity regarding whether a reward will be acquired. Thus, understanding how animals control the balance between reward and curiosity is important for clarifying the whole decision-making process. However, a method is yet to be established for quantifying the reward‚Äìcuriosity balance has yet been established. In this study, we developed a machine learning method to decode the time series of the reward‚Äìcuriosity balance from animal behavioral data.\nSome irrational behaviors emerge because of the strength of curiosity4,5. For example, conservative individuals avoid uncertainty and prefer to select an action that leads to predictable outcomes. Conversely, inquisitive individuals strongly desire to know the environment rather than rewards and prefer to select an action that leads to unpredictable outcomes. Too conservative and inquisitive natures can be interpreted as autism spectrum disorder and attention deficit\n\nhyperactivity disorder, patients with which are known to substantially avoid and seek novel information, respectively6‚Äì14. Rational individuals fall midway between these two extremes. In an ambiguous environment, they select an action to efficiently understand the environment, and if the environment becomes clear, they select an action to efficiently exploit the rewards. Therefore, curiosity has a major impact on behavioral patterns, and it is believed that animals control the balance between reward and curiosity in a context-dependent manner.\nDecision-making has been modeled primarily by reinforcement learning (RL), which is a theory for describing reward-seeking adaptive behavior in which animals not only exploit rewards but also explore the environment. In RL, explorative behavior was addressed by a passive, random choice of action15. However, animals actively explore the environment by selecting actions that minimize the uncertainty of the environment given their curiosity.\nRecently, the free energy principle (FEP) was proposed by Karl Friston under the Bayesian brain hypothesis, in which the brain optimally recognizes the outside world according to Bayesian estimation16‚Äì18. The FEP addresses not only the recognition of the external world but also the information-seeking action selection, which minimizes the uncertainty of the recognition of the external world, known as ‚Äúactive\n\n1Laboratory of Data-Driven Biology, Graduate School of Integrated Sciences for Life, Hiroshima University, Hiroshima, Japan. 2Kansei-Brain Informatics Group, Center for Brain, Mind and Kansei Sciences Research, Hiroshima University, Hiroshima, Japan. 3Theoretical Biology Research Group, Exploratory Research Center on Life and Living Systems, National Institutes of Natural Sciences, Okazaki, Japan. 4Laboratory of Theoretical Biology, Graduate School of Biostudies, Kyoto University, Kyoto, Japan. ‚Äâe-mail: nhonda@hiroshima-u.ac.jp\n\nNature Computational Science | Volume 3 | May 2023 | 418‚Äì432\n\n418\n\nArticle\n\nhttps://doi.org/10.1038/s43588-023-00439-w\n\ninference‚Äù19‚Äì21. Furthermore, FEP proposed a score of action, called expected free energy, which consists of the expected reward and curiosity with the same unit22‚Äì27. Thus, action selection can be formulated by maximizing both reward and curiosity. Note that curiosity can be regarded as information gain, that is, the extent to which we expect our recognition to be updated by the new observation through the action. However, FEP assumes that the weighting of rewards and curiosity is always even and constant. Although a previous FEP study modeled active inference in a two-choice task, it assumed a constant intensity of curiosity and thus could not treat actual animal behaviors in which the weights of rewards and curiosity are expected to change over time28. Hence, conventional theories such as RL and FEP are limited in describing the conflict between reward and curiosity.\nIdentifying the temporal variability of curiosity is important for future clarifying the neural mechanisms of the reward and curiosity conflicts in decision-making. Many FEP studies have been devoted to the construction of theory, assuming that the decision-making processes of animals are Bayes optimal. Thus, there was not even the idea that animals irrationally make decisions depending on the reward and curiosity conflicts. For this reason, a method to decode the temporal balance between reward and curiosity from behavioral data is yet to be established. Such a method would enable us to analyze neural correlates with the temporal variability of curiosity, and consequently, it would help us clarify how the brain controls the balance of reward and curiosity in a context-dependent manner.\nIn this study, we extended FEP by incorporating a meta-parameter that controls the conflict dynamics between reward and curiosity, called the reward‚Äìcuriosity decision-making (ReCU) model. The ReCU model can exhibit various behavioral patterns, such as greedy behavior toward reward, information-seeking behaviors with high curiosity and conservative behaviors avoiding uncertainty. Moreover, we developed a machine learning method called the inverse FEP (iFEP) method to estimate the internal variables of decision-making information processing. Applying the iFEP method to a behavioral time series in a two-choice task, we successfully estimated the internal variables, such as variations in curiosity, recognition of reward availability and its confidence.\nResults\nDecision-making with the reward‚Äìcuriosity dilemma Animals perceive the environment by inferring causes such as reward availability from observation, and then they make decisions based on their own inferences. In this study, we developed an ReCU model of a decision-making agent facing a dilemma between reward and curiosity in a two-choice task, wherein the agent selects either of two choices associated with the same rewards but with different reward probabilities (Fig. 1a). If the agent aims to maximize cumulative rewards, the agent must select an option with a higher reward probability. However, in animal behavioral experiments, even after they learned which option was more associated with a reward, they did not exclusively select the best choice, but also often selected the option with a smaller reward probability, which seems unreasonable.\nHere, we hypothesized the following: Animals assume that the reward probability for each option might fluctuate over time, and therefore, the continuous selection of one option decreases the confidence of the reward probability estimation for the other option. Thus, they become curious about the ambiguous option even with a smaller reward probability, and so selecting the ambiguous option is reasonable for increasing the confidence of the estimation for both options. Therefore, we considered that the agent should make decisions driven by reward and curiosity in a situation-dependent manner.\nReCU model In the ReCU model, we divided information processing in the brain into two processes. In the first process, the agent updates the recognition of the reward probability of each option (Fig. 1a, process 1). In the second\n\nprocess, the agent selects an action based on the current recognition and curiosity (Fig. 1a, process 2). The agent repeats these two processes in the two-choice task.\nWe modeled the first process by sequential Bayesian estimation, under the assumption that reward probabilities latently fluctuate in time (Fig. 1b). The agent updates the belief about the reward probabilities, which are expressed as estimation distributions, in response to actions and consequence reward observations (Fig. 1c). We derived the equations of the belief update (Fig. 1d and Methods). We modeled the second process by action selection based on two kinds of motivations: the desires to maximize the reward and to gain the information from the environment (Fig. 1e). This sum, called ‚Äòexpected net utility‚Äô in this study, can be expressed by\n\nUt (at+1) = E [Rewardt+1] + ct ‚ãÖ E [Infot+1] ,\n\n(1)\n\nwhere a and t indicate the action and trial index, respectively, and E[x] denotes the expectation value of x based on current recognition. The first and second terms represent expectations of reward and information derived from a new observation, respectively, for the next action at+1 based on current recognition. ct denotes a meta-parameter describing the intensity of curiosity, which weighs the expected information gain (see Methods for detail). We assumed an irrational mental conflict as ct varies over time (Fig. 1f). In decision-making, the agents prefer to select action at+1 with the higher expected net utility, in which the action is selected probabilistically following a sigmoidal function\n\nP (at+1)\n\n=\n\n1\n\n+\n\nexp\n\n1 (‚àíŒ≤ŒîUt\n\n)\n\n,\n\n(2)\n\nwhere ŒîUt = Ut(at‚Äâ+‚Äâ1)‚Äâ‚àí‚ÄâUt(at+1), and Œ≤ denotes the inverse temperature controlling the randomness of action selection22,29,30.\n\nRecognition and decision-making in the simulation To validate our model, we performed simulations with constant curiosity ct‚Äâ=‚Äâ1 for two cases. In the first case, where reward probabilities were constant and different between the two options (Fig. 2a), the agent preferred to select the option with the higher reward probability (Fig. 2b). The recognized reward probabilities converged to ground truths, indicating that the agent accurately recognized the reward probabilities (Fig. 2c). The recognition confidence changed over time depending on the behavior in each trial; the confidence of the selected option increased with information from the observation, whereas that of the unselected option decreased because of the agent‚Äôs assumption regarding the fluctuation in reward probabilities (Fig. 2d). Similarly, the expected information gain of an option increased and decreased when that option was selected and unselected, respectively. Thus, the expected information gain was lower for the option with higher confidence (Fig. 2e). The expected reward followed the recognized reward probability (Fig. 2f). Initially, decreasing expected information gain and increasing expected reward eventually cross at some number of trials, which correspond to switching between information exploration and reward acquisition (Supplementary Fig. 1a). These two factors are negatively correlated, indicating a trade-off relationship (Supplementary Fig. 1b). The expected net utility, which is the sum of the expected information gain and reward, represents the value of each selection (Fig. 2g), resulting in the agent‚Äôs preferentially selecting the option with the higher expected net utility.\nIn the second case, we assumed a dynamic environment with a time-dependent reward probability (Fig. 2h). In the simulation, the agent adaptively changed its recognition of the reward probability following the change in the true reward probability, and selected the option with the higher estimated reward probability (Fig. 2i,j). The confidence was affected by the uncertainty of the reward probability at each time; the confidence was high where the reward probabilities\n\nNature Computational Science | Volume 3 | May 2023 | 418‚Äì432\n\n419\n\nArticle\n\nhttps://doi.org/10.1038/s43588-023-00439-w\na\nor\n\nProcess 1: recognition\n\nProcess 2: action selection\n\nRecognition process\n\nb\n\ne\n\nLatent variable\n\nwt‚Äì2\n\nwt‚Äì1\n\nwt\n\nwt+1\n\nObservation ot‚Äì2\n\not‚Äì1\n\not\n\not+1\n\nEstimate Estimate\n\nAction\n\nat‚Äì2\n\nat‚Äì1\n\nat\n\nat+1\n\nc\n\nUpdate\n\nNew\n\nbelief\nf\n\nPrevious\n\nbelief\n\nœÉt‚Äì1\n\nœÉt\n\n¬µt‚Äì1\n\n¬µt\n\nLatent variable controlling reward probability\n\nd\n\n¬µt = ¬µt‚Äì1 + Œ± Kt {ot ‚Äì f(¬µt‚Äì1)}\n\n11\n\nœÉ\n\n2 t\n\n=\n\nKt\n\n+ f(¬µt){1 ‚Äì f(¬µt)}\n\nCuriosity\n\nAction selection process\n\nI R\n\nRI\n\nUt(at+1) = E[Reward] + ctE[Info]\n\nR\n\nI\n\nct = ct‚Äì1 + Œ≥ noise\n\nTime\n\nFig. 1 | Decision-making model for the two-choice task with reward‚Äìcuriosity dilemma. a, Decision-making in the two-choice task. Reward is provided at different probabilities for each option. The agent does not know those probabilities. Through repeated trial and error, the agent recognizes the world by inferring the latent reward probability of each option, and decides to choose the next action, that is, option, based on its own inference. b, Sequential Bayesian estimation as a recognition process. The agent assumes that the reward probabilities change over time owing to the fluctuation in the latent variable controlling reward probability. c, Belief updating. The agent recognizes the latent variable as a probability distribution. d, The update rule of the mean and\n\nvariance of the estimation distribution for each option. Œ±, Kt and f(Œºt) indicate the learning rate, Kalman gain, and the prediction of the reward probability, respectively. The second term in both equations disappears if the option is not selected. e, The action selection process by the agent. The agent evaluates the expected net utility Ut(at+1) of each action using the weighted sum of the expected reward and information gain, as shown in the equation. The agent compares the expected net utilities for both actions and prefers the option with the larger expected net utility. f, Time-dependent curiosity. The intensity of curiosity changes over time owing to the fluctuation of ct .\n\nwere near-deterministic, around 1 and 0, but low where the reward probabilities were uncertain, i.e., approximately 0.5 (Fig. 2k). The expected information gain of an option was negatively correlated with the confidence of the option (Fig. 2l), suggesting that the agent was curious about the uncertain option. The expected reward just varied depending on the recognized reward probability (Fig. 2m). In this case, we also observed a switching between information exploration and reward acquisition at the initial phase (Supplementary Fig. 1c). In contrast to the above case, the expected reward and expected information gain did not show clearly linear correlation because of the dynamic environment (Supplementary Fig. 1d). The expected net utility changed similarly to the expected reward; however, the difference between the left and right options was less pronounced because of\n\ncuriosity (Fig. 2n). These two demonstrations indicate that our model can represent the process of cognition and decision-making based on reward and curiosity.\n\nDiscrimination of passive and curiosity-dependent behaviors The behavioral difference based on curiosity is interesting. The expected net utility can be rewritten as\n\nŒîUt = ŒîE [Rewardt+1] + ct ‚ãÖ ŒîE [Infot+1] ,\n\n(3)\n\nwhere the first and second terms represent the differences between\nthe expected reward and information gain of two options, respectively. Thus, the agent is decided based on the balance between ŒîE [Rewardt+1]\n\nNature Computational Science | Volume 3 | May 2023 | 418‚Äì432\n\n420\n\nArticle\n\nhttps://doi.org/10.1038/s43588-023-00439-w\n\nand ŒîE [Infot+1]. Here, we created a diagram visualizing the selected actions in the space of ŒîE [Rewardt+1] and ŒîE [Infot+1]. Depending on the intensity of curiosity ct, the left and right actions can be separ¬≠ a¬≠ ted by a boundary of ŒîE [Rewardt+1] + ct ‚ãÖ ŒîE [Infot+1] = 0 (Fig. 2o‚Äìq). The agent with ct‚Äâ=‚Äâ0 selected an option based only on ŒîE [Rewardt+1] (Fig. 2p). In contrast, if ct was a nonzero value, the boundary leaned to a different direction depending on the positive and negative values of ct (Fig. 2o,q). These results indicate that passive choice (ct‚Äâ=‚Äâ0) and curiosity-dependent choice (ct ‚â† 0) can be discriminated based on the distributed pattern of selected actions in the space of ŒîE [Rewardt+1] and ŒîE [Infot+1].\nCuriosity-dependent irrational behaviors We examined how behavioral patterns are regulated by the intensity of curiosity and the degree of reward seeking (Fig. 3). In a scenario where the reward probabilities were zero for the left and 0.5 for the right (Fig. 3a), we simulated a model by varying the meta-parameters c and Po, which is a control parameter of the reward amount (Fig. 3b and Methods). When the agent strongly desired the reward (Po = 0.99 ) with no curiosity (c‚Äâ=‚Äâ0), the agent preferred the right option with a higher reward probability (Fig. 3c, point a). If the agent had no desire for a reward (Po = 0.5) with high curiosity (c‚Äâ=‚Äâ0), the agent preferred the option with a higher reward probability (Fig. 3c, point b). Although this behavior seems to be rational at first glance, the agent did not seek the reward, but rather sought the information (i.e., belief update) driven by curiosity, which resulted in a preference for the uncertain option. When the agent has negative curiosity (c‚Äâ=‚Äâ‚àí10), the agent continuously selected either of the two options depending on the first selection (Fig. 3c, point c). In this behavior, the agent conservatively selected the more certain option, as patients with autism spectrum disorder irrationally avoid new information and repeat the same choices6‚Äì11.\nIn addition, we obtained a nontrivial result in another scenario, where the reward probabilities were 0.5 for the left and 1 for the right (Fig. 3d‚Äìf). As in the previous scenario, the agents with a strong desire for the reward (Po = 0.99, nearly equal to 1) preferred the right option with a higher reward probability (Fig. 3f, point a). The agent with no desire for reward (Po‚Äâ=‚Äâ0.5) and high curiosity (c‚Äâ=‚Äâ10) preferred the left option with a lower reward probability (Fig. 3f, point b). This seemingly irrational behavior was the outcome of focusing on satisfying curiosity and not seeking rewards, which recalls patients with attention deficit hyperactivity disorder irrationally exploring new information12‚Äì14. In addition, as seen in the previous scenario (Fig. 3c, point c), the agents with negative curiosity (c‚Äâ=‚Äâ‚àí10), irrespective of the desire for the reward, exhibited conservative selection (Fig. 3f, point c). In combination, these results clearly indicate that behavioral patterns largely depend on the degree of conflict between reward and curiosity.\nInverse FEP: Bayesian estimation of the internal state In the above cases, we assumed a constant balance between reward and curiosity. However, our feelings swing in a context-dependent manner. Although it is important to decipher the temporal swinging of the conflict between reward and curiosity in terms of neuroscience and psychology, it is difficult to quantify the conflict because of its\n\ntemporal dynamics. Here, we addressed the inverse problem to estimate the internal states of the agent from behavioral data, known as computational phenotyping or meta-Bayesian inference31‚Äì35. To this end, we developed a machine learning method called iFEP to quantitatively decipher the temporal dynamics of the internal state including the curiosity meta-parameter from behavioral data.\nFor developing iFEP, we needed to switch the viewpoint from the agent to the observer of the agent, that is, from animals to us. In the state-space model (SSM) from the viewpoint of the agent, we described the sequential recognition of reward probabilities by the agent (Figs. 1b and 4a,b). Conversely, we developed a state-space model from the observer‚Äôs eye (the observer-SSM) to determine the internal state of the agent, for example, the intensity of curiosity ct, recognition Œºi,t and its confidence Pi,t (i.e., inverse of the variance of the estimation distribution) (Fig. 4c). In the observer-SSM, the intensity of curiosity was assumed to change continuously over time, and the agent‚Äôs recognition of the reward probability was updated by using the equations shown in Fig. 1c following the FEP; however, they were unknown to the observers. In addition, the agent‚Äôs actions were assumed to be generated depending on the intensity of its curiosity, recognition and confidence, as described in equation (2), but the observers can only monitor the agent‚Äôs action and the presence of a reward. In iFEP, based on the observer-SSM, we estimate the latent internal state of agent z from observation x in a Bayesian manner as\n\nP (z1‚à∂T|x1‚à∂T) ‚àù P (x1‚à∂T|z1‚à∂T) P (z1‚à∂T) ,\n\n(4)\n\nwhere z1‚à∂T = {Œºi,1‚à∂T, pi,1‚à∂T, c1‚à∂T} , x1‚à∂T = {a1‚à∂T, o1‚à∂T}, and the subscript 1:T indicates steps 1 to T. In this Bayesian estimation, a posterior distribu-\ntion P (z1‚à∂T|x1‚à∂T) represents the observer‚Äôs recognition of the estimated z1‚à∂T given observation x1:T with uncertainty. A prior distribution P (z1‚à∂T)represents our belief, which is expressed as the ReCU model with the random motion of the curiosity meta-parameter c as\n\nct = ct‚àí1 + œµŒ∂t,\n\n(5)\n\nwhere Œ∂t indicates the white standard Gauss noise and œµ indicates its noise intensity. The likelihood P(x1:T|z1:T) represents the probability that x1:T was observed assuming z1:T, which also follows the ReCU model. This Bayesian estimation, namely iFEP, was conducted using a particle\nfilter and Kalman backward algorithm (Methods).\n\nValidation of iFEP with artificial data We tested the validity of the iFEP method by applying it to the artificial data generated by the ReCU model. We simulated a model agent with nonconstant curiosity in the two-choice task, where reward probabilities varied temporally. We then demonstrated that iFEP estimated the ground truth of the internal state of the simulated agent, that is, the agent‚Äôs intensity of curiosity, recognition and confidence (Supplementary Fig. 2). We also confirmed that the estimation performance is robust against the value of œµ (Supplementary Fig. 3). Therefore, iFEP is in a position to provide efficient estimators of belief updating to clarify decision-making processing and the accompanying temporal swing in the conflict between reward and curiosity.\n\nFig. 2 | Simulations of the decision-making model. a, The two-choice task with constant reward probabilities. b, The selection probabilities for the left and right options, plotted as a moving average with window width of 101. c, The recognized reward probabilities for the left and right options compared with the ground truths depicted by dashed lines. d, The confidences of reward probability recognitions for left the and right options. e‚Äìg, The expected brief updates (e), expected reward (f) and expected net utility (g) for the left and right options. h, The two-choice task with constant and temporally varying reward probabilities for the left and right options. i‚Äìn, The same as b‚Äìg with parameter values of\n\nc = 1, Po = 0.8, Œ± = 0.05, Œ≤ = 2 and œÉw = 0.63. o‚Äìq, The selected options in a space of left‚Äìright differences of the expected reward and information gain. The\nReCU model was simulated with dynamically changing reward probabilities for different intensities of curiosity: c = ‚àí1 (o), c = 0 (p) and c = 3 (q). The reward probabilities were generated by the Ornstein‚ÄìUhlenbeck process of w for 1,000 trials: wi,t = wi,t‚àí1 ‚àí 0.01wi,t‚àí1 + 0.15Œæt , where Œæt indicates the standard Gauss noise. The heatmap represents the probability of action selection in the space\n(equation (2)).\n\nNature Computational Science | Volume 3 | May 2023 | 418‚Äì432\n\n421\n\nArticle\n\nhttps://doi.org/10.1038/s43588-023-00439-w\n\nInformation gain\n\nSelection probability\n\na\nb 1.00 0.75 0.50 0.25 0 0\ne 0.60 0.45 0.30 0.15 0 0\nh\n\nReward probability\n\nReward probability\n\nLeft\n\nTrials\nc\nLefLteft\n\nRigRhigt ht\n\n500 Trials\n\n1,000\n\nf\n\nRight\n\nLeft 500 Trials\n\n1,000\n\nExpected reward\n\nEstimated reward probability\n\n80%\n1.00 0.75 0.50 0.25\n0 0\n1.2 0.9 0.6 0.3\n0 0\n\n20%\nd\nLeft\n\n500 Trials\n\nRight 1,000\n\ng\nLeft\n\nRight\n\n500 Trials\n\n1,000\n\nExpected net utility\n\nConfidence\n\n44 33 22 11\n0 0\n1.40 1.05 0.70 0.35\n0 0\n\nRight Trials Left\n\nRight 500 Trials\n\n1,000\n\nLeft\n\nRight\n\n500 Trials\n\n1,000\n\nLeft Right\n\nReward probability\n\nReward probability\n\nInformation gain\n\nSelection probability\n\ni 1.00 0.75 0.50 0.25 0 0\nl 0.8 0.6 0.4 0.2 0 0\no 1.0 Right 0.5\n\nTrials\n\nj 1.00\n\nEstimated reward probability\n\nRight Left\n\n0.75 0.50 0.25\n\n0\n\n500\n\n1,000\n\n0\n\nTrials\n\nm 1.2\n\nExpected reward\n\n0.9 Left Right\n0.6\n\n0.3\n\n500 Trials\n\n0\n\n1,000\n\n0\n\np 1.0\n\n0.5\n\nRight Left\n500 Trials\nLeft Right\n500 Trials\n\nk\n1,000\n\nConfidence\n\n100 75 50 25 0 0\n\nn 1.40\n\nExpected net utility\n\n1.05\n\n0.70\n\n0.35\n\n0\n\n1,000\n\n0\n\nq 1.0\n\n0.5\n\nTrials\n\nLeft\n\nRight\n\n500 Trials\n\n1,000\n\nRight Left\n\n500 Trials\n\n1,000\n1.5 Left\n1.0\n\n‚àÜ info gain\n\n‚àÜ info gain\n\n‚àÜ info gain\n\n0\n\n0\n\n0\n\n0.5\n\n‚Äì0.5\n\n‚Äì1.0 ‚Äì1.0\n\nLeft\n\n‚Äì0.5\n\n0\n\n0.5\n\n1.0\n\n‚àÜ reward\n\n‚Äì0.5\n\nRight\n\n‚Äì1.0\n\n‚Äì1.0 ‚Äì0.5\n\n0\n\nLeft\n\n0.5\n\n1.0\n\n‚àÜ reward\n\n‚Äì0.5\n\nRight\n\n‚Äì1.0\n\n‚Äì1.0 ‚Äì0.5\n\n0\n\n0.5\n\n‚àÜ reward\n\n0 ‚Äì0.5 1.0\n\nProbability of selecting the right\n\nNature Computational Science | Volume 3 | May 2023 | 418‚Äì432\n\n422\n\nArticle\n\na\n\nb 10\n\n0% Point b\n\n50%\n\nhttps://doi.org/10.1038/s43588-023-00439-w\nd\n\n50%\n\n100%\n\ne 10\n\n1\n\nPoint b\n\nIntensity of curiosity, Ct\n\nIntensity of curiosity, Ct\n\nFraction of selecting the right\n\n0\n\nPoint a\n\n0\n\nPoint a\n\n‚Äì10 0.50\nc Point a\n1.0\n\nPoint c\n0.75 Reward intensity, Po\nPoint b 1.0\n\n1.00\n\n‚Äì10 0.50\n\nf Point a\n1.0\n\nPoint c\n0.75 Reward intensity, Po\nPoint b 1.0\n\n0 1.00\n\nRatio\n\nRatio\n\n0.5\n\n0.5\n\n0.5\n\n0.5\n\n0 Left\nPoint c 1.0\n\nRight\n\n0 Left\n1.0\n\nRight\n\n0 Left\nPoint c 1.0\n\nRight\n\n0 Left\n1.0\n\nRight\n\nRatio\n\n0.5\n\n0.5\n\n0.5\n\n0.5\n\n0 Left\n\nRight\n\n0 Left\n\nRight\n\nFig. 3 | Curiosity-dependent irrational behaviors. a, The two-choice task with\ndifferent, constant reward probabilities: 0% for the left and 50% for the right.\nb, The heatmap of the selection probability of the right option as a function of the\nparameters of curiosity and reward intensity. The probability was obtained\nempirically by running 1,000 simulations for each set of curiosity and reward.\nThree representative conditions are indicated by black dots: reward-seeking (point a; c = 0, Po = 1), information-seeking (point b; c = 10, Po = 0.5) and information-avoiding (point c; c = ‚àí10, Po = 0.75). c, Box plots showing the selection ratios of the right option for the conditions at points a‚Äìc, where the\n\n0 Left\n\nRight\n\n0 Left\n\nRight\n\ncentral line indicates the median, the edges are the lower and upper quantiles and the upper and lower whiskers represent the highest and lowest value after excluding outliers, respectively. At point c, the model agent dominantly selected either the right or left option, where the left- and right-dominantly selected simulations occurred 505 and 495 times, respectively. The box plots for point c appear crushed because the data points are too densely packed. d‚Äìf, The same as a‚Äìc for the two-choice task with different, constant reward probabilities: 50% for the left and 100% for the right. At point c in f, the left- and right-dominantly selected simulations occurred 489 and 511 times, respectively.\n\niFEP-decoded internal state behind rat behaviors We applied iFEP to actual rat behavioral data from the two-choice task experiment with temporally varying reward probabilities36 (Fig. 5a). In this experiment, once the reward probabilities were suddenly changed in a discrete manner, the rat slowly adapted to select the option with the higher reward probability (Fig. 5b), suggesting that the rat sequentially updated its recognition of the reward probability. Based on these behavioral data of the rat, iFEP estimated the internal state, that is, the intensity of curiosity, the recognized reward probabilities and their confidence levels (Fig. 5c‚Äìe). We found that the rat was not perfectly aware of the true reward probabilities but was able to recognize increases and decreases in reward probability (Fig. 5d,e). We also found that confidence increased with choice and decreased with no choice (Fig. 5f,g).\nWith iFEP, we can examine whether the rat subjectively assumed fluctuating or constant environments, that is, reward probabilities.\n\nWe estimated the degree of fluctuation of the reward probabilities the rat assumes pw = 1.785 (that is, œÉ2w = 0.560) from the rat behavioral data. This estimated value implied that the latent variable controlling the reward probabilities showed a random walk with increasing s.d. with trials as ‚àöœÉ2wt . Compared with a reward probability represented by the sigmoidal of the latent variable, the estimated reward probability can largely change from 0.5 to 0.5‚Äâ¬±‚Äâ0.4 during only ten trials (Supplementary Fig. 4). Therefore, it was suggested that the rat assumed fluctuating environments and, thus, easily forgot its recognition and lost its confidence.\nNegative curiosity and its dynamics decoded by iFEP Interestingly, the curiosity held by the rat was estimated to be negative for almost all trials (Fig. 5h). In other words, the rat conservatively preferred certain choices but did not explore uncertain choices.\n\nNature Computational Science | Volume 3 | May 2023 | 418‚Äì432\n\n423\n\nArticle\n\nObserving\n\na\nObservation Action\nc\n\nhttps://doi.org/10.1038/s43588-023-00439-w\n\nb\n\nLatent variable\n\nzt‚Äì2\n\nzt‚Äì1\n\nzt\n\nzt+1\n\nObservation\n\not‚Äì2\n\not‚Äì1\n\not\n\not+1\n\nAction\n\nat‚Äì2\n\nat‚Äì1\n\nat\n\nat+1\n\nUpdate\nOld belief\n\nNew belief\n\nI R\n\nRI\n\nI am wondering: - How much is the reward probability? - Which option should be selected?\n\nI am wondering: - How does the agent recognize the reward probability? - How much confidence does the agent have in its recognition?\n\nIntensity of curiosity\n\nct‚Äì2\n\nct‚Äì1\n\nct\n\nct+1\n\nLatent\n\n¬µt‚Äì2\n\n¬µt‚Äì1\n\n¬µt\n\n¬µt+1\n\nvariable\n\nœÉ\n\n2 t‚Äì2\n\nœÉ\n\n2 t‚Äì1\n\nœÉ\n\n2 t\n\nœÉ\n\n2 t+1\n\nObservation\n\not‚Äì2\n\not‚Äì1\n\not\n\not+1\n\nAction\n\nat‚Äì2\n\nat‚Äì1\n\nat\n\nat+1\n\nFig. 4 | The scheme of iFEP by an observer of a decision-making agent. a, An agent performing a two-choice task from the observer‚Äôs perspective. b, The observer‚Äôs assumption about the agent‚Äôs decision-making. The agent is assumed to follow the decision-making model, as described in Fig. 1. c, The SSM\n\nof the observer‚Äôs eye. For the observer, the agent‚Äôs reward‚Äìcuriosity conflict, recognized reward probabilities and their uncertainties are temporally varying latent variables, whereas the agent‚Äôs action and the presence/absence of a reward are observable. The observer estimates the latent internal states of the agent.\n\nThis negative curiosity is reasonable for the starved animals because\nanimals desired to obtain more reward with higher confidence. To\nvalidate the negative curiosity, we visualize the selected action in the space of ŒîE [Rewardt+1] and ŒîE [Infot+1] , as shown in Fig. 2o‚Äìq (Fig. 6a‚Äìc). When the estimated ct was negative, the rat dominantly selected the left action in a region with positive ŒîE [Rewardt+1] and negative ŒîE [Infot+1] (Fig. 6a for ct‚Äâ<‚Äâ‚àí1.1; Fig. 6b for ‚àí1.1 ‚â§ ct < ‚àí0.7), clearly indicating that the rat has positive subjective reward and nega-\ntive curiosity. When the estimated ct was close to 0, the rat selected both actions based on ŒîE [Rewardt+1] , independent of ŒîE [Infot+1] (Fig. 6c for ‚àí0.7 ‚â§ ct ). These results clearly supported the expected net utility with positive weight for the expected reward and time-\ndependent weight for the expected information gain. In addition, we\nstatistically tested negative curiosity (P‚Äâ<‚Äâ0.01 for Monte Carlo testing\nin Supplementary Fig. 5).\nFurther, we noticed an increase in the estimated level of curiosity at\nwhich the reward probabilities changed suddenly (Fig. 5h). This curios-\nity dynamics can be interpreted such that the rat recognized the rule\nchange and adaptively controlled the extent to which the rat sought\nnew information. We further examined how the curiosity is regulated\nby recognized environmental information. We did not detect the cor-\nrelation between the estimated curiosity and expected information\ngains (Fig. 6d,e). Moreover, we found that the temporal derivative of\n\nthe estimated curiosity highly correlated with the sum of the expected information gains for both options (Fig. 6f,g). These results implied that the rat actively upregulated the curiosity level when the uncertainty in the recognized reward probabilities increases, such as,\n\ndc dt\n\n‚àù\n\n‚àë E [Infoi].\ni\n\n(6)\n\nEvaluations of alternative models from rat behaviors Finally, to further confirm the validity of the ReCU model, we compared it with other decision-making models based on the rat behavioral data. As an alternative version of the expected net utility, we introduced the time-dependent desire for reward as\n\nUt (at+1) = dt ‚ãÖ E [Rewardt+1] + E [Infot+1] ,\n\n(7)\n\nwhere dt denotes a meta-parameter describing subjective reward (see Methods for details). With this alternative model, the time series of dt were estimated by iFEP from the rat behavioral data. We found that the estimation of the subjective reward meta-parameter dt changed dynamically and sometimes became close to zero when the rat encountered drastic changes in the reward probabilities (Supplementary Fig. 6), which indicated that the rat suddenly no longer needed rewards. This\n\nNature Computational Science | Volume 3 | May 2023 | 418‚Äì432\n\n424\n\nArticle a\n\nhttps://doi.org/10.1038/s43588-023-00439-w\n\nReward probability\n\nReward probability\n\nTrials\nb (L, R) = (0.9, 0.5) (0.5, 0.9)\n\n(0.5, 0.1)\n\nLeft\n\nP(L)\n\n1.0 0.5\n0\n\n(0.1, 0.5)\n\n(0.5, 0.9) (0.9, 0.5)\n\n(0.1, 0.5)\n\n(0.5, 0.1)\n\nTrials\nLeft Right Rewarded\n\nRight\n\n0\n\nc\n\n1.00\n\n0.75\n\nSelection probability\n\n0.50\n\n0.25\n\n0 0\n\nf\n\n24\n\n100\n\n200\n\n300\n\nTrials\n\nNon-rewarded 400\n\nd\n\n1.00\n\ne\n\n1.00\n\nRight Left\n\nEstimated reward probability (left)\n\nEstimation\n0.75\n\n0.50 0.25\n\nGround truth\n\nEstimated reward probability (right)\n\n0.75 0.50 0.25\n\nEstimation\nGround truth\n\n100\n\n200\n\n300\n\n400\n\nTrials\n\n0\n\n0\n\n100\n\n200\n\n300\n\n400\n\nTrials\n\n0\n\n0\n\n100\n\n200\n\n300\n\n400\n\nTrials\n\ng 24\n\nh\n\n1\n\n18\n\n18\n\n0\n\nIntensity of curiosity\n\nConfidence (right)\n\nConfidence (left)\n\n12\n\n12\n\n‚Äì1\n\n6\n\n6\n\n‚Äì2\n\n0\n\n0\n\n100\n\n200\n\n300\n\n400\n\nTrials\n\n0\n\n0\n\n100\n\nFig. 5 | Estimation of the rat‚Äôs internal state by iFEP. a, A rat in the two-choice task with temporally switching reward probabilities. b, Rat‚Äôs behaviors from a public dataset at https://groups.oist.jp/ja/ncu/data ref. 5. Vertical lines indicate the selections of left (L) and right (R) options, respectively. The time series indicates the moving average of the selection probability of the left option with 25 window width backward. c, The moving average of the selection probabilities for the left and right options. d,e, The rat behavior data-driven estimations of\n\n200\n\n300\n\n400\n\nTrials\n\n‚Äì3 0\n\n100\n\n200\n\n300\n\n400\n\nTrials\n\nagent-recognized reward probabilities for the left (d) and right (e) options.\nf‚Äìh, The rat behavior-driven estimations of agent‚Äôs confidence about recognized\nreward probabilities for the left (f) and right (g) options, and the agent‚Äôs curiosity (h). The estimated parameter values were Œ± = 0.058, Œ≤ = 6.991 and œÉ2w = 0.560. The number of particles was 100,000 in the particle filter. Continuous shaded\nranges represent the s.d. for all the particles in d‚Äìh.\n\nshould be unnatural for animals that were starved before the experimental task to motivate them to obtain food. Thus, the alternative model is not suitable for describing the rat behavior.\nAnother possible model is Q-learning in the framework of RL, which has been widely used to model the decision-making tasks. Following previous studies36‚Äì38, we introduced the time-dependent inverse temperature Bt, which controls the randomness of the action selection; however, it does not lead to information-seeking behavior in the ReCU model. With the Q-learning model, we estimated time series of Bt (Methods). Then, we determined that the Bt decreased when the reward probabilities suddenly changed (Supplementary Fig. 7), meaning that the rat tends to perform a random selection of actions in response to the environmental rule change. Although this dynamic behavior\n\nof the inverse temperature seems reasonable, it is unknown how it is regulated in the Q-learning model. Here, we hypothesized that Bt could be regulated by the uncertainty of our recognition. To probe this, we compared Bt and the expected information gain, where the former was estimated based on Q-learning and the latter was estimated by iFEP in the ReCU model. We found that they are positively correlated (Supplementary Fig. 7) as\n\nBt ‚àù ‚àë E [Infot,i].\ni\n\n(8)\n\nTherefore, Q-learning requires a cue regarding the uncertainty of recognition, that is, the expected information gain, which supports the ReCU model for explaining the curiosity-driven behavior.\n\nNature Computational Science | Volume 3 | May 2023 | 418‚Äì432\n\n425\n\nArticle\n\nhttps://doi.org/10.1038/s43588-023-00439-w\n\nProbability of selecting the right\n\na\n0.4 Right\n0.2\n\nLogistic regression\niFEP\n\nb\n0.4 Right\n0.2\n\nc\n0.4 iFEP\n0.2\n\n0.502 0.501\n\n‚àÜ info gain\n\n‚àÜ info gain\n\n‚àÜ info gain\n\n0 ‚Äì0.2\n\n0 ‚Äì0.2\n\n0 ‚Äì0.2\n\n0.500\n\n‚Äì0.4 Left\n\n‚Äì0.6 ‚Äì0.4 ‚Äì0.2 0\n\n0.2 0.4 0.6\n\n‚àÜ reward\n\n‚Äì0.4\n\n‚Äì0.6 ‚Äì0.4\n\nLogistic regression\n‚Äì0.2 0\n\nLeft 0.2 0.4 0.6\n\n‚àÜ reward\n\n‚Äì0.4\n\niFEP\n\nRight\n‚Äì0.6 ‚Äì0.4 ‚Äì0.2\n\nLogistic regression\n0 0.2\n\n‚àÜ reward\n\n0.499\nLeft 0.498\n0.4 0.6\n\nd 1.4\n1.2\n\nInfo gain\n\nc (intensity of curiosity)\n\n1.0\n\ne 0.9\n\n0.2\n\n0.6\n\nCorrelation coe icient\n\nInfo gain\n\nIntensity of curiosity, c\n\n1.0\n\n‚Äì0.6\n\n0.3\n\n0.8\n\n‚Äì1.4\n\n0\n\n0.6\n\n‚Äì2.2\n\n‚Äì0.3\n\n0.4 0\nf 1.4\n1.2\n\n100 Info gain\n\n200\n\n300\n\nTrials\n\n‚Äì3.0 400\n\n0.20\n\ndc/dt (temporal derivative of curiosity)\n\n0.13\n\n‚Äì0.6 ‚Äì60 ‚Äì40 ‚Äì20 0 20 40 60 Time lag\ng 0.9\n0.6\n\nCorrelation coe icient\n\nInfo gain\n\nTime derivative of curiosity, dc/dt\n\n1.0\n\n0.06\n\n0.3\n\n0.8\n\n‚Äì0.01\n\n0\n\n0.6\n\n‚Äì0.08\n\n‚Äì0.3\n\n0.4 0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\nTrials\n\nFig. 6 | Negative curiosity and its dynamics. a‚Äìc, The selected options in a space\nof left‚Äìright differences of the expected reward and information gain. All actions\nwere separated into three: when the estimated curiosity is negatively large (c ‚â§ ‚àí1.1, n = 110) (a), negatively medium (‚àí1.1 < c ‚â§ ‚àí0.7, n‚Äâ=‚Äâ93) (b) and close to 0 (‚àí0.7 < c, n = 187) (c). The left and right options were discriminated by a logistic regression with P (at = 1) = f (wRŒîE [Rewardt+1] + wIŒîE [Infot+1]), where wR and wI indicate the weight parameters. Two linear lines indicate discrimination\nboundaries using the estimated wR and wI from this scattered data and using\n\n‚Äì0.15\n\n350\n\n400\n\n‚Äì0.6 ‚Äì60 ‚Äì40 ‚Äì20 0 20 40 60 Time lag\n\nwR = 1 and wI = ‚àëct/N , which is an average of the estimated curiosity, respectively. the heatmap represents the probability of selecting the left option based on the estimated wR and wI. d,e, Time series of the intensity of curiosity and the sum of the expected information gains for both options (d), and their cross-correlation (e). f,g, Time series of the temporal derivative of curiosity and the sum of the expected information gains for both options (f), and their cross-correlation (g). The temporal derivative was computed by linear regression within a time window of seven trials.\n\nDiscussion\nThe advancement realized in this study is the modeling and decoding of mental conflict between reward and curiosity, which is yet to be quantified. The proposed approach can potentially improve our understanding of how mental conflict is regulated and highlight its neural mechanisms in the future by combining neural recording data.\nOur decoding approach has some limitations. The iFEP requires long trial behavioral data in the two-choice task because the particle filter needs certain trials for converging the estimation. Thus, the iFEP is not applicable to short behavioral data. In addition, the iFEP assumed gradual change in curiosity in time and cannot follow the pathologically rapid dynamics of curiosity in the estimation process of the particle filter.\nComparing the ReCU model and other models including Q-learning is difficult. In all these models, the action selection is commonly formulated with the sigmoidal function. Thus, any models can be made to fit the observed behaviors, i.e., increase likelihood, by adjusting the time-dependent meta-parameters. Therefore, model selection\n\nbased solely on likelihood is not very helpful and determining whether animals use the ReCU model or other models to make decisions seems inherently challenging. However, a potential advantage of the ReCU model is its ability to capture the dynamics of curiosity and the interplay between curiosity and reward-based learning.\nThere is a related theoretical model, which differs from RL and FEP. Ortega and Braun formulated FEP, which describes irrational decision making39,40. Interestingly, their formulation was based on microscopic thermodynamics and the temperature parameters controlled this irrationality. However, the thermodynamics-based FEP did not treat the sequential update of the recognition from the observations.\nFinally, it is worth discussing future perspectives of our iFEP approach in medicine. In general, mental diagnosis relies on medical interviews and has not been evaluated quantitatively. Our iFEP method can quantitatively estimate the psychological state of patients based on their behavioral data. For example, patients with social withdrawal, also known as ‚ÄòHikikomori,‚Äô have no interest in anything. In this case, social withdrawal would be characterized by a negative value of curiosity in\n\nNature Computational Science | Volume 3 | May 2023 | 418‚Äì432\n\n426\n\nArticle\n\nhttps://doi.org/10.1038/s43588-023-00439-w\n\nour FEP model. Therefore, the iFEP method could be considered effective for diagnosing mental disorders.\n\nMethods\nAmount of reward In the two-choice task, the reward is given as all-or-none; however, its intensity depends on the agent‚Äôs own feeling as\n\nR\n\n=\n\n0\n\nor\n\nln\n\nPo 1‚àíPo\n\n,\n\n(9)\n\nwhere Po represents the desired probability of how much the agent wants the reward and controls the reward intensity felt by the agent (Supplementary Fig. 8). In Friston‚Äôs formulation, the presence and absence of rewards are given by log preference with natural unit as ln Po and In(1‚Äâ‚àí‚ÄâPo), which take negative values22,23. In this equation, the reward is the difference between log preferences to ensure that the reward intensity is positive.\n\nwhere œÜt = {Œºt, respectively (Œºt\n\nŒõ=t}(,Œºa1,tn, Œºd2,Œºt)tTa; nŒõdt\n\nŒõt denote the = diag (p1,t, p2,t))\n\nmean and precision, . Q (wt|œÜt) denotes the\n\nrecogn¬≠ ition distribution. The model agent aims to update the recog¬≠\n\nnition distribution through œÜt at each time step by minimizing\nthe surprise, which is defined by ‚àí ln P (ot|o1‚à∂t‚àí1). The surprise can be decomposed as follows:\n\n‚àí ln P (ot|o1‚à∂t‚àí1)\n\n=\n\n‚à´Q (wt|œÜt) ln\n\ndw Q(wt |œÜt )\n\nP(ot ,wt |o1‚à∂t‚àí1 ,a1‚à∂t )\n\nt\n\n‚àíKL [Q (wt|œÜt) ||P (wt|o1‚à∂t, a1‚à∂t)] ,\n\n(16)\n\nwhere KL [q (x) ||p (x)] denotes the Kullback‚ÄìLeibler (KL) divergence between the probability distributions q(x) and p(x). Because of the\nnonnegativity of KL divergence, the first term is the upper bound of\nthe surprise:\n\nState space model for reward probability recognition The agent assumed that the reward is generated probabilistically from the latent cause w, and probabilities Œªi are represented by\n\nŒªi = f (wi) ,\n\n(10)\n\nwhere i indicates the indices of the options and f (x) = 1/ (1 + e‚àíx) . In addition, the agent assumed an ambiguous environment\nin which the reward probabilities are fluctuated by a random\nwalk as\n\nwi,t = wi,t‚àí1 + œÉwŒæi,t,\n\n(11)\n\nwhere t, Œæi,t , and œÉw denote the trial of the two-choice task, standard Gaussian noise and the noise intensity, respectively. Thus, the agent\nassumes an environment expressed by an SSM as\n\nP (wt|wt‚àí1) = ùí©ùí© (wt|wt‚àí1, œÉ2wI) ,\n\n(12)\n\nF (ot, œÜt) = ‚à´Q (wt|œÜt) ln Q (wt|œÜt) dwt + ‚à´Q (wt |œÜt) J (ot, wt) dwt, (17)\n\nwhich is called the free energy, where J (ot, wt) = ‚àí ln P (ot, wt|o1‚à∂t‚àí1, a1‚à∂t). The first term of the free energy corresponds to the negative entropy\nof a Gaussian distribution:\n\nF1 = ‚à´Q (wt|œÜt) ln Q (wt |œÜt) dwt.\n\n(18)\n\nThe second term is approximated as F2 = ‚à´Q (wt |œÜt) J (ot, wt) dwt\n\n‚âÖ\n\n‚à´Q (wt|œÜt) {J (ot, Œºt) +\n\ndJ dw\n\n(wt\n\n‚àí Œºt) +\n\n1 2\n\nd2 J dw2\n\n(wt\n\n‚àí Œºt)2} dwt\n\n(19)\n\nP (ot|wt, at) = ‚àè [f (wi,t)ot {1 ‚àí f (wi,t)}1‚àíot ]ai,t ,\ni\n\n(13)\n\nwhere wt and ot denote the probabilities of both options\n\nlatent at step\n\nvariables controlling the reward t (wt = (w1,t, w2,t)T) and the observa-\n\ntion of the presence of the reward (ot ‚àà {0, 1}), respectively. Meanwhile,\n\nat denotes the agent‚Äôs action at step t, which is represented by a one-hot vector (at ‚àà {(1, 0)T , (0, 1)T})‚Äâ, ùí©ùí© (x|Œº, Œ£) denotes the Gaussian distribu-\n\ntion mean Œº and variance Œ£, œÉ2w denotes the variance of the transition probability of w, I denotes an identity matrix, and f(wi,t) = 1/(1 + e‚àíwi,t ),\n\nwhich represents the probability of the reward of option i at step t. The\n\ninitial distribution of w1 is given by P (w1) = ùí©ùí© (w1|0, Œ∫I), where Œ∫ denotes variance.\n\n=\n\nJ (ot, wt)|wt=Œºt\n\n+\n\n1 2\n\nd2 J dw2\n\n||wt =Œºt\n\nŒõ‚àít 1.\n\nNote that E (ot, wt) is expanded by a second-order Taylor series\naround Œºt. At each time step, the agent updates œÜt by minimizing F (ot, œÜt).\n\nCalculation of free energy The free energy is derived as follows: F1 simply becomes\n\nF1\n\n=\n\n1 2\n\nln 2œÄp‚àí1,t1\n\n+\n\n1 2\n\nln 2œÄp‚àí2,1t\n\n+ const.\n\n(20)\n\nFor computing F2,\n\nFEP for reward probability recognition We modeled the agent‚Äôs recognition process of reward probability using sequential Bayesian updating as\nP (wt|o1‚à∂t, a1‚à∂t) ‚àù P (ot|wt, at) ‚à´P (wt|wt‚àí1) P (wt‚àí1|o1‚à∂t‚àí1, a1‚à∂t‚àí1) dwt‚àí1. (14)\n\nP (ot, wt|o1‚à∂t‚àí1, a1‚à∂t) = P (ot|wt, at) ‚à´P (wt|wt‚àí1) P (wt‚àí1|o1‚à∂t‚àí1, a1‚à∂t‚àí1) dwt‚àí1\n‚âÖ P (ot|wt, at) ‚à´P (wt|wt‚àí1) N (wt‚àí1|Œºt‚àí1, Œõ‚àít‚àí11) dwt‚àí1. (21)\n\nBecause of the non-Gaussian P(ot|wt, at), the posterior distribution of wt, P (wt|o1‚à∂t, a1‚à∂t) , becomes non-Gaussian and cannot be calculated analytically. To avoid this problem, we introduced a simple posterior\ndistribution approximated by a Gaussian distribution:\n\nQ (wt|œÜt) = ùí©ùí© (wt|Œºt, Œõ‚àít 1) ‚âíP (wt|o1‚à∂t, a1‚à∂t) ,\n\n(15)\n\nIn the second line of this equation, we use the approximated recognition distribution as the previous posterior P (wt‚àí1|o1‚à∂t‚àí1, a1‚à∂t‚àí1) ‚Äâ. This equation can be written as\nP (ot, wt|o1‚à∂t‚àí1, a1‚à∂t) ‚âÖ ‚àè [f (wi,t)ot {1 ‚àí f (wi,t)}1‚àíot ]ai,t N (wi,t|Œºi, t‚àí1, p‚àíi, t1 + œÉ2w) .\ni\n(22)\n\nNature Computational Science | Volume 3 | May 2023 | 418‚Äì432\n\n427\n\nArticle\n\nhttps://doi.org/10.1038/s43588-023-00439-w\n\nThen where\n\nE (ot, wt)|wt=Œºt = J1 (ot, Œº1,t) + J2 (ot, Œº2,t) + const., Ji (ot, Œºi,t) = ai,t [ot ln f (Œºi,t) + (1 ‚àí ot) ln {1 ‚àí f (Œºi,t)}]\n\n‚àí\n\n1 2\n\n(Œºi,t ‚àíŒºi,t‚àí1 )2 p‚àíi,t1 +œÉ2w\n\n‚àí\n\n1 2\n\nln (p‚àíi,t1 + œÉ2w) .\n\nwhich is a rewriting of equation (17). Here, we attempt to express the\nfuture free energy at time t‚Äâ+‚Äâ1, conditioned on action at +1 as (23)\nF (ot+1, at+1) = EQ(wt+1|œÜt) [ln Q (wt+1|œÜt) ‚àí ln P (ot+1, wt+1|o1‚à∂t, a1‚à∂t+1)] . (32)\n\nHowever, there is an apparent problem in this equation: ot+1 has not yet been observed because it is a future observation. To resolve this issue, (24) we take an expectation with respect to ot+1 as\n\nF (at+1) = EQ(ot+1|wt+1,at+1)Q(wt+1|œÜt) [ln Q (wt+1|œÜt) ‚àí ln P (ot+1, wt+1|o1‚à∂t, a1‚à∂t+1)] .\n\n(33)\n\nThus, F2 is calculated by substituting this equation into equation (19). Taken together,\n\nF (ot, œÜt)\n\n=\n\n‚àë\ni\n\n{Ji\n\n(ot ,\n\nŒºi,t )\n\n+\n\n1 2\n\n||| d2 Ji\ndw2i,t\n\nwi,t =Œºi,t\n\np‚àíi,t1\n\n+\n\n1 2\n\nln 2œÄp‚àíi,t1} .\n\n(25)\n\nSequential updating of the agent‚Äôs recognition\nThe updating rule for œÜt was derived by minimizing the free energy. The optimized pi,t can be computed by ‚àÇF/‚àÇp‚àíi,t1 = 0 , which leads to\n\npi,t =\n\n||| . d2Ji\ndw2i,t\n\nwi,t =Œºi,t\n\n(26)\n\nBy substituting pi,t into equation (25), the second term in the summation becomes constant, irrespective of Œºi,t. Thus, Œºi,t is updated by minimizing only the first term as\n\nŒºi,t = Œºi,t‚àí1 ‚àí Œ±Œ¥i,a\n\n||| , ‚àÇJi\n‚àÇŒºi,t Œºi,t =Œºi,t‚àí1\n\n(27)\n\nwhere Œ± is the learning rate. These two equations finally lead to\n\nŒºi,t = Œºi,t‚àí1 + Œ±Ki,t (ot ‚àí f (Œºi,t‚àí1)) ,\n\n(28)\n\npi,t = K‚àíi,t1 + f (Œºi,t) (1 ‚àí f (Œºi,t)) ,\n\n(29)\n\nwhere Ki,t = (pi,t‚àí1 + œÉ‚àíw2) / (pi,t‚àí1œÉ‚àíw2), which is called the Kalman gain. If option i was not selected, the second terms in both equations will van-\nish, which results in belief Œºi,t staying the same, while its precision decreases (that is, pi,t+1 < pi,t). If it is selected, the belief is updated by the prediction error (that is, ot ‚àí f (Œºi,t)), and its precision is improved. The confidence of the recognized reward probability should be evalu-\nbatyeŒ≥di,tn=otpiin,t/wf‚Ä≤i,(tŒºsip,t)a2c. e but in Œªi,t space; hence, the confidence is defined\n\nExpected net utility The expected net utility is described by\nUt (at+1) = ct ‚ãÖ EP(ot+1|at+1) [KL [Q (wt+1|ot+1, at+1) ||Q (wt+1|at+1)]] +EP(ot+1|at+1) [R (ot+1)] ,\n\n(30)\n\nwhere R (ot+1) = ot+1 ln (Po/ (1 ‚àí Po))‚Äâ; the first and second terms represent the expected information gain and expected reward, respectively; and\nct denotes the intensity of curiosity at time t. The heuristic idea of introducing the curiosity meta-parameter ct was also proposed in the RL field30.\nWe briefly show how to derive it based on ref. 41. The free energy\nat current time t is described by\n\nF (ot, œÜt) = EQ(wt|œÜt) [ln Q (wt|œÜt) ‚àí ln P (ot, wt|o1‚à∂t‚àí1, a1‚à∂t)] ,\n\n(31)\n\nThis can be rewritten as\n\nF (at+1) = EQ(ot+1,wt+1|o1‚à∂t,at+1) [ln Q (wt+1|œÜt) ‚àí ln P (wt+1|o1‚à∂t+1, a1‚à∂t+1) ‚àí ln P (ot+1|o1‚à∂t, a1‚à∂t+1) ]\n\n(34)\n\nAlthough P (ot+1|o1‚à∂t, a1‚à∂t+1) can be computed by the generative model as\n\nP (ot+1|o1‚à∂t, a1‚à∂t+1) = ‚à´P (ot+1|wt+1, a1‚à∂t+1) P (wt+1|o1‚à∂t) dwt+1 ‚âÖ ‚à´P (ot+1|wt+1, a1‚à∂t+1) Q (wt+1|œÜt) dwt+1,\n\n(35)\n\nit is assumed that P (ot+1|o1‚à∂t, a1‚à∂t+1) ‚Äâwas heuristically replaced with P (ot+1) as the prior agent‚Äôs preference of observing ot+1 , so that ln P (ot+1)can be addressed as an instrumental reward. The equation (34)\nwas further transformed into\n\nF (at+1) = EQ(ot+1|at+1)Q(wt+1|o1‚à∂t+1,at+1) [ln Q (wt+1|œÜt) ‚àí ln Q (wt+1|o1‚à∂t+1, a1‚à∂t+1) ‚àí ln P (ot+1) ] .\n\n(36)\n\nFinally, we obtained the so-called expected free energy as\n\nF (at+1) = EQ(ot+1|at+1) [‚àíKL [Q (wt+1|ot+1, at+1) ||Q (wt+1)] ‚àí ln P (ot+1)] . (37)\n\nwhere the KL divergence is called a Bayesian surprise, representing the\nextent to which the agent‚Äôs beliefs are updated by observation, whereas the second term ln P (ot+1) can be addressed as the agent‚Äôs prior preference of observing ot+1, which can be interpreted as an instrumental reward. Therefore, the expected free energy is derived in an ad hoc\nmanner.\nIn the first term of equation (30), the posterior and prior distribu-\ntions of wt+1 in the KL divergence are derived as\n\nQ (wt+1) = ‚à´P (wt+1|wt) Q (wt) dwt,\n\n(38)\n\nQ (wt+1|ot+1,at+1) =\n\n, P(ot+1 |wt+1 ,at+1 )Q(wt+1 |at+1 )\nP(ot+1 |at+1 )\n\n(39)\n\nrespectively. ot+1 is a future observation, and therefore, the first term was expected by P (ot+1|at+1), which can be calculated as\n\nP (ot+1|at+1) = ‚à´P (ot+1|wt+1, at+1) Q (wt+1|at+1) dwt+1.\n\n(40)\n\nIn the second term, the reward is quantitatively interpreted as the desired probability of ot+1. For the two-choice task, we use\n\nP\n\n(ot+1 )\n\n=\n\nPot+1\no\n\n(1\n\n‚àí\n\nPo )(1‚àíot+1 )\n\n,\n\n(41)\n\nwhere Po indicates the desired probability of the presence of a reward. According to the probabilistic interpretation of reward22,23,\n\nNature Computational Science | Volume 3 | May 2023 | 418‚Äì432\n\n428\n\nArticle\n\nhttps://doi.org/10.1038/s43588-023-00439-w\n\nthe presence and absence of a reward can be evaluated by ln Po and ln(1‚Äâ‚àí‚ÄâPo), respectively.\nIn this study, we changed the sign of the expected free energy as the expected net utility, because we modeled decision-making as the maximization of the expected free energy. We ventured to introduce the curiosity meta-parameter ct to express irrational decision-making. Because rewards are relative, in this study, we set ln {Po/ (1 ‚àí Po)} and 0 for the presence and absence of a reward, respectively. Thus, our expected net utility can be described by equation (30), in which there is a gap between the original expected free energy and our expected net utility.\n\nModel for action selection The agent probabilistically selects a choice with higher expected net utility as\n\nP (at+1) =\n\nexp(Œ≤U(at+1 )) ‚àëa exp(Œ≤U(a))\n\n,\n\n(42)\n\nwhere U (at+1) indicates the expected net utility of action at+1. Equation (42) is equivalent to equation (2). To derive equation (42), we\nconsidered the expectation of the expected net utility with respect to\nprobabilistic action as\n\nE [U] = EQ(at+1) [U (at+1) ‚àí Œ≤‚àí1 ln Q (at+1)] ,\n\n(43)\n\nwhere Œ≤ indicates an inverse temperature, and the entropic constraint of action probability is introduced in the second term. This equation can be rewritten as\n\nE [U] = ‚àíŒ≤‚àí1KL [Q (at+1) ‚Äñ exp (Œ≤U (at+1)) /Z] + Œ≤‚àí1 ln Z,\n\n(44)\n\nwhere Z indicates a normalization constant. Thus, its maximization\nrespective to Q (at+1) leads to the optimal action probability as shown in equation (42).\n\nH (ot+1|wt+1) = ‚àíEQ(wt+1|at+1) [‚àëi ai,t+1g (wi,t+1)] ,\n\n(49)\n\nwhere\n\ng (w) = f (w) ln f (w) + (1 ‚àí f (w)) ln (1 ‚àí f (w)) .\n\n(50)\n\nHere, we approximately calculate this equation by using the second-order Taylor expansion as\n\nH\n\n(ot+1 |wt+1 )\n\n‚âÖ\n\n‚àíEQ(wt+1 |at+1 )\n\n‚é°‚é¢‚é¢‚é¢‚àëi ‚é£\n\nai,t+1\n\n‚éß‚é™ ‚é®‚é™‚é©\n\ng\n\n(Œºi,t+1 )\n\n+\n\n‚àÇg ‚àÇwi,t+1\n\n(wi,t+1\n\n‚àí\n\nŒºi,t+1 )\n\n+\n\n1 2\n\n‚àÇ2 g ‚àÇw2i,t+1\n\n(wi,t+1\n\n‚àí Œºi,t+1)2\n\n‚é´‚é™‚é¨‚é™‚é≠‚é§‚é•‚é•‚é•‚é¶\n\n,\n\n(51)\n\nwhich leads to\n\nH (ot+1|wt+1) =\n\n‚àí\n\n‚àë\ni\n\nai,t+1\n\n‚é°‚é¢‚é¢‚é¢‚é¢ ‚é£\n\n+\n\n1 2\n\nf (Œºi,t+1) lnf (Œºi,t+1) + (1 ‚àí f (Œºi,t+1)) ln (1 ‚àí f (Œºi,t+1))\n\n{f\n\n(Œºi,t+1\n\n)\n\n(1\n\n‚àí\n\nf\n\n(Œºi,t+1\n\n))\n\n(1\n\n+\n\n(1\n\n‚àí\n\n2f\n\n(Œºi,t+1\n\n))\n\nln\n\nf(Œºi,t+1 ) 1‚àíf(Œºi,t+1\n\n)\n\n)}\n\n(p‚àíi,t1 + p‚àíw1)\n\n‚é§‚é•‚é•‚é•‚é•. ‚é¶\n\n(52)\n\nThe marginal entropy H (ot+1) can be calculated as\n\nH (ot+1) = ‚àí ‚àë ai,t+1 {P (ot+1 = 0|at+1) ln P (ot+1 = 0|at+1) ,\ni\n+P (ot+1 = 1|at+1) ln P (ot+1 = 1|at+1)}\n\n(53)\n\nwhere\n\nP (ot+1|at+1) = ‚à´P (ot+1|wt+1, at+1) Q (wt+1|at+1) dwt+1\n\nAlternative expected net utility For comparison, we consider an alternative expected net utility by intro¬≠ ducing a time-dependent meta-parameter in the second term as follows:\n\n= ‚à´‚àè {f (wi,t+1)ot+1 (1 ‚àí f (wi,t+1))1‚àíot+1 }ai,t+1 Q (wt+1|at+1) dwt+1\ni\n\nU (at+1) = EP(ot+1|at+1) [KL [Q (wt+1|ot+1, at+1) ||Q (wt+1|at+1)]] + dt ‚ãÖ EP(ot+1|at+1) [R (ot+1)] ,\n\n(45)\n\nwhere dt denotes the subjective intensity of reward at time t. In this case, the agent with high dt will show more exploitative behavior, whereas the agent with dt = 0 shows more explorative behavior driven by the expected information gain.\n\nCalculation of expected net utility Here, we present the calculation of the expected net utility. The KL divergence in the first term of equation (30) can be transformed into\n\nEP(ot+1 |at+1 )\n\n[KL\n\n[Q\n\n(wt+1 |ot+1 ,\n\nat+1 )\n\n||Q\n\n(wt+1 |at+1 )]]\n\n=\n\nH\n\n(ot+1 )\n\n‚àí\n\nH\n\n(ot+1|wt+1) , (46)\n\n= ‚àè[ i +1ot+1\n\n(‚àí1)1‚àíot+1\n\nf (Œºi,t+1)ot+1 (1 ‚àí f (Œºi,t+1))1‚àíot+1\n\n1 2\n\nf\n\n(Œºi,t+1 )\n\n{1\n\n‚àí\n\nf\n\n(Œºi,t+1 )}\n\n{1\n\n‚àí\n\n2f\n\n(Œºi,t+1 )}\n\n(p‚àíi,t1\n\nai ,t+1\n]. + p‚àíw1)\n\n(54)\n\nThe second term of the expected net utility (equation (36)) is calculated as\n\nEP(ot+1|at+1) [lnP (ot+1)] = EP(ot+1|at+1) [ot+1lnPo + (1 ‚àí ot+1) ln (1 ‚àí Po)]\n\n(55)\n\n= P (ot+1 = 0|at+1) ln (1 ‚àí Po) + P (ot+1 = 1|at+1) ln (1 ‚àí Po) .\n\nwhere the first and second terms represent the conditional and marginal entropies, respectively:\n\nH (ot+1|wt+1) = EP(ot+1|wt+1,at+1)Q(wt+1|at+1) [‚àí ln P (ot+1|wt+1, at+1)] ,\n\n(47)\n\nH (ot+1) = EP(ot+1|at+1) [‚àí ln P (ot+1|at+1)] .\n\n(48)\n\nThe conditional entropy H (ot+1|wt+1)can be calculated by substituting equation (13) into equation (47) as\n\nObserver-SSM We constructed the observer-SSM, which describes the temporal transitions of the latent internal state z of agent and the generation of action, from the viewpoint of the observer of the agent. This is depicted graphically in Fig. 4. As prior information, we assumed that the agent acts based on the internal state, that is, the intensity of curiosity, the recognized reward probabilities and their confidence levels. The intensity of curiosity was assumed to change temporally as a random walk:\n\nct = ct‚àí1 + œµŒ∂t,\n\n(56)\n\nNature Computational Science | Volume 3 | May 2023 | 418‚Äì432\n\n429\n\nArticle\n\nhttps://doi.org/10.1038/s43588-023-00439-w\n\nwhere Œ∂t denotes white noise with zero mean and unit variance, and œµ denotes its noise intensity. Other internal states, that is, Œºi and pi, were assumed to update as equations (28) and (29). The transition of the\ninternal state is expressed by the probability distribution\n\nP (zt|zt‚àí1) = ùí©ùí© (zt|F (zt‚àí1, at‚àí1) , Œì ) ,\n\n(57)\n\nF\n\n(zt‚àí1 ,\n\nat‚àí1 )\n\n=\n\n‚é°‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢\n\n0 h (Œº1,t‚àí1, p1,t‚àí1, ot, a1) h (Œº2,t‚àí1, p2,t‚àí1, ot, a2)\nk (Œº1,t‚àí1, p1,t‚àí1, a1)\n\n‚é§‚é•‚é•‚é•‚é•‚é•‚é•‚é•\n\n,\n\n‚é£ k (Œº2,t‚àí1, p2,t‚àí1, a2) ‚é¶\n\n(58)\n\niFEP by particle filter and Kalman backward algorithm\nBased on the observer-SSM, we estimated the posterior distribution\nof the latent internal state of agent zt given all observations from 1 to T ( x1‚à∂T) in a Bayesian manner, that is, P (zt|x1‚à∂T). This estimation was done by forward and backward algorithms, which are called filtering and\nsmoothing, respectively.\nIn filtering, the posterior distribution of zt given observations until t ( x1‚à∂t) is sequentially updated in a forward direction as\n\nP (zt|x1‚à∂t) ‚àù P (xt|zt, Œ∏) ‚à´P (zt|zt‚àí1, Œ∏) P (zt‚àí1|x1‚à∂t‚àí1) dzt‚àí1,\n\n(66)\n\nwhere xt = (aTt , ot)T and Œ∏ = {œÉ2, Œ±, Po}. The prior distribution of z1 is\n\nwhere zt = (ct, ŒºTt , pTt )T and Œì = œµ2diag(1, 0, 0, 0, 0) ‚Äâ. h (Œºi,t‚àí1, pi,t‚àí1, ot, ai) and k (Œºi,t‚àí1, pi,t‚àí1, ai) represent the right-hand sides of equations (28) and (29), respectively; and Œì and diag (x) denote the variance‚Äìcovari-\nance matrix and square matrix whose diagonal component is x, respec-\ntively. In addition, the agent was assumed to select an action at+1 based on the expected net utilities, as follows:\n\nP (at+1) =\n\nexp(Œ≤U(at+1 )) ‚àëa exp(Œ≤U(a))\n\n,\n\n(59)\n\nand the reward was obtained by the following probability distribution:\n\nP\n\n(ot |at )\n\n=\n\n‚àè\ni\n\n{Œªoi,tt (1\n\n‚àí\n\nŒªi,t)1‚àíot }ai,t\n\n.\n\n(60)\n\nQ-leaning in two-choice task and its observer-SSM The decision-making in the two-choice task was also modeled by Q-learning. Reward prediction for the ith option Qi,t is updated as\n\nQi,t = Qi,t‚àí1 + Œ±t‚àí1 (rtai,t‚àí1 ‚àí Qi,t‚àí1) ,\n\n(61)\n\nwhere Œ±t indicates a learning rate at trail t. The agents selected action following a softmax function:\n\nP (ai,t\n\n= 1) =\n\n, exp(Bt Qi,t )\n‚àëi exp(Bt Qi,t )\n\n(62)\n\nwhere Bt indicates the inverse temperature at trail t controlling the randomness of the action selection.\nThe time-dependent parameters Œ±t and Bt in Q-learning were estimated from behavioral data37. These parameters were assumed\nto change temporally as a random walk:\n\nŒ∏t = Œ∏t‚àí1 + œµŒ∏Œ∂Œ∏,t,\n\n(63)\n\nwhere Œ∏ ‚àà {Œ±, B} , Œ∂Œ∏,t denotes white noise with zero mean and unit variance and œµŒ∏ denotes its noise intensity. Thus, the transition of the internal state is expressed by the probability distribution\n\nP (zt|zt‚àí1) = ùí©ùí© (zt|F (zt‚àí1, at‚àí1) , Œì) ,\n\n(64)\n\nF\n\n(zt‚àí1 ,\n\nat‚àí1 )\n\n=\n\n‚é°‚é¢‚é¢‚é¢‚é¢‚é¢\n\nh1\n\n(Œ±t ,\n\n0 0 Q1,t‚àí1 ,\n\nat‚àí1 )\n\n‚é§‚é•‚é•‚é•‚é•‚é•\n\n,\n\n‚é£ h2 (Œ±t, Q2,t‚àí1, at‚àí1) ‚é¶\n\n(65)\n\nwhere zt = (Œ±t, Bt, Q1,t, Q2,t)T , Œì = œµ2diag (1, 1, 0, 0) , hi (Œ±t, Qi,t‚àí1, at) represents the right-hand side of equation (61); and Œì and diag (x)denote the variance‚Äìcovariance matrix and square matrix whose diagonal com-\nponent is x, respectively.\n\nP (z1) = [‚àè ùí©ùí©ùí©Œºi,1|Œº0, œÉ2Œº)Gam(pi,1|ag, bg)] Uni(c1|au, bu),\ni\n\n(67)\n\nwhere Œº0 and œÉ2Œº denote means and variances, Gam(x|ag, bg) indicates the Gamma distribution with shape parameter ag and scale parameter bg, and Uni (x|au, bu) indicates uniform distribution from au to bu. We used a particle filter42 to sequentially calculate the posterior\nP (zt|x1‚à∂t), which cannot be analytically derived because of the nonlinear transition probability.\nAfter the particle filter, the posterior distribution of zt given all observations (x1‚à∂T) is sequentially updated in a backward direction as\n\nP (zt|x1‚à∂T) = ‚à´P (zt+1|x1‚à∂T) P (zt|zt+1, x1‚à∂t, Œ∏) dzt+1\n\n(68)\n\n=\n\n‚à´P (zt+1|x1‚à∂T)\n\nP(zt+1 |zt ,Œ∏)P(zt |x1‚à∂t ,Œ∏)\n‚à´P (zt+1 |zt ,Œ∏)P(zt |x1‚à∂t ,Œ∏)dzt\n\ndzt+1 .\n\nHowever, this backward integration is intractable because of\nthe non-Gaussian P (zT|x1‚à∂T), which was represented by the particle ensemble in the particle filter, and the nonlinear relationship between\nzt and zt+1 in P (zt+1|zt, Œ∏) (equation (57)). Thus, we approximated P (zt|x1‚à∂t) as ùí©ùí©(zt|mt, Vt), where mt and Vt denote a sample mean and a sample variance of the particles at t, whereas we linearized\nP (zt|zt‚àí1, Œ∏) as\n\nP (zt|zt‚àí1, Œ∏) ‚âÖ ùí©ùí© (zt|Azt‚àí1 + b, Œì) ,\n\n(69)\n\nA=\n\n||| , ‚àÇF(zt‚àí1,at‚àí1)\n\n‚àÇzt‚àí1\n\nmt\n\n(70)\n\nb = F (mt, at‚àí1) ‚àí Amt,\n\n(71)\n\nwhere A denotes a Jacobian matrix. Because these approximations make the integration of equation (68) tractable, the posterior distribution P (zt|x1‚à∂T) can be computed by a Gaussian distribution as\n\nP (zt|x1‚à∂T) = ùí©ùí© (zt|mÀÜ t, VÀÜt)\n\n(72)\n\nwhose mean and variance were analytically updated by Kalman backward algorithms as43\n\nmÀÜ t = mt + Jt {mÀÜ t+1 ‚àí (Amt + b)} ,\n\n(73)\n\nVÀÜt = Vt + Jt {mÀÜ t+1 ‚àí (AVtAT + Œì)} JTt ,\n\n(74)\n\nwhere\n\nJt = VtAT (AVtAT + Œì)‚àí1 .\n\n(75)\n\nNature Computational Science | Volume 3 | May 2023 | 418‚Äì432\n\n430\n\nArticle\n\nhttps://doi.org/10.1038/s43588-023-00439-w\n\nImpossibility of model discrimination In the ReCU and Q-learning models, the action selections were formulated with the same softmax functions as\n\nWe used the rat behavioral data published in ref. 36, which is publicly\navailable at https://groups.oist.jp/ja/ncu/data. These rat behavioral data are also included in the Source Data for Fig. 5 and on Zenodo45.\n\nP (ai,t\n\n= 1) =\n\n. exp(Œ≤(E[Rewardi,t ]+ct E[Infoi,t ]))\n‚àëj exp(Œ≤(E[Rewardj,t ]+ct E[Infoj,t ]))\n\n(76)\n\nP (ai,t\n\n= 1) =\n\n, exp(Œ≤t Qi,t )\n‚àëi exp(Œ≤t Qi,t )\n\n(77)\n\nwhich correspond to that of the ReCU and Q-learning models, respectively. These equations contain the time-dependent meta-parameters, that is, ct and Œ≤t. For both models, the goodness of fit (that is, likelihood) for the actual behavioral data can be freely improved by tuning the time-dependent meta-parameters. Thus, discrimination between the ReCU and Q-learning models must be essentially impossible.\n\nEstimation of parameters in iFEP The ReCU model has several parameters: œÉ2w, Œ±, Œ≤, Po and œµ. In the estimation, we set œµ to 1, which was the optimal value for estimation in the artificial data (Supplementary Fig. 3). We assumed the unit intensity\nof reward, that is, ln Po/(1 ‚àí Po) = 1, because it is impossible to estimate both Po and Œ≤ caused by multiplying Œ≤ and ln Po/(1 ‚àí Po) in the expected net utility (equations (30) and (42)). This treatment is suitable for rela-\ntive comparison between the curiosity meta-parameter and the reward. In addition, we addressed Œ≤ct as a latent variable as cÃÇt = Œ≤ct because of the multiplication of Œ≤ in the expected net utility (equations (30)\nand (42)). Thus, the estimation of ct can be obtained by dividing the estimated cÃÇt by the estimated Œ≤. Therefore, the hyperparameters to be estimated were œÉ2w, Œ± and Œ≤.\nTo estimate these parameters Œ∏ = {œÉ2w, Œ±, Œ≤} , we extended the observer-SSM to a self-organizing SSM44 in which Œ∏ was addressed as\nconstant latent variables:\n\nP (zt, Œ∏|x1‚à∂t) ‚àù P (xt|zt) ‚à´P (zt|zt‚àí1, Œ∏) P (zt‚àí1, Œ∏|x1‚à∂t‚àí1) dzt‚àí1,\n\n(78)\n\nwhere P (Œ∏) = Uni (œÉ2|aœÉ, bœÉ) Uni(Œ±|aŒ±, bŒ±)ùí©ùí©ùí©Œ≤|mŒ≤, vŒ≤). To sequentially calculate the posterior P (zt, Œ∏|x1‚à∂t) using the particle filter, we used 100,000 particles and augmented the state vector of all particles by\nadding the parameter Œ∏, which was not updated from randomly sam-\npled initial values.\nThe hyperparameter values used in this estimation were Œº0 = 0, œÉ2Œº = 0.012, ag = 10, bg = 0.001, au = ‚àí15, bu = 15, aœÉ = 0.2, bœÉ = 0.7, aŒ± = 0.04, bŒ± = 0.06, aŒ≤ = 0 and bŒ≤ = 50, which were heuristically given as parameters\ncorrectly estimated using the artificial data (Supplementary Fig. 2).\n\nStatistical testing with Monte Carlo simulations Supplementary Fig. 5 shows statistical testing of the negative curiosity estimated in Fig. 5. A null hypothesis is that an agent has no curiosity (that is, ct‚Äâ=‚Äâ0) decides on a choice only depending on its recognition of the reward probability. Under the null hypothesis, model simulations were repeated 1,000 times under the same experimental conditions as in Fig. 5 and the curiosity was estimated for each using iFEP. We adopted the temporal average of the estimated curiosity as a test statistic and plotted the null distribution of the test statistic. Compared with the estimated curiosity of the rat behavior, we computed the P value for a one-sided left-tailed test.\n\nReporting summary Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.\nData availability\nSource data for Figs. 2, 3, 5 and 6 are available with this paper. Source data for Supplementary Figures are available in Supplementary Data.\n\nCode availability\nThe computer simulation and data analysis were performed using MATLAB (version R2020b) software. The code used for this work is available on GitHub at https://github.com/YukiKonaka/Konaka_ Honda_2023. The specific version used to produce the results in this manuscript is also available on Zenodo45.\nReferences\n1. Helmholtz, H. Handbuch der Physiologischen Optik (Andesite Press, 1867).\n2. Yuille, A. & Kersten, D. Vision as Bayesian inference: analysis by synthesis? Trends Cogn. Sci. 10, 301‚Äì308 (2006).\n3. Millett, J. D. & Simon, H. A. Administrative behavior: a study of decision-making processes in administrative organization. Polit. Sci. Q. 62, 621 (1947).\n4. Dubey, R. & Griffiths, T. L. Understanding exploration in humans and machines by formalizing the function of curiosity. Curr. Opin. Behav. Sci. 35, 118‚Äì124 (2020).\n5. Kidd, C. & Hayden, B. Y. The psychology and neuroscience of curiosity. Neuron 88, 449‚Äì460 (2015).\n6. Klein, U. & Nowak, A. J. Characteristics of patients with autistic disorder (AD) presenting for dental treatment: a survey and chart review. Spec. Care Dentist. 19, 200‚Äì207 (1999).\n7. Lockner, D. W., Crowe, T. K. & Skipper, B. J. Dietary intake and parents‚Äô perception of mealtime behaviors in preschoolage children with autism spectrum disorder and in typically developing children. J. Am. Diet. Assoc. 108, 1360‚Äì1363 (2008).\n8. Schreck, K. A. & Williams, K. Food preferences and factors influencing food selectivity for children with autism spectrum disorders. Res. Dev. Disabil. 27, 353‚Äì363 (2006).\n9. Esposito, M. et al. Sensory processing, gastrointestinal symptoms and parental feeding practices in the explanation of food selectivity: clustering children with and without autism. Int. J. Autism Relat. Disabil. 2, 1‚Äì12 (2019).\n10. Hobson, R. P. Autism and the development of mind. Essays Dev. Psychol. (Routledge, 1993).\n11. Burke, R. Personalized recommendation of PoIs to people with autism. Commun. ACM 65, 100 (2022).\n12. Ghanizadeh, A. Educating and counseling of parents of children with attention-deficit hyperactivity disorder. Patient Educ. Couns. 68, 23‚Äì28 (2007).\n13. Sedgwick, J. A., Merwood, A. & Asherson, P. The positive aspects of attention deficit hyperactivity disorder: a qualitative investigation of successful adults with ADHD. ADHD Atten. Deficit Hyperact. Disord. 11, 241‚Äì253 (2019).\n14. Redshaw, R. & McCormack, L. ‚ÄòBeing ADHD‚Äô: a qualitative study. Adv. Neurodev. Disord. 6, 20‚Äì28 (2022).\n15. Sutton, R. S. & Barto, A. G. Reinforcement Learning: an Introduction (MIT Press, 1998).\n16. Friston, K. A theory of cortical responses. Philos. Trans. R. Soc. B 360, 815‚Äì836 (2005).\n17. Friston, K., Kilner, J. & Harrison, L. A free energy principle for the brain. J. Physiol. Paris 100, 70‚Äì87 (2006).\n18. Friston, K. The free-energy principle: a unified brain theory? Nat. Rev. Neurosci. 11, 127‚Äì138 (2010).\n19. Lindley, D. V. On a measure of the information provided by an experiment. Ann. Math. Stat. 27, 986‚Äì1005 (1956).\n20. MacKay, D. J. C. Information-based objective functions for active data selection. Neural Comput. 4, 590‚Äì604 (1992).\n21. Berger, J. O. Statistical Decision Theory and Bayesian Analysis, Springer Series in Statistics (Springer, 2011).\n\nNature Computational Science | Volume 3 | May 2023 | 418‚Äì432\n\n431\n\nArticle\n\nhttps://doi.org/10.1038/s43588-023-00439-w\n\n22. Friston, K. et al. Active inference and epistemic value. Cogn. Neurosci. 6, 187‚Äì214 (2015).\n23. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P. & Pezzulo, G. Active inference: a process theory. Neural Comput. 29, 1‚Äì49 (2017).\n24. Attias, H. Planning by probabilistic inference in Proc. 9th Int. Work. Artif. Intell. Stat. 4, 9‚Äì16 (2003).\n25. Botvinick, M. & Toussaint, M. Planning as inference. Trends Cogn. Sci. 16, 485‚Äì488 (2012).\n26. Kaplan, R. & Friston, K. J. Planning and navigation as active inference. Biol. Cybern. 112, 323‚Äì343 (2018).\n27. Matsumoto, T. & Tani, J. Goal-directed planning for habituated agents by active inference using a variational recurrent neural network. Entropy 22, (2020).\n28. Schwartenbeck, P. et al. Computational mechanisms of curiosity and goal-directed exploration. eLife 8, 1‚Äì45 (2019).\n29. Millidge, B., Tschantz, A. & Buckley, C. L. Whence the expected free energy? Neural Comput. 33, 447‚Äì482 (2021).\n30. Houthooft, R. et al. VIME: variational information maximizing exploration. Adv. Neural Inf. Process. Syst. 0, 1117‚Äì1125 (2016).\n31. Smith, R. et al. Greater decision uncertainty characterizes a transdiagnostic patient sample during approach-avoidance conflict: a computational modelling approach. J. Psychiatry Neurosci. 46, E74‚ÄìE87 (2021).\n32. Smith, R. et al. Long-term stability of computational parameters during approach-avoidance conflict in a transdiagnostic psychiatric patient sample. Sci Rep. 11, 1‚Äì13 (2021).\n33. Schwartenbeck, P. & Friston, K. Computational phenotyping in psychiatry: a worked example. eNeuro 3, 1‚Äì18 (2016).\n34. Daunizeau, J. et al. Observing the observer (I): meta-Bayesian models of learning and decision-making. PLoS ONE 5, e15554 (2010).\n35. Patzelt, E. H., Hartley, C. A. & Gershman, S. J. Computational phenotyping: using models to understand individual differences in personality, development, and mental illness. Personal. Neurosci. 1, e18 (2018).\n36. Ito, M. & Doya, K. Validation of decision-making models and analysis of decision variables in the rat basal ganglia. J. Neurosci. 29, 9861‚Äì9874 (2009).\n37. Samejima, K., Doya, K., Ueda, Y. & Kimura, M. Estimating internal variables and parameters of a learning agent by a particle filter. Adv. Neural Inf. Process. Syst. 16 (2003).\n38. Samejima, K., Ueda, Y., Doya, K. & Kimura, M. Neuroscience: representation of action-specific reward values in the striatum. Science (80-.) 310, 1337‚Äì1340 (2005).\n39. Ortega, P. A. & Braun, D. A. Thermodynamics as a theory of decision-making with information-processing costs. Proc. R. Soc. London. A 469, 20120683 (2013).\n40. Gottwald, S. & Braun, D. A. The two kinds of free energy and the Bayesian revolution. PLoS Comput. Biol. 16, (2020).\n41. Parr, T. & Friston, K. J. Generalised free energy and active inference. Biol. Cybern. 113, 495‚Äì513 (2019).\n42. Kitagawa, G. Monte Carlo filter and smoother for non-Gaussian nonlinear state space models. J. Comput. Graph. Stat. 5, 1‚Äì25 (1996).\n43. Bishop, C. M. Pattern Recognition and Machine Learning (Springer, 2006).\n44. Kitagawa, G. A self-organizing state-space model. J. Am. Stat. 93, 1203‚Äì1215 (1998).\n45. Konaka, Y. & Naoki, H. Codes for Konaka and Honda 2023. Zenodo https://doi.org/10.5281/zenodo.7722905 (2023)\n\nAcknowledgements\nWe are grateful to K. Doya and M. Ito for providing rat behavioral data. We thank the organizers of the tutorial on the free energy principle in 2019, which inspired this research, and I. Higashino and M. Fujiwara-Yada for carefully checking all the equations in the manuscript. This study was supported in part by a Grant-in-Aid for Transformative Research Areas (B) (no. 21H05170), AMED (grant no. JP21wm0425010), Moonshot R&D‚ÄìMILLENNIA program (grant no. JPMJMS2024-9) by JST, the Cooperative Study Program of Exploratory Research Center on Life and Living Systems (ExCELLS) (program no. 21-102) and the grant of Joint Research by the National Institutes of Natural Sciences (NINS program no. 01112102).\nAuthor contributions\nH.N. conceived of the project. Y. K. and H.N. developed the method, and Y.K. implemented the model simulation. Y.K. and H.N. wrote the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary information The online version contains supplementary material available at https://doi.org/10.1038/s43588-023-00439-w.\nCorrespondence and requests for materials should be addressed to Honda Naoki.\nPeer review information Nature Computational Science thanks Junichiro Yoshimoto and Karl Friston for their contribution to the peer review of this work. Primary Handling Editors: Ananya Rastogi and Jie Pan, in collaboration with the Nature Computational Science team.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher‚Äôs note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article‚Äôs Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article‚Äôs Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons. org/licenses/by/4.0/.\n¬© The Author(s) 2023\n\nNature Computational Science | Volume 3 | May 2023 | 418‚Äì432\n\n432\n\n"}