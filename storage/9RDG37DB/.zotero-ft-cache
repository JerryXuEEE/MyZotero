nature computational science

Article

https://doi.org/10.1038/s43588-023-00439-w

Decoding reward–curiosity conflict in

decision-making from irrational behaviors

Received: 21 June 2022 Accepted: 29 March 2023 Published online: 15 May 2023
Check for updates

Yuki Konaka    1 & Honda Naoki    1,2,3,4 
Humans and animals are not always rational. They not only rationally exploit rewards but also explore an environment owing to their curiosity. However, the mechanism of such curiosity-driven irrational behavior is largely unknown. Here, we developed a decision-making model for a twochoice task based on the free energy principle, which is a theory integrating recognition and action selection. The model describes irrational behaviors depending on the curiosity level. We also proposed a machine learning method to decode temporal curiosity from behavioral data. By applying it to rat behavioral data, we found that the rat had negative curiosity, reflecting conservative selection sticking to more certain options and that the level of curiosity was upregulated by the expected future information obtained from an uncertain environment. Our decoding approach can be a fundamental tool for identifying the neural basis for reward–curiosity conflicts. Furthermore, it could be effective in diagnosing mental disorders.

Animals and humans perceive the external world through their sensory systems and make decisions accordingly1,2. Generally, they cannot make optimal decisions because of the uncertainty of the environment as well as the limited computational capacity of the brain and time constraints associated with decision-making3. In fact, they perform irrational actions. For example, people play lotteries and gamble despite low reward expectations. In this case, they face a dilemma between low expected reward and curiosity regarding whether a reward will be acquired. Thus, understanding how animals control the balance between reward and curiosity is important for clarifying the whole decision-making process. However, a method is yet to be established for quantifying the reward–curiosity balance has yet been established. In this study, we developed a machine learning method to decode the time series of the reward–curiosity balance from animal behavioral data.
Some irrational behaviors emerge because of the strength of curiosity4,5. For example, conservative individuals avoid uncertainty and prefer to select an action that leads to predictable outcomes. Conversely, inquisitive individuals strongly desire to know the environment rather than rewards and prefer to select an action that leads to unpredictable outcomes. Too conservative and inquisitive natures can be interpreted as autism spectrum disorder and attention deficit

hyperactivity disorder, patients with which are known to substantially avoid and seek novel information, respectively6–14. Rational individuals fall midway between these two extremes. In an ambiguous environment, they select an action to efficiently understand the environment, and if the environment becomes clear, they select an action to efficiently exploit the rewards. Therefore, curiosity has a major impact on behavioral patterns, and it is believed that animals control the balance between reward and curiosity in a context-dependent manner.
Decision-making has been modeled primarily by reinforcement learning (RL), which is a theory for describing reward-seeking adaptive behavior in which animals not only exploit rewards but also explore the environment. In RL, explorative behavior was addressed by a passive, random choice of action15. However, animals actively explore the environment by selecting actions that minimize the uncertainty of the environment given their curiosity.
Recently, the free energy principle (FEP) was proposed by Karl Friston under the Bayesian brain hypothesis, in which the brain optimally recognizes the outside world according to Bayesian estimation16–18. The FEP addresses not only the recognition of the external world but also the information-seeking action selection, which minimizes the uncertainty of the recognition of the external world, known as “active

1Laboratory of Data-Driven Biology, Graduate School of Integrated Sciences for Life, Hiroshima University, Hiroshima, Japan. 2Kansei-Brain Informatics Group, Center for Brain, Mind and Kansei Sciences Research, Hiroshima University, Hiroshima, Japan. 3Theoretical Biology Research Group, Exploratory Research Center on Life and Living Systems, National Institutes of Natural Sciences, Okazaki, Japan. 4Laboratory of Theoretical Biology, Graduate School of Biostudies, Kyoto University, Kyoto, Japan.  e-mail: nhonda@hiroshima-u.ac.jp

Nature Computational Science | Volume 3 | May 2023 | 418–432

418

Article

https://doi.org/10.1038/s43588-023-00439-w

inference”19–21. Furthermore, FEP proposed a score of action, called expected free energy, which consists of the expected reward and curiosity with the same unit22–27. Thus, action selection can be formulated by maximizing both reward and curiosity. Note that curiosity can be regarded as information gain, that is, the extent to which we expect our recognition to be updated by the new observation through the action. However, FEP assumes that the weighting of rewards and curiosity is always even and constant. Although a previous FEP study modeled active inference in a two-choice task, it assumed a constant intensity of curiosity and thus could not treat actual animal behaviors in which the weights of rewards and curiosity are expected to change over time28. Hence, conventional theories such as RL and FEP are limited in describing the conflict between reward and curiosity.
Identifying the temporal variability of curiosity is important for future clarifying the neural mechanisms of the reward and curiosity conflicts in decision-making. Many FEP studies have been devoted to the construction of theory, assuming that the decision-making processes of animals are Bayes optimal. Thus, there was not even the idea that animals irrationally make decisions depending on the reward and curiosity conflicts. For this reason, a method to decode the temporal balance between reward and curiosity from behavioral data is yet to be established. Such a method would enable us to analyze neural correlates with the temporal variability of curiosity, and consequently, it would help us clarify how the brain controls the balance of reward and curiosity in a context-dependent manner.
In this study, we extended FEP by incorporating a meta-parameter that controls the conflict dynamics between reward and curiosity, called the reward–curiosity decision-making (ReCU) model. The ReCU model can exhibit various behavioral patterns, such as greedy behavior toward reward, information-seeking behaviors with high curiosity and conservative behaviors avoiding uncertainty. Moreover, we developed a machine learning method called the inverse FEP (iFEP) method to estimate the internal variables of decision-making information processing. Applying the iFEP method to a behavioral time series in a two-choice task, we successfully estimated the internal variables, such as variations in curiosity, recognition of reward availability and its confidence.
Results
Decision-making with the reward–curiosity dilemma Animals perceive the environment by inferring causes such as reward availability from observation, and then they make decisions based on their own inferences. In this study, we developed an ReCU model of a decision-making agent facing a dilemma between reward and curiosity in a two-choice task, wherein the agent selects either of two choices associated with the same rewards but with different reward probabilities (Fig. 1a). If the agent aims to maximize cumulative rewards, the agent must select an option with a higher reward probability. However, in animal behavioral experiments, even after they learned which option was more associated with a reward, they did not exclusively select the best choice, but also often selected the option with a smaller reward probability, which seems unreasonable.
Here, we hypothesized the following: Animals assume that the reward probability for each option might fluctuate over time, and therefore, the continuous selection of one option decreases the confidence of the reward probability estimation for the other option. Thus, they become curious about the ambiguous option even with a smaller reward probability, and so selecting the ambiguous option is reasonable for increasing the confidence of the estimation for both options. Therefore, we considered that the agent should make decisions driven by reward and curiosity in a situation-dependent manner.
ReCU model In the ReCU model, we divided information processing in the brain into two processes. In the first process, the agent updates the recognition of the reward probability of each option (Fig. 1a, process 1). In the second

process, the agent selects an action based on the current recognition and curiosity (Fig. 1a, process 2). The agent repeats these two processes in the two-choice task.
We modeled the first process by sequential Bayesian estimation, under the assumption that reward probabilities latently fluctuate in time (Fig. 1b). The agent updates the belief about the reward probabilities, which are expressed as estimation distributions, in response to actions and consequence reward observations (Fig. 1c). We derived the equations of the belief update (Fig. 1d and Methods). We modeled the second process by action selection based on two kinds of motivations: the desires to maximize the reward and to gain the information from the environment (Fig. 1e). This sum, called ‘expected net utility’ in this study, can be expressed by

Ut (at+1) = E [Rewardt+1] + ct ⋅ E [Infot+1] ,

(1)

where a and t indicate the action and trial index, respectively, and E[x] denotes the expectation value of x based on current recognition. The first and second terms represent expectations of reward and information derived from a new observation, respectively, for the next action at+1 based on current recognition. ct denotes a meta-parameter describing the intensity of curiosity, which weighs the expected information gain (see Methods for detail). We assumed an irrational mental conflict as ct varies over time (Fig. 1f). In decision-making, the agents prefer to select action at+1 with the higher expected net utility, in which the action is selected probabilistically following a sigmoidal function

P (at+1)

=

1

+

exp

1 (−βΔUt

)

,

(2)

where ΔUt = Ut(at + 1) − Ut(at+1), and β denotes the inverse temperature controlling the randomness of action selection22,29,30.

Recognition and decision-making in the simulation To validate our model, we performed simulations with constant curiosity ct = 1 for two cases. In the first case, where reward probabilities were constant and different between the two options (Fig. 2a), the agent preferred to select the option with the higher reward probability (Fig. 2b). The recognized reward probabilities converged to ground truths, indicating that the agent accurately recognized the reward probabilities (Fig. 2c). The recognition confidence changed over time depending on the behavior in each trial; the confidence of the selected option increased with information from the observation, whereas that of the unselected option decreased because of the agent’s assumption regarding the fluctuation in reward probabilities (Fig. 2d). Similarly, the expected information gain of an option increased and decreased when that option was selected and unselected, respectively. Thus, the expected information gain was lower for the option with higher confidence (Fig. 2e). The expected reward followed the recognized reward probability (Fig. 2f). Initially, decreasing expected information gain and increasing expected reward eventually cross at some number of trials, which correspond to switching between information exploration and reward acquisition (Supplementary Fig. 1a). These two factors are negatively correlated, indicating a trade-off relationship (Supplementary Fig. 1b). The expected net utility, which is the sum of the expected information gain and reward, represents the value of each selection (Fig. 2g), resulting in the agent’s preferentially selecting the option with the higher expected net utility.
In the second case, we assumed a dynamic environment with a time-dependent reward probability (Fig. 2h). In the simulation, the agent adaptively changed its recognition of the reward probability following the change in the true reward probability, and selected the option with the higher estimated reward probability (Fig. 2i,j). The confidence was affected by the uncertainty of the reward probability at each time; the confidence was high where the reward probabilities

Nature Computational Science | Volume 3 | May 2023 | 418–432

419

Article

https://doi.org/10.1038/s43588-023-00439-w
a
or

Process 1: recognition

Process 2: action selection

Recognition process

b

e

Latent variable

wt–2

wt–1

wt

wt+1

Observation ot–2

ot–1

ot

ot+1

Estimate Estimate

Action

at–2

at–1

at

at+1

c

Update

New

belief
f

Previous

belief

σt–1

σt

µt–1

µt

Latent variable controlling reward probability

d

µt = µt–1 + α Kt {ot – f(µt–1)}

11

σ

2 t

=

Kt

+ f(µt){1 – f(µt)}

Curiosity

Action selection process

I R

RI

Ut(at+1) = E[Reward] + ctE[Info]

R

I

ct = ct–1 + γ noise

Time

Fig. 1 | Decision-making model for the two-choice task with reward–curiosity dilemma. a, Decision-making in the two-choice task. Reward is provided at different probabilities for each option. The agent does not know those probabilities. Through repeated trial and error, the agent recognizes the world by inferring the latent reward probability of each option, and decides to choose the next action, that is, option, based on its own inference. b, Sequential Bayesian estimation as a recognition process. The agent assumes that the reward probabilities change over time owing to the fluctuation in the latent variable controlling reward probability. c, Belief updating. The agent recognizes the latent variable as a probability distribution. d, The update rule of the mean and

variance of the estimation distribution for each option. α, Kt and f(μt) indicate the learning rate, Kalman gain, and the prediction of the reward probability, respectively. The second term in both equations disappears if the option is not selected. e, The action selection process by the agent. The agent evaluates the expected net utility Ut(at+1) of each action using the weighted sum of the expected reward and information gain, as shown in the equation. The agent compares the expected net utilities for both actions and prefers the option with the larger expected net utility. f, Time-dependent curiosity. The intensity of curiosity changes over time owing to the fluctuation of ct .

were near-deterministic, around 1 and 0, but low where the reward probabilities were uncertain, i.e., approximately 0.5 (Fig. 2k). The expected information gain of an option was negatively correlated with the confidence of the option (Fig. 2l), suggesting that the agent was curious about the uncertain option. The expected reward just varied depending on the recognized reward probability (Fig. 2m). In this case, we also observed a switching between information exploration and reward acquisition at the initial phase (Supplementary Fig. 1c). In contrast to the above case, the expected reward and expected information gain did not show clearly linear correlation because of the dynamic environment (Supplementary Fig. 1d). The expected net utility changed similarly to the expected reward; however, the difference between the left and right options was less pronounced because of

curiosity (Fig. 2n). These two demonstrations indicate that our model can represent the process of cognition and decision-making based on reward and curiosity.

Discrimination of passive and curiosity-dependent behaviors The behavioral difference based on curiosity is interesting. The expected net utility can be rewritten as

ΔUt = ΔE [Rewardt+1] + ct ⋅ ΔE [Infot+1] ,

(3)

where the first and second terms represent the differences between
the expected reward and information gain of two options, respectively. Thus, the agent is decided based on the balance between ΔE [Rewardt+1]

Nature Computational Science | Volume 3 | May 2023 | 418–432

420

Article

https://doi.org/10.1038/s43588-023-00439-w

and ΔE [Infot+1]. Here, we created a diagram visualizing the selected actions in the space of ΔE [Rewardt+1] and ΔE [Infot+1]. Depending on the intensity of curiosity ct, the left and right actions can be separ­ a­ ted by a boundary of ΔE [Rewardt+1] + ct ⋅ ΔE [Infot+1] = 0 (Fig. 2o–q). The agent with ct = 0 selected an option based only on ΔE [Rewardt+1] (Fig. 2p). In contrast, if ct was a nonzero value, the boundary leaned to a different direction depending on the positive and negative values of ct (Fig. 2o,q). These results indicate that passive choice (ct = 0) and curiosity-dependent choice (ct ≠ 0) can be discriminated based on the distributed pattern of selected actions in the space of ΔE [Rewardt+1] and ΔE [Infot+1].
Curiosity-dependent irrational behaviors We examined how behavioral patterns are regulated by the intensity of curiosity and the degree of reward seeking (Fig. 3). In a scenario where the reward probabilities were zero for the left and 0.5 for the right (Fig. 3a), we simulated a model by varying the meta-parameters c and Po, which is a control parameter of the reward amount (Fig. 3b and Methods). When the agent strongly desired the reward (Po = 0.99 ) with no curiosity (c = 0), the agent preferred the right option with a higher reward probability (Fig. 3c, point a). If the agent had no desire for a reward (Po = 0.5) with high curiosity (c = 0), the agent preferred the option with a higher reward probability (Fig. 3c, point b). Although this behavior seems to be rational at first glance, the agent did not seek the reward, but rather sought the information (i.e., belief update) driven by curiosity, which resulted in a preference for the uncertain option. When the agent has negative curiosity (c = −10), the agent continuously selected either of the two options depending on the first selection (Fig. 3c, point c). In this behavior, the agent conservatively selected the more certain option, as patients with autism spectrum disorder irrationally avoid new information and repeat the same choices6–11.
In addition, we obtained a nontrivial result in another scenario, where the reward probabilities were 0.5 for the left and 1 for the right (Fig. 3d–f). As in the previous scenario, the agents with a strong desire for the reward (Po = 0.99, nearly equal to 1) preferred the right option with a higher reward probability (Fig. 3f, point a). The agent with no desire for reward (Po = 0.5) and high curiosity (c = 10) preferred the left option with a lower reward probability (Fig. 3f, point b). This seemingly irrational behavior was the outcome of focusing on satisfying curiosity and not seeking rewards, which recalls patients with attention deficit hyperactivity disorder irrationally exploring new information12–14. In addition, as seen in the previous scenario (Fig. 3c, point c), the agents with negative curiosity (c = −10), irrespective of the desire for the reward, exhibited conservative selection (Fig. 3f, point c). In combination, these results clearly indicate that behavioral patterns largely depend on the degree of conflict between reward and curiosity.
Inverse FEP: Bayesian estimation of the internal state In the above cases, we assumed a constant balance between reward and curiosity. However, our feelings swing in a context-dependent manner. Although it is important to decipher the temporal swinging of the conflict between reward and curiosity in terms of neuroscience and psychology, it is difficult to quantify the conflict because of its

temporal dynamics. Here, we addressed the inverse problem to estimate the internal states of the agent from behavioral data, known as computational phenotyping or meta-Bayesian inference31–35. To this end, we developed a machine learning method called iFEP to quantitatively decipher the temporal dynamics of the internal state including the curiosity meta-parameter from behavioral data.
For developing iFEP, we needed to switch the viewpoint from the agent to the observer of the agent, that is, from animals to us. In the state-space model (SSM) from the viewpoint of the agent, we described the sequential recognition of reward probabilities by the agent (Figs. 1b and 4a,b). Conversely, we developed a state-space model from the observer’s eye (the observer-SSM) to determine the internal state of the agent, for example, the intensity of curiosity ct, recognition μi,t and its confidence Pi,t (i.e., inverse of the variance of the estimation distribution) (Fig. 4c). In the observer-SSM, the intensity of curiosity was assumed to change continuously over time, and the agent’s recognition of the reward probability was updated by using the equations shown in Fig. 1c following the FEP; however, they were unknown to the observers. In addition, the agent’s actions were assumed to be generated depending on the intensity of its curiosity, recognition and confidence, as described in equation (2), but the observers can only monitor the agent’s action and the presence of a reward. In iFEP, based on the observer-SSM, we estimate the latent internal state of agent z from observation x in a Bayesian manner as

P (z1∶T|x1∶T) ∝ P (x1∶T|z1∶T) P (z1∶T) ,

(4)

where z1∶T = {μi,1∶T, pi,1∶T, c1∶T} , x1∶T = {a1∶T, o1∶T}, and the subscript 1:T indicates steps 1 to T. In this Bayesian estimation, a posterior distribu-
tion P (z1∶T|x1∶T) represents the observer’s recognition of the estimated z1∶T given observation x1:T with uncertainty. A prior distribution P (z1∶T)represents our belief, which is expressed as the ReCU model with the random motion of the curiosity meta-parameter c as

ct = ct−1 + ϵζt,

(5)

where ζt indicates the white standard Gauss noise and ϵ indicates its noise intensity. The likelihood P(x1:T|z1:T) represents the probability that x1:T was observed assuming z1:T, which also follows the ReCU model. This Bayesian estimation, namely iFEP, was conducted using a particle
filter and Kalman backward algorithm (Methods).

Validation of iFEP with artificial data We tested the validity of the iFEP method by applying it to the artificial data generated by the ReCU model. We simulated a model agent with nonconstant curiosity in the two-choice task, where reward probabilities varied temporally. We then demonstrated that iFEP estimated the ground truth of the internal state of the simulated agent, that is, the agent’s intensity of curiosity, recognition and confidence (Supplementary Fig. 2). We also confirmed that the estimation performance is robust against the value of ϵ (Supplementary Fig. 3). Therefore, iFEP is in a position to provide efficient estimators of belief updating to clarify decision-making processing and the accompanying temporal swing in the conflict between reward and curiosity.

Fig. 2 | Simulations of the decision-making model. a, The two-choice task with constant reward probabilities. b, The selection probabilities for the left and right options, plotted as a moving average with window width of 101. c, The recognized reward probabilities for the left and right options compared with the ground truths depicted by dashed lines. d, The confidences of reward probability recognitions for left the and right options. e–g, The expected brief updates (e), expected reward (f) and expected net utility (g) for the left and right options. h, The two-choice task with constant and temporally varying reward probabilities for the left and right options. i–n, The same as b–g with parameter values of

c = 1, Po = 0.8, α = 0.05, β = 2 and σw = 0.63. o–q, The selected options in a space of left–right differences of the expected reward and information gain. The
ReCU model was simulated with dynamically changing reward probabilities for different intensities of curiosity: c = −1 (o), c = 0 (p) and c = 3 (q). The reward probabilities were generated by the Ornstein–Uhlenbeck process of w for 1,000 trials: wi,t = wi,t−1 − 0.01wi,t−1 + 0.15ξt , where ξt indicates the standard Gauss noise. The heatmap represents the probability of action selection in the space
(equation (2)).

Nature Computational Science | Volume 3 | May 2023 | 418–432

421

Article

https://doi.org/10.1038/s43588-023-00439-w

Information gain

Selection probability

a
b 1.00 0.75 0.50 0.25 0 0
e 0.60 0.45 0.30 0.15 0 0
h

Reward probability

Reward probability

Left

Trials
c
LefLteft

RigRhigt ht

500 Trials

1,000

f

Right

Left 500 Trials

1,000

Expected reward

Estimated reward probability

80%
1.00 0.75 0.50 0.25
0 0
1.2 0.9 0.6 0.3
0 0

20%
d
Left

500 Trials

Right 1,000

g
Left

Right

500 Trials

1,000

Expected net utility

Confidence

44 33 22 11
0 0
1.40 1.05 0.70 0.35
0 0

Right Trials Left

Right 500 Trials

1,000

Left

Right

500 Trials

1,000

Left Right

Reward probability

Reward probability

Information gain

Selection probability

i 1.00 0.75 0.50 0.25 0 0
l 0.8 0.6 0.4 0.2 0 0
o 1.0 Right 0.5

Trials

j 1.00

Estimated reward probability

Right Left

0.75 0.50 0.25

0

500

1,000

0

Trials

m 1.2

Expected reward

0.9 Left Right
0.6

0.3

500 Trials

0

1,000

0

p 1.0

0.5

Right Left
500 Trials
Left Right
500 Trials

k
1,000

Confidence

100 75 50 25 0 0

n 1.40

Expected net utility

1.05

0.70

0.35

0

1,000

0

q 1.0

0.5

Trials

Left

Right

500 Trials

1,000

Right Left

500 Trials

1,000
1.5 Left
1.0

∆ info gain

∆ info gain

∆ info gain

0

0

0

0.5

–0.5

–1.0 –1.0

Left

–0.5

0

0.5

1.0

∆ reward

–0.5

Right

–1.0

–1.0 –0.5

0

Left

0.5

1.0

∆ reward

–0.5

Right

–1.0

–1.0 –0.5

0

0.5

∆ reward

0 –0.5 1.0

Probability of selecting the right

Nature Computational Science | Volume 3 | May 2023 | 418–432

422

Article

a

b 10

0% Point b

50%

https://doi.org/10.1038/s43588-023-00439-w
d

50%

100%

e 10

1

Point b

Intensity of curiosity, Ct

Intensity of curiosity, Ct

Fraction of selecting the right

0

Point a

0

Point a

–10 0.50
c Point a
1.0

Point c
0.75 Reward intensity, Po
Point b 1.0

1.00

–10 0.50

f Point a
1.0

Point c
0.75 Reward intensity, Po
Point b 1.0

0 1.00

Ratio

Ratio

0.5

0.5

0.5

0.5

0 Left
Point c 1.0

Right

0 Left
1.0

Right

0 Left
Point c 1.0

Right

0 Left
1.0

Right

Ratio

0.5

0.5

0.5

0.5

0 Left

Right

0 Left

Right

Fig. 3 | Curiosity-dependent irrational behaviors. a, The two-choice task with
different, constant reward probabilities: 0% for the left and 50% for the right.
b, The heatmap of the selection probability of the right option as a function of the
parameters of curiosity and reward intensity. The probability was obtained
empirically by running 1,000 simulations for each set of curiosity and reward.
Three representative conditions are indicated by black dots: reward-seeking (point a; c = 0, Po = 1), information-seeking (point b; c = 10, Po = 0.5) and information-avoiding (point c; c = −10, Po = 0.75). c, Box plots showing the selection ratios of the right option for the conditions at points a–c, where the

0 Left

Right

0 Left

Right

central line indicates the median, the edges are the lower and upper quantiles and the upper and lower whiskers represent the highest and lowest value after excluding outliers, respectively. At point c, the model agent dominantly selected either the right or left option, where the left- and right-dominantly selected simulations occurred 505 and 495 times, respectively. The box plots for point c appear crushed because the data points are too densely packed. d–f, The same as a–c for the two-choice task with different, constant reward probabilities: 50% for the left and 100% for the right. At point c in f, the left- and right-dominantly selected simulations occurred 489 and 511 times, respectively.

iFEP-decoded internal state behind rat behaviors We applied iFEP to actual rat behavioral data from the two-choice task experiment with temporally varying reward probabilities36 (Fig. 5a). In this experiment, once the reward probabilities were suddenly changed in a discrete manner, the rat slowly adapted to select the option with the higher reward probability (Fig. 5b), suggesting that the rat sequentially updated its recognition of the reward probability. Based on these behavioral data of the rat, iFEP estimated the internal state, that is, the intensity of curiosity, the recognized reward probabilities and their confidence levels (Fig. 5c–e). We found that the rat was not perfectly aware of the true reward probabilities but was able to recognize increases and decreases in reward probability (Fig. 5d,e). We also found that confidence increased with choice and decreased with no choice (Fig. 5f,g).
With iFEP, we can examine whether the rat subjectively assumed fluctuating or constant environments, that is, reward probabilities.

We estimated the degree of fluctuation of the reward probabilities the rat assumes pw = 1.785 (that is, σ2w = 0.560) from the rat behavioral data. This estimated value implied that the latent variable controlling the reward probabilities showed a random walk with increasing s.d. with trials as √σ2wt . Compared with a reward probability represented by the sigmoidal of the latent variable, the estimated reward probability can largely change from 0.5 to 0.5 ± 0.4 during only ten trials (Supplementary Fig. 4). Therefore, it was suggested that the rat assumed fluctuating environments and, thus, easily forgot its recognition and lost its confidence.
Negative curiosity and its dynamics decoded by iFEP Interestingly, the curiosity held by the rat was estimated to be negative for almost all trials (Fig. 5h). In other words, the rat conservatively preferred certain choices but did not explore uncertain choices.

Nature Computational Science | Volume 3 | May 2023 | 418–432

423

Article

Observing

a
Observation Action
c

https://doi.org/10.1038/s43588-023-00439-w

b

Latent variable

zt–2

zt–1

zt

zt+1

Observation

ot–2

ot–1

ot

ot+1

Action

at–2

at–1

at

at+1

Update
Old belief

New belief

I R

RI

I am wondering: - How much is the reward probability? - Which option should be selected?

I am wondering: - How does the agent recognize the reward probability? - How much confidence does the agent have in its recognition?

Intensity of curiosity

ct–2

ct–1

ct

ct+1

Latent

µt–2

µt–1

µt

µt+1

variable

σ

2 t–2

σ

2 t–1

σ

2 t

σ

2 t+1

Observation

ot–2

ot–1

ot

ot+1

Action

at–2

at–1

at

at+1

Fig. 4 | The scheme of iFEP by an observer of a decision-making agent. a, An agent performing a two-choice task from the observer’s perspective. b, The observer’s assumption about the agent’s decision-making. The agent is assumed to follow the decision-making model, as described in Fig. 1. c, The SSM

of the observer’s eye. For the observer, the agent’s reward–curiosity conflict, recognized reward probabilities and their uncertainties are temporally varying latent variables, whereas the agent’s action and the presence/absence of a reward are observable. The observer estimates the latent internal states of the agent.

This negative curiosity is reasonable for the starved animals because
animals desired to obtain more reward with higher confidence. To
validate the negative curiosity, we visualize the selected action in the space of ΔE [Rewardt+1] and ΔE [Infot+1] , as shown in Fig. 2o–q (Fig. 6a–c). When the estimated ct was negative, the rat dominantly selected the left action in a region with positive ΔE [Rewardt+1] and negative ΔE [Infot+1] (Fig. 6a for ct < −1.1; Fig. 6b for −1.1 ≤ ct < −0.7), clearly indicating that the rat has positive subjective reward and nega-
tive curiosity. When the estimated ct was close to 0, the rat selected both actions based on ΔE [Rewardt+1] , independent of ΔE [Infot+1] (Fig. 6c for −0.7 ≤ ct ). These results clearly supported the expected net utility with positive weight for the expected reward and time-
dependent weight for the expected information gain. In addition, we
statistically tested negative curiosity (P < 0.01 for Monte Carlo testing
in Supplementary Fig. 5).
Further, we noticed an increase in the estimated level of curiosity at
which the reward probabilities changed suddenly (Fig. 5h). This curios-
ity dynamics can be interpreted such that the rat recognized the rule
change and adaptively controlled the extent to which the rat sought
new information. We further examined how the curiosity is regulated
by recognized environmental information. We did not detect the cor-
relation between the estimated curiosity and expected information
gains (Fig. 6d,e). Moreover, we found that the temporal derivative of

the estimated curiosity highly correlated with the sum of the expected information gains for both options (Fig. 6f,g). These results implied that the rat actively upregulated the curiosity level when the uncertainty in the recognized reward probabilities increases, such as,

dc dt

∝

∑ E [Infoi].
i

(6)

Evaluations of alternative models from rat behaviors Finally, to further confirm the validity of the ReCU model, we compared it with other decision-making models based on the rat behavioral data. As an alternative version of the expected net utility, we introduced the time-dependent desire for reward as

Ut (at+1) = dt ⋅ E [Rewardt+1] + E [Infot+1] ,

(7)

where dt denotes a meta-parameter describing subjective reward (see Methods for details). With this alternative model, the time series of dt were estimated by iFEP from the rat behavioral data. We found that the estimation of the subjective reward meta-parameter dt changed dynamically and sometimes became close to zero when the rat encountered drastic changes in the reward probabilities (Supplementary Fig. 6), which indicated that the rat suddenly no longer needed rewards. This

Nature Computational Science | Volume 3 | May 2023 | 418–432

424

Article a

https://doi.org/10.1038/s43588-023-00439-w

Reward probability

Reward probability

Trials
b (L, R) = (0.9, 0.5) (0.5, 0.9)

(0.5, 0.1)

Left

P(L)

1.0 0.5
0

(0.1, 0.5)

(0.5, 0.9) (0.9, 0.5)

(0.1, 0.5)

(0.5, 0.1)

Trials
Left Right Rewarded

Right

0

c

1.00

0.75

Selection probability

0.50

0.25

0 0

f

24

100

200

300

Trials

Non-rewarded 400

d

1.00

e

1.00

Right Left

Estimated reward probability (left)

Estimation
0.75

0.50 0.25

Ground truth

Estimated reward probability (right)

0.75 0.50 0.25

Estimation
Ground truth

100

200

300

400

Trials

0

0

100

200

300

400

Trials

0

0

100

200

300

400

Trials

g 24

h

1

18

18

0

Intensity of curiosity

Confidence (right)

Confidence (left)

12

12

–1

6

6

–2

0

0

100

200

300

400

Trials

0

0

100

Fig. 5 | Estimation of the rat’s internal state by iFEP. a, A rat in the two-choice task with temporally switching reward probabilities. b, Rat’s behaviors from a public dataset at https://groups.oist.jp/ja/ncu/data ref. 5. Vertical lines indicate the selections of left (L) and right (R) options, respectively. The time series indicates the moving average of the selection probability of the left option with 25 window width backward. c, The moving average of the selection probabilities for the left and right options. d,e, The rat behavior data-driven estimations of

200

300

400

Trials

–3 0

100

200

300

400

Trials

agent-recognized reward probabilities for the left (d) and right (e) options.
f–h, The rat behavior-driven estimations of agent’s confidence about recognized
reward probabilities for the left (f) and right (g) options, and the agent’s curiosity (h). The estimated parameter values were α = 0.058, β = 6.991 and σ2w = 0.560. The number of particles was 100,000 in the particle filter. Continuous shaded
ranges represent the s.d. for all the particles in d–h.

should be unnatural for animals that were starved before the experimental task to motivate them to obtain food. Thus, the alternative model is not suitable for describing the rat behavior.
Another possible model is Q-learning in the framework of RL, which has been widely used to model the decision-making tasks. Following previous studies36–38, we introduced the time-dependent inverse temperature Bt, which controls the randomness of the action selection; however, it does not lead to information-seeking behavior in the ReCU model. With the Q-learning model, we estimated time series of Bt (Methods). Then, we determined that the Bt decreased when the reward probabilities suddenly changed (Supplementary Fig. 7), meaning that the rat tends to perform a random selection of actions in response to the environmental rule change. Although this dynamic behavior

of the inverse temperature seems reasonable, it is unknown how it is regulated in the Q-learning model. Here, we hypothesized that Bt could be regulated by the uncertainty of our recognition. To probe this, we compared Bt and the expected information gain, where the former was estimated based on Q-learning and the latter was estimated by iFEP in the ReCU model. We found that they are positively correlated (Supplementary Fig. 7) as

Bt ∝ ∑ E [Infot,i].
i

(8)

Therefore, Q-learning requires a cue regarding the uncertainty of recognition, that is, the expected information gain, which supports the ReCU model for explaining the curiosity-driven behavior.

Nature Computational Science | Volume 3 | May 2023 | 418–432

425

Article

https://doi.org/10.1038/s43588-023-00439-w

Probability of selecting the right

a
0.4 Right
0.2

Logistic regression
iFEP

b
0.4 Right
0.2

c
0.4 iFEP
0.2

0.502 0.501

∆ info gain

∆ info gain

∆ info gain

0 –0.2

0 –0.2

0 –0.2

0.500

–0.4 Left

–0.6 –0.4 –0.2 0

0.2 0.4 0.6

∆ reward

–0.4

–0.6 –0.4

Logistic regression
–0.2 0

Left 0.2 0.4 0.6

∆ reward

–0.4

iFEP

Right
–0.6 –0.4 –0.2

Logistic regression
0 0.2

∆ reward

0.499
Left 0.498
0.4 0.6

d 1.4
1.2

Info gain

c (intensity of curiosity)

1.0

e 0.9

0.2

0.6

Correlation coe icient

Info gain

Intensity of curiosity, c

1.0

–0.6

0.3

0.8

–1.4

0

0.6

–2.2

–0.3

0.4 0
f 1.4
1.2

100 Info gain

200

300

Trials

–3.0 400

0.20

dc/dt (temporal derivative of curiosity)

0.13

–0.6 –60 –40 –20 0 20 40 60 Time lag
g 0.9
0.6

Correlation coe icient

Info gain

Time derivative of curiosity, dc/dt

1.0

0.06

0.3

0.8

–0.01

0

0.6

–0.08

–0.3

0.4 0

50

100

150

200

250

300

Trials

Fig. 6 | Negative curiosity and its dynamics. a–c, The selected options in a space
of left–right differences of the expected reward and information gain. All actions
were separated into three: when the estimated curiosity is negatively large (c ≤ −1.1, n = 110) (a), negatively medium (−1.1 < c ≤ −0.7, n = 93) (b) and close to 0 (−0.7 < c, n = 187) (c). The left and right options were discriminated by a logistic regression with P (at = 1) = f (wRΔE [Rewardt+1] + wIΔE [Infot+1]), where wR and wI indicate the weight parameters. Two linear lines indicate discrimination
boundaries using the estimated wR and wI from this scattered data and using

–0.15

350

400

–0.6 –60 –40 –20 0 20 40 60 Time lag

wR = 1 and wI = ∑ct/N , which is an average of the estimated curiosity, respectively. the heatmap represents the probability of selecting the left option based on the estimated wR and wI. d,e, Time series of the intensity of curiosity and the sum of the expected information gains for both options (d), and their cross-correlation (e). f,g, Time series of the temporal derivative of curiosity and the sum of the expected information gains for both options (f), and their cross-correlation (g). The temporal derivative was computed by linear regression within a time window of seven trials.

Discussion
The advancement realized in this study is the modeling and decoding of mental conflict between reward and curiosity, which is yet to be quantified. The proposed approach can potentially improve our understanding of how mental conflict is regulated and highlight its neural mechanisms in the future by combining neural recording data.
Our decoding approach has some limitations. The iFEP requires long trial behavioral data in the two-choice task because the particle filter needs certain trials for converging the estimation. Thus, the iFEP is not applicable to short behavioral data. In addition, the iFEP assumed gradual change in curiosity in time and cannot follow the pathologically rapid dynamics of curiosity in the estimation process of the particle filter.
Comparing the ReCU model and other models including Q-learning is difficult. In all these models, the action selection is commonly formulated with the sigmoidal function. Thus, any models can be made to fit the observed behaviors, i.e., increase likelihood, by adjusting the time-dependent meta-parameters. Therefore, model selection

based solely on likelihood is not very helpful and determining whether animals use the ReCU model or other models to make decisions seems inherently challenging. However, a potential advantage of the ReCU model is its ability to capture the dynamics of curiosity and the interplay between curiosity and reward-based learning.
There is a related theoretical model, which differs from RL and FEP. Ortega and Braun formulated FEP, which describes irrational decision making39,40. Interestingly, their formulation was based on microscopic thermodynamics and the temperature parameters controlled this irrationality. However, the thermodynamics-based FEP did not treat the sequential update of the recognition from the observations.
Finally, it is worth discussing future perspectives of our iFEP approach in medicine. In general, mental diagnosis relies on medical interviews and has not been evaluated quantitatively. Our iFEP method can quantitatively estimate the psychological state of patients based on their behavioral data. For example, patients with social withdrawal, also known as ‘Hikikomori,’ have no interest in anything. In this case, social withdrawal would be characterized by a negative value of curiosity in

Nature Computational Science | Volume 3 | May 2023 | 418–432

426

Article

https://doi.org/10.1038/s43588-023-00439-w

our FEP model. Therefore, the iFEP method could be considered effective for diagnosing mental disorders.

Methods
Amount of reward In the two-choice task, the reward is given as all-or-none; however, its intensity depends on the agent’s own feeling as

R

=

0

or

ln

Po 1−Po

,

(9)

where Po represents the desired probability of how much the agent wants the reward and controls the reward intensity felt by the agent (Supplementary Fig. 8). In Friston’s formulation, the presence and absence of rewards are given by log preference with natural unit as ln Po and In(1 − Po), which take negative values22,23. In this equation, the reward is the difference between log preferences to ensure that the reward intensity is positive.

where φt = {μt, respectively (μt

Λ=t}(,μa1,tn, μd2,μt)tTa; nΛdt

Λt denote the = diag (p1,t, p2,t))

mean and precision, . Q (wt|φt) denotes the

recogn­ ition distribution. The model agent aims to update the recog­

nition distribution through φt at each time step by minimizing
the surprise, which is defined by − ln P (ot|o1∶t−1). The surprise can be decomposed as follows:

− ln P (ot|o1∶t−1)

=

∫Q (wt|φt) ln

dw Q(wt |φt )

P(ot ,wt |o1∶t−1 ,a1∶t )

t

−KL [Q (wt|φt) ||P (wt|o1∶t, a1∶t)] ,

(16)

where KL [q (x) ||p (x)] denotes the Kullback–Leibler (KL) divergence between the probability distributions q(x) and p(x). Because of the
nonnegativity of KL divergence, the first term is the upper bound of
the surprise:

State space model for reward probability recognition The agent assumed that the reward is generated probabilistically from the latent cause w, and probabilities λi are represented by

λi = f (wi) ,

(10)

where i indicates the indices of the options and f (x) = 1/ (1 + e−x) . In addition, the agent assumed an ambiguous environment
in which the reward probabilities are fluctuated by a random
walk as

wi,t = wi,t−1 + σwξi,t,

(11)

where t, ξi,t , and σw denote the trial of the two-choice task, standard Gaussian noise and the noise intensity, respectively. Thus, the agent
assumes an environment expressed by an SSM as

P (wt|wt−1) = 𝒩𝒩 (wt|wt−1, σ2wI) ,

(12)

F (ot, φt) = ∫Q (wt|φt) ln Q (wt|φt) dwt + ∫Q (wt |φt) J (ot, wt) dwt, (17)

which is called the free energy, where J (ot, wt) = − ln P (ot, wt|o1∶t−1, a1∶t). The first term of the free energy corresponds to the negative entropy
of a Gaussian distribution:

F1 = ∫Q (wt|φt) ln Q (wt |φt) dwt.

(18)

The second term is approximated as F2 = ∫Q (wt |φt) J (ot, wt) dwt

≅

∫Q (wt|φt) {J (ot, μt) +

dJ dw

(wt

− μt) +

1 2

d2 J dw2

(wt

− μt)2} dwt

(19)

P (ot|wt, at) = ∏ [f (wi,t)ot {1 − f (wi,t)}1−ot ]ai,t ,
i

(13)

where wt and ot denote the probabilities of both options

latent at step

variables controlling the reward t (wt = (w1,t, w2,t)T) and the observa-

tion of the presence of the reward (ot ∈ {0, 1}), respectively. Meanwhile,

at denotes the agent’s action at step t, which is represented by a one-hot vector (at ∈ {(1, 0)T , (0, 1)T}) , 𝒩𝒩 (x|μ, Σ) denotes the Gaussian distribu-

tion mean μ and variance Σ, σ2w denotes the variance of the transition probability of w, I denotes an identity matrix, and f(wi,t) = 1/(1 + e−wi,t ),

which represents the probability of the reward of option i at step t. The

initial distribution of w1 is given by P (w1) = 𝒩𝒩 (w1|0, κI), where κ denotes variance.

=

J (ot, wt)|wt=μt

+

1 2

d2 J dw2

||wt =μt

Λ−t 1.

Note that E (ot, wt) is expanded by a second-order Taylor series
around μt. At each time step, the agent updates φt by minimizing F (ot, φt).

Calculation of free energy The free energy is derived as follows: F1 simply becomes

F1

=

1 2

ln 2πp−1,t1

+

1 2

ln 2πp−2,1t

+ const.

(20)

For computing F2,

FEP for reward probability recognition We modeled the agent’s recognition process of reward probability using sequential Bayesian updating as
P (wt|o1∶t, a1∶t) ∝ P (ot|wt, at) ∫P (wt|wt−1) P (wt−1|o1∶t−1, a1∶t−1) dwt−1. (14)

P (ot, wt|o1∶t−1, a1∶t) = P (ot|wt, at) ∫P (wt|wt−1) P (wt−1|o1∶t−1, a1∶t−1) dwt−1
≅ P (ot|wt, at) ∫P (wt|wt−1) N (wt−1|μt−1, Λ−t−11) dwt−1. (21)

Because of the non-Gaussian P(ot|wt, at), the posterior distribution of wt, P (wt|o1∶t, a1∶t) , becomes non-Gaussian and cannot be calculated analytically. To avoid this problem, we introduced a simple posterior
distribution approximated by a Gaussian distribution:

Q (wt|φt) = 𝒩𝒩 (wt|μt, Λ−t 1) ≒P (wt|o1∶t, a1∶t) ,

(15)

In the second line of this equation, we use the approximated recognition distribution as the previous posterior P (wt−1|o1∶t−1, a1∶t−1)  . This equation can be written as
P (ot, wt|o1∶t−1, a1∶t) ≅ ∏ [f (wi,t)ot {1 − f (wi,t)}1−ot ]ai,t N (wi,t|μi, t−1, p−i, t1 + σ2w) .
i
(22)

Nature Computational Science | Volume 3 | May 2023 | 418–432

427

Article

https://doi.org/10.1038/s43588-023-00439-w

Then where

E (ot, wt)|wt=μt = J1 (ot, μ1,t) + J2 (ot, μ2,t) + const., Ji (ot, μi,t) = ai,t [ot ln f (μi,t) + (1 − ot) ln {1 − f (μi,t)}]

−

1 2

(μi,t −μi,t−1 )2 p−i,t1 +σ2w

−

1 2

ln (p−i,t1 + σ2w) .

which is a rewriting of equation (17). Here, we attempt to express the
future free energy at time t + 1, conditioned on action at +1 as (23)
F (ot+1, at+1) = EQ(wt+1|φt) [ln Q (wt+1|φt) − ln P (ot+1, wt+1|o1∶t, a1∶t+1)] . (32)

However, there is an apparent problem in this equation: ot+1 has not yet been observed because it is a future observation. To resolve this issue, (24) we take an expectation with respect to ot+1 as

F (at+1) = EQ(ot+1|wt+1,at+1)Q(wt+1|φt) [ln Q (wt+1|φt) − ln P (ot+1, wt+1|o1∶t, a1∶t+1)] .

(33)

Thus, F2 is calculated by substituting this equation into equation (19). Taken together,

F (ot, φt)

=

∑
i

{Ji

(ot ,

μi,t )

+

1 2

||| d2 Ji
dw2i,t

wi,t =μi,t

p−i,t1

+

1 2

ln 2πp−i,t1} .

(25)

Sequential updating of the agent’s recognition
The updating rule for φt was derived by minimizing the free energy. The optimized pi,t can be computed by ∂F/∂p−i,t1 = 0 , which leads to

pi,t =

||| . d2Ji
dw2i,t

wi,t =μi,t

(26)

By substituting pi,t into equation (25), the second term in the summation becomes constant, irrespective of μi,t. Thus, μi,t is updated by minimizing only the first term as

μi,t = μi,t−1 − αδi,a

||| , ∂Ji
∂μi,t μi,t =μi,t−1

(27)

where α is the learning rate. These two equations finally lead to

μi,t = μi,t−1 + αKi,t (ot − f (μi,t−1)) ,

(28)

pi,t = K−i,t1 + f (μi,t) (1 − f (μi,t)) ,

(29)

where Ki,t = (pi,t−1 + σ−w2) / (pi,t−1σ−w2), which is called the Kalman gain. If option i was not selected, the second terms in both equations will van-
ish, which results in belief μi,t staying the same, while its precision decreases (that is, pi,t+1 < pi,t). If it is selected, the belief is updated by the prediction error (that is, ot − f (μi,t)), and its precision is improved. The confidence of the recognized reward probability should be evalu-
batyeγdi,tn=otpiin,t/wf′i,(tμsip,t)a2c. e but in λi,t space; hence, the confidence is defined

Expected net utility The expected net utility is described by
Ut (at+1) = ct ⋅ EP(ot+1|at+1) [KL [Q (wt+1|ot+1, at+1) ||Q (wt+1|at+1)]] +EP(ot+1|at+1) [R (ot+1)] ,

(30)

where R (ot+1) = ot+1 ln (Po/ (1 − Po)) ; the first and second terms represent the expected information gain and expected reward, respectively; and
ct denotes the intensity of curiosity at time t. The heuristic idea of introducing the curiosity meta-parameter ct was also proposed in the RL field30.
We briefly show how to derive it based on ref. 41. The free energy
at current time t is described by

F (ot, φt) = EQ(wt|φt) [ln Q (wt|φt) − ln P (ot, wt|o1∶t−1, a1∶t)] ,

(31)

This can be rewritten as

F (at+1) = EQ(ot+1,wt+1|o1∶t,at+1) [ln Q (wt+1|φt) − ln P (wt+1|o1∶t+1, a1∶t+1) − ln P (ot+1|o1∶t, a1∶t+1) ]

(34)

Although P (ot+1|o1∶t, a1∶t+1) can be computed by the generative model as

P (ot+1|o1∶t, a1∶t+1) = ∫P (ot+1|wt+1, a1∶t+1) P (wt+1|o1∶t) dwt+1 ≅ ∫P (ot+1|wt+1, a1∶t+1) Q (wt+1|φt) dwt+1,

(35)

it is assumed that P (ot+1|o1∶t, a1∶t+1)  was heuristically replaced with P (ot+1) as the prior agent’s preference of observing ot+1 , so that ln P (ot+1)can be addressed as an instrumental reward. The equation (34)
was further transformed into

F (at+1) = EQ(ot+1|at+1)Q(wt+1|o1∶t+1,at+1) [ln Q (wt+1|φt) − ln Q (wt+1|o1∶t+1, a1∶t+1) − ln P (ot+1) ] .

(36)

Finally, we obtained the so-called expected free energy as

F (at+1) = EQ(ot+1|at+1) [−KL [Q (wt+1|ot+1, at+1) ||Q (wt+1)] − ln P (ot+1)] . (37)

where the KL divergence is called a Bayesian surprise, representing the
extent to which the agent’s beliefs are updated by observation, whereas the second term ln P (ot+1) can be addressed as the agent’s prior preference of observing ot+1, which can be interpreted as an instrumental reward. Therefore, the expected free energy is derived in an ad hoc
manner.
In the first term of equation (30), the posterior and prior distribu-
tions of wt+1 in the KL divergence are derived as

Q (wt+1) = ∫P (wt+1|wt) Q (wt) dwt,

(38)

Q (wt+1|ot+1,at+1) =

, P(ot+1 |wt+1 ,at+1 )Q(wt+1 |at+1 )
P(ot+1 |at+1 )

(39)

respectively. ot+1 is a future observation, and therefore, the first term was expected by P (ot+1|at+1), which can be calculated as

P (ot+1|at+1) = ∫P (ot+1|wt+1, at+1) Q (wt+1|at+1) dwt+1.

(40)

In the second term, the reward is quantitatively interpreted as the desired probability of ot+1. For the two-choice task, we use

P

(ot+1 )

=

Pot+1
o

(1

−

Po )(1−ot+1 )

,

(41)

where Po indicates the desired probability of the presence of a reward. According to the probabilistic interpretation of reward22,23,

Nature Computational Science | Volume 3 | May 2023 | 418–432

428

Article

https://doi.org/10.1038/s43588-023-00439-w

the presence and absence of a reward can be evaluated by ln Po and ln(1 − Po), respectively.
In this study, we changed the sign of the expected free energy as the expected net utility, because we modeled decision-making as the maximization of the expected free energy. We ventured to introduce the curiosity meta-parameter ct to express irrational decision-making. Because rewards are relative, in this study, we set ln {Po/ (1 − Po)} and 0 for the presence and absence of a reward, respectively. Thus, our expected net utility can be described by equation (30), in which there is a gap between the original expected free energy and our expected net utility.

Model for action selection The agent probabilistically selects a choice with higher expected net utility as

P (at+1) =

exp(βU(at+1 )) ∑a exp(βU(a))

,

(42)

where U (at+1) indicates the expected net utility of action at+1. Equation (42) is equivalent to equation (2). To derive equation (42), we
considered the expectation of the expected net utility with respect to
probabilistic action as

E [U] = EQ(at+1) [U (at+1) − β−1 ln Q (at+1)] ,

(43)

where β indicates an inverse temperature, and the entropic constraint of action probability is introduced in the second term. This equation can be rewritten as

E [U] = −β−1KL [Q (at+1) ‖ exp (βU (at+1)) /Z] + β−1 ln Z,

(44)

where Z indicates a normalization constant. Thus, its maximization
respective to Q (at+1) leads to the optimal action probability as shown in equation (42).

H (ot+1|wt+1) = −EQ(wt+1|at+1) [∑i ai,t+1g (wi,t+1)] ,

(49)

where

g (w) = f (w) ln f (w) + (1 − f (w)) ln (1 − f (w)) .

(50)

Here, we approximately calculate this equation by using the second-order Taylor expansion as

H

(ot+1 |wt+1 )

≅

−EQ(wt+1 |at+1 )

⎡⎢⎢⎢∑i ⎣

ai,t+1

⎧⎪ ⎨⎪⎩

g

(μi,t+1 )

+

∂g ∂wi,t+1

(wi,t+1

−

μi,t+1 )

+

1 2

∂2 g ∂w2i,t+1

(wi,t+1

− μi,t+1)2

⎫⎪⎬⎪⎭⎤⎥⎥⎥⎦

,

(51)

which leads to

H (ot+1|wt+1) =

−

∑
i

ai,t+1

⎡⎢⎢⎢⎢ ⎣

+

1 2

f (μi,t+1) lnf (μi,t+1) + (1 − f (μi,t+1)) ln (1 − f (μi,t+1))

{f

(μi,t+1

)

(1

−

f

(μi,t+1

))

(1

+

(1

−

2f

(μi,t+1

))

ln

f(μi,t+1 ) 1−f(μi,t+1

)

)}

(p−i,t1 + p−w1)

⎤⎥⎥⎥⎥. ⎦

(52)

The marginal entropy H (ot+1) can be calculated as

H (ot+1) = − ∑ ai,t+1 {P (ot+1 = 0|at+1) ln P (ot+1 = 0|at+1) ,
i
+P (ot+1 = 1|at+1) ln P (ot+1 = 1|at+1)}

(53)

where

P (ot+1|at+1) = ∫P (ot+1|wt+1, at+1) Q (wt+1|at+1) dwt+1

Alternative expected net utility For comparison, we consider an alternative expected net utility by intro­ ducing a time-dependent meta-parameter in the second term as follows:

= ∫∏ {f (wi,t+1)ot+1 (1 − f (wi,t+1))1−ot+1 }ai,t+1 Q (wt+1|at+1) dwt+1
i

U (at+1) = EP(ot+1|at+1) [KL [Q (wt+1|ot+1, at+1) ||Q (wt+1|at+1)]] + dt ⋅ EP(ot+1|at+1) [R (ot+1)] ,

(45)

where dt denotes the subjective intensity of reward at time t. In this case, the agent with high dt will show more exploitative behavior, whereas the agent with dt = 0 shows more explorative behavior driven by the expected information gain.

Calculation of expected net utility Here, we present the calculation of the expected net utility. The KL divergence in the first term of equation (30) can be transformed into

EP(ot+1 |at+1 )

[KL

[Q

(wt+1 |ot+1 ,

at+1 )

||Q

(wt+1 |at+1 )]]

=

H

(ot+1 )

−

H

(ot+1|wt+1) , (46)

= ∏[ i +1ot+1

(−1)1−ot+1

f (μi,t+1)ot+1 (1 − f (μi,t+1))1−ot+1

1 2

f

(μi,t+1 )

{1

−

f

(μi,t+1 )}

{1

−

2f

(μi,t+1 )}

(p−i,t1

ai ,t+1
]. + p−w1)

(54)

The second term of the expected net utility (equation (36)) is calculated as

EP(ot+1|at+1) [lnP (ot+1)] = EP(ot+1|at+1) [ot+1lnPo + (1 − ot+1) ln (1 − Po)]

(55)

= P (ot+1 = 0|at+1) ln (1 − Po) + P (ot+1 = 1|at+1) ln (1 − Po) .

where the first and second terms represent the conditional and marginal entropies, respectively:

H (ot+1|wt+1) = EP(ot+1|wt+1,at+1)Q(wt+1|at+1) [− ln P (ot+1|wt+1, at+1)] ,

(47)

H (ot+1) = EP(ot+1|at+1) [− ln P (ot+1|at+1)] .

(48)

The conditional entropy H (ot+1|wt+1)can be calculated by substituting equation (13) into equation (47) as

Observer-SSM We constructed the observer-SSM, which describes the temporal transitions of the latent internal state z of agent and the generation of action, from the viewpoint of the observer of the agent. This is depicted graphically in Fig. 4. As prior information, we assumed that the agent acts based on the internal state, that is, the intensity of curiosity, the recognized reward probabilities and their confidence levels. The intensity of curiosity was assumed to change temporally as a random walk:

ct = ct−1 + ϵζt,

(56)

Nature Computational Science | Volume 3 | May 2023 | 418–432

429

Article

https://doi.org/10.1038/s43588-023-00439-w

where ζt denotes white noise with zero mean and unit variance, and ϵ denotes its noise intensity. Other internal states, that is, μi and pi, were assumed to update as equations (28) and (29). The transition of the
internal state is expressed by the probability distribution

P (zt|zt−1) = 𝒩𝒩 (zt|F (zt−1, at−1) , Γ ) ,

(57)

F

(zt−1 ,

at−1 )

=

⎡⎢⎢⎢⎢⎢⎢⎢

0 h (μ1,t−1, p1,t−1, ot, a1) h (μ2,t−1, p2,t−1, ot, a2)
k (μ1,t−1, p1,t−1, a1)

⎤⎥⎥⎥⎥⎥⎥⎥

,

⎣ k (μ2,t−1, p2,t−1, a2) ⎦

(58)

iFEP by particle filter and Kalman backward algorithm
Based on the observer-SSM, we estimated the posterior distribution
of the latent internal state of agent zt given all observations from 1 to T ( x1∶T) in a Bayesian manner, that is, P (zt|x1∶T). This estimation was done by forward and backward algorithms, which are called filtering and
smoothing, respectively.
In filtering, the posterior distribution of zt given observations until t ( x1∶t) is sequentially updated in a forward direction as

P (zt|x1∶t) ∝ P (xt|zt, θ) ∫P (zt|zt−1, θ) P (zt−1|x1∶t−1) dzt−1,

(66)

where xt = (aTt , ot)T and θ = {σ2, α, Po}. The prior distribution of z1 is

where zt = (ct, μTt , pTt )T and Γ = ϵ2diag(1, 0, 0, 0, 0)  . h (μi,t−1, pi,t−1, ot, ai) and k (μi,t−1, pi,t−1, ai) represent the right-hand sides of equations (28) and (29), respectively; and Γ and diag (x) denote the variance–covari-
ance matrix and square matrix whose diagonal component is x, respec-
tively. In addition, the agent was assumed to select an action at+1 based on the expected net utilities, as follows:

P (at+1) =

exp(βU(at+1 )) ∑a exp(βU(a))

,

(59)

and the reward was obtained by the following probability distribution:

P

(ot |at )

=

∏
i

{λoi,tt (1

−

λi,t)1−ot }ai,t

.

(60)

Q-leaning in two-choice task and its observer-SSM The decision-making in the two-choice task was also modeled by Q-learning. Reward prediction for the ith option Qi,t is updated as

Qi,t = Qi,t−1 + αt−1 (rtai,t−1 − Qi,t−1) ,

(61)

where αt indicates a learning rate at trail t. The agents selected action following a softmax function:

P (ai,t

= 1) =

, exp(Bt Qi,t )
∑i exp(Bt Qi,t )

(62)

where Bt indicates the inverse temperature at trail t controlling the randomness of the action selection.
The time-dependent parameters αt and Bt in Q-learning were estimated from behavioral data37. These parameters were assumed
to change temporally as a random walk:

θt = θt−1 + ϵθζθ,t,

(63)

where θ ∈ {α, B} , ζθ,t denotes white noise with zero mean and unit variance and ϵθ denotes its noise intensity. Thus, the transition of the internal state is expressed by the probability distribution

P (zt|zt−1) = 𝒩𝒩 (zt|F (zt−1, at−1) , Γ) ,

(64)

F

(zt−1 ,

at−1 )

=

⎡⎢⎢⎢⎢⎢

h1

(αt ,

0 0 Q1,t−1 ,

at−1 )

⎤⎥⎥⎥⎥⎥

,

⎣ h2 (αt, Q2,t−1, at−1) ⎦

(65)

where zt = (αt, Bt, Q1,t, Q2,t)T , Γ = ϵ2diag (1, 1, 0, 0) , hi (αt, Qi,t−1, at) represents the right-hand side of equation (61); and Γ and diag (x)denote the variance–covariance matrix and square matrix whose diagonal com-
ponent is x, respectively.

P (z1) = [∏ 𝒩𝒩𝒩μi,1|μ0, σ2μ)Gam(pi,1|ag, bg)] Uni(c1|au, bu),
i

(67)

where μ0 and σ2μ denote means and variances, Gam(x|ag, bg) indicates the Gamma distribution with shape parameter ag and scale parameter bg, and Uni (x|au, bu) indicates uniform distribution from au to bu. We used a particle filter42 to sequentially calculate the posterior
P (zt|x1∶t), which cannot be analytically derived because of the nonlinear transition probability.
After the particle filter, the posterior distribution of zt given all observations (x1∶T) is sequentially updated in a backward direction as

P (zt|x1∶T) = ∫P (zt+1|x1∶T) P (zt|zt+1, x1∶t, θ) dzt+1

(68)

=

∫P (zt+1|x1∶T)

P(zt+1 |zt ,θ)P(zt |x1∶t ,θ)
∫P (zt+1 |zt ,θ)P(zt |x1∶t ,θ)dzt

dzt+1 .

However, this backward integration is intractable because of
the non-Gaussian P (zT|x1∶T), which was represented by the particle ensemble in the particle filter, and the nonlinear relationship between
zt and zt+1 in P (zt+1|zt, θ) (equation (57)). Thus, we approximated P (zt|x1∶t) as 𝒩𝒩(zt|mt, Vt), where mt and Vt denote a sample mean and a sample variance of the particles at t, whereas we linearized
P (zt|zt−1, θ) as

P (zt|zt−1, θ) ≅ 𝒩𝒩 (zt|Azt−1 + b, Γ) ,

(69)

A=

||| , ∂F(zt−1,at−1)

∂zt−1

mt

(70)

b = F (mt, at−1) − Amt,

(71)

where A denotes a Jacobian matrix. Because these approximations make the integration of equation (68) tractable, the posterior distribution P (zt|x1∶T) can be computed by a Gaussian distribution as

P (zt|x1∶T) = 𝒩𝒩 (zt|mˆ t, Vˆt)

(72)

whose mean and variance were analytically updated by Kalman backward algorithms as43

mˆ t = mt + Jt {mˆ t+1 − (Amt + b)} ,

(73)

Vˆt = Vt + Jt {mˆ t+1 − (AVtAT + Γ)} JTt ,

(74)

where

Jt = VtAT (AVtAT + Γ)−1 .

(75)

Nature Computational Science | Volume 3 | May 2023 | 418–432

430

Article

https://doi.org/10.1038/s43588-023-00439-w

Impossibility of model discrimination In the ReCU and Q-learning models, the action selections were formulated with the same softmax functions as

We used the rat behavioral data published in ref. 36, which is publicly
available at https://groups.oist.jp/ja/ncu/data. These rat behavioral data are also included in the Source Data for Fig. 5 and on Zenodo45.

P (ai,t

= 1) =

. exp(β(E[Rewardi,t ]+ct E[Infoi,t ]))
∑j exp(β(E[Rewardj,t ]+ct E[Infoj,t ]))

(76)

P (ai,t

= 1) =

, exp(βt Qi,t )
∑i exp(βt Qi,t )

(77)

which correspond to that of the ReCU and Q-learning models, respectively. These equations contain the time-dependent meta-parameters, that is, ct and βt. For both models, the goodness of fit (that is, likelihood) for the actual behavioral data can be freely improved by tuning the time-dependent meta-parameters. Thus, discrimination between the ReCU and Q-learning models must be essentially impossible.

Estimation of parameters in iFEP The ReCU model has several parameters: σ2w, α, β, Po and ϵ. In the estimation, we set ϵ to 1, which was the optimal value for estimation in the artificial data (Supplementary Fig. 3). We assumed the unit intensity
of reward, that is, ln Po/(1 − Po) = 1, because it is impossible to estimate both Po and β caused by multiplying β and ln Po/(1 − Po) in the expected net utility (equations (30) and (42)). This treatment is suitable for rela-
tive comparison between the curiosity meta-parameter and the reward. In addition, we addressed βct as a latent variable as ĉt = βct because of the multiplication of β in the expected net utility (equations (30)
and (42)). Thus, the estimation of ct can be obtained by dividing the estimated ĉt by the estimated β. Therefore, the hyperparameters to be estimated were σ2w, α and β.
To estimate these parameters θ = {σ2w, α, β} , we extended the observer-SSM to a self-organizing SSM44 in which θ was addressed as
constant latent variables:

P (zt, θ|x1∶t) ∝ P (xt|zt) ∫P (zt|zt−1, θ) P (zt−1, θ|x1∶t−1) dzt−1,

(78)

where P (θ) = Uni (σ2|aσ, bσ) Uni(α|aα, bα)𝒩𝒩𝒩β|mβ, vβ). To sequentially calculate the posterior P (zt, θ|x1∶t) using the particle filter, we used 100,000 particles and augmented the state vector of all particles by
adding the parameter θ, which was not updated from randomly sam-
pled initial values.
The hyperparameter values used in this estimation were μ0 = 0, σ2μ = 0.012, ag = 10, bg = 0.001, au = −15, bu = 15, aσ = 0.2, bσ = 0.7, aα = 0.04, bα = 0.06, aβ = 0 and bβ = 50, which were heuristically given as parameters
correctly estimated using the artificial data (Supplementary Fig. 2).

Statistical testing with Monte Carlo simulations Supplementary Fig. 5 shows statistical testing of the negative curiosity estimated in Fig. 5. A null hypothesis is that an agent has no curiosity (that is, ct = 0) decides on a choice only depending on its recognition of the reward probability. Under the null hypothesis, model simulations were repeated 1,000 times under the same experimental conditions as in Fig. 5 and the curiosity was estimated for each using iFEP. We adopted the temporal average of the estimated curiosity as a test statistic and plotted the null distribution of the test statistic. Compared with the estimated curiosity of the rat behavior, we computed the P value for a one-sided left-tailed test.

Reporting summary Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.
Data availability
Source data for Figs. 2, 3, 5 and 6 are available with this paper. Source data for Supplementary Figures are available in Supplementary Data.

Code availability
The computer simulation and data analysis were performed using MATLAB (version R2020b) software. The code used for this work is available on GitHub at https://github.com/YukiKonaka/Konaka_ Honda_2023. The specific version used to produce the results in this manuscript is also available on Zenodo45.
References
1. Helmholtz, H. Handbuch der Physiologischen Optik (Andesite Press, 1867).
2. Yuille, A. & Kersten, D. Vision as Bayesian inference: analysis by synthesis? Trends Cogn. Sci. 10, 301–308 (2006).
3. Millett, J. D. & Simon, H. A. Administrative behavior: a study of decision-making processes in administrative organization. Polit. Sci. Q. 62, 621 (1947).
4. Dubey, R. & Griffiths, T. L. Understanding exploration in humans and machines by formalizing the function of curiosity. Curr. Opin. Behav. Sci. 35, 118–124 (2020).
5. Kidd, C. & Hayden, B. Y. The psychology and neuroscience of curiosity. Neuron 88, 449–460 (2015).
6. Klein, U. & Nowak, A. J. Characteristics of patients with autistic disorder (AD) presenting for dental treatment: a survey and chart review. Spec. Care Dentist. 19, 200–207 (1999).
7. Lockner, D. W., Crowe, T. K. & Skipper, B. J. Dietary intake and parents’ perception of mealtime behaviors in preschoolage children with autism spectrum disorder and in typically developing children. J. Am. Diet. Assoc. 108, 1360–1363 (2008).
8. Schreck, K. A. & Williams, K. Food preferences and factors influencing food selectivity for children with autism spectrum disorders. Res. Dev. Disabil. 27, 353–363 (2006).
9. Esposito, M. et al. Sensory processing, gastrointestinal symptoms and parental feeding practices in the explanation of food selectivity: clustering children with and without autism. Int. J. Autism Relat. Disabil. 2, 1–12 (2019).
10. Hobson, R. P. Autism and the development of mind. Essays Dev. Psychol. (Routledge, 1993).
11. Burke, R. Personalized recommendation of PoIs to people with autism. Commun. ACM 65, 100 (2022).
12. Ghanizadeh, A. Educating and counseling of parents of children with attention-deficit hyperactivity disorder. Patient Educ. Couns. 68, 23–28 (2007).
13. Sedgwick, J. A., Merwood, A. & Asherson, P. The positive aspects of attention deficit hyperactivity disorder: a qualitative investigation of successful adults with ADHD. ADHD Atten. Deficit Hyperact. Disord. 11, 241–253 (2019).
14. Redshaw, R. & McCormack, L. ‘Being ADHD’: a qualitative study. Adv. Neurodev. Disord. 6, 20–28 (2022).
15. Sutton, R. S. & Barto, A. G. Reinforcement Learning: an Introduction (MIT Press, 1998).
16. Friston, K. A theory of cortical responses. Philos. Trans. R. Soc. B 360, 815–836 (2005).
17. Friston, K., Kilner, J. & Harrison, L. A free energy principle for the brain. J. Physiol. Paris 100, 70–87 (2006).
18. Friston, K. The free-energy principle: a unified brain theory? Nat. Rev. Neurosci. 11, 127–138 (2010).
19. Lindley, D. V. On a measure of the information provided by an experiment. Ann. Math. Stat. 27, 986–1005 (1956).
20. MacKay, D. J. C. Information-based objective functions for active data selection. Neural Comput. 4, 590–604 (1992).
21. Berger, J. O. Statistical Decision Theory and Bayesian Analysis, Springer Series in Statistics (Springer, 2011).

Nature Computational Science | Volume 3 | May 2023 | 418–432

431

Article

https://doi.org/10.1038/s43588-023-00439-w

22. Friston, K. et al. Active inference and epistemic value. Cogn. Neurosci. 6, 187–214 (2015).
23. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P. & Pezzulo, G. Active inference: a process theory. Neural Comput. 29, 1–49 (2017).
24. Attias, H. Planning by probabilistic inference in Proc. 9th Int. Work. Artif. Intell. Stat. 4, 9–16 (2003).
25. Botvinick, M. & Toussaint, M. Planning as inference. Trends Cogn. Sci. 16, 485–488 (2012).
26. Kaplan, R. & Friston, K. J. Planning and navigation as active inference. Biol. Cybern. 112, 323–343 (2018).
27. Matsumoto, T. & Tani, J. Goal-directed planning for habituated agents by active inference using a variational recurrent neural network. Entropy 22, (2020).
28. Schwartenbeck, P. et al. Computational mechanisms of curiosity and goal-directed exploration. eLife 8, 1–45 (2019).
29. Millidge, B., Tschantz, A. & Buckley, C. L. Whence the expected free energy? Neural Comput. 33, 447–482 (2021).
30. Houthooft, R. et al. VIME: variational information maximizing exploration. Adv. Neural Inf. Process. Syst. 0, 1117–1125 (2016).
31. Smith, R. et al. Greater decision uncertainty characterizes a transdiagnostic patient sample during approach-avoidance conflict: a computational modelling approach. J. Psychiatry Neurosci. 46, E74–E87 (2021).
32. Smith, R. et al. Long-term stability of computational parameters during approach-avoidance conflict in a transdiagnostic psychiatric patient sample. Sci Rep. 11, 1–13 (2021).
33. Schwartenbeck, P. & Friston, K. Computational phenotyping in psychiatry: a worked example. eNeuro 3, 1–18 (2016).
34. Daunizeau, J. et al. Observing the observer (I): meta-Bayesian models of learning and decision-making. PLoS ONE 5, e15554 (2010).
35. Patzelt, E. H., Hartley, C. A. & Gershman, S. J. Computational phenotyping: using models to understand individual differences in personality, development, and mental illness. Personal. Neurosci. 1, e18 (2018).
36. Ito, M. & Doya, K. Validation of decision-making models and analysis of decision variables in the rat basal ganglia. J. Neurosci. 29, 9861–9874 (2009).
37. Samejima, K., Doya, K., Ueda, Y. & Kimura, M. Estimating internal variables and parameters of a learning agent by a particle filter. Adv. Neural Inf. Process. Syst. 16 (2003).
38. Samejima, K., Ueda, Y., Doya, K. & Kimura, M. Neuroscience: representation of action-specific reward values in the striatum. Science (80-.) 310, 1337–1340 (2005).
39. Ortega, P. A. & Braun, D. A. Thermodynamics as a theory of decision-making with information-processing costs. Proc. R. Soc. London. A 469, 20120683 (2013).
40. Gottwald, S. & Braun, D. A. The two kinds of free energy and the Bayesian revolution. PLoS Comput. Biol. 16, (2020).
41. Parr, T. & Friston, K. J. Generalised free energy and active inference. Biol. Cybern. 113, 495–513 (2019).
42. Kitagawa, G. Monte Carlo filter and smoother for non-Gaussian nonlinear state space models. J. Comput. Graph. Stat. 5, 1–25 (1996).
43. Bishop, C. M. Pattern Recognition and Machine Learning (Springer, 2006).
44. Kitagawa, G. A self-organizing state-space model. J. Am. Stat. 93, 1203–1215 (1998).
45. Konaka, Y. & Naoki, H. Codes for Konaka and Honda 2023. Zenodo https://doi.org/10.5281/zenodo.7722905 (2023)

Acknowledgements
We are grateful to K. Doya and M. Ito for providing rat behavioral data. We thank the organizers of the tutorial on the free energy principle in 2019, which inspired this research, and I. Higashino and M. Fujiwara-Yada for carefully checking all the equations in the manuscript. This study was supported in part by a Grant-in-Aid for Transformative Research Areas (B) (no. 21H05170), AMED (grant no. JP21wm0425010), Moonshot R&D–MILLENNIA program (grant no. JPMJMS2024-9) by JST, the Cooperative Study Program of Exploratory Research Center on Life and Living Systems (ExCELLS) (program no. 21-102) and the grant of Joint Research by the National Institutes of Natural Sciences (NINS program no. 01112102).
Author contributions
H.N. conceived of the project. Y. K. and H.N. developed the method, and Y.K. implemented the model simulation. Y.K. and H.N. wrote the manuscript.
Competing interests
The authors declare no competing interests.
Additional information
Supplementary information The online version contains supplementary material available at https://doi.org/10.1038/s43588-023-00439-w.
Correspondence and requests for materials should be addressed to Honda Naoki.
Peer review information Nature Computational Science thanks Junichiro Yoshimoto and Karl Friston for their contribution to the peer review of this work. Primary Handling Editors: Ananya Rastogi and Jie Pan, in collaboration with the Nature Computational Science team.
Reprints and permissions information is available at www.nature.com/reprints.
Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons. org/licenses/by/4.0/.
© The Author(s) 2023

Nature Computational Science | Volume 3 | May 2023 | 418–432

432

