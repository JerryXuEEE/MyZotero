bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
Representational drift as a result of implicit regularization
Aviv Ratzon1,2*, Dori Derdikman1, and Omri Barak1,2
1Rappaport Faculty of Medicine, Technion - Israel Institute of Technology, Haifa 31096, Israel 2Network Biology Research Laboratory, Technion - Israel Institute of Technology, Haifa 32000, Israel,
*Aviv.Ratzon@hotmail.com
Recent studies show that, even in constant environments, the tuning of single neurons changes over time in a variety of brain regions. This representational drift has been suggested to be a consequence of continuous learning under noise, but its properties are still not fully understood. To uncover the underlying mechanism, we trained an artificial network on a simplified navigational task, inspired by the predictive coding literature. The network quickly reached a state of high performance, and many neurons exhibited spatial tuning. We then continued training the network and noticed that the activity became sparser with time. We observed vastly different time scales between the initial learning and the ensuing sparsification. We verified the generality of this phenomenon across tasks, learning algorithms, and parameters. This sparseness is a manifestation of the movement within the solution space - the networks drift until they reach a flat loss landscape. This is consistent with recent experimental results demonstrating that CA1 neurons increase sparseness with exposure to the same environment and become more spatially informative. We conclude that learning is divided into three overlapping phases: Fast familiarity with the environment, slow implicit regularization, and a steady state of null drift. The variability in drift dynamics opens the possibility of inferring learning algorithms from observations of drift statistics.
1 What do we mean when we say that the brain represents the external world? One interpretation is the 2 existence of neurons whose activity is tuned to world variables. Such neurons have been observed in
1

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
3 many contexts: place cells [1, 2] – which are tuned to position in a specific context, visual cells [3] – 4 which are tuned to specific visual cues, neurons that are tuned to the execution of actions [4] and more. 5 This tight link between the external world and neural activity might suggest that, in the absence of 6 environmental or behavioral changes, neural activity is constant. In contrast, recent studies show that, 7 even in constant environments, the tuning of single neurons to outside world variables gradually changes 8 over time in a variety of brain regions, even long after good representations of the stimuli were achieved. 9 This phenomenon has been termed representational drift, and has changed the way we think about the 10 stability of memory and perception, but its driving forces and properties are still unknown [5, 6, 7, 8] 11 (see [9, 10] for an alternative account). 12 There are at least two immediate theoretical questions arising from the observation of drift – why 13 does it happen, and whether and how behavior is resistant to it [11, 12]? One mechanistic explanation 14 is that the underlying anatomical substrates are themselves undergoing constant change, such that drift 15 is a direct manifestation of this structural morphing [13]. A normative interpretation posits that drift 16 is a solution to a computational demand, such as temporal encoding [14], ’drop-out’ regularization [15], 17 exploration of the solution space [16], or re-encoding during continual learning [11]. Several studies also 18 address the resistance question, providing possible explanations on how behavior can be robust to such 19 phenomena [17, 18, 19, 20]. 20 Here, we focus on the mechanistic question, and leverage analyses of drift statistics for this purpose. 21 Specifically, recent studies showed that representational drift in the CA1 is driven by active experience 22 [21, 22]. Namely, rate maps decorrelate more when mice are active for a longer time in a given context. 23 This implies that drift is not just a passive process, but rather an active learning one. As drift seems to 24 occur after an adequate representation has formed, it seems fitting to model it as a form of a continuous 25 learning process. 26 This approach has been recently explored by [23, 24]. They considered continuous learning in noisy, 27 overparameterized neural networks. Because the system is overparameterized, a manifold of zero-loss 28 solutions exists. [23] showed that for feedforward neural networks (FNNs) trained using Hebbian learning 29 with added parameter noise, neurons change their tuning over time. This was due to an undirected 30 random walk within the manifold of solutions. The coordinated drift of neighboring place fields was 31 used as evidence to support this view. The phenomenon of undirected motion within the space of
2

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
32 solutions seems plausible, as all members of this space achieve equally good performance (Fig 1A left). 33 However, there may be other properties of the solutions (Fig 1B) that vary along this manifold, which 34 could potentially bias drift in a certain direction (Fig 1A right). It is likely that the drift observed in 35 experiments is a combination of both an undirected and directed movement. We will now introduce 36 theoretical results from machine learning that support the possibility of directed drift.
Figure 1: Two types of possible movements within the solution space. (A) Two options of how drift may look in the solution space. Random walk within the space of equally good solutions that is either undirected (left) or directed (right). (B) The qualitative consequence of the two movement types. For an undirected random walk, all properties of the solution will remain roughly constant (left). For the directed movement there should be a given property that is gradually increasing or decreasing (right).
37 Recent work provided a tractable analytical framework for the learning dynamics of Stochastic 38 Gradient Descent (SGD) with added noise and an overparameterized regime [25, 26]. These studies 39 showed that, after the network has converged to the zero-loss manifold, a second-order effect biases 40 the random walk along a specific direction within this manifold. This direction reduces an implicit
3

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
41 regularizer, determined by the type of noise the network is exposed to. The regularizer is related to the 42 Hessian of the loss – a measure of the flatness of the loss landscape in the vicinity of the solutions. Since 43 this directed movement is a second-order effect, its timescale is orders of magnitude larger than that of 44 the initial convergence. 45 Consider a biological neural network performing a task. The ML implicit regularization mentioned 46 above requires three components: an overparameterized regime, noise, and SGD. Both biological and 47 artificial networks possess a large number of synapses, or parameters, and hence can reasonably be 48 expected to be overparameterized. Noise can emerge from the external environment or from internal 49 biological elements. It is not reasonable to assume that a precise form of gradient descent is implemented 50 in the brain [27], thereby casting doubt on the third element. Nevertheless, biologically plausible rules 51 could be considered as noisy versions of gradient descent, as long as there is a coherent improvement 52 in performance [28, 29]. Motivated by this analogy, we explore representational drift in models and 53 experimental data. 54 Because drift is commonly observed in spatially-selective cells, we base our analysis on a model 55 which has been shown to contain such cells [30]. Specifically, we trained artificial neural networks on a 56 predictive coding task in the presence of noise. In this task, an agent moves along a linear track while 57 receiving visual input from the walls, such that the goal is to predict the subsequent input. We observed 58 that neurons became tuned to the latent variable, which is position, in accordance with previous results 59 [30]. We continued training and found that in addition to the gradual change of tuning curves, similar 60 to [23], we witnessed that the number of active neurons decreased slowly while their tuning specificity 61 increased. These results align with recent experimental observations [21]. Finally, we demonstrated the 62 connection between this sparsificiation effect and changes to the Hessian, in accordance with ML theory.
63 Results
64 Spontaneous sparsification in a predictive coding network
65 To model representational drift in the CA1 area, we chose a simple model that could give rise to spatially66 tuned cells [30]. In this model, an agent traverses a corridor while slightly modulating its angle with 67 respect to the main axis (Fig 2A). The walls are textured by a fixed smooth noisy signal, and the agent
4

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.

68 receives this as input according to its current field of view. The model itself is a single hidden layer 69 feedforward network, with the velocity and visual field as inputs. The desired output is the predicted 70 visual input in the next time step. The model equations are given by:

yˆt = σ(xtmT + b)nT ,

(1)

71 where m and n are the input and output matrices respectively, b is the bias vector, and σ is the ReLU 72 activation function. The task is for the network’s output, y, to match the visual input, x of the following 73 time step, resulting in the following loss function:

f (m, n, b) = Et(yˆt − xt+1)2.

(2)

74 We train the network using Gradient Descent (GD), while adding update noise to the learning 75 dynamics:

θτ +1

=

θτ

−

η ∂f (θτ ) ∂θτ

+

ξτupdate,

(3)

76 where θ = (m, n, b) is the vectorized parameters-vector, τ is the current training step and ξτupdate is 77 Gaussian noise. We let the network converge to a good solution, demonstrated by a loss plateau, and

78 continue training for an additional period. Note that this additional period can be orders of magnitude

79 longer than the initial training period. The network quickly converged to a low loss and stayed at the

80 same loss during the additional training period (Fig 2B). Surprisingly, when looking at the activity within

81 the hidden layer, we noticed that it slowly became sparse. This sparsification did not hurt performance,

82 because individual units became more informative, as quantified by the average mutual information

83 between unit activity and the position of the agent (Fig 2C). When looking at the rate maps of neurons,

84 i.e. their tuning to position, one can observe an image similar to representational drift observed in

85 experiments [5] – namely that neurons changed their tuning over time (Fig 2D). Additionally, their

86 tuning specificity increased in accordance with the information increase. By observing the correlation

87 matrix of the rate maps over time, it is apparent that there was a gradual change that slowed down

5

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
88 (Fig 2E). To summarize, we observed a spontaneous sparsification over a timescale much longer than the 89 initial convergence, without introducing any explicit regularization. This is comparable to experimental 90 data from [21], where indeed drift was characterized by a decrease in the fraction of active place cells, 91 and an increase in cells’ information while the decoding error for the position of the mouse stayed 92 relatively constant (Fig 2F). [31] also reported a decrease in CA1 neural activity and rise in specificity 93 with environment familiarity. Another recent study further demonstrated an increase in information 94 over days [32].
95 Generality of the phenomenon
96 To explore the sensitivity of our results to specific modeling choices, we systematically varied many of 97 them (Fig 3A). Specifically, we replaced the task with either a simplified predictive coding, random 98 mappings or smoothed random mappings. Noise was introduced to the outputs (label noise), instead 99 of the update noise. We simulated different activation functions. Perhaps most important, we varied 100 the learning rules, as SGD is not a biologically plausible one. We used both Adam [33] and RMSprop 101 [34], from the ML literature. We also used Stochastic Error-Descent (SED) [35], which does not require 102 gradient calculation and is more biologically plausible (5). All cases demonstrated an initial, fast, phase 103 of convergence to low loss, followed by a much slower phase of directed random motion within the 104 low-loss space. 105 The results of the simulations supported our main conclusion, though several qualitative phe106 nomenons could be observed. First of all, sparsification dynamics were not sensitive to most of the 107 parameters. The main qualitative difference observed was that the timescales could vary by orders of 108 magnitude as a function of the noise scale (Fig 3B bottom). Note that we calculate the timescale of 109 sparsification by fitting an exponential curve to the fraction of active units over time, and take the time 110 constant of the fitted exponential. Additionally, apart from simulations that did not converge due to 111 too big timescales, the final sparsity was the same for all networks of the same size (Fig 3B top), in 112 accordance with results from [23]. In a sense, once noise is introduced the network is driven to maximal 113 sparsification. For Adam, RMSprop and SED sparsification ensued in the absence of any added noise. 114 For SED the explanation is straightforward, as the parameter updates are driven by noise. For Adam 115 and RMSprop, we suggest that in the vicinity of the zero-loss manifold, the second moment acts as noise.
6

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
Figure 2: Noisy learning leads to spontaneous sparsification. (A) Illustration of an agent in a corridor receiving high-dimensional visual input from the walls. (B) Log loss as a function of training steps, log loss of 0 corresponds to a mean estimator. The loss rapidly decreases, and then remains roughly constant. (C) Information (blue) and fraction of units with non-zero activation for at least one input (red) as a function of training steps. (D) Rate maps sampled at four different time points. Maps in each row are sorted according to a different time point. Sorting is done based on the peak tuning value to the latent variable. (E) Correlation of rate maps between different time points along training. Only active units are used. (F) Figures reproduced from [21] where mice spent different amount of time in two environments. Fraction of place cells in the beginning relative to the end of the experiment (left), average Spatial Information (SI) per cell in the beginning relative to the end of the experiment (middle) and the decoding error for the position of the mouse (right).
7

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
Figure 3: Generality of the results. Summary of 1117 simulations with various parameters (see Table 1). (A) Histogram of fraction of active units after 107 training steps for each simulation. (B) Subset of 178 simulations with the same parameters and varying noise variance, each point represents a single simulation. Fraction of active units as a function of the variance of the noise (top), the log of sparsification time scale as a function of the variance of the noise (bottom). (C) Learning a similarity matching task with Hebbian and anti-Hebbian learning with published code from [23]. Performance of the network (blue) and fraction of active units (red) as a function of training steps. Note that the loss axis does not start at zero, and the dynamic range is small.
116 For label noise, the dynamics were qualitatively different, the fraction of active units did not reduce, 117 but the activity of the units did sparsify. In some cases, the networks quickly collapsed to a sparse 118 solution, most likely as a result of the learning rate being too high, in relation to the input statistics 119 [36]. Importantly, for GD without noise, there was no change after the initial convergence. 120 As a further test of the generality of this phenomenon, we consider the recent simulation from [23]. 121 The learning rule used in this work was very different from the ones we applied. We, therefore, simulated
8

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
122 that network using the published code. We found the same type of dynamics as shown above, namely that 123 the network initially converged to a good solution followed by a longer period of sparsification (Fig 3C). 124 Note that in The original publication [23] the focus was on the stage following this sparsification, in 125 which the network indeed maintained a constant fraction of active cells. 126 In conclusion, we see that noisy learning leads to three phases under rather general conditions. First, 127 fast learning of the task and convergence to the manifold of low-loss solutions. The second phase is 128 directed movement on this manifold driven by a second-order effect of implicit regularization. The third 129 phase is an undirected random walk within the sub-manifold of low loss and maximum regularization.
130 Mechanism of sparsification
131 What are the mechanisms that give rise to sparsification? As illustrated in Fig. 1, different solutions 132 in the zero-loss manifold might vary in some of their properties. The specific property suggested from 133 theory [25] is the flatness of the loss landscape in the vicinity of the solution. This can be demonstrated 134 with a simple example. Consider a two-dimensional loss function. The function is shaped like a valley 135 with a continuous one-dimensional zero-loss manifold at it’s bottom (Fig 4A). Crucially, the loss on the 136 entire manifold is exactly zero, while the vicinity of the manifold becomes systematically flatter in one 137 direction. We simulated gradient descent with added noise on this function from a random starting 138 point (red dot). The trajectory quickly converged to the zero-loss manifold, and began a random walk 139 on it. This walk was clearly biased towards the flatter area of the manifold, as can be seen by the spread 140 of the trajectory. This bias could be comprehended by noting that the gradient was orthogonal to the 141 contour lines of the loss, and therefore had a component directed towards the flat region. 142 In higher dimensions, flatness is captured by the eigenvalues of the Hessian of the loss. Because these 143 eigenvalues are a collection of numbers, different scenarios could lead to minimizing different aspects 144 of this collection. Specifically, according to [25], update noise should regularize the sum of the log 145 of the non-zero eigenvalues while label noise should do the same for the sum of eigenvalues. In our 146 predictive coding example, where update noise was added, each inactivated unit translates into a set 147 of zero-rows in the Hessian, and thus also into a set of zero-eigenvalues (Fig 4B). The slope of the 148 regularizer approaches infinity as the eigenvalue approaches zero, and thus small eigenvalues are driven 149 to zero much faster than large eigenvalues (Fig 4C). So in this case, update noise leads to an increase
9

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
150 in the number of zero eigenvalues, which are manifested as a sparse solution. Another, perhaps more 151 intuitive, way to understand these results is that units below the activation threshold are insensitive to 152 noise perturbations. In other scenarios, in which we simulated with label noise, we indeed observed a 153 gradual decrease in the sum of eigenvalues (Fig 4D).
154 Discussion
155 We showed that representational drift could arise from ongoing learning in the presence of noise, af156 ter a network has already reached good performance. We suggest that learning is divided into three 157 overlapping phases: a fast initial phase, where good performance is achieved, a second slower phase in 158 which directed drift along the low-loss manifold leads to an implicit regularization and finally, a third 159 undirected phase ensues once the regularizer is minimized. In our results, the directed component was 160 associated with sparsification of the neural code, a phenomenon we also observed in experimental data. 161 Interpreting drift as a learning process has recently been suggested by [23, 24]. Both studies focused 162 on the final phase in which the statistics of the representations were constant. Experimentally, [7] 163 reported a decrease in activity at the beginning of the experiment, which they suggested was correlated 164 with some behavioral change, but we believe it could also be a result of the directed drift phase. [37] 165 also reported a slow directed change in representation long after familiarity with the stimuli. There 166 is another consequence of the timescale separation. Unlike in the setting of drift experiments, natural 167 environments are never truly constant. Thus, it is possible that the second phase of learning never stops 168 because the task is slowly changing. This would imply that the second, directed, phase may be the 169 natural regime in which neural networks reside. 170 Here, we reported directed drift in the space of solutions of neural networks. This drift could be 171 observed by examining changes to the representation of external world variables, and hence is related to 172 the phenomenon of representational drift. Note, however, that representations are not a full description 173 of a network’s behavior [38]. The statistics of representational changes can be used as a window into 174 changes of network dynamics and function. 175 The phenomenon of directed drift is very robust to various modeling choices, and also consistent 176 with recent theoretical results [25, 26] The details of the direction of the drift, however, are dependent
10

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
Figure 4: Noisy learning leads to a flat landscape. (A) Gradient Descent dynamics over a two-dimensional loss function with a one-dimensional zero-loss manifold. Note that the loss is identically zero along the horizontal axis, but the left area is flatter. The orange trajectory begins at the red dot. Note the asymmetric extension into the left area. (B) Fraction of active units as a function of the number of non-zero eigenvalues. (C) Log of non-zero eigenvalues at two consecutive time points. Note that eigenvalues do not correspond to one another when calculated at two different time points, and this plot demonstrates the change in their distribution rather than changes in eigenvalues corresponding to specific directions. The distribution of larger eigenvalues hardly changes, while the distribution of smaller eigenvalues is pushed to smaller values. (D) Sum of the Hessian’s eigenvalues as a function of time for learning with label noise.
11

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
177 on specific choices. Specifically, which aspects of the Hessian are minimized during the second phase of 178 learning, as well as the timescale of this phase, depend on the specifics of the learning rule and the noise 179 in the system. This suggests an exciting opportunity – inferring the learning rule of a network from the 180 statistics of representational drift. 181 Our explanation of drift invoked the concept of a low-loss manifold – a family of network config182 urations that have identical performance on a task. The definition of low-loss, however, depends on 183 the specific task and context analyzed. Challenging a system with new inputs could dissociate two 184 configurations that otherwise appear identical [39]. It will be interesting to explore whether various en185 vironmental perturbations could uncover the motion along the low-loss manifold in the CA1 population. 186 For instance, remapping was interpreted as an indicator of the detection of a context switch [40]. One 187 can therefore speculate that the probability for remapping given the same environmental change will 188 systematically vary as the network moves to flatter areas of the loss landscape. 189 Machine learning has been suggested as a model tool for neuroscience research [41, 42, 43]. However, 190 the implicit regularization in ML has not been studied to explain representational drift in neuroscience, 191 and may have been done without awareness of this phenomenon. It’s worth noting that this isn’t a 192 phenomenon specific to neural networks, but rather a general property of overparameterized systems 193 that optimize a cost function. Importing insights from this domain into neuroscience shows the utility 194 of studying general phenomena in systems that learn. For example, another complex learning system in 195 which a similar idea has been proposed is evolution – ”survival of the flattest” suggests that, under a 196 high mutation rate, the fittest replicators are not just the ones with the highest fitness, but also with a 197 flat fitness function which is more robust to mutations [44]. One can hope that more such insights will 198 arise as we open our eyes.
199 Materials and methods
200 Predictive coding task
201 The agent is moving in an arena of size (Lx, Ly), with constant velocity in the y direction of V0. The 202 agent’s heading direction is θ and it changes at every time step by ∆θ ∼ G(0, σθ2), the agent’s visual 203 field has an angle θvis and is represented as a vector of size Lvis. The texture of the walls is generated
12

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.

204 from a random Gaussian vector of size Lwalls = 2(Lx + Ly)Lvis, smoothed with a Gaussian filter with 205 σ2 = KsmoothLwalls. At each time step the agent receives the visual input from the walls, determined by 206 the intersection points of it’s visual field with the walls. When the agent reaches a distance of LyLbuffer 207 from the wall, it turns to the opposite direction.
208 Tuning properties of units
209 For each unit we calculated a tuning curve. We divided the arena into 100 equal bins and computed the 210 number of time steps in each bin and the mean unit activation. We then obtained the tuning curve by 211 dividing the mean activity for each bin by the occupancy. We treated movement in each direction as a 212 separate location. We calculated the spatial information (SI) of the tuning curves for each unit:

SI =

pi

ri r¯

log2

ri r¯

(4)

i

213 where i is the index of the bin, pi is the probability of being in the bin, ri is the value of the tuning 214 curve in the bin and r¯ is the unit’s mean activity rate. Active unit was defined as a unit with non-zero

215 activation for at least one input.

216 Simulations
217 For the random simulations, we train each network for 107 training steps while choosing random learning 218 algorithm and parameters. The ranges and relevant values of parameters are specified in Table 1. For 219 Adam and SED there was no added noise.

220 Stochastic Error Descent
221 The equation for parameter updates under this learning rule is given by:

θτ+1 = θτ − η(f (θτ + ξτ ) − f (θτ ))ξτ

(5)

222 In this learning rule, the parameters are randomly perturbed at each training step by a Gaussian 223 noise denoted by ξτ and then updated in proportion to the change in loss.
13

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.

Table 1: Parameter ranges for random simulations.

Parameter

Possible values

learning algorithm

{SGD, Adam, SED}

noise type

{update, label}

number of samples

[1,20]

initialization regime

{lazy, rich}

task

{abstract predictive, random, random smoothed}

input dimension

[1,20]

output dimension

[1,20]

noise variance (label/update) [0.1,1]/[0.01,0.1]

hidden layer size

100

224 Label noise
225 Label noise is introduced to the loss function given by the following formula:

f (xt) = (yˆt − xt+1 + ξτlabel)2,

(6)

226 where ξτlabel is Gaussian noise.

227 Gradient descent dynamics around the zero-loss manifold
228 The function we used for the two-dimensional example was given by:

L(x, y) = (xy)2,

(7)

229 which has zero loss on the x and y axes. For small enough update noise, GD will converge to the vicinity 230 of this manifold (the axes). We consider a point on the x axis: (x0, 0), and calculate the direction of the 231 gradient near that point. Because we are interested in motion along the zero-loss manifold, we consider 232 a small perturbation in the orthogonal direction (x0, 0 + ∆y) where x0 >> 1 and |∆y| << 1. Any
14

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.

233 component of the gradient in the x direction will lead to motion along the manifold. The update step 234 at this point is given by:



(∆y)2

−∇L(x0, 0 + ∆y) = −2x0 

.

(8)

x0∆y

235 One can observe that the step has a large component in the y direction, quickly returning to the 236 manifold. There is also a smaller component in the x direction, reducing the value of x. Reducing x 237 also reduces the Hessian’s eigenvalues:



00

HL(x0, 0) = 2  

(9)

0 x20

 

λ1,2

=

{0,

x20},

v1,2

=

1 { 

,

0  }.

(10)

01

238 Thus, it becomes clear that the trajectory will have a bias that reduces the curvature in the y 239 direction. 240 For general loss functions and various noise models, rigorous proofs can be found in [25], and a 241 different approach can be found in [26]. Here, we will briefly outline the intuition for the general case. 242 Consider again the update rule for GD:

θ ← θ − η∇L(θ).

(11)

243 In order to understand the dynamics close to the zero-loss manifold, we consider a point θ, for which 244 L(θ) = 0 expand the loss around it:

L(θ + δθ) = L(θ) + ∇T L(θ)δθ + 1 δθT Hδθ.

(12)

2

245 We can then take the gradient of this expansion with respect to θ:

15

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.

∇θL(θ + δθ)

=

∇θL(θ)

+

∇θ ∇Tθ

L(θ)δθ

+

∇θ(

1 2

δθT

Hδθ)

(13)

=

0

+

H

δθ

+

∇θ

(

1 2

δθT

H

δθ).

(14)

246 The first term is zero, because the gradient is zero on the manifold. The second term is the largest

247 one, as it linear in δθ. Note that the Hessian matrix has zero eigenvalues in directions on the zero-loss

248 manifold, and non-zero eigenvalues in other directions. Thus, the second term corresponds to projecting

249 δθ in a direction that is orthogonal to the zero-loss manifold. The third term can be interpreted as

250 the gradient of some auxiliary loss function. Thus, we expect gradient descent to minimize this new

251 loss, which corresponds to a quadratic form with the Hessian. This is the reason for the implicit

252 regularization along the manifold. Note that the auxiliary loss function is defined by δθ, and thus

253 different noise statistics will correspond, on average, to different implicit regularizations. In conclusion,

254 the update step will have a large component that moves the parameter vector towards the zero-loss

255 manifold, and a small component that moves the parameter vector on the manifold in a direction that

256 minimizes some measure of the Hessian.

257 Hessian and sparseness
258 In the main text, we show that the implicit regularization of the Hessian leads to sparse representa259 tions. Here, we show this relationship for a single-hidden layer feed-forward neural network with ReLU 260 activation and Mean Squared Error loss:

f (xi) = σ(xtmT + b)nT

(15)

261 The gradient and Hessian at the zero-loss manifold are given by [45]:





∂f ∂m

n ⊙ 1(xi; θ) ⊗ xi





∇θf (xi)

=

 ∂f  =   ∂b  

n ⊙ 1(xi; θ)

 

(16)





∂f ∂n

(xi · nT + b) ⊙ 1(xi; θ)

∇2θL(x; θ) =

∇θf (xi)∇θf (xi)T ,

(17)

i

16

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
262 where 1(xi; θ) is an indicator vector denoting whether each unit is active for some input xi. Sparseness
263 means that a unit has become inactive for all inputs. All the partial derivatives of input, output and 264 bias weights associated with such a unit are zero, and thus the relevant rows of the Hessian are zero as 265 well. Thus, every inactive unit leads to several zero eigenvalues.
266 Acknowledgments
267 We thank Ron Teichner and Kabir Dabholkar for comments on the manuscript. This research was 268 supported by the ISRAEL SCIENCE FOUNDATION (grants Nos. 2655/18 and 2183/21 to DD, and 269 1442/21to OB), by the German-Israeli Foundation (GIF I-1477-421.13/2018) to DD, by a grant from 270 the US-Israel Binational Science Foundation (NIMH-BSF CRCNS BSF:2019807, NIMH:R01 MH125544271 01 to DD), by an HFSP research grant (RGP0017/2021) to OB, A Rappaport Institute Collaborative 272 research grant to DD, by Israel PBC-VATAT and by the Technion Center for Machine Learning and 273 Intelligent Systems (MLIS) to DD and OB, by the Prince Center for the Aging Brain, and by a University 274 of Michigan – Israel Partnership for Research and Education Collaborative Research stipend to DK. 275 (data science)
References
[1] John O’keefe and Lynn Nadel. The hippocampus as a cognitive map. Behavioral and Brain Sciences, 2(4):487–494, 1979.
[2] John O’Keefe and Jonathan Dostrovsky. The hippocampus as a spatial map: preliminary evidence from unit activity in the freely-moving rat. Brain research, 1971.
[3] David H Hubel and Torsten N Wiesel. Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex. The Journal of physiology, 160(1):106, 1962.
[4] Bruce L McNaughton, SJY Mizumori, CA Barnes, BJ Leonard, M Marquis, and EJ Green. Cortical representation of motion during unrestrained spatial navigation in the rat. Cerebral Cortex, 4(1):27– 39, 1994.
17

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
[5] Yaniv Ziv, Laurie D Burns, Eric D Cocker, Elizabeth O Hamel, Kunal K Ghosh, Lacey J Kitch, Abbas El Gamal, and Mark J Schnitzer. Long-term dynamics of ca1 hippocampal place codes. Nature neuroscience, 16(3):264–266, 2013.
[6] Laura N. Driscoll, Noah L. Pettit, Matthias Minderer, Selmaan N. Chettih, and Christopher D. Harvey. Dynamic Reorganization of Neuronal Activity Patterns in Parietal Cortex. Cell, 170(5):986– 999, 8 2017.
[7] Daniel Deitch, Alon Rubin, and Yaniv Ziv. Representational drift in the mouse visual cortex. Current biology, 31(19):4327–4339, 2021.
[8] Carl E Schoonover, Sarah N Ohashi, Richard Axel, and Andrew J P Fink. Representational drift in primary olfactory cortex. Nature, 594, 2021.
[9] William A Liberti, 3rd, Tobias A Schmid, Angelo Forli, Madeleine Snyder, and Michael M Yartsev. Publisher correction: A stable hippocampal code in freely flying bats. Nature, 606(7914):E6, June 2022.
[10] Sadra Sadeh and Claudia Clopath. Contribution of behavioural variability to representational drift. Elife, 11:e77907, 2022.
[11] Michael E Rule, Timothy O’Leary, and Christopher D Harvey. Causes and consequences of representational drift. Curr. Opin. Neurobiol., 58:141–147, October 2019.
[12] Laura N Driscoll, Lea Duncker, and Christopher D Harvey. Representational drift: Emerging theories for continual learning and experimental future directions. Current Opinion in Neurobiology, 76:102609, 2022.
[13] Noam E Ziv and Naama Brenner. Synaptic tenacity or lack thereof: spontaneous remodeling of synapses. Trends in neurosciences, 41(2):89–99, 2018.
[14] Alon Rubin, Nitzan Geva, Liron Sheintuch, and Yaniv Ziv. Hippocampal ensemble dynamics timestamp events in long-term memory. elife, 4:e12247, 2015.
[15] Kyle Aitken, Marina Garrett, Shawn Olsen, and Stefan Mihalas. The geometry of representational drift in natural and artificial neural networks. PLOS Computational Biology, 18(11):e1010716, 2022.
[16] David Kappel, Stefan Habenschuss, Robert Legenstein, and Wolfgang Maass. Network plasticity as bayesian inference. PLoS computational biology, 11(11):e1004485, 2015.
18

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
[17] Uri Rokni, Andrew G Richardson, Emilio Bizzi, and H Sebastian Seung. Motor learning with unstable neural representations. Neuron, 54(4):653–666, 2007.
[18] Lee Susman, Naama Brenner, and Omri Barak. Stable memory with unstable synapses. Nature communications, 10(1):4441, 2019.
[19] Gianluigi Mongillo, Simon Rumpel, and Yonatan Loewenstein. Intrinsic volatility of synaptic connections—a challenge to the synaptic trace theory of memory. Current opinion in neurobiology, 46:7–13, 2017.
[20] Yaroslav Felipe Kalle Kossio, Sven Goedeke, Christian Klos, and Raoul-Martin Memmesheimer. Drifting assemblies for persistent memory: Neuron transitions and unsupervised compensation. Proceedings of the National Academy of Sciences, 118(46):e2023832118, 2021.
[21] Dorgham Khatib, Aviv Ratzon, Mariell Sellevoll, Omri Barak, Genela Morris, and Dori Derdikman. Active experience, not time, determines within-day representational drift in dorsal ca1. Neuron, 2023.
[22] Nitzan Geva, Daniel Deitch, Alon Rubin, and Yaniv Ziv. Time and experience differentially affect distinct aspects of hippocampal representational drift. Neuron, 2023.
[23] Shanshan Qin, Shiva Farashahi, David Lipshutz, Anirvan M Sengupta, Dmitri B Chklovskii, and Cengiz Pehlevan. Coordinated drift of receptive fields in hebbian/anti-hebbian network models during noisy representation learning. Nature Neuroscience, pages 1–11, 2023.
[24] Farhad Pashakhanloo and Alexei Koulakov. Stochastic gradient descent-induced drift of representation in a two-layer neural network. arXiv preprint arXiv:2302.02563, 2023.
[25] Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process. In Conference on learning theory, pages 483–513. PMLR, 2020.
[26] Zhiyuan Li, Tianhao Wang, and Sanjeev Arora. What happens after sgd reaches zero loss?–a mathematical framework. arXiv preprint arXiv:2110.06914, 2021.
[27] Yoshua Bengio, Dong-Hyun Lee, Jorg Bornschein, Thomas Mesnard, and Zhouhan Lin. Towards biologically plausible deep learning. arXiv preprint arXiv:1502.04156, 2015.
19

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
[28] Yuhan Helena Liu, Arna Ghosh, Blake Richards, Eric Shea-Brown, and Guillaume Lajoie. Beyond accuracy: generalization properties of bio-plausible temporal credit assignment rules. Advances in Neural Information Processing Systems, 35:23077–23097, 2022.
[29] Owen Marschall, Kyunghyun Cho, and Cristina Savin. A unified framework of online learning algorithms for training recurrent neural networks. The Journal of Machine Learning Research, 21(1):5320–5353, 2020.
[30] Stefano Recanatesi, Matthew Farrell, Guillaume Lajoie, Sophie Deneve, Mattia Rigotti, and Eric Shea-Brown. Predictive learning as a network mechanism for extracting low-dimensional latent space representations. Nature Communications, 12(1), 2021.
[31] Mattias P Karlsson and Loren M Frank. Network dynamics underlying the formation of sparse, informative representations in the hippocampus. Journal of Neuroscience, 28(52):14271–14281, 2008.
[32] Liron Sheintuch, Alon Rubin, and Yaniv Ziv. Bias-free estimation of information content in temporally sparse neuronal activity. PLoS computational biology, 18(2):e1009832, 2022.
[33] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[34] Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. Cited on, 14(8):2, 2012.
[35] Gert Cauwenberghs. A fast stochastic error-descent algorithm for supervised learning and optimization. Advances in neural information processing systems, 5, 1992.
[36] Rotem Mulayoff, Tomer Michaeli, and Daniel Soudry. The implicit bias of minima stability: A view from function space. Advances in Neural Information Processing Systems, 34:17749–17761, 2021.
[37] Nghia D Nguyen, Andrew Lutas, Jesseba Fernando, Josselyn Vergara, Justin McMahon, Jordane Dimidschstein, and Mark L Andermann. Cortical reactivations predict future sensory responses. bioRxiv, pages 2022–11, 2022.
[38] Romain Brette. Is coding a relevant metaphor for the brain? Behavioral and Brain Sciences, 42:e215, 2019.
20

bioRxiv preprint doi: https://doi.org/10.1101/2023.05.04.539512; this version posted July 2, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
[39] Elia Turner, Kabir V Dabholkar, and Omri Barak. Charting and navigating the space of solutions for recurrent neural networks. Advances in Neural Information Processing Systems, 34:25320–25333, 2021.
[40] Honi Sanders, Matthew A Wilson, and Samuel J Gershman. Hippocampal remapping as hidden state inference. Elife, 9:e51140, 2020.
[41] Blake A Richards, Timothy P Lillicrap, Philippe Beaudoin, Yoshua Bengio, Rafal Bogacz, Amelia Christensen, Claudia Clopath, Rui Ponte Costa, Archy de Berker, Surya Ganguli, et al. A deep learning framework for neuroscience. Nature neuroscience, 22(11):1761–1770, 2019.
[42] Adam H Marblestone, Greg Wayne, and Konrad P Kording. Toward an integration of deep learning and neuroscience. Frontiers in computational neuroscience, page 94, 2016.
[43] Andrew Saxe, Stephanie Nelli, and Christopher Summerfield. If deep learning is the answer, what is the question? Nature Reviews Neuroscience, 22(1):55–67, 2021.
[44] Francisco M Codon˜er, Jos´e-Antonio Daro´s, Ricard V Sol´e, and Santiago F Elena. The fittest versus the flattest: experimental confirmation of the quasispecies effect with subviral pathogens. PLoS pathogens, 2(12):e136, 2006.
[45] Mor Shpigel Nacson, Rotem Mulayoff, Greg Ongie, Tomer Michaeli, and Daniel Soudry. The implicit bias of minima stability in multivariate shallow reLU networks. inproceedings, 2023.
21

