An Introductory Course in Computational Neuroscience
Paul Miller
The MIT Press Cambridge, Massachusetts London, England

© 2018 Massachusetts Institute of Technology All rights reserved No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher. This book was set in Times by Toppan Best-set Premedia Limited Printed and bound in the United States of America. Library of Congress Cataloging-in-Publication Data Names: Miller, Paul, 1969- author. Title: An introductory course in computational neuroscience / Paul Miller. Description: Cambridge, MA : The MIT Press, 2018. | Series: Computational neuroscience series | Includes bibliographical references and index. Identifiers: LCCN 2018003118 | ISBN 9780262038256 (hardcover : alk. paper) eISBN 9780262347556 Subjects: LCSH: Computational neuroscience--Textbooks. | Neurosciences--Mathematics. Classification: LCC QP357.5 .M55 2018 | DDC 612.8/233--dc23 LC record available at https://lccn.loc.gov/2018003118 ePub Version 1.0

Table of Contents
Series page Title page Copyright page Series Foreword Acknowledgments Preface 1: Preliminary Material 2: The Neuron and Minimal Spiking Models 3: Analysis of Individual Spike Trains 4: Conductance-Based Models 5: Connections between Neurons 6: Firing-Rate Models and Network Dynamics 7: An Introduction to Dynamical Systems 8: Learning and Synaptic Plasticity 9: Analysis of Population Data References Index Computational Neuroscience

List of tables
Table 1.1  Analogy between flows of electrical charges and fluids Table 1.2  Dynamics of a falling stone Table 2.1  Ions and their properties Table 2.2  List of symbols used in chapter 2 Table 3.1  Structure of code for tutorial 3.1, part A Table 4.1  Parameter values for H-H model (figures 4.2–4.10 and tutorial 4.1) Table 4.2  Gating variables of the H-H model (figures 4.2–4.10 and tutorial 4.1) Table 4.3  Parameter values for the Connor-Stevens model (figure 4.11) Table 4.4  Gating variables of the Connor-Stevens model (figure 4.11) Table 4.5  Parameter values for thalamic rebound model (figure 4.12 and tutorial 4.2) Table 4.6  Gating variables of the thalamic rebound model (figure 4.12 and tutorial 4.2) Table 4.7  Parameter values used for intrinsic bursting in figures 4.14 and 4.15 Table 4.8  Gating variables of the Pinsky-Rinzel model Table 4.9  Parameter values used for the burster with Ih current in figure 4.16 Table 5.1  For each three-neuron motif (figure 5.6), the number of distinct sets of connections between the three cells that produce the same motif is given as the multiplicity

Table 8.1  A three-component plasticity rule that can be used for supervised learning in problems such as the weatherprediction task
Table 8.2  Properties of different cell types in the cerebellar eye-conditioning model of tutorial 8.4

List of figures
Figure 1.1  Annotation of a differential equation. The equation is evaluated at each point in time (each point on lower curve), to determine the slope of the curve, which is the rate of change of the variable, . By knowing the rate of change, the complete curve of as a function of time can be determined. The parameters , , and are fixed electrical properties of the neuron, which would be provided in any question, or must be measured in any experiment.
Figure 1.2  Structure of a neuron. Arrows on dendrites and axon depict the standard direction of information flow: Inputs are received on the dendrites and, once sufficient electrical charge accumulates on the surface of the soma, a spike of voltage propagates away from the soma down the axon. Reverse (retrograde) propagation is also possible.
Figure 1.3  Structures of some distinct cells. (A) Layer 3 pyramidal cell from the frontal cortex of a mouse (courtesy of the Luebke Laboratory, Boston University). Cells like these are the predominant excitatory cells of the cerebral cortex. (B) A human Purkinje cell, the inhibitory output cell of the cerebellum, as drawn by Ramón y Cajal (from Popular Science Monthly 71, 1907). (C) A neuron from the stomatogastric ganglion of the crab. In this case, the axon branches away from the dendrites instead of being attached directly to the cell body (courtesy of the Marder laboratory, Brandeis University).
Figure 1.4  Regions and topographic maps. (A) Cerebral cortex with gyri, from H. Gray, Anatomy of the Human Body (1918), illustrated by H. V. Carter. (B) The shading of an orientation selectivity map indicates that neurons in primary visual cortex are clustered according to the orientation of a bar to which they most strongly respond (see the mapping from orientation to shading below). Insets show pinwheel centers (see section 6.9) and boundaries. This figure panel was originally published as

part B of figure 1 in W. H. Bosking, Y. Zhang, B. Schofield, and D. Fitzpatrick (1997), “Orientation selectivity and the arrangement of horizontal connections in tree shrew striate cortex,” Journal of Neuroscience, 17(6), 2212–2221. Reprinted with permission.
Figure 1.5  Pump and reservoir analogy to electrical circuits. Open channels have a low resistance, permitting high current flow across a potential difference from high potential to low potential. Pumps (effectively batteries) can maintain the potential difference, but without them the passive current flow would tend to erase any potential difference.
Figure 1.6  Dynamics of a falling stone. (A) Velocity, , decreases linearly with time. (B) As a result, height, , decreases quadratically with time. Crosses are data points from table 1.2. This figure was produced by the available online code falling_stone.m.
Figure 1.7  Time-dependent velocity. (A) Velocity of a hypothetical sprinter increases from zero toward a maximum, , at which it remains. (B) The velocity corresponds to the gradient of the position versus time curve, which starts instantaneously flat, then steepens to lie parallel, but below the dotted line .
Figure 1.8  Position-dependent velocity. (A) Profile of a hill so that with increasing distance the terrain progresses from steep uphill to shallow uphill to flat to shallow downhill to steep downhill. (B) The velocity of a child on a bicycle is slowest at the steepest uphill and increases monotonically with position, to become fastest at the steep downhill. The velocity as a function of position might be approximated as .
Figure 1.9  Rate of change of temperature as a linear function of temperature. (A) The diagonal straight line indicates how the rate of change of temperature depends linearly on temperature in a simplified temperature-regulation system. Horizontal dotted lines indicate the direction and magnitude of the rate of temperature change. They point toward the equilibrium temperature, , where . (B) Since does not have a fixed value, a plot of temperature, , against time, , does not have a fixed gradient and so is nonlinear.

Figure 1.10  In the Venn diagram, left, the entire top circle represents , the entire bottom circle represents , and the overlap is . The total shaded region is whose area is the sum of the areas of each circle minus the area of intersection, which is otherwise counted twice.
Figure 1.11  Probability of multiple independent events. If a coin is tossed multiple times, the number of possible outcomes for the sequence of tosses doubles following each toss, so there are eight outcomes after three tosses. The probability of heads or tails is 1/2 for every toss, independent of the number of prior heads or tails. Each of the final sequences (, , etc.) is equally probable, with a probability of , where is the number of coin tosses.
Figure 1.12  Annotation of rule 3.
Figure 1.13  Probabilities for each of four choices of target by a soccer player taking a penalty. In this example, the probability that the target is bottom-left is . This can be calculated as the probability the player aims left, , multiplied by the probability the player shoots low when aiming left, . Illustration by Jaleel Draws of Pencil on Paper (POP).
Figure 1.14  Annotation of Bayes’ theorem, rule 4. The formula is typically used once we know that event A has occurred. The terms P(B) and P(A) are priors, since these are respectively the probabilities of events B and A occurring, before we know whether A occurred.
Figure 1.15  Flipping a biased or unbiased coin (example i). The probability of selecting a biased coin, , is 1/5, and the probability of selecting an unbiased coin, , is 4/5. The probability of tossing three successive heads when the biased coin is selected, , while the probability of tossing three successive heads when the unbiased coin is selected, . Combining these yields the probability of selecting the biased coin and tossing three heads as , while the probability of selecting the unbiased coin and tossing three successive heads is .
Figure 1.16  Euler method. At each time point, the gradient of the curve of versus is evaluated as from the relevant ordinary

differential equation. Then a small step in time, of size is taken and the value of is increased by an amount .
Figure 2.1  The equivalent circuit representation of the neuron’s membrane. Various ion channels, each type represented by a resistor in series with a battery, span the cell membrane. All operate in parallel with each other—so their conductances sum—and with a capacitor, which represents the insulating cell membrane. The battery for a type of ion channel is directed to bring the membrane potential (electrical potential on the inside of the membrane minus the potential on the outside) to the Nernst equilibrium potential for that ion. Types of ion channels with a variable total conductance—sodium (), calcium (), and potassium ()—are represented by the arrow through the resistor, whereas the leak conductance, , is defined as a constant. The vertically directed arrows indicate the direction of current flow when the membrane potential is at zero or another intermediate value. If all channels with variable conductance are closed so that no current flows through the central parallel paths in the above circuit, then current will flow through the leak channels (charging the capacitor) until the inside of the cell membrane is at the leak potential .
Figure 2.2  Exponential decay of the membrane potential. A leak potential of mV (dashed line) is reached by exponential decay with a time constant of ms. The solid curves follow equation 2.8, but differ in their initial conditions, , which is mV (upper curve) or mv (lower curve). This figure is created by the online code exponential.m.
Figure 2.3  Behavior of the leaky integrate-and-fire neuron. Membrane potential of the model neuron (equation 2.9) in response to three different 200 ms current pulses (top) is depicted in the middle row, with spike times indicated by vertical lines in the bottom row. Parameters of the model are: pF, nS, mV, mV, and mV. Combining these parameters yields a time constant of ms and pA. This figure was created by the online code LIF_model.m.
Figure 2.4  Three methods for incorporating a refractory period in an LIF model. (A) Method 1, a hard reset with the membrane

potential fixed at the reset value for the refractory period. (B) Method 2, a refractory conductance brings the membrane potential to reset. Its effect lingers as the conductance decays while the membrane potential rises, slowing the rate of spikes. (C) Method 3, following a hard reset the membrane potential can rise immediately, with the refractory period ensured by a raised threshold (dashed line). Solid line = membrane potential; dashed line = threshold voltage. Left column, input current is 240 pA. Right column, input current is 400 pA. This figure was created by the online code LIF_model_3trefs.m.
Figure 2.5  Spike-rate adaptation in a model LIF neuron. (A) The membrane potential as a function of time in response to applied current steps of duration 200 ms and of amplitude 240 pA (left) or 400 pA (right). Note the increasing separation between spike times. (B) The potassium conductance accumulates over successive spikes until its decay between spikes matches its increment following a spike (right), in which case a regular interspike interval is reached. This figure is created by the online code, LIF_model_sra.m.
Figure 2.6  Spike generation in the ELIF model. The ELIF model (right) produces the typical inflexion of the voltage trace that is absent in the LIF model (left). Parameters are as in figure 2.3 with pA. For the ELIF model mV and mV. This figure was created by the online code LIFspike_ELIF.m.
Figure 2.7  Response of the AELIF model to a current step. (A) The applied current. (B) The membrane potential. (C) The adaptation current. Note the very strong lengthening of the interspike interval following the first two spikes. Parameters used are: pF, nS, mV, mV, mV, mV, mV, nS, pA, ms, and nA. This figure was created by the online code AELIF_code.m.
Figure 2.8  Firing rate curve for AELIF model. The solid line is the firing rate after any initial transient when the cell fires periodically. Circles denote the reciprocal of the first interspike interval, effectively the initial firing rate, evaluated when a current step produces two or more spikes. Asterisks correspond to current steps that produce a single spike. Parameters are as in

figure 2.7. This figure was produced by the online code AELIF_code_loop.m.
Figure 2.9  Response of an altered AELIF model to produce a refractory period. Possible solution of tutorial 2.3, question 3. (A) Response of membrane potential to applied current of 160 pA. (B) Firing rate as a function of applied current (currents in the tens of nA produce failure rather than high rates in this model). (C, D) Mean membrane potential increases monotonically with firing rate (C) and with applied current (D). This figure is produced by the online code Tutorial_2_3_Q3.m.
Figure 2.10  Calculation of Nernst equilibrium potential. (A) When a species of ion, such as potassium, has a greater concentration inside than outside the neuron, there would be a net outward flow of ions through any open channel (i.e., upward in the diagram) unless countered by a flow in the opposite direction due to a potential difference (in this case the outside of the cell should be at a higher electrical potential than the inside, in order to drive positive ions back into the cell). (B) In a simplified model, when the outside of the cell is at a higher potential than the inside, external ions that reach the channel opening will pass through it, but internal ions that reach the channel opening will pass through only if they have enough velocity, , in the direction perpendicular to the channel opening. The speed criterion for exiting the cell is that the component of their kinetic energy perpendicular to the channel opening, , is greater than the potential energy the ion would acquire when passing through the channel, , where is the total charge on the ion and is the membrane potential. The distribution of is exponential, such that the probability any ion has is , for .
Figure 3.1  Sketches of receptive fields and their general structure. (A) A neuron in primary somatosensory cortex is most active when a particular area of skin is touched (marked dark). (B) A ganglion cell in the retina responds best to a centersurround arrangement of visual stimulus, either light center with dark surround (upper panel, “On-center”) or a dark center with a light surround (lower panel, “Off-center”). White, light; black, darkness. (C) The spectrotemporal receptive field of a

neuron in auditory cortex can indicate a response to a particular, fixed, frequency of a tone (upper panel) or to a sweep of frequency (a downward sweep in the lower panel). The response is accentuated if there is no sound intensity at surrounding frequencies. White, high intensity; dark, low intensity.
Figure 3.2  PSTH for an example of oscillating activity. (A) Each of the 10 rows contains the series of spike times from a single neuron in one trial. Spikes are produced as an inhomogeneous Poisson process (see text or code) with the firing rate in Hertz as where is time in seconds. (B) Spikes are accumulated across trials in consecutive time bins of width 50 ms. (C) Spikes are accumulated across trials in consecutive time bins of width 200 ms. (D) Spikes are accumulated across trials in a sliding bin of width 100 ms. (E) Each spike is filtered (smoothed) with a Gaussian of width 100 ms to produce a continuous, smooth function. In this example, because the rate is itself defined as a smooth function, the smoothed spike train produces the best approximation to the underlying rate. This figure was created by the available online code PSTH_osc.m.
Figure 3.3  Response of a center-surround linear filter or kernel. The response of many neurons in primary sensory areas can be modeled as a linear filter of the stimulus. In the simplified example shown, the neuron filters a spatial stimulus by multiplying the stimulus amplitude at each spatial location (left) by the corresponding value of the filter—also called the kernel—(center) to produce a response (right). (A) For the center-surround filter shown (middle) a spatially constant stimulus (left) generates no response (right) because the positive contribution at the center of the filter is canceled by the negative contributions of the surround. (B) Such a filter produces a strong response to a spatially localized stimulus.
Figure 3.4  Producing a spike-triggered average. A time-varying signal (solid continuous line, top) produces a sequence of eight spikes (vertical solid lines below the stimulus). The stimulus is then realigned to the time of each spike (vertical dashed lines top and middle) to produce eight samples of the stimulus, one sample around the time of each spike. These samples are

averaged to produce the spike-triggered average (STA, bottom). The STA preceding the neuron’s spike (set to the time of zero) is the average stimulus to cause a spike. Since the spike cannot cause the stimulus, the period after the spike time over which the STA decreases to zero is an indication of the timescale of correlations within the stimulus. Any appreciable decay time— indicating stimulus correlations that do not rapidly decay to zero —is a confound if this method is used to estimate properties of the neuron.
Figure 3.5  Model of a spatiotemporal receptive field of a V1 simple cell. A single spatial dimension is plotted (y-axis), corresponding to the location of a bar of optimal orientation, combined with a time dimension (x-axis). The center-surround spatial response combines with a biphasic temporal response. Light indicates high positive stimulus, dark indicates negative stimulus. This figure was generated by the online code spatial_AELIF_randomI.m.
Figure 3.6  Downsampling and upsampling data. The original data stream (center) is a series of entries as a function of time. It can be downsampled by averaging across bins (bottom), reducing time resolution and using fewer time bins, or it can be upsampled (top) to create more time bins.
Figure 3.7  Spike trains, ISI distributions, and statistics. (A) A regular spike train with a fixed ISI of 50 ms. (B) A spike train with ISIs arising randomly from a normal distribution with mean 50 ms and standard deviation of 20 ms. (C) A Poisson spike train with mean ISI of 50 ms. (D) A regular spike train with ISIs of 150 ms switches to a regular spike train with ISIs of 10 ms. (E) A spike train with bursts of three spikes, with 150 ms between bursts and 4 or 5 ms between spikes within a burst. (Left) A1–E1. Example of 1 s of each spike train. (Right) A2–E2. Histograms show the distribution of ISIs generated by a 300-s simulation. and of each distribution is given above each example. This figure was created by the online code spikes_cv_isis.m.
Figure 3.8  The ROC curve. (A) Histograms of the number of spikes produced by a noisy AELIF neuron across 1000 trials

with a time window of 0.5 s and a stimulus of 0.1 nA (right histogram) or no stimulus (left histogram) combined with input noise with pA.s0.5. (B) The cumulative sum of each histogram is calculated and divided by the total number of trials, then subtracted from 1 to obtain the fraction of trials with more spikes than a given spike count. (C) The y-values ( and ) from each of the two curves in B) at the same x-value are plotted against each other to produce the ROC curve as the x-value varies. That is, each point on the ROC curve can be produced by choosing a threshold at a particular spike count (x-value) such that a positive (ve) test result is produced if the spike count is above that threshold. The fraction of trials with above-threshold spike counts in the absence of stimulus is the false positive value on the x-axis (). The fraction of trials with above-threshold spike counts in the presence of a stimulus is the true positive value (or hit rate) on the y-axis (). Parameters for these data are those given in tutorial 3.3, question 1. This figure is produced by the online code AELIF_noise_ROC.m.
Figure 3.9  Optimal position of the threshold for stimulus detection. (A) If both possibilities are equally likely a priori, then the optimal threshold is at the intersection of the two probability distributions. (B) Raising the threshold above the optimum reduces the fraction of errors that are false positives () but reduces true positives () (i.e., increases the fraction of errors that are “misses”) by a greater amount.
Figure 3.10  ROC analysis to reveal the stimulus-response distribution. (A) Three alternative stimulus-response distributions are depicted. (A1) The response to the stimulus is bimodal, with a strong response on a fraction of trials (recall), but otherwise no distinction from the absence of a stimulus (guessing). (A2) The response is also bimodal, but the weak response (recognition) differs from the no-stimulus condition. (A3) The stimulus response arises from a single Gaussian distribution with increased mean and variance compared to the no-stimulus condition. (B) ROC curves produced from the distributions in 3.10A (solid) with the chance diagonal line (dotted). (B1) The fraction of trials with high response produces a nonzero y-intercept. Other trials arise from the same

distribution as the no-stimulus condition, so the curve is linear with gradient equal to the ratio of the sizes of the lower distributions. (B2) The fraction of high-response trials still produces a positive y-intercept, but now the low-response trials are sampled from a distribution with higher mean than the nostimulus condition, so the curve is concave. (B3) A single broad stimulus-distribution produces a similarly shaped ROC curve to 3.10B2, making it nontrivial to distinguish distributions 3.10A2 and 3.10A3 from ROC curves alone. (C) ROC curves plotted on z-score coordinates, where z-score is number of standard deviations above or below the mean assuming a single Gaussian distribution. (C1, C2) Sampling from two Gaussians for response to the stimulus leads to a curved plot. (C3) When both stimuluspresent and stimulus-absent conditions each produce a single Gaussian distribution in 3.10A, then a plot of the z-scores against each other is linear. The gradient is the ratio of the standard deviations. Code to produce these figures is available online as ROC_distributions.m.
Figure 3.11  Relating to . Any combination of events that contributes to the probability of three events in an interval— such as the combination labeled 3a—is one event away from each of combinations of events that contribute to the probability of four events in an interval, like those labeled 4a, 4b, and 4c. However, each of those four-event combinations can be arrived at four different ways—for example, the combination labeled 4c is reached by adding one event to any of the combinations labeled 3a, 3b, 3c, or 3d. Therefore, the actual number of fourevent combinations is times the number of three-event combinations. Since addition of an extra event occurs with probability , we find—by multiplying the relative probabilities of a single combination by the relative numbers of combinations— that the probability of four events, , is related to the probability of three events, , by the formula . When considering a general whole number of events, , this formula becomes via similar logic.
Figure 3.12  Random selection from a known distribution. The area to the left of a randomly chosen interval, , is the sum of the areas of strips of height and width . If a point is chosen at random with equal probability from within the total area of the

distribution, its likelihood to fall on a given strip is equal to the area of that strip, which is , so proportional to the corresponding probability. Such selection can be made by choosing a random number from a uniform distribution between zero and 1 and requiring either the area to the left, or the area to the right of the chosen interval, , is equal to that random number.
Figure 4.1  A mechanical calculating machine, the Brunsviga, of the type used by Huxley. For each mathematical operation, Huxley would need to set levers in place and turn a crank to produce an answer. The huge amount of effort required to simulate the model equations gave Hodgkin and Huxley the impetus to use sophisticated mathematical methods to minimize the calculation time. Photo from www.vintagecalculators.com, used with permission.
Figure 4.2  Stages of the action potential. (A) Sodium activation, m, causes a rapid upswing of the membrane potential, V, by opening sodium channels. (B) Slower sodium inactivation, h, closes sodium channels and potassium activation, n, opens potassium channels at high-V to bring about the fall in membrane potential. (C) Deactivation of sodium channels, m, as V falls below threshold. (D) Slower deinactivation of sodium channels, h, and deactivation of potassium channels, n, at hyperpolarized V allows V to rise again ready for a new spike. This figure is produced by the online code HH_old_base.m.
Figure 4.3  Rate constants for the gating variables of the Hodgkin-Huxley model. (A) Rates of sodium activation, , and deactivation, . (B) Rates of potassium activation, , and deactivation, . (C) Rates of sodium deinactivation, , and inactivation, . Note the twentyfold increase in the y-axis scale for the activation variable of sodium, which responds most rapidly. Equations are provided in table 4.2. This figure was produced by the online code HHab_plotnewV.m.
Figure 4.4  Steady state of the gating variables in the HodgkinHuxley model. The activation variables approach 1 at positive membrane potentials, whereas the inactivation variable approaches one at very negative membrane potentials. The

small region where sodium activation, , and inactivation, , are both greater than zero provides a voltage “window” in which a sodium current may be sustained. Hence the sodium current can be called a “window current.” This figure was produced by the online code HHab_plotnewV.m.
Figure 4.5  Dynamics of gating variables during a single spike. In this example (as in figure 4.2), the peak of the spike is at 70 ms, almost simultaneous with the peak of sodium activation, (dotted). The sodium inactivation variable, (dashed) reaches a minimum and potassium activation, (solid) reaches a maximum over 1 ms later. The neuron is not able to spike again until and have increased and has decreased back to near baseline. This figure is produced by the online code HH_old_base.m.
Figure 4.6  Type-II behavior of the Hodgkin-Huxley model. (A) Current steps that are applied in panels (B–E). (B) A 0.2 nA applied current produces a decaying subthreshold oscillation in the membrane potential. (C) A 0.3 nA applied current produces a single spike followed by decaying sub-threshold oscillations. (D) A 0.6 nA applied current produces two spikes, the second of lower amplitude than the first, followed by more slowly decaying subthreshold oscillations. (E) A 0.624 nA applied current produces a succession of six spikes of near constant interspike interval, but not sustained firing. Subthreshold oscillations follow the last spike. (F) An applied current of 0.626 nA is just sufficient to produce sustained spiking, but at a high frequency of over 50 Hz. This figure is produced by the online code HH_manyItrials.m.
Figure 4.7  Firing rate curve of Hodgkin-Huxley model. (A) Firing rate in response to current steps from a baseline of 0 nA, as used in figure 4.6. Small solid circles indicate a single spike is produced; crosses indicate multiple spikes are produced without sustained firing. (B) Firing rate in response to a protocol of current steps that successively increase from the previous value without reset, then successively decrease. For a wide range of currents the system is bistable, with the neuron remaining quiescent (zero firing rate) if initially quiescent, or sustaining

firing if spiking is already initiated. The code used to generate this figure can be found online as HH_new_f_I_bi.m.
Figure 4.8  Anode break, a spike from hyperpolarization. (A) Applied current is negative (outward and hyperpolarizing) for a period of 50 ms. (B) When the applied current is released and returns abruptly to its baseline value of zero an action potential is produced. (C) A plot of the gating variables reveals that the period of hyperpolarization increased the inactivation variable, , in a process called deinactivation. In combination with a reduced potassium conductance due to the reduction in potassium activation, , the deinactivation dramatically reduces the threshold for a sodium spike. This figure was produced by the online code HH_old_anode_break_3plot.m.
Figure 4.9  Subthreshold oscillations following step changes in current. (A) A subthreshold step of current is applied for 50 ms. (B) The membrane potential responds with a brief period of damped oscillations. This figure was produced by the online code: HH_old_sub_thresh.m.
Figure 4.10  Resonant response to a frequency sweep or “ZAP” current. (A) The injected current is given by a sine wave of linearly increasing frequency, where , Hz, Hz and s. (B) With nA, the membrane potential oscillates in response at the same frequency as the applied current, with an amplitude that peaks near 0.8 s when the frequency is 64 Hz. (C) With nA, the membrane potential responds with action potentials when the oscillation frequency is within the resonant range. Notice that at the resonant frequency for subthreshold oscillations (at a time near 0.8 s) the neuron spikes on alternate current peaks because it takes the neuron longer to recover from the large deviation of a spike (bottom) than the smaller subthreshold oscillation (center). This figure was produced by the online code HH_new_zap.m.
Figure 4.11  The Connor-Stevens model. (A) The spike train with an applied current of 850 pA between s and s. The delay to first spike is typical of a type-I model. (B) Type-I behavior is further exemplified by the f-I curve, which increases from zero without any discontinuity of firing-rate. (C) The A-current (,

dotted) becomes stronger than the delayed rectifier current (, solid) shortly after a spike. Deinactivation of the outward Acurrent (b) counteracts deinactivation of the inward sodium current (h), preventing the rebound that causes oscillations in the membrane potential and type-II behavior. (D) Steady states of the gating variables show that at low membrane potential the conductance of the A current can remain high (both and are nonzero at mV). Hyperpolarization can even cause the steadystate of the A-type conductance to increase via deinactivation ( rises) while the conductance of the delayed rectifier current falls dramatically via deactivation ( falls). This figure is produced by the online code CS_figure.m.
Figure 4.12  Deinactivation of T-type calcium channels produces postinhibitory rebound. (A) The current is stepped up every 250 ms from an initial inhibitory current of –100 pA in increments of +50 pA. (B) The membrane potential responds to the first step up to –50 pA with a burst of five spikes, but it does not respond with a spike following the third step from 0 pA to +50 pA. With higher currents the neuron responds with regular (tonic) spikes. (C) The inhibitory currents cause deinactivation of T-type calcium channels (a rise in ) allowing for the current to flow following hyperpolarization (left) but not depolarization (right). For parameters, see tutorial 4.2. This figure is produced by the online code PIR_steps.m.
Figure 4.13  Simplifying a pyramidal cell with a multicompartmental model. (A) The traced structure of a pyramidal cell with a long apical dendrite containing a large fraction of the cell’s surface area (at the top) and other basal branches leaving the soma (at the bottom). (B) The dendritic structure is modeled as five separate compartments connected either to each other or to the soma’s separate compartment. The axon is often not modeled, because once the action potential is generated in its initial segment (which is typically incorporated in the soma in a model) the axon simply transmits the spike with minimal attenuation or modulation. This image first appeared as figure 2.1 in the Book of Genesis, published under the GNU Free Documentation License, and was kindly provided by Dave Beeman, University of Colorado at Boulder.

Figure 4.14  The Pinsky-Rinzel model is a two-compartmental model of a bursting neuron. The two compartments with their intrinsic currents are shown. The soma is similar to the standard Hodgkin-Huxley model with an inward fast sodium current (), an outward delayed rectifier potassium current (), as well as a small leak current () that is mostly outward. The two compartments are connected by an electrical resistor, which conveys a link current () that flows from the compartment with higher membrane potential to the one with lower membrane potential at a given time. The dendritic compartment receives inward calcium current (), whose activation provides the positive feedback needed to generate a dendritic calcium spike. The outward calcium-dependent potassium current () is relatively fast and can terminate a dendritic spike, whereas the outward after-hyperpolarization current () is slower and largely determines the interval between dendritic spikes. The dendritic compartment has its own small leak current ().
Figure 4.15  Intrinsic bursting in a variant of the PR model. (A) The somatic membrane potential produces bursts of highfrequency sodium spikes with an interval of hundreds of ms between bursts. (B) The dendritic membrane potential produces regular broad calcium spikes that are essential for the bursts in the soma. Parameters are given in table 4.7 and equations for rate constants of the gating variables in table 4.8. The code used to produce this figure can be found online as PR_euler_final.m.
Figure 4.16  Dynamics of a single burst. (A) In this burst, eight sodium spikes (solid line) occur within 25 ms. The first sodium spike produces a subthreshold potential rise in the dendrite (dotted line). The second sodium spike initiates a full, broad calcium spike in the dendrite. During the calcium spike the sodium spikes are rapid and do not return fully to baseline. As the calcium spike decays the frequency of sodium spikes gradually decreases until the end of the burst. (B) is the current from the dendritic to somatic compartment. During a burst, there can be a “ping-pong” effect, a shuttling of current back and forth between the two compartments. In this example, the burst is initiated with a net flow of current from soma to dendrite (), while during the burst the prolonged calcium spike in the

dendrite produces a net current flow from dendrite to soma (). (C) Intracellular calcium rises dramatically within a burst and decays between bursts. (Note the extended timescale compared to A and B.) (D) Hyperpolarizing potassium currents activate in the dendritic compartment during the burst. The faster, calcium-activated potassium current, , which is also voltagedependent, brings about the termination of the burst. The slower after-hyperpolarization current, , is also calciumdependent, and after a burst must decay sufficiently before the next burst can commence. Parameters are those of figure 4.15. The code used to produce this figure can be found online as PR_euler_final.m.
Figure 4.17  Ih gating variables. (A) Steady state of the activation variable, , plotted from equation 4.23, increases from 0 to 1 with hyperpolarization between mV and mV. (B) Time constant for the Ih-conductance is in the 100s of ms, which is greater than typical burst durations. This figure was produced by the online code IH_PR_loop.m.
Figure 4.18  Impact of the hyperpolarization-activated current (Ih) on bursting. (A) The somatic membrane potential of a simulated bursting neuron without Ih. (B) Ih is added with nS, causing more than doubling of the burst frequency with little change in burst waveform. (C) The burst contains an extra prespike with increased Ih with nS. (D) For higher values of nS, regular bursting is lost with intermittent periods of prolonged high-frequency spiking. (Left) 4 seconds of bursting activity shown. (Right) A 50 ms zoom-in of a burst, aligned to the upswing of dendritic membrane potential. This figure is produced using a modified Pinsky-Rinzel model (tables 4.8 and 4.9) with an additional Ih current produced using the method of Liu et al.33 This figure is produced by the online code IH_PR_loop.m.
Figure 4.19  Neural activity depends on location of inputs. (A) The model neuron is based on the Pinsky-Rinzel model (figure 4.13) but with three identical dendritic compartments (D1, D2, and D3) instead of just one. (B1–2) Response of the membrane potential in the soma (top) and apical dendrite (bottom), when

regular pulses of applied current are injected simultaneously to all three dendritic compartments. (C1–2) Responses of the membrane potential in the soma (top) and apical dendrite (bottom) when two-thirds of the total current injected to the neuron in B is injected into a single dendritic compartment (the apical compartment, D1). This figure is produced by the online code PR3branch.m.
Figure 5.1  Sketch of a simplified chemical synapse. Vesicles containing neurotransmitter in the presynaptic axon terminal (left) can be docked, such that they abut the presynaptic cell’s membrane and are ready to release neurotransmitter into the synaptic cleft following an action potential. Once they have released neurotransmitter they must be replaced by other nearby vesicles and recycled to make new vesicles. Neurotransmitter diffuses rapidly (in a few nanoseconds) across the synaptic cleft and binds to receptors in the postsynaptic neuron. The receptors shown here are on a dendritic spine, which is a small compartment with a narrow neck attaching it to the dendritic shaft, the main component of the dendrite. (Excitatory inputs to pyramidal cells, the most abundant excitatory cells in the cortex, are predominantly via synapses on spines.) The narrow neck permits flow of electrical current but can restrict diffusion such that spines are partially chemically isolated from each other while they are not electrically isolated.
Figure 5.2  Conductance , following equation 5.4 in response to a series of three spikes at intervals of 50 ms. Parameters are nS, ms, and ms. This figure is produced by the online code synaptic_opening.m.
Figure 5.3  Model of synaptic transmission by graded release. (A) Steady-state conductance is a continuous function of presynaptic membrane potential. (B) Time constant is slow for unbinding of neurotransmitter at low presynaptic membrane potential and fast for binding at high presynaptic membrane potential. (C) Response of the model synaptic conductance (solid) to a single burst of presynaptic spikes (dotted, produced by the Pinsky-Rinzel model, figure 4.16). Parameters for this

figure are nS, mV, mV, ms, and ms. This figure is produced by the online code graded_release.m and the file PR_VS.mat.
Figure 5.4  Connections between cells can be represented by a matrix. In this example, entries of 1 or 0 in the matrix indicate respectively the presence or absence of a connection between two cells. The row number of the matrix corresponds to the presynaptic cell and the column number the postsynaptic cell. (A) A feedforward circuit can be hidden within the connectivity matrix until (B) appropriate rearrangement of neurons reveals an upper-triangular connectivity matrix. (C) Disconnected groups of neurons produce a block diagonal connectivity matrix (D) after appropriate rearrangement. In the transition from C to D, cells 2 and 3 are swapped, so in the connectivity matrix the second and third columns are swapped and the second and third rows are swapped.
Figure 5.5  Examples of connectivity matrices with different network architecture. (A) The globally feedforward network is upper-triangular (no nonzero elements on the diagonal or below it). (B) This locally feedforward network is globally recurrent, because a chain of connections commencing from the first cell eventually returns to the first cell. It is locally feedforward because neurons can be arranged in a circle with connections only directed clockwise. (C) In this recurrent and local network the neurons can be arranged in a circle with connections only to nearest neighbors. (D) In this disconnected network, none of the first four neurons are connected with any of the last two neurons.
Figure 5.6  Connectivity motifs for groups of three neurons. Sixteen connection patterns are possible, differing in the numbers of unidirectional or bidirectional connections or in the pattern of cells so connected. Any other pattern can be rearranged into one of the depicted forms by swapping the positions of one or two pairs of cells (which does not affect the motif, since the positions are arbitrary). Empirical data from rat visual and somatosensory cortices14,15 show that the triply connected motifs on the bottom row are overrepresented compared to chance.

Figure 5.7  Hidden structure revealed by rearranging labels. The connectivity matrix in A contains two disconnected recurrent circuits that can be revealed by reordering the cells as shown in B. The resulting matrix is block-diagonal, meaning that entries are zero apart from specific squares along the diagonal, in this case a 5 × 5 square, then a 4 × 4 square. The two matrices correspond to identical circuit structure, but the particular ordering on the right makes it easier for us to visualize such underlying structure.
Figure 5.8  Images producing bistable percepts. (A) The Necker cube can be seen with its front face as bottom-right or top-left. The 3D percept produced by such a 2D line drawing is experience-dependent—a fifty-year-old who gained sight after being blind from birth did not perceive a cube. We will consider how experience-dependent synaptic strengthening can produce multistable circuits in chapter 8. (B) Rubin’s vase can appear as a single white vase in the center or two black faces looking at each other. Danish psychologist Edgar Rubin created the illusion in 1915.16 (C) The image by Joseph Jastrow in 1899 can appear either as a leftward-facing duck or a rightward-facing rabbit.17
Figure 5.9  (A) The half-center oscillator. Activity of two neurons connected by cross-inhibition can oscillate in antiphase.18 Spike trains are produced by the online code coupled_PIR.m. (B) The triphasic rhythm. Activity of three neurons oscillates with a phase relationship determined by the connectivity pattern. Order of activation is 1–2–3–1– … with inhibition from the active cell suppressing the cell that precedes it in the cycle. Spike trains produced by the online code three_coupled_PIR.m.
Figure 5.10  The phase response curve (PRC). A phase response curve can be produced for any system undergoing periodic activity—i.e., an oscillator (top)—such as a regularly spiking or bursting neuron. If the oscillator is perturbed by an input, such as a brief pulse of excitation (vertical arrow), it can be advanced in phase (a positive phase response, upper curve) or delayed in phase (a negative phase response, lower curve). The phase

response curve (bottom) is produced by measuring the change in phase (horizontal arrows) as a function of the point in the cycle at which the perturbation arises. Regularly bursting and type-II neural oscillators typically have biphasic phase response curves similar to the one sketched here.
Figure 5.11  Phase response curves of simulated neurons. Model neurons in a regularly oscillating state (left) receive a 10 pS pulse of excitatory conductance for 5 ms at different points in their oscillating cycle, with a phase of 0 defined to be at the upswing of a spike or the first spike of a burst. Shaded regions on the right indicate where stable entrainment to a periodic sequence of such pulses is possible. (A) Hodgkin-Huxley model, a type-II neuron with biphasic PRC. (B) Connor-Stevens model, a type-I neuron with only phase-advance. (C) Pinsky-Rinzel model with current applied to the dendrite. While biphasic, the delay portion of the PRC is of longer duration and twentyfold smaller amplitude than the advance portion. (D) Thalamic rebound model with altered parameters to render it regularly oscillating produces a more intricate PRC, with the timing of the pulse with respect to individual spikes within a burst impacting the phase shift, on top of a general delay then advance when input arrives between bursts. These panels are produced by the online codes A: Phase_Response_HH.m, B: Phase_Response_CS.m, C: Phase_Response_IHPR.m, D: Phase_Response_PIR.m.
Figure 6.1  Examples of input-output functions for firing-rate models. (A) Power-law function, with amplitude , exponent and threshold . The square brackets equate to 0 if and to otherwise. (B) An example of a sigmoid, the logistic function, with maximum output, , input for half-max, , and inverse steepness, . (C) Threshold linear response function, with for , for , and for . Saturation is at , responsive range has , and the threshold is (D) Binary response function, with for and for . Saturation is at and the threshold is . This figure is produced by the available online code firing_rate_curves.m.
Figure 6.2  Annotation of equation 6.4. The reduced firing-rate model describes the rate of change of firing rate of each unit in

terms of the sum over all of its inputs from other units.
Figure 6.3  Firing-rate model unit with recurrent feedback. A unit of many neurons (triangles) produces spikes at a mean rate, , which is a function of the total synaptic input, . The total synaptic input, , depends on the mean rate, , via the term , which corresponds to the fraction of open synaptic channels when the presynaptic cells fire at the mean rate, . The fraction of open channels, , which varies between 0 and 1, is multiplied by the maximum effective feedback strength, , to produce the total synaptic input, . The superscript “” in denotes excitatory feedback to excitatory cells in this model.
Figure 6.4  Recurrent feedback strength determines the stable states of a circuit. (A1, B1, C1) Firing rate response of the unit is a sigmoidal function (solid line). For the feedback function (dashed straight line) the firing rate on the y-axis determines the level of synaptic input (x-axis). The stronger the connection strength, the greater the synaptic input produced by a given firing rate, so the shallower the feedback curve. (A2, B2, C2) Dynamics of firing rate as a function of time for the models with rates initialized at zero and a pulse of input between 0.5 s and 0.55 s (gray bar). Note the switch in activity, representing memory of the stimulus in B2. (A) With weak feedback, , only a low-rate state is stable. (B) With medium feedback, , both a lowrate and a high-rate state are stable so a transient stimulus can switch the system between states. (C) With strong feedback, , only a high-rate state is stable. This figure is produced by the available online code bistable1.m.
Figure 6.5  Persistent activity at lower rates via synaptic saturation. When synaptic feedback saturates (dashed curves become vertical at high rates) bistability is possible without the more active state’s firing rate being so unrealistically high. Synaptic saturation occurs at lower rates (C) if the fraction of receptors bound per spike, , is high rather than low. (A1-A2) , . (B1-B2) , . (C1-C2) , . For all curves, and ms. This figure is produced by the available online code bistable2.m and are in the same format with the same stimuli as figure 6.4.

Figure 6.6  Activity states in a circuit with excitatory feedback and short-term synaptic dynamics. (A) Synaptic depression alone can destabilize the persistent activity state and lead to slow oscillations. (B) Synaptic facilitation enhances the stability of a bistable system, so the low-rate state can have activity above 10 Hz. (C) Facilitation and depression combine to generate a bistable system even when synaptic saturation is absent. Format of the figure is the same as that of figure 6.4 and with the same stimulus. This figure is produced by the available online code bistable_fac_dep.m.
Figure 6.7  Connection between integration and parametric memory. (A) Transient stimuli of different amplitudes are provided as possible inputs to an integrator. (B) The rate of change of an integrator depends on its inputs and is zero in the absence of input. The integrator’s output increases at a stimulus-dependent rate to reach a level that depends on the amplitude of the stimulus. The output remains as a constant when the stimulus is removed. Therefore—so long as input duration is fixed and the output is reset to zero before any input —the final output of the integrator provides a memory of the amplitude of its prior input. In practice the output could be simply the mean firing rate or, more generally, any function of the firing rates of a group of neurons. This figure is produced by the available online code integrator_cartoon.m.
Figure 6.8  Decision-making circuit. Two units, that are each separately bistable via recurrent excitation of strength , are coupled by cross-inhibition of strength . If one unit becomes active it suppresses the activity of the other unit in a “winnertakes-all” manner, with the active unit representing the decision. If noisy inputs and are applied to units 1 and 2 respectively, the unit with greater mean input is more likely to become the active one.
Figure 6.9  Dynamics of a decision-making circuit. (A) Upon stimulus delivery, the firing rate of unit 1 (solid) rises gradually over the course of almost a second until the threshold rate of 40 Hz is reached. The firing rate of unit 2 (dotted) rises initially, but is then suppressed by inhibition from unit 1. The gray shaded

region indicates the time of input, during which unit 1 receives slightly greater excitatory input than unit 2. (B) The firing rate of unit 1 on four successive trials demonstrates considerable variability in the time to reach threshold. (C) The mean rate as a function of time from onset of the stimulus. (D) The mean rate as a function of time aligned to the threshold-crossing point— the curve of unit 1 is steeper, and the suppression of the rate of unit 2 is clearer, than in C (dotted). These figures are produced using the circuitry of figure 6.8, by the available online code decision_making_dynamics_figure.m.
Figure 6.10  The speed-accuracy trade-off. (A) The fraction of trials in which unit 1 reaches threshold first increases in a sigmoidal manner from 0 to 1 as the stimulus difference, , increases. With reduced input conductance (solid line versus dashed line) a smaller stimulus difference is required for accurate responses. (B) The cost of improved accuracy is a longer time to reach threshold when input conductance is weaker (solid line versus dashed line). Responses are slowest when the stimuli are most equal. These figures are produced using the circuitry of figure 6.8, by the available online code decision_making_vary_g_stim.m.
Figure 6.11  Two-unit oscillator. An excitatory unit (E) and an inhibitory unit (I) are coupled to produce an oscillator. The Eunit has strong self-excitation but also receives disynaptic inhibitory feedback, because it excites the I-unit, which in return inhibits the E-unit. The inputs, and , determine the excitability of the units, and can be altered to switch on or off oscillations and control their frequency.
Figure 6.12  Gamma frequency oscillations via a PING mechanism. The firing rate of excitatory cells (solid) rises because of recurrent self-excitation. The inhibitory cells (dotted) are strongly excited, so rise with little delay. The inhibitory firing is so strong it proceeds to shut down the excitatory cells, after which inhibitory firing decays. The behavior shown here is only possible if units comprise many cells, because the mean firing rate of excitatory cells shown here is 13 Hz, below the oscillation frequency of 40 Hz. This ratio means that any individual cell

fires spikes on only about 1/3 of the gamma cycles. This figure is produced by the available online code oscillator_single_stim.m.
Figure 6.13  High-frequency oscillations in an all-inhibitory network with delayed feedback. (A) A single unit comprising inhibitory cells with feedback can generate oscillations if a delay is included between the firing rate and the inhibitory synaptic feedback. Such a delay arises from a summation of the time for action potential propagation, the time for dendritic input current to reach the soma, and a rise time for maximal synaptic channel opening to be reached following vesicle release. These separate components can be included individually in spiking models. (B) Firing rate oscillations from such an inhibitory circuit at a rate primarily determined by the synaptic feedback delay, . Results are produced by the available online code ineuron_oscillator_single.m with ms.
Figure 6.14  Background input affects the frequency and amplitude of an oscillating circuit. (A) The frequency of oscillation increases within the gamma range, from below 30 Hz to 50 Hz, as excitatory input to the excitatory unit is increased. (B) The amplitude of oscillation, in particular of the inhibitory cells, rises as more inhibition is needed to overcome the activity of the E-unit when the latter receives greater input. (C) The mean firing rates of both units increase as the oscillation amplitude increases, though mean firing rate of the E-unit remains below the oscillation frequency, an indication that individual cells fire spikes on intermittent gamma cycles. This figure is produced by the available online code EI_oscillator.m.
Figure 6.15  Feedforward excitatory input from thalamic cells increases with contrast across all orientations. (A) The circular receptive fields of five LGN neurons are indicated as circles. A bar of light (rectangle) at low contrast (top) or high contrast (bottom) causes those neurons to become active (lighter circles indicate greater activity) when passing through the center of the receptive fields of those LGN neurons. The vertically orientated ellipse indicates the receptive cell of a cortical neuron that receives feedforward input from the three LGN neurons within the ellipse. Importantly, when the bar of light appears at the null

orientation at high contrast (bottom-right), the central, highly active LGN neuron could provide as much input to the cortical neuron as the three aligned less active LGN neurons when the bar of light is at the preferred orientation but at low contrast (top left). (B) The feedforward excitatory input from the LGN to a cortical neuron scales up across all orientations with increasing contrast. Given that the cortex receives only excitatory projections from the LGN, panel B shows how the total input from LGN to cortex depends on orientation. If a cortical neuron’s activity were determined by this input alone, then its tuning curve would broaden as input increases, because the range of orientations that produce input above any fixed threshold would increase. Such an “iceberg effect” is not observed in neural recordings.
Figure 6.16  Circuit motifs for investigation of contrastinvariant orientation selectivity in V1. (A) Feedforward excitation to two excitatory cells (labeled E) with opposite orientation preferences. Tuning curves of excitatory cells are unaffected by cortical connections. (B) Feedforward excitation and inhibition to excitatory cells. Inhibitory input to excitatory units is from interneurons (labeled I) receiving oppositely tuned LGN input. (C) Feedforward and feedback excitation and inhibition. Feedback is via recurrent excitatory and inhibitory connections from similarly tuned cells within the cortex. (A–C) In all figures, just two pairs of cells are shown, whereas the complete set would be arranged according to orientation preference in a ring, with all orientations present. Only the strongest connections are shown, yet in all models the local connections also spread to neurons with similar, not just identical or opposite, orientation preferences, as shown in figure 6.17.
Figure 6.17  Structured connectivity in a ring model. (A) Structured excitatory-to-excitatory network. The strength of recurrent excitatory connection from neuron to neuron , is in a ring model, producing cyclic structure. For an orientation selectivity model the preferred input of unit is given by (since rotation by produces an identical stimulus). For a spatial location model (such as head direction, where rotation by results

in no change) the preferred input of unit is given by . In the example shown (used to produce the results of figure 6.20 and found in the code ring_attractor.m) the mean connection strength, where the number of units, . (B) Structured inhibitoryto-inhibitory network. In an alternative circuit containing only inhibitory neurons that are spontaneously active, crossinhibition can produce a similar connectivity profile and similar network behavior. The strength of inhibitory connection from neuron i to neuron j, is , which is of maximal strength (but negative) to neurons of opposite tuning. In this example, the mean connection strength is and other parameters are defined as in A.
Figure 6.18  Pinwheel arrangement of neurons in V1. The figure represents an area of cortex containing numerous neurons shaded by their preferred orientations, with two pinwheel centers (marked by crosses) contained within the region. Neurons are arranged by orientation preference (indicated by labels on the figure) counterclockwise around the left pinwheel center and clockwise around the right one. Brightness of shading indicates the absolute value of a neuron’s preferred orientation, which ranges from – to . The pinwheel arrangement observed in many mammals33,34 is efficient in ensuring the neurons most likely to be connected with each other in a ring model—those with similar orientation preferences—are nearby each other in the physical space of V1. This figure was made with the online code pin_wheel.m.
Figure 6.19  Local excitation and surround inhibition produced in the ring model. (A, B) Broad excitation to excitatory cells (A) and to inhibitory cells (B) decays with tuning difference. (C) Inhibition from inhibitory cells decays with tuning difference to the inhibited excitatory cell. (D) Combining the effects of E-to-I (B) and I-to-E (C) through convolution, produces a disynaptic inhibitory effect from excitatory cells to excitatory cells that does not drop to zero for oppositely tuned cells. Convolution takes into account and sums together every combination of two successive differences in tuning (labeled on the x-axes of B and C) to produce each single resulting difference in tuning on the xaxis of (D). (E) The sum of direct E-to-E (A) and indirect E-to-E

(D) produces the total effective connectivity between excitatory cells, with net excitation between similarly tuned cells and net inhibition between oppositely tuned cells. This figure was produced by the online code ring_attractor.m.
Figure 6.20  Stationary and nonstationary states in a ring model. (A) Both inactive and active states are stable. When a cosine stimulus with peak centered at unit 25 is applied at a time of 2 s and is removed at a time of 4 s, the model produces a bump of activity as in the orientation selectivity network. However, after the stimulus is removed, the bump of activity remains in place. (B) Only a stationary active “bump” state is stable. Increased excitatory feedback causes a bump to form in the absence of a stimulus. The same stimulus used in A causes the bump of activity to shift toward the peak of the stimulus. (C) Only a moving “bump” state is stable. Asymmetric inhibitory feedback suppresses one side of the bump more than the other, causing it to drift with constant velocity over time in the absence of any stimulus. Firing rates of the excitatory neurons are shown. This figure is produced by the online code ring_attractor.m.
Figure 6.21  Angular integration in a ring attractor network. Angular velocity is approximately a linear function of input bias, so that position of the bump is approximately the integral of input bias over time. The ring network with only inhibitory neurons39 (figure 6.17B) produces integration over a wider range of angular velocities than the standard excitatoryinhibitory ring network (figure 6.19). The code to produce this figure can be found online as angular_integration.m.
Figure 7.1  Stable and unstable fixed points in a single-variable firing rate model. (A) A plot of against , where
is obtained from a firing-rate model with recurrent feedback of strength (see section 6.3). In this example, there are three fixed points (denoted by circles where ). Fixed points separate ranges of where (shaded gray) from ranges where (unshaded). Arrows on the x-axis indicate the direction of change of (rightward to increase where and leftward to decrease where ). The solid circles are stable fixed points, around which the direction of is

toward the fixed point, while the open circle at 50 Hz is an unstable fixed point, around which the direction of is away from the fixed point. The system is bistable and can be a basic memory unit. (B) If the differential equation is simulated with multiple starting points, the trajectories of move away from the unstable fixed point and toward one of the stable fixed points. Notice that since depends only on , if one were to take any horizontal line of fixed , the gradient of the trajectories crossing that line would all be the same (fixed at a fixed ). Parameters used are ms, nA/Hz, nA, nA. This figure was generated by the online code drdt_plot.m.
Figure 7.2  Increase of feedback strength causes loss of low firing-rate fixed point. (A) The bistable system of figure 7.1. (B) With increased feedback, , becomes more positive, so the curve shifts up and the lower fixed points move closer together. (C) At a critical value of feedback the lower two fixed points collide. (D) With even greater feedback, only one fixed point remains. These curves were produced using the online code drdt_plot_manyW.m.
Figure 7.3  A bifurcation curve shows the stable and unstable states as a function of a control parameter. The feedback strength, , controls whether the system has a single stable state of low firing rate (at low ), a single stable state of high firing rate (at high ), or two stable states with an intermediate unstable fixed point (at intermediate ). Two bifurcations occur with increasing , one at the appearance, the other at the disappearance of the unstable fixed point. The x-axis crossingpoints of the curves in figure 7.2 produce points on this curve: three points at feedback strengths of and (figure 7.2A, B); one point of high rate at (figure 7.2D), with (figure 7.2C) corresponding to the bifurcation point, at the value of where the two lower fixed points collide. This figure was generated by the online code bifurcation_varyW.m.
Figure 7.4  Nullclines showing the fixed points of a bistable system with threshold-linear firing-rate units. The nullcline for is shown as a dotted line, and for as a solid line. The nullclines cross at five fixed points, three of which are stable (solid circles)

and two of which are unstable (open circles). The axes are extended to negative rates—which can never be reached in practice—so that the complete “Z” shape of the nullclines can be seen. The shapes of the curves are similar to reflections of figure 7.3 (reflected, because rate of one unit provides negative input to the other). In this case, the fixed points all lie on one of the axes. The online code nullcline_bistable.m was used to generate this figure. It can be altered so that the parameter-dependence of the nullclines and fixed points is easily studied.
Figure 7.5  Vector fields indicate the dynamics of a system on a phase plane. (A) The multistable system shown in figure 7.4, with arrows indicating how the firing rates change together. Depending on the starting point, the system moves to one of the three stable fixed points (solid circles). (B) With adjustment of thresholds and connection strengths, with one excitatory unit of rate , and one inhibitory unit of rate , the nullclines cross at a single point. In this case an unstable fixed points is produced, around which the firing rates oscillate to produce an orbit in the phase plane. This circuit is equivalent to the one that produced PING oscillations (figures 6.11–6.12 and tutorial 6.3), although here the neural responses are threshold-linear rather than sigmoidal. In both panels, the nullcline for , where , is the solid line, while the nullcline for , where , is the dotted line. The code to produce this figure can be found online as vector_field.m.
Figure 7.6  The surprising response to inputs of an inhibitionstabilized network. (A, B) The circuit consists of two units, one excitatory and one inhibitory, with strong excitatory (solid triangles) and inhibitory (solid circles) reciprocal connections. In A, there is no external input; in B, there is inhibitory input to the inhibitory unit. (C, D) The nullclines and vector fields are plotted as in figure 7.5. Notice the similarity to the oscillator of figure 7.5B, yet in this case the fixed point where the nullclines cross is stable. In D, the inhibitory input lowers the nullcline for (dotted), that is, reducing for any fixed level of , but with the consequence that the crossing point of the two nullclines is raised, producing increased as well as increased . (E) The firing rates settle at the fixed point in the absence of external input. (F) The firing rates settle to the higher fixed point (increased rate

for both units) when the inhibitory unit is inhibited. The code to produce these results is vector_field.m. Parameters are , , , , Hz, Hz, with Hz for both units, ms and otherwise linear f-I curves as in equation 7.4.
Figure 7.7  Alternating visual percepts are produced by ambiguous images. (A) The Necker cube can be perceived with either the upper square or the lower square front-facing. (B) Either a white vase or two black faces are perceived (first created by the Danish psychologist Edgar Rubin in 1915).12 If we gaze at either image, our percepts alternate.
Figure 7.8  Transitions between attractor states in a bistable system. (A) An adaptation current is added to the winner-takesall network of figure 6.8. The time for the adaptation current to build up in the highly active unit depends on that unit’s firing rate, while the time for the adaptation to decay in the silent unit is equal to the time constant (of 250 ms in this example). The resulting switching of the dominant unit is periodic, with nearidentical state durations (D). (B) Instead of an adaptation current, external noise is added to both units in a bistable winner-takes-all network. State durations (times between transitions) are highly variable, producing an exponential distribution (E). (C) A combination of weaker adaptation current and weaker noise produces variable state durations. Very brief state durations are rare because the adaptation must build up enough for the noise to have a significant chance of causing a transition. The resulting shape of the distribution of state durations (F) matches that of percept durations during perceptual rivalry tasks.20 (D–F) The distribution of the number of occurrences of different state durations over, in each panel, a 10,000-s trial of the activity shown in A–C respectively. The code to generate this figure is available online as bistable_percept.m.
Figure 7.9  The FitzHugh Nagumo model. The cubic -nullcline (solid line) separates regions where (unshaded) from regions where (shaded gray). When the system is oscillating, (dashed trajectory) it changes slowly when its variables lie on or near the -nullcline, but it jumps rapidly (horizontal arrows) between the

high- and low- branches of the nullcline when the system reaches the bifurcation points (maxima and minima of the nullcline). In this manner, the membrane potential cycles around the range of adaptation, , where, if were fixed, would be bistable. This figure was produced by the available online code FHNmodel.m.
Figure 7.10  The FitzHugh-Nagumo model behaves like a typeII neuron. (A) With small applied current, , a single spike occurs, followed by subthreshold oscillations. (A1) Nullclines are shown with solid and dotted. The trajectory of versus , dashed, depicts the spike as a single excursion jumping between stable sections of the V-nullcline, but ultimately resting at the stable fixed point (solid circle) where the two nullclines intersect. (A2) Membrane potential versus time and (A3) adaptation variable versus time for the same applied current of . (B) With a larger applied current, , the -nullcline shifts up to higher values of (B1) and regular spiking occurs (B2), with repeated loops between the higher and lower branches of the -nullcline followed. Axes of B1–B3 are identical to those of A1–A3. (C) With very large applied current, , the -nullcline intersects the -nullcline on its upper branch, producing a stable fixed point at high membrane potential. Spike-like oscillations are no longer possible. Axes in C1–C3 are shifted to higher values of . The figures depict solutions of equation 7.7, with parameters mV, mV, mV, mV−2, ms, ms, and the applied currents shown. This figure was produced by the available online code FHNmodel.m.
Figure 7.11  Example of a saddle point in a circuit with crossinhibition. (A) Circuit architecture with cross-inhibition between two units similar to that of a decision-making network (section 6.5) (solid circles = inhibitory connections; solid triangle = excitatory connection). (B, C) Nullclines cross in three places with stable fixed points shown as solid circles and the intermediate unstable fixed point, which is a saddle point, as an open circle. Arrows indicate direction of change of the system’s firing rates. Dashed lines are trajectories, which initially approach the saddle point then deviate to one of the stable fixed points. (D, E) The firing rate curves that correspond to the trajectories in B and C respectively. Note the period of slower

rate of change and the nonmonotonic variation of one of the firing rates, a hallmark of approach toward and then departure from a saddle point. The code to produce these results is vector_field_saddle.m. Parameters are , , , Hz, Hz, with Hz for both units, ms and otherwise linear f-I curves as in equation 7.4.
Figure 7.12  Chaotic behavior of a circuit of three firing-rate model units. (A) Circuit architecture is based on two coupled oscillators, with units 1 and 2 forming one excitatory-inhibitory oscillator and units 3 and 2 forming a separate one. Arrows represent excitatory connections, solid balls represent inhibitory connections. (B) Oscillations of units 1 and 2 grow in amplitude, but are reset by irregular, transient bursts of firing of unit 3. (C) The initial conditions are altered by less than one part in 1,000— the initial rate of unit 1 is shifted from 15 Hz in (B) to 15.01 Hz in C. The system begins to follow almost the same trajectory as in B, but gradually diverges. Once a time of 1 second has passed, the system’s state in C is completely uncorrelated with its state at that time in B, and the time of bursts of activity of unit 3 in C cannot be predicted from the prior simulation in B. The code producing this figure is available online as chaotic_3units.m.
Figure 7.13  Chaotic neural activity in high dimensions. Firing rate as a function of time is shown for three excitatory units selected from a balanced network of 200 excitatory and 200 inhibitory units with sigmoidal firing rate curves and random, sparse connections between units. Unlike figure 7.12, no vestige of regular oscillation remains in the chaotic activity. (A) Unperturbed activity with initial rates evenly spaced in the range from 0 to 100 Hz. (B) Perturbed activity with the initial rate of a single unit shifted from 0 to Hz, while the initial rates of other units remain identical to those in A. The activity in B initially appears unchanged from that of A, but deviations are visible after a few hundred milliseconds. The timescale of divergence is independent of the particular unit that is perturbed initially. The code producing this figure is available online as highD_chaos.m.
Figure 7.14  The divergence of firing rates grows exponentially following a small perturbation in a chaotic system. (A) The mean of the absolute difference in firing rates of all excitatory units of

the network used to produce figure 7.13, is plotted as a function of time. The initial perturbation of Hz in one unit out of 200 corresponds to a mean change of Hz across all excitatory units. The change in network activity appears negligible until after 200 milliseconds the exponential growth brings the change up to the size of the system. (B) The logarithm (base 10) of the plot in A reveals the exponential period of growth as a straight line. The code producing this figure is available online as highD_chaos.m.
Figure 7.15  Propagation of activity as “avalanches” indicates whether a network is in the critical state. (A) In a subcritical network, the effect of each spike is to produce, on average, less than one extra spike in all other neurons, so avalanches typically decay. Two avalanches are shown, one of size 2 and duration 2 (on the left), the other of size 1 and duration 1 (on the right). (B) In the critical state, each spike causes, on average, exactly one extra spike in another neuron, so avalanches can propagate or decay quickly. Two avalanches are shown, one of size 5 and duration 4 (on the left) and one of size 2 and duration 2 (on the right). (C) In a supercritical network, each spike causes on average more than one extra spike in other neurons, so avalanches commonly propagate and can extend to the full size of the system. The two avalanches initiated in the first timestep merge and quickly reach the system’s size.
Figure 7.16  Analysis of avalanche data from a simple “birthdeath” model of neural activity. (A) A log-log plot of the number of avalanches as a function of avalanche duration. Gradient of the straight line yields , the exponent of the power law. (B) A log-log plot of the number of avalanches as a function of the total size of the avalanche. Gradient of the straight line yields , the exponent of the power law. (C) A log-log plot of the mean size of avalanches as a function of their duration (i.e., x-values of panel B, plotted against x-values of panel A). Gradient of the straight line yields , the exponent of the power law. However, in this case the scaling relation of a critical system is not quite satisfied, as . (D) The instantaneous size as a function of time is calculated for all avalanches, which are combined according to duration into five distinct groups. (E) The shape of the profile is similar for all avalanches, as seen by replotting the data shown

in D, with the timescale normalized by the duration, , (i.e., ) and after the size of each avalanche is scaled by a power-law of the duration (i.e., ), in which . (F) The scale-free behavior is seen more clearly by using a fitted value of , , obtained from the mean value of each curve. The code that generated this figure is available online as avalanche_data.m.
Figure 8.1  Alternative forms of Hebbian plasticity. (A) Potentiation only (symmetric). If both the presynaptic and the postsynaptic cells’ activities are above-threshold, then connections between the two cells strengthen. Otherwise no change in synaptic weight occurs. To prevent runaway excitation a global normalization is needed. (Rule 1, tutorial 8.1) (B) Potentiation and depression (symmetric). Potentiation occurs as in A, but if one neuron’s activity is above-threshold while the other neuron’s activity is below-threshold, then any connection between the two cells weakens. If both neurons have low activity there is no change (rule 2, tutorial 8.1). (C, D) Potentiation and depression (asymmetric). Potentiation occurs as in A. Depression occurs only if the presynaptic cell’s activity is abovethreshold while the postsynaptic cell’s activity is belowthreshold in C, while depression occurs only if the postsynaptic cell’s activity is above-threshold while the presynaptic cell’s activity is below threshold in D. This figure is produced by the online code plasticity_four_rules.m.
Figure 8.2  Plasticity rule with quadratic dependence on postsynaptic firing rate. The crossover from synaptic depression to potentiation occurs when the postsynaptic firing rate increases above the threshold rate, which is 15 Hz in this example. In some models the threshold rate can vary slowly over time, increasing if the postsynaptic cell has a long-term high firing rate and decreasing if the postsynaptic cell has a long-term low firing rate. Such metaplasticity, as proposed in the BCM model,48 was an early suggestion of homeostasis (see section 8.6) to counter the instabilities of Hebbian plasticity. This figure is produced by the online code quadratic_plasticity_figure.m.
Figure 8.3  Pattern completion and pattern separation of corrupted exemplars following Hebbian learning. Each small

square in the 17 × 17 array corresponds to a unit. The coordinated activity of a set of units can be entirely produced by input (top row) or, after learning, can represent a particular memory stored in the connections between units (bottom row). In this example, inputs generate an activity pattern with approximately 80 percent overlap with one of the “ideal” templates (20 percent of units have their input switched sign, top row). After learning, the structure of feedback connections within the network causes the units to retrieve the activity corresponding to the closest “ideal” template (bottom row). That is, the network has learned an “ideal” template, even though the exact template was never presented during Hebbian learning— the corrupted exemplars were sufficient. See tutorial 8.1. This figure was produced by the online code Tutorial_8_1.m.
Figure 8.4  Spike-timing dependent plasticity. (A) The amplitude of excitatory postsynaptic current (EPSC) can change following a pairing protocol. The change in amplitude can be sustained for tens of minutes or many hours following the protocol (the experiments are in vitro, so there is a time limit for sustaining healthy cells). The protocols comprise several minutes of combined presynaptic spikes and postsynaptic depolarization or hyperpolarization or spiking, and they last several minutes. Depending on the protocol long-term potentiation (LTP, an increase in evoked EPSC) or long-term depression (LTD, a decrease in evoked EPSC) can be produced. (B) In a protocol using 10 Hz trains of presynaptic spikes (a spike every 100 ms) the change in synaptic strength between two excitatory cells is positive if the postsynaptic cell is caused to spike within 20 ms following each presynaptic spike (prebefore-post), and is negative if it is caused to spike in a similar time window preceding each presynaptic spike (post-beforepre). The dependence on the time difference between the two spikes can be fitted as two exponentials. Such spike-timing dependent plasticity demonstrates a causal requirement for a change in connection, a requirement that was present in Hebb’s original postulate.1
Figure 8.13  Rate-dependence of STDP with nearest-neighbor rule. A plot of using equation 8.16, using parameters , , ms, Hz,

and variable postsynaptic rate, . Notice the similarity in shape with figure 8.2. This figure was produced by the online code STDP_Poisson_nn.m.
Figure 8.5  Continuous update rule for STDP and triplet-STDP (3-STDP). Synaptic weights can increase (LTP) at the time of a postsynaptic spike (row 2) and decrease (LTD) at the time of a presynaptic spike (row 1). In STDP (row 6) the amount of LTD is given by (row 3), which depends on the time since the prior postsynaptic spike, whereas the amount of LTP is given by (row 4), which depends on the time since the prior presynaptic spike. In triplet STDP (bottom row) the amount of LTP is also multiplied by an extra factor, (row 5), which depends on prior postsynaptic spikes. This leads to minimal LTP at the time of the second postsynaptic spike but enhanced LTP at the time of the third postsynaptic spike that shortly follows.
Figure 8.6  Encoding of sequences and paired associations using triplet STDP. Four excitatory neurons are stimulated repeatedly by external input (gray shaded rectangles), either in sequence (A1) or simultaneously as pairs of two neurons (B1). During training the connections between the four neurons undergo synaptic plasticity using a mechanism of triplet STDP (figure 8.5 and equation 8.6). After training, stimulation of a single neuron (single gray shaded rectangle) causes a replay of the sequence (A2) or of activity in the neuron paired with the stimulated neuron (B2) according to the prior training. These responses arise because triplet STDP can encode asymmetric synaptic weights in a unidirectional pattern following sequence training (A3), or autoassociate the paired neurons in a symmetric, bidirectional pattern following paired training (B3). This figure is based on the works of Pfister, Clopath, Busing, Vasilaki, and Gerstener.39,41 The code used to generate this figure is available online as STDP_recurrent_final.m. Feedback inhibition is used in the code to enhance stability and it is worth noting that following further training in pattern B, the pairs of neurons would maintain activity in the delay between stimulations. Thereafter the sequence of stimulations lacks a silent period of activity between pairings, so the pairs of neurons connect with each other in sequence (as in A) until eventually all

neurons are highly connected. Therefore, the simple model used here is ultimately unstable.
Figure 8.7  NMDA receptor activation as a temporal-order detector. (A1) If the presynaptic spike precedes the postsynaptic spike, the transient postsynaptic depolarization (solid line, scale on right axis) overlaps with the period of glutamate binding to the NMDA receptor (dotted line, scale on left axis). (B1) As a result, NMDA receptors open rapidly to admit calcium. (C1) Calcium concentration then rises rapidly to a high level in a manner that could lead to LTP. (A2) If the postsynaptic spike precedes the presynaptic spike, then the rapid membrane depolarization is over before glutamate binds to the NMDA receptor. (B2) Receptors open only partially. (C2) Calcium concentration rises slowly to a lower peak value in a manner that could lead to LTD. This figure is produced by the online code NMDA_activation.m.
Figure 8.8  Example of the need for homeostasis in a neuron that responds to coincident inputs. (A1–2) Synaptic inputs 1 and 2 to the cell. These should not produce a response individually (first spike in each panel) but only when they arrive together (second spike in each panel). (B, E) If synaptic conductance is too low (B) or A-type potassium conductance too high (E), then the neuron never responds. (C, F) At the standard level of synaptic and A-type potassium conductance, the neuron responds with a spike only to coincident inputs. (D, G) If synaptic conductance is too high (D) or A-type potassium conductance is too low (G), then the neuron responds to every input, even single ones. Hebbian plasticity can rescue neither the unresponsive case (B, E) nor the overly responsive case (D, G). Figure produced by the online code CS_3spikesin.m.
Figure 8.9  Need for homeostasis to produce intrinsic bursts. The somatic membrane potential , indicates the pattern of neural firing in the Pinsky-Rinzel model (section 4.7) when the maximum calcium conductance in the dendrite is: (A) μS, or (B) μS, or (C) μS, or (D) μS, or (E) μS (where is relative dendritic area). Here, mean membrane potential is nonmonotonic in the maximum calcium conductance (with lowest mean membrane

potential in panel C) so a feedback mechanism based on the membrane potential or firing rate would not be able to return dendritic calcium channel density to its ideal level (panel C) following all deviations. For example, panels A and D are similar, but require opposite homeostatic responses. This figure was produced by the online code PR_euler_5cellversions.m.
Figure 8.10  Need for homeostasis to control frequency of an intrinsically bursting neuron. The frequency of bursting depends on the maximum after-hyperpolarization conductance, a potassium conductance in the dendrites, with values of: (A) nS; (B) nS; and (C) nS, where is the relative dendritic area. In these examples the mean potassium current across the dendritic membrane decreases (because bursts are less frequent) even as this particular potassium conductance increases. The figure is produced by the online code PR_euler_3cellversions_freq.m.
Figure 8.11  The weather-prediction task: learning to respond in an uncertain environment. The task is a two-alternative forced choice task that could be considered as similar to the choice of clothing to wear based on a prediction of the day’s weather—often an uncertain affair. In each trial of the task, one of the responses is considered correct and rewarded. Depending on the correct response, each cue appears with a given probability. For example, cue A appears with 95 percent probability if choice 1 is correct but only with 5 percent probability if choice 2 is correct, while cue E appears with 5 percent probability if choice 1 is correct but 95 percent probability if choice 2 is correct. Cue C is uninformative, appearing with 50 percent probability, whichever choice is correct. Cues B and D are intermediate, with probabilities of 75 percent versus 25 percent and 25 percent versus 75 percent respectively if choice 1 versus choice 2 is correct. In the standard version of the task each choice is correct with 50 percent probability on any trial. The model circuit used to solve the task —i.e., to produce correct choices given the uncertain cues—relies on reward-dependent plasticity at the excitatory synapses between the cue-responsive units and the decision-making units. Initially the noise in the decision-making circuit generates random choices (the exploration stage), but the synaptic

plasticity causes biases so that eventually the response is dominated by activity of units responsive to the most predictive cues.
Figure 8.12  Simplified model of cerebellar eyeblink conditioning. (A) The conditioned stimulus (CS) is typically a tone, which stays on during a trial. The unconditioned stimulus (US), a puff of air into the eye, arrives a fixed time interval later. (B) In the simplified model circuit, the CS provides input to excitatory Granule Cells (GrC), which are in a recurrent circuit with inhibitory Golgi cells (GoC), and which provide input to the inhibitory Purkinje cell (PC) via parallel fibers. The US, as well as producing the automatic response of eyelid closure, also initiates the reinforcement signal for learning. To this end, it generates transient activity in the excitatory cells of the red nucleus (RN), whose activity is transmitted via the climbing fiber (CF), which winds around the dendrites of the Purkinje cell. The CF provides a reinforcement signal to the synapses from the Granule cells to the Purkinje cell. The output of the anterior interpositus nucleus (AIN) causes the conditioned response, when a temporary pause in the Purkinje cell’s activity causes disinhibition, while it is receiving activity from the granule cells (model connections from GrC to AIN are not present empirically, but in this simplified model, they are necessary to ensure the AIN can only be active when the CS is on). The model is based on the work of Medina et al.60
Figure 9.1  Separation of neural spikes by PCA. (A) When a spike is detected, the waveform is characterized by a number of values of the voltage (small circles), which together describe the characteristic shape produced on each electrode. Three spikes are depicted, each spike characterized by fifteen values on each of two electrodes, which would produce a single point in thirty dimensions for each spike. In this illustration, spikes 1 and 3 are similar because they arise from the same neuron, whereas spike 2 is significantly different because it arises from a different neuron. (B) Example traces of the multiple waveforms produced by several hundred spikes on a single electrode implanted in rat gustatory cortex. These traces group around two distinct forms, so most likely arise from two distinct neurons. Notice that the

voltage trace, dependent on electric fields outside the neurons, can have a very different shape from the membrane potential of the neuron producing a spike. (C) Each spike appears as a dot in high-dimensional space. The spikes separate into two clusters, representing the two putative neurons. Following PCA, the separation of the clusters is visible here when just the first two principal components of the data are plotted against each other. Forty data points per spike from a single electrode were used for this analysis. (Data provided by the laboratory of Don Katz, Brandeis University.)
Figure 9.2  Selecting new axes with principal component analysis. (A) Each data point corresponds to a specific measurement and comprises a pair of firing rates, and . (B) The axis corresponding to the first principal component, , contains the most variance in the dataset. The second axis, , is perpendicular to the first.
Figure 9.3  Projection of data. (A) A set of data points in two dimensions can be projected to a single dimension in multiple ways. (B–D) Projection of each data point onto an axis is shown as a dashed line. The resulting set of projected data falls on a single line, so each data point is represented by a single number, its y-value in B, its x-value in C, or the scaled difference between the two in D.
Figure 9.4  Eigenvectors and eigenvalues. (A) The matrix has an eigenvector directed along the x-axis with an eigenvalue of 2, because multiplication of that eigenvector by the matrix simply scales the vector by a factor of 2. The second eigenvector, , is perpendicular to the first, directed along the y-axis, and has an eigenvalue of 1. Notice that the eigenvectors could equally have been chosen to be in the opposite direction, e.g., is an eigenvector, and they would still be perpendicular, each with an unchanged eigenvalue. (B) The matrix has eigenvectors rotated by 45°. Since eigenvectors should be of unit length, the actual eigenvectors are and , with eigenvalues of 3 and 1 respectively. Notice that when the matrix multiplies a vector that is not an eigenvector, the result is not a scalar multiple. e.g., means a

vector along the x-axis is rotated clockwise as well as scaled by the matrix.
Figure 9.5  Detecting a change in firing rate. (Top) Each vertical line indicates the time of a spike generated by a Poisson process whose rate changes from 5 Hz (white region) to 10 Hz (gray region) at a time of 6 s (the change point). (Bottom) The log likelihood ratio is the log of the probability of the spikes being produced by a Poisson process whose rate changes at that given point of time divided by the probability of those spikes being produced by a process whose rate does not change. In practice, it is rare that the log likelihood ratio exceeds 4 in the absence of a rate change. This figure was produced by the online code single_jumprate_detect.m.
Figure 9.6  Example of a hidden Markov model. A model with four states (ovals), as shown, would be defined by a 4 × 4 transition matrix, indicating the probability per time bin of a transition from one state to another (represented as arrows between ovals). In models of neural activity, the strongest transition is found to be from a state to itself, indicating the persistence of a state across many time bins. Each state is defined by the probability of spike emissions, defined by the firing rate of each cell. In this example, rates of five neurons (labeled A–E) define the states and are depicted as histograms. The set of rates is unique to each state.
Figure 9.7  HMM extracts trial-specific state sequences. (A) Sketch of spike trains from two cells, which respond preferentially to one of the two percepts that arise when a stimulus induces the bistable state. On separate trials the duration of percepts and the initial percept can vary. (B) A hidden Markov model of the system would, most likely, extract three states (indicated by the shade of gray), one prestimulus and poststimulus, the other two corresponding to the distinct activity states present during the distinct percepts when the stimulus is present. The firing rates, shown as continuous bold lines, transition between distinct values, one value for each cell within each state (see figure 9.6). State transitions are evaluated probabilistically, with short periods without shading between

states representing periods when the model does not predict a single state’s presence with high probability. (C) A peristimulus time histogram (PSTH), like other standard analyses, which begin by averaging data across trials aligned to stimulus onset, would obscure evidence of these state transitions.
Figure 9.8  Algorithm for decoding position from place fields. (A) In stage 1, a place field (bottom) is calculated for each neuron, by combining the observed position of the animal as a function of time (trajectory, top left) with the spikes emitted by a neuron (dots, top right). (B) In stage 2, the position at each time point is estimated as a probability distribution, which is updated every timestep. An initial prior distribution is assumed to be flat, with no evidence for a particular location of the animal (top). If a neuron emits a spike (left), the neuron’s place field is multiplied by the prior, to obtain an updated estimate of the animal’s position (bottom left). The updated estimate becomes the new prior in the next timestep. When a neuron does not emit a spike (right), the estimate of position is also impacted, and a new prior is produced (bottom right). In this figure, the darker shading of a region indicates the greater the firing rate of the neuron when the animal is at that location in A, and the greater the probability of the animal being at that location in B.
Figure 9.9  Decoding of position using the spikes produces by place-cells. (A1–A4) Circular place fields of simulated cells with the neurons’ firing rates peaked at a particular location (higher firing rate is indicated by the darker shading—see color bars, with rate indicated in Hz). (B1–B2) An adaptive decoder allows spikes from the four cells represented in (a) to update estimates of the position (gray curves, Est.), tracking the actual position (black curves, Act.) with a high degree of accuracy. Indeed, the error between estimates and actual position is generally a lot lower than the standard deviations of the mostly nonoverlapping place fields shown in A. (B1) Estimate and actual value of the xcoordinate as a function of time. (B2) Estimate and actual value of the y-coordinate as a function of time. The second half (shown) of an 1800-s trial was used for decoding (figure 9.8B), while the first half (not shown) was used for estimation of the place fields (figure 9.8A). This figure was produced by the online

code place_decoding.m. If you run the code, you will see the two-dimensional probability distribution for the estimate of position evolve over time.


 
 




Series Foreword
Computational neuroscience is an approach to understanding the development and function of nervous systems at many different structural scales, including the biophysical, the circuit, and the systems levels. Methods include theoretical analysis and modeling of neurons, networks, and brain systems and are complementary to empirical techniques in neuroscience. Areas and topics of particular interest to this book series include computational mechanisms in neurons, analysis of signal processing in neural circuits, representation of sensory information, systems models of sensorimotor integration, computational approaches to biological motor control, and models of learning and memory. Further topics of interest include the intersection of computational neuroscience with engineering, from representation and dynamics, to observation and control.
Terrence J. Sejnowski
Tomaso Poggio



Acknowledgments
I am grateful to the following people for their constructive comments and suggestions, which helped improve this book: Jonathan Cannon, Irv Epstein, John Ksander, Stephen Lovatt, Eve Marder, Alexandra Miller, Candace Miller, Ray Morin, Narendra Muckerjee, Alireza Soltani, Stephen Van Hooser, Ryan Young; and the following members of the Brandeis University Computational Neuroscience Classes (Spring 2017 and Spring 2018): Taniz Abid, Rabia Anjum, Apoorva Arora, Sam Aviles, Remi Boros, Brian Cary, Kieran Cooper, Ron Gadot, Sophie Grutzner, Noah Guzman, Lily He, Dahlia Kushinsky, Jasmine Quynh Le, Andrew Lipnick, Cherubin Manokaran, Sigal Sax, Nathan Schneider, Daniel Shin, Elizabeth Tilden, David Tresner-Kirsch, Nick Trojanowski, Vardges Tserunyan, and Jeffrey Zhu.
Several tutorials in this book evolved from course materials produced by Larry Abbott, Tim Vogels, and Xiao-Jing Wang, to whom I am grateful for introducing me to neuroscience.
I am particularly thankful to Candace Miller for her encouragement during this enterprise and to Brandeis University for its support.



Preface

I designed this book to help beginning students access the exciting

and blossoming field of computational neuroscience and lead them

to the point where they can understand, simulate, and analyze the

quite complex behaviors of individual neurons and brain circuits. I

was motivated to write the book when progressing to the “flipped” or

“inverted” classroom approach to teaching, in which much of the

time in the classroom is spent assisting students with the computer

tutorials while the majority of information-delivery is via students

reading the material outside of class. To facilitate this process, I

assume less mathematical background of the reader than is required

for many similar texts (I confine calculus-based proofs to

appendices) and intersperse the text with computer tutorials that can

be used in (or outside of) class. Many of the topics are discussed in

more depth in the book Theoretical Neuroscience by Peter Dayan

and Larry Abbott, the book I used to learn theoretical neuroscience

and which I recommend for students with a strong mathematical

background.

The majority of figures, as well as the tutorials, have associated

computer

codes

available

online,

at

github,

https://github.com/primon23/Intro-Comp-Neuro, at my website,

http://people.brandeis.edu/~pmiller, and at the website of MIT

Press, https://mitpress.mit.edu/computationalneuroscience. I hope

these codes may be a useful resource for anyone teaching or wishing

to further their understanding of neural systems.

1  

Preliminary Material
When using this book for a course without prerequisites in calculus or computer coding, the first two weeks of the course (at a minimum) should be spent covering the preliminary material found in chapter 1. The contents of the different sections of this chapter are introduced here.

1.1  Introduction
1.1.1  The Cell, the Circuit, and the Brain
In my experience, many students who enjoy solving mathematical or computational problems take a course such as computational neuroscience as their first introduction to neuroscience, or even as their first university-level course in the life sciences. For such students, section 1.2 offers a very basic summary of the meaning and relevance of biological and neurological terms that are used but not introduced elsewhere in the book. The newcomer to neuroscience should read section 1.2 before commencing the course.
1.1.2  Physics of Electrical Circuits
The ability of neurons to convey and process information depends on their electrical properties, in particular the spatial and temporal characteristics of the potential difference across the neuron’s membrane—its membrane potential. Nearly all of single neuron modeling revolves around calculating the causes and effects of changes in the membrane potential. To understand fully the relevant chapters in this book, it is first necessary for the reader to appreciate some of the underlying physics, so a background is provided in section 1.3 of this chapter.

Membrane potential,  
The potential difference across the membrane of a cell, which is highly variable in neurons, ranging over a scale of tens of millivolts.
1.1.3  Mathematical Preliminaries
The universe runs on differential equations, thanks to the continuity of space and time. The same applies to the brain, so at the heart of this computational modeling course is the requirement to write computer codes that solve differential equations. This may sound daunting, but it is in fact a lot easier than solving the same differential equations by the analytical methods one might find in a mathematics course. As a preliminary to delving into the various specific ordinary differential equations that we will find in this course, it is first important to understand what an ordinary differential equation is and what it means.

Variable 
A property of the system that changes with time.

Parameter 
A property of the system that is fixed during an experiment or simulation.

Differential equation 
An equation describing the rate of change of a variable in terms of all quantities—which may include the current value of the variable—that impact its rate of change.
Differential equations describe rates of change in terms of the current properties of a system. Those properties that change are variables of the system. A complete description of the system requires one differential equation for each variable. Typically, in neuroscience, the rate of change of one variable depends on other variables, in which case the description of the system comprises coupled (i.e., connected) ordinary differential equations.

Leak potential,
  The value that the membrane potential a neuron returns to after any temporary charge imbalance leaks away.

Leak conductance,  
The ability of charge to leak in and out of the cell through its membrane, the inverse of the membrane resistance. increases with the surface area of the cell.
Figure 1.1  Annotation of a differential equation. The equation is evaluated at each point in time (each point on
lower curve), to determine the slope of the curve, which is the rate of change of the variable, . By knowing the rate of change, the complete curve of as a function of time can be determined. The parameters , , and are fixed electrical properties of the neuron, which would be provided in any question, or must be measured in any experiment.

Membrane capacitance,  
The capacity of the cell’s membrane to store electrical charge, a quantity that increases with the surface area of the cell. For now, we will focus on a single variable changing in time. For example, the most common form of equation we will come across in this course can be written (see figure 1.1):

or equivalently

(1.1) 

(1.2) 

(the first form with

is due to Leibniz, the second with is due

to Newton—these were the two feuding originators of calculus). Note

that a single differential equation has just a single variable, in this

case the membrane potential, , whose rate of change at any point in time is given by evaluating the quantity on the right-hand side. In the absence of other information, we must assume the other

quantities: in this case leak conductance, , leak potential, , and

membrane capacitance, , are all fixed parameters. The equation is not simple to solve, because as the membrane potential changes, so too does the term on the right-hand side, which tells us its rate of change. A little calculus will tell us the solution, but if we are writing a computer code we can proceed without calculus.
Section 1.4 provides more mathematical background for this process. Section 1.6 provides methods for solving such equations computationally.

1.1.4  Writing Computer Code
A major aim of this book is to enable students with limited or no prior coding background to solve interesting scientific problems by

computer simulation or to analyze large datasets with sophisticated methods. For those to whom MATLAB or writing computer code is a new enterprise, it is essential to spend as much time as necessary, at least several hours, getting comfortable with the coding language before attempting tutorial 2.1. To aid in this process, section 1.5 provides an introductory MATLAB tutorial (there are many others available online) that focuses on the essential skills needed for this course. Once the tutorial in section 1.5 is completed, the new programmer will find section 1.6 a helpful first step in solving many of the problems found in the tutorials of this book.

1.2  The Neuron, the Circuit, and the Brain
This section is designed for students who have not taken any prior courses in neuroscience and perhaps have little background in cell biology.
1.2.1  The Cellular Level
Neurons are cells found in animals, so they possess all of the standard components of animal cells: a nucleus containing chromosomes of DNA where transcription takes place (production of RNA in the first step of protein synthesis); the nucleus is surrounded by a nuclear envelope outside of which is the cytoplasm containing various specialized subcompartments called organelles, such as mitochondria, the “power packs” of the cell; the cytoplasm itself is surrounded by a cell membrane marking the boundary of the cell. The cell membrane, also called the plasma membrane, consists of a lipid bilayer and is an electrical insulator. Within the membrane are situated transmembrane proteins, the most important of which for neural function are those that bind together to form ion channels. Ion channels allow specific ions to flow into and/or out of the cell.

Soma 
The main body of a cell, containing the nucleus.

Process 
A part of a cell that is elongated, extending far from the soma.

Dendrites 
Branched processes of a neuron that usually receive input from other cells or receptors and transfer the input to the soma.

Axons 
Branched processes of a neuron, usually thinner so less visible than dendrites, which usually convey neural activity generated in the soma or in the part of the axon adjacent to the soma (its initial segment, also called the “axon hillock”), to other neurons.
Neurons are unusual in their spatial structure, which allows them to connect with specificity to other neurons that may not be close neighbors. While the nucleus resides in the cell’s body (its soma) branching structures (processes) that connect to other cells are contiguous with the soma (figure 1.2). Dendrites are such processes, named after their treelike structure, that extend over hundreds of microns to receive inputs from other cells. Another branching process, the axon, can extend for over a meter in some animals.

Figure 1.2  Structure of a neuron. Arrows on dendrites and axon depict the standard direction of information flow: Inputs are received on the dendrites and, once sufficient electrical charge accumulates on the surface of the soma, a spike of voltage propagates away from the soma down the axon. Reverse (retrograde) propagation is also possible.
Neurons are also unusual in their active electrical properties, the most prominent of which is their ability to produce and transmit a large pulse of deviation in the electrical potential difference across the cell membrane. Such a pulse, called an action potential or simply “spike,” is carried away from the cell body down the axon. The voltage spike is caused by active ion channels, which open or close in

response to changes in the potential difference across the cell membrane. In chapter 4 we will study and simulate the cause and properties of such a spike.

Vesicle 
A small spherical container of biochemical molecules.
At the terminals of the axon, the spike in the membrane potential causes voltage-gated channels to open and admit a brief pulse of calcium ions. These calcium ions activate proteins that lead to release of one or more vesicles—spheres surrounded in lipid and containing a cargo. In this case, the cargo is a neurotransmitter, which upon release of a vesicle diffuses rapidly to bind to receptors of a neighboring neuron. The binding to the neighboring neuron initiates the opening of ion channels in that neuron, completing the passing of a message that allows the changes in the membrane potential of one neuron to impact the membrane potential of any other neuron to which it is connected.

Glial cells 
The general name for many cells in the brain that are not neurons, with multiple roles, mostly thought to support the activity of neurons, but also with their own electrical properties.
Neurons are by no means the only cells in the brain. We deliberately focus on them in this book, because their electrical properties have a clear, direct role in mental processing and they are the best studied; voltage spikes are easily detectable by electrodes within brains, and the electrical fields generated by currents flowing into and out of neurons produce the strongest signals detectable outside the brain. However, it should be noted that the most significant difference between Einstein’s brain and the average person’s is an increased abundance of glial cells, which are normally thought of as support cells for neurons. Recent experiments have shown that glia produce their own electrical signals and communicate with each other. It is reasonable to assume that glia respond to overall activity in a local circuit and to some extent modulate this activity, but a great deal more remains to be uncovered. I hope that after completing this book you will have gained sufficient confidence in computational modeling to be ready to simulate more novel models testing how glia contribute to information processing in the brain.
1.2.2  The Circuit Level
The amount of branching—or ramification—of the processes of a neuron is strongly correlated with the number of cells that connect with it. Pyramidal cells (figure 1.3A), the most common excitatory cell in the cortex and hippocampus of mammalian brains, receive on the order of 10,000 inputs. Purkinje cells (figure 1.3B), the major output cells of the cerebellum, possess the most highly branched of dendrites: Each cell receives on the order of 100,000 inputs. Its axon, on the other hand, is relatively unbranched: After

accumulating so much input, it sends its output to just a few cells in the cerebellar nuclei.
Figure 1.3  Structures of some distinct cells. (A) Layer 3 pyramidal cell from the frontal cortex of a mouse (courtesy of the Luebke Laboratory, Boston University). Cells like these are the predominant excitatory cells of the cerebral cortex. (B) A human Purkinje cell, the inhibitory output cell of the cerebellum, as drawn by Ramón y Cajal (from Popular Science Monthly 71, 1907). (C) A neuron from the stomatogastric ganglion of the crab. In this case, the axon branches away from the dendrites instead of being attached directly to the cell body (courtesy of the Marder laboratory, Brandeis University).

Connectome 
The complete circuit diagram, in terms of neurons and their connections, of a particular region of a central nervous system, or the entire neural circuitry of an animal.

Central nervous system 
The brain, spinal cord, and other areas with significant neural processing, such as the retina, of an animal.
Neurons do not just connect with other cells at random. Many invertebrates have a fixed number of neurons with stereotypical connections. In a small number of species, the entire circuit—called the connectome—is known and is reproducible across different members of the species. The first such circuit to be mapped was for Caenorhabditis elegans, a small worm with 302 neurons. Small motor circuits in marine creatures such as lamprey and lobsters have been fully described and, more important, matched with their function. A full description of the connectome of Drosophila melanogaster, a fruit fly with more than 80,000 neurons, is close to being complete.
In mammals, the description of the connectome is more of a statistical process. In each small circuit the numbers and locations of each different type of cell can be evaluated, in combination with the average numbers of connections between the different cell types. Thus, one can build up stereotypical circuits in different regions of the central nervous system, with complete descriptions now available for isolated areas such as the retina. Given the 80 billion ( ) neurons and the 100 trillion ( ) connections of a human brain, it is currently infeasible to produce a circuit at the neural level for humans. Moreover, even the statistical approach is not straightforward: Different individuals of a mammalian species have different connectomes, with the existence of a connection and its strength in any particular animal correlated with the properties of a number of different cells. Therefore, it is quite likely that a connectome acquired by averaging over many individuals, missing some of these crucial correlations, would produce a nonfunctioning brain.
1.2.3  The Regional Level

Cerebral cortex 
The outer region of mammalian brains that is highly folded in humans.
At the grossest scale, brains can be divided into regions according to their structure and function. These regions can be further subdivided as dictated by differences in the relative abundances of cell types or differences in the neurons’ functional properties. For example, the most recent evolutionary addition to mammals’ brains is the cerebral cortex, which forms the outer, upper surface of our brains (figure 1.4A). It contains gray matter mostly comprising neurons and is supported by underlying white matter comprising other cell types such as glia. In primates, it is highly folded with ridges (gyri) and crevasses (sulci) as a consequence of its large surface area. These bumps and dips, combined with location inside the skull, allow anatomists to identify equivalent areas in different brains. The areas themselves can be named by their anatomical location or by their function, which reliably maps to location across individuals. For example, motor planning areas are found in front of the central sulcus, whereas sensory processing areas for visual, auditory, and touch stimuli are found behind the central sulcus.

Figure 1.4  Regions and topographic maps. (A) Cerebral cortex with gyri, from H. Gray, Anatomy of the Human Body (1918), illustrated by H. V. Carter. (B) The shading of an orientation selectivity map indicates that neurons in primary visual cortex are clustered according to the orientation of a bar to which they most strongly respond (see the mapping from orientation to shading below). Insets show pinwheel centers (see section 6.9) and boundaries. This figure panel was originally published as part B of figure 1 in W. H. Bosking, Y. Zhang, B. Schofield, and D. Fitzpatrick (1997), “Orientation selectivity and the arrangement of horizontal connections in tree shrew striate cortex,” Journal of Neuroscience, 17(6), 2212–2221. Reprinted with permission.
Perhaps the most studied area in mammals is the visual cortex, which is right at the back of the brain (part of the occipital cortex). The visual cortex is further subdivided (primary, V1; secondary, V2; etc.) because: (1) these subregions are structurally distinct; (2) a complete representation of the visual scene is found in each subregion; and (3) neurons have distinct types of response in the different subregions (cells in V1 respond primarily to contrast boundaries, whereas cells in V2 respond better to more complex combinations of stimuli). Furthermore, a single subregion such as V1 can be subdivided in some animals according to which eye the neurons in a location respond to more, which orientation direction they respond to, whenever such features are clustered by spatial location. This latter level of spatial subdivision is called a topographic map (figure 1.4B).

Topographic map 
An area of the brain where the neurons are positioned in an organized arrangement that is directly related to the arrangement of the different stimulus features to which they respond.
The divisions of cortex into subregions described above are based on the different properties of different areas of the cortical sheet. The folded cortical sheet also has a thickness of 1–2 mm within which distinct layers of cells can be identified. Six layers are enumerated, but layer 1 lacks cell bodies (soma); layers 2 and 3 are typically combined, with cells therein having similar properties; and layer 4 can be expanded into sublayers in some sensory areas (such as V1), whereas it is absent from other areas (such as prefrontal cortex). While cells in different layers of a column may respond to related stimulus features (e.g., an edge in the same location of the visual scene), there are clear differences in the dominant input cells (from other parts of the brain, or other areas of cortex, or from a higher or lower layer in the same location) and output cells of the neurons in these different layers. Thus, the layer-specific connections in a region of cortex form a stereotypical circuit, whose functional properties are an ongoing subject of research.

1.3  Physics of Electrical Circuits
We usually think of electrical circuits as requiring wires and connections made of metal. However, wherever electrical charge is moved or is able to move from one place to another, an electrical circuit can be produced. Whereas in metals the movement of electrical charge is due to free electrons, in solutions—such as the intracellular and extracellular fluids of living organisms—the charge carriers are ions. This leads to some important distinctions between the operation of neurons and of common electrical circuits. Before focusing on the biologically important distinctions, first we review the more general features and terminology of electrical circuits.
1.3.1  Terms and Properties
Charges 
Many chemicals are soluble and dissociate (split up) into complementary positively and negatively charged ions in solution. Oppositely charged ions attract each other, while similarly charged ions repel each other. The motion of the charges is mediated by an electric field, which is the spatial gradient of an electric potential. Positive charges move to a more negative potential, while negative charges move to a more positive potential. Each charge produces its own electric field such that approach to a positive charge is to a more positive potential and approach to a negative charge is to a more negative potential.
Currents 
An electrical current is a flow of electrical charge from one place to another. Of particular interest in neuroscience is the electrical current through a particular channel in the cell’s membrane or the total current across a patch of membrane. In this case the inward

membrane current, , is equal to the rate of change of charge, , on the inside of the cell’s membrane. As an equation, this reads:

(1.3) 

Potential Difference (Voltage) 
A potential difference generates the drive for charges to move (hence it is also known as electromotive force, or EMF). Significant potential differences arise between the inner and outer surfaces of the cell membrane of neurons. Within the cell, smaller differences in potential arise because charge can more easily flow to balance out such differences.

Capacitance (Capacity to Store Charge) 
The capacitance, , of a surface relates the amount of charge, , stored on a surface to the potential difference, , produced across that surface via:
. (1.4) 

Conductance (Ability to Let Current Flow) 
The conductance, , between two regions determines how easily current flows from one region to the other to equalize any potential difference between the regions. A high conductance means that current is high when there is a potential difference between the two regions, such that charge flows rapidly in order to counteract any potential difference between the two regions. The inverse of conductance is resistance, , so the two equations:

are equivalent to each other.

(1.5) 

1.3.2  Pumps, Reservoirs, and Pipes

It can be helpful to make the analogy of water flowing through pipes when thinking about electrical currents (see table 1.1 and figure 1.5). Many of the words used, such as pumps, current flow, channel, and potential, have equivalent meanings in the two settings. Water flows downhill from a high potential energy to a low potential energy. If two water containers are connected with a pipe, or channel, the water will flow until the water level is the same in both containers, or at the same potential. If a pump is continuously operating, water can be maintained at a higher level in one container than another. If the channel (pipe) is closed, the higher level remains in the absence of further pumping. The closed pipe is equivalent to a high resistance that prevents current flow. If the pipe is opened, then its resistance drops and current flows in the direction that will reduce the potential difference (i.e., water flows downhill).
Table 1.1  Analogy between flows of electrical charges and fluids

Figure 1.5  Pump and reservoir analogy to electrical circuits. Open channels have a low resistance, permitting high current flow across a potential difference from high potential to low potential. Pumps (effectively batteries) can maintain the potential difference, but without them the passive current flow would tend to erase any potential difference.
The main limitation of the analogy is that charge can be either positive or negative. When one considers negative charges, the physical movement of the actual ions is opposite to the net electric current produced. However, if one sticks with positive charges then the analogy of water flowing through pipes between containers can be helpful. Opening holes in the cell membrane (ion channels) allows current to flow. This corresponds to a decrease in membrane resistance or, equivalently, an increase in membrane conductance. Pumps in cells can cause systematic deviations from equilibrium, moving ions to regions of higher chemical or electrical potential energy (respectively to regions of higher concentration or to a more positive electrical potential for a positive charge), just as they can move water to a higher potential energy (uphill).
1.3.3  Some Peculiarities of the Electrical Properties of Neurons
Charge flow in neurons is due to movement of ions—usually atoms, but also molecules, that have gained or lost one or more electrons. The necessity of physical movement of these ions allows for the cell to control charge flow and in so doing, to control the potential

difference between the inside and the outside of the cell. Such control is achieved via proteins in the cell membrane, which can interact with specific types of ions in a manner that permits movement of some ions and not others.
1. Ion pumps preferentially pump into or out of the cell specific ions, producing a concentration difference across the cell membrane that is particular to each type of ion. Functionally, the process is equivalent to making a series of mini-batteries— whose strength and direction depends on ion type—along the cell membrane.
2. Ion channels permit flow of a subset of ions, often only one type of ion. As these channels open and close, the effect is to functionally connect or disconnect the corresponding minibatteries. Active channels are those that open or close in response to biophysical properties, such as local potential difference, concentration of a particular chemical, or temperature.
Aqueous solutions like the cell’s cytoplasm are a million or more times worse than good conductors at transferring charge (for example, the electrical resistivity of sea water is 0.2 m—in the range of semiconductors—whereas the electrical resistivity of copper is
m). Therefore, charge that enters a cell can leak out before spreading out equally within the cell, causing charge imbalances within the cell, so the potential difference across the cell membrane in its distal dendrites (figure 1.2) can be quite different from that in the soma. Many models of neurons focus solely on the dynamical properties of the membrane potential and ignore this spatial feature, treating the neuron as a pointlike object with a single membrane potential. Multicompartment models (section 4.7) are spatially extensive and take into account intracellular variations in membrane potential.
A Note on Time Constants 
Strictly, a time constant is defined by the amount of time needed for a system to get closer to equilibrium by a factor of (where is Euler’s constant, used in exponential notation). Even when a system

is not approaching equilibrium, one can often calculate “instantaneous time constants” that tell us how quickly things are changing (compared to the state the system is trying to reach) at that point in time. When we are thinking about current flowing across the membrane of a cell, the two key quantities that set the time constant are the amount of charge that is to flow and the rate of flow of that charge. Amount divided by rate gives the timescale. Both of these are proportional to the potential difference, while the amount of charge is also proportional to the capacitance (equation 1.4) and the rate of flow is also proportional to the conductance (equation 1.5). Therefore, the timescale is given by capacitance divided by the conductance, or equivalently, the capacitance multiplied by the resistance. This makes sense in terms of the water analogy—the time to empty a bath goes up if the bath is bigger, because it stores more water, but it goes down if the drain is wider, because water flows out more quickly. A larger bathtub has greater capacity to store water so is like a larger cell with greater capacitance. A larger drain, which allows water to flow out more quickly, is like a larger conductance. If the area of the drainage were scaled with the area of the bathtub, the time to empty would be independent of size of bathtub. Similarly, for a neuron, if the number of open channels in a cell membrane scales up in proportion to the total surface area where the charge is stored, then the time constant does not change.
Note that the time constant of a neuron is not typically static. The more channels that are open at any time, the more rapidly charge can flow into or out of the cell, so the shorter its effective time constant.

1.4  Mathematical Background
This section is intended to supply those readers who have not taken a college-level calculus course with the intuition and fundamental results needed to complete an introductory course in computational neuroscience.
1.4.1  Ordinary Differential Equations
A hallmark of all living things is that we change over time—that is, we are dynamical systems. Differential equations are used to describe dynamical systems. The structure of a differential equation reflects the continuous nature of space and time, combined with the fact that for a system to get from one state to another it must pass through a continuous path of intervening states. That is, in spatial terms, there is no such thing as teleportation. The position of an object a tiny amount of time in the future can be calculated from its current position and current velocity. Differential equations quantify this procedure.
In this book, we will only deal with ordinary differential equations (ODEs), which describe how individual properties of a system change with time, or along a single spatial dimension, but not with both together (the combination of time and space requires partial differential equations). We treat spatial variation by coupling together ordinary differential equations that represent the dynamics at different points in space—such as in different neurons or different compartments of a single neuron—and include their effects on each other.
We will find that even though using mathematics to solve ODEs can become inordinately difficult, writing computer codes to solve them is straightforward.
Example 1: A Falling Stone 
Gravity produces a constant acceleration, so that if we neglect air resistance, a dropped stone’s speed is proportional to the time for

which it has been falling. We will define the downward velocity as negative (since the stone’s height decreases with time) and can ask how the position of the stone changes with time. In doing this we will carry out integration twice—once to obtain velocity from acceleration, then again to obtain position from velocity. That is:

(1.6)  (in words, the rate of change of velocity is equal to the acceleration) and

(1.7) 

(in words, the rate of change of y-position is equal to the velocity).

Table 1.2 indicates how velocity and position accumulate over time,

if we approximate the effect of gravity with

ms–2. In the third

row, the velocity is given by the accumulation over time of

acceleration, which is a fixed quantity. The result of such

accumulation is a linear dependence: Velocity is simply acceleration

times time,

. In the last row, position is obtained by

accumulating the average velocity across each timestep. That is, the

value of position is equal to its previous value plus the mean of the

current value of velocity and the prior value of velocity multiplied by

the change in time. Such a procedure can be carried out by

computers extremely rapidly and with very small timesteps. Indeed,

by the end of chapter 1 you should understand how to program a

computer to produce tables with a similar form to table 1.2 and to

plot them as graphs such as figure 1.6.

Table 1.2  Dynamics of a falling stone

Note: As time increases (top row) the velocity (third row) accumulates the mean of successive values of acceleration (second row) and the position (bottom row) accumulates the mean of successive values of position.

Figure 1.6  Dynamics of a falling stone. (A) Velocity, , decreases linearly with time. (B) As a result, height, , decreases quadratically with time. Crosses are data points from table 1.2. This figure was produced by the available online code falling_stone.m.

Example 2: The Sprinter 
Let’s next consider the example of a sprinter in a 100-meter race. Initially the sprinter is stationary and so has zero velocity. We can assume the sprinter accelerates up to a maximal velocity and maintains it until finishing the event. We might want to know how long the sprinter takes to complete 100 meters, which we can do if we know the velocity as a function of time. Perhaps the velocity is provided by the following equation:

(1.8) 

where is the maximum velocity (e.g.,

m/s) and indicates

how long it takes to approach maximum velocity (e.g., s).

The shape of the curve in equation 1.8 is shown in figure 1.7A. The

ordinary differential equation to describe the position, , of the

sprinter along the track is then:

(1.9) 

Figure 1.7  Time-dependent velocity. (A) Velocity of a hypothetical sprinter increases from zero toward a maximum, , at which it remains. (B) The velocity corresponds to the gradient of the position versus time curve, which starts instantaneously flat, then steepens to lie parallel, but below the dotted line .

Equation 1.9 states that the differential of position, , with respect to time, , is the velocity at that point in time. To put it another way, the rate of change of position is the velocity. Throughout this course, we will be using the fact that the rate of change of a variable can be described in terms of its current value and/or the current values of other variables.
For this course, you do not need to know how to use mathematics
to solve the equation for position, . Instead, section 1.6 explains how a computer code can be written to solve the equation. Rather, it is important to know what various functions look like, so that you can see if the behavior makes sense. For example, the exponential
function, , has a value of 1 for , and decays to become

negligible for . That means the function

has the value of

zero if and the value of 1 if , as necessary for to commence

at zero and reach a maximum value, , in equations 1.8 and 1.9.

In this example, is a simple enough function that I can write down the solution:

(1.10) 

This means that the position is a little less than (see figure 1.7B).
is the position reached if the sprinter were able to run at maximum velocity from the starting line (as may happen in a relay).
If the distance “lost” by acceleration is (or meters) in the current example. This implies the time to complete 100 meters would be approximately 11 s.
In section 1.6, we will see how to solve equations like equation 1.9, using a computer simulation. When an analytic (i.e., mathematical) solution like equation 1.10 is also known, the exact analytic solution can be compared with simulations to test the accuracy of various simulation methods.

Example 3. Cycling over a Hill 
In the first two examples, the velocity was determined purely by the time elapsed since the start of the process. More generally, the velocity may have a known dependence on the variable being calculated, such as position. For example, when a child cycles over a hill she may ride five times faster on the downhill than the uphill. In this case, the rate of change of position will depend directly on the current position as opposed to the current time.
We will consider a hill of total distance 2 miles, shaped steepest at the bottom, so that the cyclist’s velocity increases with position along

the hill as

(see figure 1.8). Then the rate of change of

position is written as:

(1.11) 

Figure 1.8  Position-dependent velocity. (A) Profile of a hill so that

with increasing distance the terrain progresses from steep uphill to

shallow uphill to flat to shallow downhill to steep downhill. (B) The

velocity of a child on a bicycle is slowest at the steepest uphill and

increases monotonically with position, to become fastest at the steep

downhill. The velocity as a function of position might be

approximated as

.

Notice that writing in equation 1.11, rather than just , is done simply to remind the reader that the velocity does depend on time, even though it is explicitly written only in terms of position, .

Similarly, is written to remind the reader of the time-dependence of position, .
A first course in calculus will teach you how to find the analytic

expression for the solution of equation 1.11 as

, which then

tells us the time taken to complete the 2-mile journey is

.

While the analytic method for solving such an equation is

significantly trickier than for solving examples 1 and 2, the

computational method for solving example 3 requires only a trivial

alteration from example 1. Again, for this course it is much more

important to understand what the equations mean and how their

solutions make sense than worrying about finding exact formulas.

The one formula for the solution of an ordinary differential equation

that you should get to know is provided in example 4.

Example 4. Exponential Decay to a Point of Equilibrium—
Cooling a House 
Any process with an equilibrium point and a rate of change that is directly proportional to the difference between the system’s current state and its equilibrium point follows an exponential decay. Such processes are very common in chemistry and physics, so we will consider a couple of examples.
For this example, we consider the change in temperature of a house on a hot day when air conditioners are first switched on. The key to the change in temperature is that while air conditioners extract heat from the house at a constant rate, the high temperature outside of the house causes heat to enter the house at a rate proportional to the difference between inside and outside temperatures. So, as the house cools, the flow of heat from outside to inside increases until the air conditioners can no longer keep up and an equilibrium is reached (we assume that if there is a thermostat it is set to a lower temperature than this eventual equilibrium, so the air conditioners never switch off). The equilibrium is dynamic—as it is for most biological and chemical equilibria—based on a balance of outward heat flow matching the inward heat flow.
Mathematically we can write the rate of change of temperature, , as

(1.12)  where is the heat capacity of the house (greater for a bigger house),
is the rate of heat extraction (proportional to the power of the air

conditioner), is the outside air temperature, and is a measure of how easily heat flows through the walls of the house, or “leakiness” (better insulation reduces ). Note that we are using upper case “ ” to represent temperature in equation (1.12), while the lower case “ ”

always represents time in this book. Therefore

means rate of

change of temperature, so would have standard units of degrees

Celsius per second, though more manageable units for a building

would be degrees per hour.

Figure 1.9  Rate of change of temperature as a linear function of

temperature. (A) The diagonal straight line indicates how the rate of

change of temperature depends linearly on temperature in a

simplified temperature-regulation system. Horizontal dotted lines

indicate the direction and magnitude of the rate of temperature

change. They point toward the equilibrium temperature, , where

. (B) Since

does not have a fixed value, a plot of

temperature, , against time, , does not have a fixed gradient and so

is nonlinear.

Equation (1.12) is both an ODE and linear in the variable of

interest. That is, if we plotted

as a function of , we would get a

straight line (see figure 1.9A). The full name of this type of equation

is therefore a linear first-order ordinary differential equation (ODE).

It is worth knowing the general solution of such equations, since they

are so common.

The most important point in understanding the behavior of

equation 1.12 is to realize that while the rate of change of

temperature is a linear function of temperature, if we plot the

resulting temperature as a function of time, we get a nonlinear

function—as the room heats or cools its temperature changes, and

thus the gradient of temperature as a function of time changes, until

it reaches a point where rate of change of temperature is zero (

) when temperature stops changing. In all such equations, we can calculate the equilibrium point—in

this case, the temperature, , as the value of at which

.

Following equation 1.12, we set

(1.13) 

when = , which leads to
(1.14)  This result means that the equilibrium temperature is below the external temperature by an amount that is proportional to the power of the air conditioner (A) and inversely proportional to the “leakiness” of the house (B). Of course, this makes sense—stronger air conditioners or better insulation will allow the house to be cooler on hot days. Just as any straight line can be written in terms of two parameters —its gradient and its intercept on the y-axis—so too can any linear first-order ODE be written in terms of two parameters. The important parameters are the equilibrium point and a time constant. The equilibrium point found above is the intersection on the x-axis (figure 1.9A), whereas the time constant is the inverse of the gradient. In our example, we can rewrite the straight line in equation
1.12 representing as

(1.15) 

so we see the gradient is

and the y-intercept is . In our

example, we have already found and only a little algebra is

required to show that

. (We use a very similar looking, but

distinct symbol, , the Greek “tau,” for the inverse-gradient, because

it becomes the time constant of the process, and time constants are

traditionally given the symbol “ .”)

The final property to notice is that the equilibrium point is stable.

The requirement for stability is that the temperature returns to its

equilibrium following small deviations. In fact, for a linear system it

does not matter how far from equilibrium the system is; if the

equilibrium is stable, the system will return to equilibrium. In our

example, the key property is that the gradient of the

versus

line is negative (figure 1.9A), so that if is greater than its

equilibrium value it will decrease over time (negative ), whereas if is lower than its equilibrium value it will increase over time

(positive ), as shown in figure 1.9B.

Once we know a linear system has a stable equilibrium point, we know it will reach that equilibrium as an exponential decay (proof to
follow). If the initial temperature is at the time when the air conditioners are first switched on, then the temperature will change as a function of time according to:

(1.16)  We will see this equation many times in this book. You should

convince yourself it behaves as expected, so that by setting , we

find

, and if then

.

Finally, for our example, we can assume the initial temperature of

the house is the external temperature, so

, in which case

and the solution can be written as:

(1.17) 

Proof of Exponential Solution* 
For those with a little calculus background, we will use the formula

for the integral of

with respect to as

in order to

prove the exponential solution of a linear first-order ODE. This

proof, like all of those in the appendices of this book, can be skipped

without loss of content, but should be followed by those who want to

deepen their mathematical understanding and have sufficient

background in calculus.

In general, if an equation can be written in the form of equation

1.15 as:

(1.18)  we can rearrange it in a format suitable for integration as

After integrating we find

(1.19) 

which becomes

(1.20) 

Equation 1.21 can be solved to find both sides:

(1.21)  by taking the exponential of

(1.22)  Rearrangement of equation 1.22 gives the solution,
(1.23)  The solution proves that is the time constant and is the equilibrium point reached after a time long compared to the time constant. Therefore, knowing this solution, all that must be found when solving any linear first-order differential equation is the equilibrium point and the time constant. We shall use such a shortcut in example 5 below.

Example 5. Dynamics of Gating Variables 
The term “gating variable” is used to represent the fraction of a particular type of ion channel in a state that allows current to flow. We assume the channel consists of a protein with two states, which we will call “closed” and “open.” The key parameters are the rates for closed channels to open and the rates for open channels to close.
These rate constants are and respectively and have units of
inverse time ( ). The chemical equation is simply

. (1.24) 

If we write as the number of open channels and as the

number of closed channels then equation 1.24 tells us that the rate of

closed channels opening is

while the rate of open channels

closing is . If we write the total number of channels as then the

number of closed channels is given by

, so rate of closed

channels opening is

. Therefore, we can write the rate of

change of the number of open channels as the difference between

opening and closing rates as

. (1.25) 

Notice that if all channels are open, then the first term on the right

of equation 1.25 is zero (no more channels can open and cannot

increase), whereas if all channels are closed the second term on the

right is zero (no more channels can close and cannot decrease).

We define the gating variable, , as the fraction of channels in the

open state,

, such that the dynamics of is found after

dividing equation 1.25 by to give:

(1.26)  Basic algebra allows us to rewrite equation 1.26 as

so long as we identify

(1.27) 

(1.28)  and

(1.29)  Finally, we can write down the solution of the equation 1.27 (following the methods of example 4) to give:
(1.30) 

It is worth noting that the following properties of the solution (equations 1.28–1.30) match our expectations or intuition:

1. The equilibrium value,

, can only vary between 0

and 1 (since and are positive rate constants);

2. approaches 1 if (if opening rate is much greater than closing rate, nearly all channels are open);

3. approaches 0 if (if closing rate is much greater than opening rate, nearly all channels are closed);

4. The time constant,

, is dominated by the largest

rate constant, while remaining inversely proportional to rates.

Equation 1.30 fully describes the behavior of a two-state system, for which the channel can be in only either a closed or open state. However, the solution can be applied to systems with multiple pairs of such states, so long as the transitions within one pair of states do not impact the transition rates within other pairs.
For example, in chapter 4, when we focus on conductance-based models, we will consider ion channels with separate activation and inactivation variables. These variables can be related to the states of individual proteins that comprise the (mostly) independent subunits of a channel. In most models, each of the variables has two states, one of which allows the channel to open and the other of which keeps it closed. Multiplication of these distinct gating variables together to evaluate the total probability of a channel being open—proportional to the expected instantaneous conductance of many such channels— is valid if each variable is independent. Indeed, for the sodium
conductance, the total conductance is proportional to a term of , which assumes three independent activation variables, , and a
single independent inactivation variable, , each of which follow the dynamics of equation 1.30.

1.4.2  Vectors, Matrices, and Their Basic Operations
When dealing with a large quantity of variables—say, the value of the firing rate of many neurons, each at many different points in time—it becomes essential to store them together in vectors or matrices.

Computer codes written in MATLAB can perform calculations on all of the variables very efficiently when they are combined together in a matrix (the “Mat” in MATLAB stands for Matrix). Therefore, for this course it is important for you to learn how matrices and vectors are combined by addition or multiplication.
A vector is a list of numbers, with each number labeled according to its position on the list. A matrix is a rectangular grid of numbers, with each number labeled according to its position in the grid, in particular via its row index then column index. It is worth noting that each row of a matrix is a vector, as is each column—vectors can be rows or columns. A matrix can be thought of as a vector of vectors— either a row vector of column vectors, or a column vector of row vectors.
In this book, a single line under the variable’s name indicates it is a vector. For example, the vector could represent a list of the firing rates of many cells. The underline is not present when a particular

entry of the vector is used; for example, or would mean the firing rate of the third cell in the list and that firing rate is a single value, not itself a vector.
Similarly, two lines under a variable’s name indicate that it is a

matrix. For example, could represent the set of connection strengths between all pairs of cells in a group, each row labeling the presynaptic cell and each column labeling the postsynaptic cell. Again, no underlines are used when writing a particular entry in the

matrix, so

or would be the strength of connection from

cell 2 to cell 3, which is a single value.

Examples of Matrices and Vectors 

a.  If

