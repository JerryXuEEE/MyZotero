THE

llllllllllllllllllllllllllllllllllllllllllllllllllll 123456789123456789123456789123456789123456789l
The Computational Brain

The Computational Brain
25th Anniversary Edition
Patricia S. Churchland and Terrence J. Sejnowski
The MIT Press Cambridge, Massachusetts London, England

Â© 2017 Massachusetts Institute of Technology

All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher.

This book was set in Palatino LT by Toppan Best-set Premedia Limited. Printed and bound in the United States of America.

Library of Congress Cataloging-in-Publication Data

Names: Churchland, Patricia Smith, author. | Sejnowski, Terrence J. (Terrence

Joseph) author.

Title: The computational brain / Patricia S. Churchland and Terrence J.

Sejnowski.

Description: 25th Anniversary edition. | Cambridge, MA : The MIT Press,

[2017] | Series: Computational neuroscience series | Includes bibliographical

references and index.

Identifiers: LCCN 2016034531 | ISBN

(pbk. : alk. paper)

Subjects: LCSH: Brain--Computer simulation. | Neural networks

(Neurobiology)

Classification: LCC QP356 .C48 2017 | DDC 612.8/2--dc23 LC record available

at https://lccn.loc.gov/2016034531

10 9 8 7 6 5 4 3 2 1

Contents

Series Foreword

vii

25th Anniversary Preface

ix

Preface

xvii

1 Introduction

1

2 Neuroscience Overview

17

Introduction

17

Levels in nervous systems

18

Structure at various levels of organization

27

A short list of brain facts

48

3 Computational Overview

61

Introduction

61

Looking up the answer

69

Linear associators

77

Constraint satisfaction: Hopfield networks and Boltzmann

machines

82

Learning in neural nets

96

Competitive learning

102

Curve fitting

105

Feedforward nets: Two examples

107

Recurrent nets

115

From toy world to real world

125

What good are optimization procedures to neuroscience?

130

Models: Realistic and abstract

136

Concluding remarks

137

4 Representing the World

141

Introduction

141

Constructing a visual world

142

Thumbnail sketch of the mammalian visual system

148

Representing in the brain: What can we learn from the

visual system?

157

What is so special about distribution?

163

World enough and time

174

Shape from shading: A neurocomputational study

183

Stereo vision

188

Computational models of stereo vision

199

Hyperacuity: From mystery to mechanism

221

Vector averaging

233

Concluding remarks

237

5 Plasticity: Cells, Circuits, Brains, and Behavior

239

Introduction

239

Learning and the hippocampus

243

Donald Hebb and synaptic plasticity

250

Memories are made of this: Mechanisms of neuronal

plasticity

254

Cells and circuits

281

Decreasing synaptic strength

289

Back to systems and behavior

295

Being and timing

305

Development of nervous systems

307

Modules and networks

316

6 Sensorimotor Integration

331

Introduction

331

LeechNet

341

Computation and the vestibulo-ocular reflex

353

Time and time again

379

The segmental swimming oscillator

388

Modeling the neuron

399

Concluding remarks

411

7 Concluding and Beyond

413

Afterword

425

Appendix Anatomical and Physiological Techniques

427

Permanent lesions

427

Reversible lesions and microlesions

430

Imaging techniques

432

Gross electrical and magnetic recording

437

Single-unit recording

440

Anatomical tract tracing

442

Notes

445

Glossary

457

References

479

Index

525

vi

Contents

Series Foreword
Computational neuroscience is an approach to understanding the development and function of nervous systems at many different structural scales, including the biophysical, the circuit, and the systems levels. Methods include theoretical analysis and modeling of neurons, networks, and brain systems and are complementary to empirical techniques in neuroscience. Areas and topics of particular interest to this book series include computational mechanisms in neurons, analysis of signal processing in neural circuits, representation of sensory information, systems models of sensorimotor integration, computational approaches to biological motor control, and models of learning and memory. Further topics of interest include the intersection of computational neuroscience with engineering, from representation and dynamics, to observation and control.
Terrence J. Sejnowski Tomaso Poggio

25th Anniversary Preface
When The Computational Brain was published 25 years ago, most of what we knew about the properties of neurons and neural systems was derived by recording from single neurons, one at a time. This technique revealed the sensory features that neurons respond to and neural activity that correlated with behavior from a limited sample of neurons. As a consequence, conceptual frameworks for brain function were based on single neurons, as exemplified by the grandmother cell theory for visual perception, according to which your perception of your grandmother depends on a single neuron that selectively responds to your grandmother. In retrospect, this narrow scope is similar to the problem faced when looking at a scene through a soda straw, sampling one pixel at a time from random locations. Without knowing the complex nature of how the world projects onto the eye, which is encoded in the higher-order relationships between pixels, it would be difficult to extract the global properties of objects in the scene and their configuration in depth. Extracting motion from the sequence of images in a movie would be even more demanding.
Our goal in writing The Computational Brain was to develop a different conceptual framework, one based on large populations of neurons. The insights motivating this goal had their origin in advances made by learning algorithms for artificial neural network models in the 1980s. By being trained on examples with a learning algorithm for updating the connectivity between units, these models could solve difficult computational problems by developing distributed representations in hidden layers between the inputs and outputs. The patterns of activity among the hidden units of a trained network had properties that resembled those recorded from populations of neurons recorded one at a time. For some this was a revelation, but others depressingly concluded that we would never be able to understand brain function by randomly sampling neurons from a distributed population, since you would need to record from all of them to uncover the transformation (Robinson,

1992). This raises the issue of how many neurons we need to record from at the same time to make progress.
What a difference 25 years can make. On April 2, 2013, the BRAIN Initiative was announced by the White House, heralding a new era in the study of neural populations based on innovative neurotechnologies. Neuroscientists now routinely record from hundreds of neurons using optics rather than electricity. What before was seen only in a reconstruction from many different cells in different animals can now be seen in the same animal at the same time. Misha Ahrens, using genetically-encoded calcium sensors, has recorded activity from 80,000 neurons in the transparent larval zebrafish brain (Ahrens et al., 2013). The goal of the BRAIN Initiative is to record from a million neurons in many different brain areas (BRAIN 2025, 2014). Recordings from the cortex of humans undergoing operations for epilepsy to identify the focus of seizures has given us unprecedented opportunities to observe large-scale populations during cognitive tasks that only humans perform, such as language processing (Pasley et al., 2012). New algorithms are needed for analyzing these large datasets (Sejnowski et al., 2014).
We were gratified by the reception that The Computational Brain received when it was published in 1992 and the influence it has had on a generation of researchers. Even though there have been many important advances in both neuroscience and computational studies over the last 25 years, the main message of this book and the examples in it are still relevant today. Some aspects of the book have taken on a life of their own, such as the space-time diagram of methods in Figure A.1 on page 428. The original diagram left a lot of open space that has since been filled in with new methods such as functional magnetic resonance imaging (fMRI) and optical recording. The most recent version (Figure 0.1) is barely recognizable, and continues to be transformed as new neurotechnologies are developed by the BRAIN Initiative.
Just as we were finishing The Computational Brain, Peter Dayan and Read Montague arrived in La Jolla as postdoctoral fellows. If they had arrived a few years earlier, there would have been a chapter in The Computational Brain based on their analysis of transient signals in dopamine neurons as a reflection of reward prediction error (Montague et al. 1996). Classical conditioning is found in almost all species, from insects to mammals and has been well studied by psychologists. In the iconic experiment by Pavlov, a bell was sounded just before food was delivered, which induced salivation. After pairing, the bell could itself induce salivation. Dopamine cells in the ventral tegmental area of the midbrain are a neural correlate of this conditioning: They transiently respond to an unexpected reward, such as food. However, after pairing a

x

25th Anniversary Preface

0.0001

0.001

0.01

0.1

1

10

100

1000

10000

100000 1000000

Brain Lobe

1000 2014
100

EEG and MEG

PET imaging

1000 100

Map 10
Nucleus 1
Layer

VSD imaging

TMS

Microstimulation

Optogenetics

fMRI imaging

2-DG imaging

10 Brain lesions
1

Size (mm)

0.1

Light microscopy

0.1

Field potentials

Neuron 0.01

Single units

Dendrite

0.001

Synapse

0.0001

Patch clamp

Calcium imaging

Electron microscopy

0.0001

0.001

0.01

Millisecond

0.1

1

10

100

Time (s)

Second

Minute

1000

Hour

Day

Month

1988

Figure 0.1 The spatiotemporal domain of neuroscience and of the main methods available for the study of the nervous system in 2014. Each region represents the useful domain of spatial and temporal resolution for one method available for the study of the brain. Open regions represent measurement techniques; filled regions, perturbation techniques. Inset, a cartoon rendition of the methods available in 1988, notable for the large gaps where no useful method existed. The regions allocated to each domain are somewhat arbitrary and represent our own estimates. EEG, electroencephalography; MEG, magnetoencephalography; PET, positron emission tomography; VSD, voltage-sensitive dye; TMS, transcranial magnetic stimulation; 2-DG, 2-deoxyglucose. (From Sejnowski et al. 2014)

neutral sensory stimulus, such as a sound, with a reward, the sound will itself elicit a transient dopamine response, but no longer to the reward. Peter and Read modeled this using a reinforcement learning algorithm called temporal differences (TD), which learns that the sound predicts a future reward. In TD learning, the input from the reward to the dopamine neuron is still present, but is canceled by a learned input from the sensory stimulus predicting the future reward. This was confirmed in an experiment with a monkey by Wolfram Schultz, who withheld the reward in some trials and discovered that there was a transient decrease in the firing rate of the dopamine cell when the reward should have appeared (Schultz, et al., 1997). Brain imaging experiments have confirmed these results in humans.

xi

25th Anniversary Preface

TD learning is a weak learner, in the sense that the feedback signal from the reward is a scalarâindicating more or less reward than expected. However, by trial and error a value function can be learned that predicts future rewards for sensory states, which allows an animal to make a sequence of decisions to optimize future rewards. A dramatic example of this was a program designed by Gerald Tesauro, called TD-Gammon, which learned how to play backgammon, a game that depends on an uncertain roll of the dice (Tesauro, 1995). After a million of games, TD-Gammon played at world champion levels. Although this may seem like a lot of games, in fact the program only saw an infinitesimal fraction of all possible board positions (100,000,000,000,000,000,000). This means that TDGammon had to generalize via the value function to new board positions on almost every move.
Simulations of neural network models in the 1980s and 1990s were performed on computers that had less computing power than that of a watch today. The networks had only a few hundred model neurons and a few thousand connections between them. By contrast, todayâs networks have millions of model neurons and billions of connections, which can be organized in a hierarchy with dozens of layers. Deep learning in these networks has produced high levels of performance in tasks that previously only humans could do well, such as speech recognition and object recognition in images (LeCun et al., 2015). Deep recurrent networks have achieved impressive feats in natural language processing such as language translation and scene captioning. What we did not know in the early 1980s when these learning algorithms were developed is how well they would scale with the size of the network. Very few algorithms scale this successfully. In retrospect, perhaps we should have expected some algorithms for learning from examples to scale well. After all, the cerebral cortex, a prodigious learner from examples, greatly expanded during the evolution of mammals, with minimal changes to the architecture of the cortex.
If TD learning is combined with deep learning it might be possible to solve even more difficult problems. AlphaGo is a program that learned to play Go at a championship level using an architecture similar to TD-Gammon, combined with a Monte Carlo tree search, a value function with many more layers of hidden units and another deep learning network for deciding policyâwhich move to make from a given board position (Silver et al., 2016). Go is a famously complex game that is much more difficult than chess, since the board is much bigger (19x19) and requires multiple battles to be waged on different parts of the board. In March, 2016, AlphaGo beat the South Korean Go champion, Lee Sedol, 4 games to 1. This match rightly captured the attention of the world, as did the chess match Deep Blue won over reigning world chess champion Gary

xii

25th Anniversary Preface

Kasparov in 1997. The difference between the achievements is that while Deep Blue used brute force search to look more moves into the future than humans could, AlphaGo learned how to play much as humans do. Some of the moves made by AlphaGo were revolutionary. On the 37th move in the matchâs second game, AlphaGo made a brilliantly creative play that surprised Lee Sedol, who took nearly fifteen minutes to respond and never recovered.
In 1971, Noam Chomsky wrote a devastating attack on behaviorism (Chomsky, 1971). His argument boiled down to the statement that he could not imagine that behavioral training based on classical conditioning could achieve anything as complex as language, an opinion that had a decisive influence on cognitive science in the 1970s. By the 1980s the symbol processing approach to cognition advocated by Chomsky and others was âthe only game in town.â Despite the dismissal of conditioning as a dead end, learned highdimensional models have proved to be a better match to the complexity of the world than low-dimensional logical models, even for language. In retrospect this was a classic case of a failure of imagination and is a good illustration of Leslie Orgelâs Second Law: âEvolution is smarter than you are.â
AlphaGo lives in a protective bubble, overseen by doting parents and programmers with power provided by the grid. Powerful though the models are, they are not yet able to accommodate features important for how animals make a livingâfeatures such as motivation, drives, sociality, and aggression. Important aspects of decision-making depend precisely on the emotional and motivational state of the animal, as well as on its past learning and present predictions. If, as it seems, artificial neural models are epistemological giants, they are nevertheless motivational cripples. Although they are powerful categorizers and predictors, knowledge without the modulation and inspiration of suitable motivation can take you only so far. An epistemologically brilliant rat who feels neither fear nor hunger will not pass on his genes. For many practical purposes, mere knowledge may be enough, and the controller can provide the setting and switch on the system âon.â But to capture the autonomy of animals and how they use knowledge to make a living in a causally and socially complex world will require the broader warp and woof of goals, motivation, caring and wanting.
The new conceptual framework for understanding brain function that is emerging is a more radical departure from the past than simply shifting the focus from single neurons to populations of neurons. Traditional distinctions such as memory, attention, and decision-making were based on psychological studies that were undertaken long before anything was known about neural function. These concepts have guided research into neural mechanisms

xiii

25th Anniversary Preface

but on closer examination it is often difficult to know where attention ends and memory begins, or why neurons in the visual system represent value as well as visual features (Churchland et al., 1994). Brain systems evolved over many millions of years by gradual tinkering, not by engineered design. We have put labels on neural signals and brain areas, such as reward and vision, but in the new framework information is mixed and transformed, and signals traverse feedback loops as well as the more familiar feedforward pathways. The brain is a dynamical system, internally generating shifting spatiotemporal patterns of âspontaneous activityâ whose significance we are just beginning to appreciate.
There is a general feeling that the major advances being made in our understanding of the brain and the dramatic impact of deep learning and TD learning on AI has led us to a tipping point. The current network models are based on what we knew about the brain in the 1960s. The next generation of neural network models will include many more brain systems and be based on a much better understanding of brain architecture and neural plasticity that covers a wider range of time scales. This has been accelerated by the BRAIN Initiative and other international brain programs including the European Human Brain Project and the Chinese Brain Project. The irony is that many of the machine learning algorithms that are now used to analyze big neuroscience data were inspired 30 years ago by the brain itself.
It is difficult to imagine what we will know about the brain when the third edition of The Computational Brain appears in 25 years.
Patricia S. Churchland Terrence J. Sejnowski La Jolla, CA
References
Ahrens, M. B., Orger, M. B., Robson, D. N., Li, J. M., & Keller, P. J. (2013). Wholebrain functional imaging at cellular resolution using light-sheet microscopy. Nature Methods, 10(5), 413â420.
BRAIN 2025. (2014). BRAIN Initiative (Brain Research through Advancing Innovative Neurotechnologies), https://www.nih.gov/science/brain/2025/index.htm
Chomsky, N. (1971). The case against B.F. Skinner. New York Review of Books, 17, 18â24.
Churchland, P. S., Ramachandran, V. S., & Sejnowski, T. J. (1994). A critique of pure vision. In Large-Scale Neuronal Theories of the Brain, Koch, C., Davis, J. (Ed.) Cambridge, MA: MIT Press, 23â60.
LeCun, Y., Bengio, Y., & Hinton, G. E. (2015). Deep learning, Nature, 521, 436â444.

xiv

25th Anniversary Preface

Montague, P. R., Dayan, P., & Sejnowski, T. J. (1996). A framework for mesencephalic dopamine systems based on predictive Hebbian learning. Journal of Neuroscience, 16(5), 1936â1947.
Pasley, B. N., David, S. D., Mesgarani, N., Flinker, A., Shamma, S. A., Crone, N. E., Knight, R. T., & Chang, E. F. (2012). Reconstructing speech from human auditory cortex, PLoS Biol., 10(1): e100125.
Robinson, D. A. (1992). How far into brain function can neural networks take us? Behavioral and Brain Sciences, 15(4), 823â825.
Schultz, W., Dayan, P., & Montague, P. R. (1997). A neural substrate of prediction and reward. Science, 275, 1593â1599.
Sejnowski, T. J., Churchland, P. S., & Movshon, J. A. (2014). Putting big data to good use in neuroscience. Nature Neuroscience, 17, 1440â1441.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., et al. (2016). Mastering the game of Go with deep neural networks and tree search. Nature, 529, 484â489.
Tesauro, G. (1995). Temporal difference learning and TD-Gammon. Communications of the ACM, 38(3), 58â68.

xv

25th Anniversary Preface

Preface
To understand how neurons give rise to a mental life, we must know what they do, both individually as single cells and collectively as coherent systems of cells. The idea that brains are computational in nature has spawned a range of explanatory hypotheses in theoretical neurobiology. This book represents one slant on current computational research relevant to neurobiology, a slant both on the conceptual foundations and the benchmark studies. We mustered our plans for this book against the backdrop of several earlier projects: Parallel Distributed Processing, edited by Dave Rumelhart and Jay McClelland (1986), and Neurophilosophy (P. S. Churchland, 1986). Since those books went to press, much has changed. Powerful new tools for modeling neurons and circuits of neurons have become available, and a conceptual framework for neurocomputational projects has been steadily greening out. Puzzling questions abound on every side, however, concerning such matters as algorithms for weight-setting in neural nets and the extent to which they can be valuable in neuromodeling; concerning biological realism in neural net models and what degree of realism is necessary to make a model useful; and highly focused questions such as what exactly is âHebbian learningâ and what are âgrandmotherâ cells.
The questions that became pivotal in The Computational Brain were questions that have been biting our heels more or less incessantly. The book is thus shaped by what has bothered or beguiled us, individually and jointly. We learned a great deal from the conversations in the laboratory, some of which extended over many months. Francis Crick launched the institution of afternoon tea in the Computational Neurobiology Laboratory at the Salk, and teatime quickly became the daily occasion for close discussion of ideas and data, flying untried balloons, and giving the broad questions a hearing. It was a time for emerging from the comfortable burrows of safe detail into the wide-open prairie of no-holdsbarred. Crick characteristically pushed the questions about how the brain works further and more relentlessly. Moreover, it was

typically his hunches, breadth, and steel-edged skepticism that supplied a sense of balance both when we thought we knew what we were doing and when we were pretty sure we didnât. Virtually everyone who visited the Computational Neurobiology Lab was coaxed or bullied into dilating on the philosophical (grand-scale, background, or fuzzy) questions facing computational neuroscience. From these âconfessions,â we drew ideas and inspiration, and garnered the pluck to stick our necks out a bit.
Several explanations-cum-apologies are in order. The first is for our decision to facilitate easy reading by including only the unavoidable minimum of references in the text itself. We found that long lists of authors in the text make the reader stumble, and hence we elected to use notes for many references rather than follow standard practice in technical writing. Despite our best efforts to refer as fully as possible in the notes, we undoubtedly have missed some essential references, and we apologize in advance for unwitting omissions. The next apology is owed because in choosing instances of research to exemplify a point, we inevitably found ourselves drawing on the research that was most familiar to us, and that often meant research based in California, especially in San Diego. Important and interesting work in computational neuroscience is going on all over the globe, and to have done an exhaustive survey before beginning to write would have meant a deadline receding faster than the progress line. We therefore apologize if we seem rather provincial in our selection preferences. The third apology is for the length. We began the project with the strict understanding that primers are best if brief. In the execution, alas, it became impossible to live within the bounds. As it is, a number of additional topics might well have been included but for permitting the book an embarrassing girth. We therefore apologizeâboth because the book is too long and because it is too short. Fourth, we decided in the interests of smooth reading to abide by the practice of using âheâ as the third-person pronoun referring indifferently to males and females. This reflects nothing ideological. If anything, it is a concession to Mrs. Lundy, whose unflinching dictum in grammar school was that ideological shoe-horning frustrates readability.
Many people helped enormously in writing the book; it simply could not have been done by just the two of us. Most particularly, Paul Churchland gave unstintingly of his imagination and ideas; the daily ritual was to think through everything, page by page, model by model, over capuccino at Il Fornaio. Antonio and Hanna Damasio talked through every major issue with us; they broadened and deepened our perspective in all dimensions, but especially in thinking about what neuropsychological results could tell us about micro-organization. Beatrice Golomb, V. S. Ramachandran, Diane

xviii

Preface

Rogers-Ramachandran, Alexandre Pouget, Karen Dobkins, and Tom Albright helped with representation in general and visual representations in particular; Rodolfo Llinas helped with many issues, but especially in thinking about time; Gyori Buzsaki, Larry Squire, David Amaral, Wendy Suzuki, and Chuck Stevens with plasticity; Carver Mead with thinking about the nature of computation, time, and representation. Shawn Lockery, Steve Lisberger, Tom Anastasio, Al Selverston, Thelma Williams, Larry Jordan, Susan Shefchyk, and James Buchanan gave us much useful advice on sensorimotor coordination. Mark Konishi and Roderick Corriveau gave us invaluable criticism and advice on many chapters and saved us from several embarrassments. Many thanks are also owed to Paul Bush for preparing the glossary, Shona Chatterji for drawing and cheerfully redrawing many figures, Mark Churchland for the cover and for useful criticism, Georg Schwarz for manuscript preparation, and David Lawrence for rescues from macfrazzles. A special debt is owed to Rosemary Miller, whose wit and wisdom kept the boat afloat Others who helped in indispensable ways include: Richard Adams, Dana Ballard, Tony Bell, Anne Churchland, Hillary Chase Benedetti, Richard Gregory, Geoff Hinton, Harvey Karten, Christof Koch, Bill Lytton, Steve Nowlan, Leslie Orgel, Hal Pashler, Steve Quartz, Paul Rhodes, Paul Viola, Ning Qian, and Jack Wathey.
P.S.C. was supported by a University of California Presidentâs Humanities Fellowship, a grant from the National Science Foundation (87-06757), and the James S. McDonnell Foundation. T.J.S. was supported by the Howard Hughes Medical Institute and grants from the Drown Foundation, the Mathers Foundation, the National Science Foundation, and the Office of Naval Research.

xix

Preface

1

Introduction

Major advances in science often consist in discovering how macroscale phenomena reduce to their microscale constituents. These latter are often counterintuitive conceptually, invisible observationally, and troublesome experimentally. Thus, for example, temperature in a gas turned out to be mean kinetic energy of the constituent molecules; the varied properties displayed by matter turned out to be a function of the component atoms and their arcane properties such as electron shells; bacteria-not Divine vengeance-were found to be the proximal cause of smallpox and bubonic plague; and the reproduction of organisms, we now know, depends on the arrangement of four bases in the molecule DNA.
Our psychological life, too, is a natural phenomenon to be understood. Here as well, the explanations will draw on properties of the infrastructure that are certainly veiled and probably arcane, an infrastructure whose modus operandi may seem alien to our customary self-conception. Perhaps this is inevitable, since the very brain we wish to understand is also the brain whose unaided observation is focused at the macrolevel and whose design seems to favor large-scale concepts for the explanation of its own behavior; for example, superstructure concepts such as "is hungry," "wants food," "believes honey is in the hole up the oak tree," and "sees the grizzly bear approaching."
Neurons are the basic structural components of the brain. A neuron is an individual cell, specialized by architectural features that enable fast changes of voltage across its membrane as well as voltage changes in neighboring neurons. Brains are assemblies of just such cells, and while an individual neuron does not see or reason or remember, brains regularly do. How do you get from ion movement across cell membranes to memory or perception in brains7 What is the nature of neuron-neuron connectivity and interadivity7 What makes a clump of neurons a nervous system?
At this stage in the evolution of science, it appears highly probable that psychological processes are in fact processes of the physical brain, not, as Descartes concluded, processes of a nonphysical soul or mind. Since this issue has been discussed at length elsewhere (for example, P.M. Churchland 1984, P. S. Churchland 1986), and since Cartesian dualism is not taken very seriously either in mainstream philosophy or mainstream neuroscience, it is not necessary to repeat the details of the arguments here. Suffice it to say that the

Cartesian hypothesis fails to cohere with current physics, chemistry, evolutionary biology, molecular biology, embryology, immunology, and neuroscience. To be sure, materialism is not an established fact, in the way that the four-base helical structure of DNA, for example, is an established fact. It is possible, therefore, that current evidence notwithstanding, dualism might actually be true. Despite the rather remote possibility that new discoveries will vindicate Descartes, materialism, like Darwinian evolution, is the more probable working hypothesis. That being so, it does not seem worthwhile to modify the basic neuroscience research program and its scaffolding of physicalistic presuppositions to accommodate the Cartesian hypothesis, though scientific tolerance counsels that the door not be closed until the facts themselves well and truly close it. Whether modifications to micro/nano/pico level sciences such as quantum physics will be called for as a result of advances in neuropsychology is likewise conceivable (Penrose 1989), but so far there is no moderately convincing reason to expect that they will.
Arguments from ignorance are to be especially guarded against in this context. Their canonical form is this: neuroscience is ignorant of how to explain X (consciousness, for instance) in terms of the nervous system; therefore
it cannot be so explained. Rather, it can eventually be explained in terms of Y
(pick your favorite thing, for example, quantum wave packets, psychons, ectoplasmic retrovibrations, etc.). The canonical form lends itself to endless seductive variations, particularly ones in which failures of imagination massage intuition: 'We cannot imagine how to explain consciousness in terms of neuronal activity ... ; how could physical processes like ions crossing membranes explain the awfulness of pain?" In its denuded rendition, the argument from ignorance is not mildly tempting, but in full regalia, it may seem beguiling and exactly what reharmonizes such "intuition dissonance" as is provoked by reflecting on the physical basis of the mental. A version of the argument convinced the German mathematician and philosopher, Leibniz (1714), and in the past two decades, variations on Leibniz' basic theme have surfaced as the single most popular and appealing justification for concluding thai: neurobiological explanations of psychological phenomena are impossible. (For instances of the argument in many different and alluring guises, see Thomas Nagel 1974, }. C. Eccles 1977, John Searle 1980, 1990, and Roger Penrose 1989.) From the revolutions wrought by Copernicus, Galileo, Darwin, and Einstein, it is all too apparent that "intuition dissonance" is a poor indicator of truth; it is a good indicator only of how one idea sits with well-favored others. Establishing truth or probability requires rather more.
The working hypothesis underlying this book is that emergent properties are high-level effects that depend on lower-level phenomena in some systematic way. Turning the hypothesis around to its negative version, it is highly improbable that emergent properties are properties that cannot be explained by low-level properties (Popper 1959), or that they are in some sense irreducible, causally sui generis, or as philosophers are wont to say, "nomologically autonomous," meaning, roughly, "not part of the rest of science" (Fodor 1974, Pylyshyn 1984). The trouble with characterizing certain properties as irreduc-

2

Chapter 1

ibly emergent is that it assumes we can tell in advance whether something can be explained-ever explained. Obviously such a claim embodies a prediction, and as the history of science shows all too clearly, predictions grounded in ignorance rather than knowledge often go awry. In advance of a much more highly developed neurobiology than currently exists, it is much too soon to be sure that psychological phenomena cannot be explained in terms of neurobiological phenomena. Although a given phenomenon such as protein folding or awareness of visual motion cannot now be explained, it might yield to explanation as time and science go on. Whether it does or not is a matter of empirical fact, not a matter of a priori divination. Searching for reductive explanations of emergent properties does not entail that we should expect the explanations to be simpleminded or breezily cobbled up or straightforwardly readable off the data points; it means only that the betting man keeps going.
Two groundbreaking discoveries in the nineteenth century established the foundations for a science of nervous systems: (1) macro effects displayed by nervous systems depend on individual cells, whose paradigm anatomical structures include both long tails (axons) for sending signals and treelike proliferations (dendrites) for receiving signals (figure 1.1); (2) these cells are essentially electrical devices; their basic business is to receive and transmit signals by causing and responding to electric current. Within this elegantly simple framework, truly spectacular progress has been made in unravelling the intricate story of exactly how neurons work. In this century, and especially within the

Figure 1.1 Drawing by Cajal based on his Golgi-stained sections of the superior part of the cerebral hemispheres and corpus callosum of a mouse of 20 days. A, corpus callosum; B, antero-
posterior fibers; C lateral ventricle; a, large pyramidal cell; b, callosal fiber bifurcating into a
branch that is arborized in the gray matter and another that continues in the corpus callosum; c, callosal fiber that comes from an axon of the white matter; d, callosal fiber the originates in a pyramidal cell; e. axons of lateral pyramidal cells which follow a descending course in the corpus
callosum without forming part of the commissure; f, f', the two final branches coming from a
fiber of the corpus callosum and arborizing in the gray matter; g, epithelial cells; h, fiber from a large pyramid, giving off a fine collateral to the corpus callosum; i, fusiform cells whose axons ascend to the molecular layer; j, terminal arborization of a callosal fiber originating on the opposite side. (With permission. Santiago Ramon y Cajal, 1890. Reprinted in Defelipe and Jones, eds., 1988, Caja/ on the Cerebral Cortex. Oxford: Oxford University Press.)

3

Introduction

last three decades, an enormous amount has been learned about neurons: about their electrophysiology, microanatomy, connectivity, and development; about the large assortment of neurochemicals that mediate signaling from one neuron to the next; inside a neuron, about the cell's membrane, its roster of channel types, and their specific roles in receiving, integrating, and sending signals; about ,transmitter release, and about the range, structure, and mechanisms of receptors. Even the genetics of the proteins that constitute the various receptors is now steadily coming into view. (Nathans 1987, 1989, Gasic and Heinemann, 1991, Heinemann et al. 1990).
Recent progress in neuroscience is genuinely breathtaking and deservedly captivating. But, the naif might wonder why, if we know so much about neurons, do we not yet understand how the brain works-or at least how, say, the visual system or the motor system works? Assuming that detailed knowledge of the parts automatically confers (or nearly so) knowledge of the whole, then we ought to understand-more or less, at least in silhouette-how animals see, learn, and take action. In fact, however, we do not. Perhaps the hitch is that microlevel progress notwithstanding, we still do not know nearly enough about the fine-grained neural facts. All that is needed, runs this argument, is more of the same-indeed, much, much more of the same. This strategy is sometimes referred to as the pure bottom-up approach. It counsels that if brains are, after all, just assemblies of cells, then once we truly understand every facet of cell function, the principles of brain function will be evident, by and large. Perhaps. But perhaps not.
The overarching contention of this book is that knowledge of the molecular and cellular levels is essential, but on its own it is not enough, rich and thorough though it be. Complex effects, such as representing visual motion, are the outcome of the dynamics of neural networks. This means that while network properties are dependent on the properties of the neurons in the network, they are nevertheless not identical to cellular properties, nor to simple combinations of cellular properties. Interaction of neurons in networks is required for complex effects, but it is dynamical, not a simple wind-up doll affair.
A telling illustration derives from research by Allen Selverston (1988) on the stomatogastric ganglion of the spiny lobster (figure 1.2).1 The network in question contains about 28 neurons and serves to drive the muscles controlling the teeth of the gastric mill so that food can be ground up for digestion. The output of the network is rhythmic, and hence the muscular action and the grinders' movements are correspondingly rhythmic.
The basic electrophysiological and anatomical features of the neurons have been catalogued, so that the microlevel vitae for each cell in the network is impressively detailed. What is not understood is how the cells interact to constitute a circuit that produces the rhythmic pattern. No one cell is responsible for the network's rhythmic output; no one cell is itself the repository of properties displayed by the network as a whole. Where then does the rhythmicity come from? Very roughly speaking, from the pattern of interactions among cells and the intrinsic properties of component cells. What, more precisely speaking, is that? How does the network create rhythm? How is it

4

Chapter 1

Figure 1.2 Diagram of the circuit in the stomatogastric ganglion of the spiny lobster. The circuit normally has 28 neurons, and for each, its connectivity (whom it affects and who affects it), sign of connectivity (excitatory or inhibitory), and mode of effect (chemical or electrical) have been discovered. Labels on cell bodies stand for their individual names. (Courtesy Allen Selverston.)
that the network can produce different rhythms under different biochemical conditions?
Research on the stomatogastric ganglion is legendary in neurobiology, partly because it is a fair test case for the bottom-up strategy: if the purely bottomup approach works anywhere, it should work on the stomatogastric ganglion. If the macrolevel answers are supposed to fall out of the microlevel data, they ought to do so here. Yet we are disappointed. As Selverston ruefully points out, the purely bottom-up strategy has all the earmarks of a half-strategy. Moreover, the plea, "If only more microlevel details of the neurons were discovered, then the explanation would be evident," tends now to fall on skeptical ears. What the stomatogastric ganglion seems to be telling us is that we need to figure out the interactive principles governing the system, and that although interactive hypotheses should be constrained by microlevel data, their job is to characterize higher-level features. Boiled down, the lesson is that microlevel data are necessary to understand the system, but not sufficient. To echo a remark of Maxwell Cowan, even if we did know about all the synapses, all the transmitters, all the channels, all the response patterns for each cell, and so forth, still, we would not know how an animal sees and smells and walks. 2
There is a broader rationale for modeling that goes beyond neuroscience in particular and applies to science generally. Why bother with models at alL one might ask? Why not just perform experiments and record the observations? Though the answers may be obvious, they are perhaps worth listing. First,

5

Introduction

models help organize the data and motivate experiments; they suggest how data might fit together to yield an explanation of a phenomenon. It is, therefore, better to have some model than none at all. In fad, of course, scientists do always have some hypothesis or other that provides the motivational and interpretive framework for their research, though background hypotheses may be neither cleanly articulated nor well-honed. A quantitative model is a step forward because it brings background assumptions to the light of day and permits a more exacting analysis of why they might or might not be true. The further philosophical point is that models increase in believability as they survive tough experimental tests (Popper 1959, P. S. Churchland 1986). Especially in the pioneering days of a discipline, when data are relatively sparse, progress is closely tied to ruling out a class of models and hypotheses. Indefinitely many models can be equally consistent with a set of data; to make real strides one must seek to falsify an ostensibly plausible model. Consequently, models that suggest potentially falsifying experiments are critical.3 Should a model survive a demanding experimental test, to that degree it is more probable; saved from the scrap heap of dead hypotheses, it lives on to be tested against yet further experimental data. Should it be falsified, it then becomes a springboard for the next model.
Computational neuroscience is an evolving approach that aims to discover the properties characterizing and the principles governing neurons and networks of neurons. It draws on both neurobiological data and computational ideas to investigate how neural networks can produce complex effects such as stereo vision, learning, and auditory location of sound-emitting objects. To put it crudely, it has one foot in neuroscience and one foot in computer science. A third foot is firmly planted in experimental psychology, and at least a toe is in philosophy, so evidently the enterprise is multipedal. Of which more anon.
Probably the closest academic kin of computational neuroscience is systems neurobiology, a branch of neuroscience that traditionally has focused on much the same set of problems, but did not explicitly ally itself with computer modeling or with an avowedly information-processing framework for theories. A precocious ancestor went by the name of "cybernetics," which, inversely to systems neurobiology, generally leaned more heavily on the engineering and psychophysical sides, and more lightly on the neurobiological side. Coined more recently, "connectionism" usually refers to modeling with networks that bear only superficial similarities to real neural networks, while "neural net modeling" can cover a broad range of projects. Ironically perhaps, "neural net modeling" is usually identified with computer modeling of highly artificial nonneuronal networks, often with mainly technological significance such as medical diagnoses in emergency wards.4 "PDP" ("parallel distributed processing") is generally the preferred label of cognitive psychologists and some computer scientists who seek to model rather high-level activity such as face recognition and language learning rather than lower-level activity such as visual motion detection or defensive bending in the leech.
As we use the term, "computational neuroscience" aims for biological realism in computational models of neural networks, though en route, rather sim-

6

Chapter I

plified and artificial models may be used to help test and explore computational principles. Academic garden-plotting is a comically imprecise trade because the carrots regularly wander in with turnips and the turnips with the potatoes. Each of us (P.S.C. and T.J.S.) is cheerfully guilty of wandering into neuroscience from his mother discipline, so we emphatically do not mean to tut-tut academic "cross-fielding." On the contrary, we view the blurring of the disciplinary boundaries between neuroscience, computer science, and psychology as a healthy development to be wisely encouraged. In any case, perhaps a crude survey will help orient the greenhorn-or even the old hand-to the clustering of goals, tactics, and prejudices manifest in the "network" game.
The expression "computational" in computational neuroscience reflects the role of the computer as a research tool in modeling complex systems such as networks, ganglia, and brains. Using the word in that sense, one could have also computational astronomy or computational geology. In the present context, however, the word's primary force is its descriptive connotation, which here betokens the deep-seated conviction that what is being modeled by a computer is itself a kind of computer, albeit one quite unlike the serial, digital machines on which computer science cut its teeth. That is, nervous systems and probably parts of nervous systems are themselves naturally evolved computers-organically constituted, analog in representation, and parallel in their processing architecture. They represent features and relations in the world and they enable an animal to adapt to its circumstances. They are a breed of computer whose modus operandi still elude us but are the mother lode, so to speak, of computational neuroscience.
A number of broad dues about computation ih nervous systems are available. First, unlike a digital computer which is general purpose and can be programmed to run any algorithm, the brain appears to be an interconnected collection of special-purpose systems that are very efficient at performing their tal)ks but limited in their flexibility. Visual cortex, for example, does not appear able to assume the functions of the cerebellum or the hippocampus. Presumably this is not because visual cortex contains cells that are essentially and intrinsically visual in what they do (or contain "visons" instead of "auditons"), but rather it is mainly because of their morphological specialization and of their place in the system of cells in visual cortex, i.e., relative to their input cells, their intracortical and subcortical connections, their output cells, and so on. Put another way, a neuron's specialization is a function of the neuron's computational roles in the system, and evolution has refined the cells better to perform those roles.
Second, the clues about the brain's computational principles that can be gleaned from studying its microstructure and organization are indispensable to figuring out its computational organization because the nervous system is a product of evolution, not engineering design. Evolutionary modifications are always made within the context of an organization and architecture that are already in place. Quite simply, Nature is not an intelligent engineer. It cannot dismantle the existing configuration and start from scratch with a preferred design or preferred materials. It cannot mull the environmental conditions

7

Introduction

and construct an optimal device. Consequently, the computational solutions evolved by Nature may be quite unlike those that an intelligent human would invent, and they may well be neither optimal nor predictable from orthodox engineering assumptions.
Third, human nervous systems are by no means exclusively cognitive devices, though the infatuation with cognition fosters a tacit tendency to assume so. Nervous systems must also manage such matters as thermoregulation-a very complex function for mammals-growth, aspects of reproduction, respiration, regulation of hunger, thirst, and motor controL and maintenance of behavioral state, such as sleeping, dreaming, being awake, and so forth. Thus an evolutionary modification that results in a computational improvement in vision, say, might seem to have the earmarks of an engineering prizewinner. But if it cannot mesh with the rest of the brain's organization, or if it marginalizes critical functions such as thermoregulation, the animal and its "prizewinning" vision genes will die. Given these reasons, reverse engineering, where the device is taken apart to see how it works, is a profitable strategy with respect to the brain. By contrast, a pureiy a priori approach, based entirely on reasonable principles of engineering design, may lead us down a blind alley.
Fourth, it is prudent to be aware that our favorite intuitions about these matters may be misleading, however "self-evident" and compelling they be. More specifically, neither the nature of the computational problems the nervous system is solving nor the difficulty of the problems confronting the nervous system can be judged merely by introspection. Consider, for example, a natural human activity such as walking-a skill that is typically mastered in the first year or so of life. One might doubt whether this is a computational problem at all, or if it is, whether it is a problem of sufficient complexity to be worth one's reflection. Since walking is virtually effortless, unlike, say, doing algebra, which many people do find a strain, one might conclude from casual observation that walking is a computationally easy task-easier, at least, than doing algebra. The preconception that walking is computationally rather trivial is, however, merely an illusion. It is easy enough for toy manufacturers to make a doll that puts one foot in front of the other as long as she is held by the child. But for the doll to walk as we do, maintaining balance as we do, is a completely different task Locomotion turns out to be a complicated matter, the ease implied by introspection notwithstanding.
Another computational issue of critical importance in generating hypotheses in computational neuroscience concerns the time available for performing the computation. From the point of view of the nervous system, it is not enough to come up with solutions that merely give the correct output for a given input. The solutions must also be available within milliseconds of. the problem's presentation, and applications must be forthcoming within a few hundred milliseconds. It is important that nervous systems can routinely detect signals, recognize patterns, and assemble responses within one second. The ability of nervous systems to move their encasing bodies appropriately and swiftly was typically selected at every stage of evolution, since by and large natural selection would favor those organisms that could flee or fight preda-

8

Chapter 1

tors, and catch and cache prey. Ceteris paribus, slow nervous systems become dinner for faster nervous systems. Even if the computational strategies used by the brain should tum out not to be elegant or beautiful but to have a sort of evolutionary do-it-yourself quality, they are demonstrably very fast. This tiny response-time rules out as just too slow many kinds of ostensibly elegant computational architectures and clever computational principles. This point is all the more significant when it is considered that events in an electronic computer happen in the nanosecond (10- 9 ) range, whereas events in neurons happen in the millisecond (10-3 ) range.
A related consideration is that organic computers such as brains are constrained in the amount of space available for the essential elements-cell bodies, dendrites, axons, glial cells, and vascularization-and the cranial capacity is in tum limited by the mechanisms of reproduction. In mammals, for example, the size of the pelvic cavity of the mother constrains head size of offspring, and therefore brain size of offspring. What this all means IS that the length of wiring in nervous systems must also be limited-evolution cannot just help itself to indefinite lengths of connecting wire but must make every centimeter count. In a human brain, for example, the total length of wiring is about 108 meters and it has l:o be packed into a volume of about 1.5 liters. The spatial configuration of sense organs and muscles on the body and the relative position of the afferent and efferent systems will also be relevant to the computational genre that has been selected in the evolution of nervous systems (figure 1.3). One strategy the brain uses to economize on wire is to map the processing units so that neighboring units process similar representations. Another strategy involves sharing wire, meaning that the same wire (axon) can be used in coding a large range of representations (Mead 1989). The computal:ional genre adopted for a nervous system will, therefore, be constrained not only by temporal factors but also by spatial factors.
Computation is also limited by power consumption, and on this matter too the brain is impressively efficient. For example, a neuron uses roughly 10-15 joules of energy per operation (e.g., one neuron activating another at a synapse). By contrast, the most efficient silicon l:echnology currenlly requires about 10-7 joules per operation (multiply, add, etc.) (Mead 1989). Using the criterion of joules per operation, the brain is about 7 or 8 orders of magnitude more power efficient than the best of the silicon chips. A direct consequence of their energy efficiency is that brains can perform many more operations per second than even the newest supercomputers. The fastest digital computers are capable of around 109 operations per second; the brain of the common housefly, for example, performs about 1011 operations per second when merely resting.
Finally, there are constraints imposed by the materials of construction. That is, cells are made out of proteins and lipids, they have to rely on mitochondria for their energy supply; nervous systems must have the substances and dispositions necessary for growth and development, and they must exploit such features as the membrane properties of cells and the available chemicals in order to function as an organic computer. Additionally, the nervous system

9

Introduction

.',â¢i',
C. ARCHER FISH

D. RACCOON

Figure 1.3 Evolutionary specializations of manipulation. The octopus (A) can manipulate objects with its tentacles, which are modified limbs; the ant (B) moves things with its pincers, which are modified jaws (mandibles). The archer fish (C) can use its mouth and pharynx to shoot droplets of water at airborne insects, an elementary form of tool use. The raccoon (D) performs dextrous manipulations of foodstuffs with its handlike paws. (From Shepherd 1987.)
needs a constant supply of oxygen and a reliable supply of nutrients. Evolution has to make what it can out of proteins, lipids, membranes, amino acids, etc. This is not altogether unlike the engineering make-do game where the given materials are limited (a finite number of popsicle sticks, rubber bands, and paper clips), and the task, for example, is to build a weight-supporting bridge. Indeed, John Allman (1990) has suggested that brain expansion in homeotherms was spurred by the need to engage in intense prey-catching in order to keep the home fires burning, as it were. In the competition for large amounts of fueL homeotherms with sophisticated neural machinery that upgraded preycatching and predator avoidance would have had an advantage.
Two conceptual ideas have structured much of how we tend to conceive of problems in computational neuroscience. First is the notion of levels, and the

10

Chapter 1

1m

10 em

1 em

Maps

1 mm

100 ~-tm

1gm
0
1 A
Figure 1.4 Schematic illustration of levels of organization in the nervous system. The spatial scales at which anatomical organizations can be identified varies over many orders of magnitude. Icons to the right represent structures at distinct levels: (top) a subset of visual areas in visual cortex (van Essen and Maunsell1980); (middle) a network model of how ganglion cells could be connected to simple cells in visual cortex (Hubel and Wiesel, 1962), and (bottom) a chemical synapse (Kandel and Schwartz, 1985). (From Churchland and Sejnowski 1988.)
second concerns the co-evolution of research on different levels. In the brain, there is both large-scale and small-scale organization, and different functions take place on higher and lower levels (figure 1.4). One sort of account will explain how signals are integrated in dendrites; a different account will explain the interaction. of neurons in a network or the interaction of networks in a system. 5 A model that captures the salient features of learning in networks will have a different face from a model that describes the NMDA channel. Nevertheless, the theories on one level must mesh with the theories of levels both higher and lower, because an inconsistency or a lacuna somewhere in the tale means that some phenomenon has been misunderstood. After all, brains are assemblies of cells, and something would be seriously amiss if neurons under one description had properties incompatible with the same neurons under another description.
That there are levels of organization is a matter of fad; co-evolution of research, on the other hand, is a matter of research strategy in light of the presumed fad. The hallmark of co-evolution of theories is that research at one level provides correction, constraints, and inspiration for research at higher and at lower levels (figure 1.5). Computational space is undoubtedly vast, and the possible ways to perform any task are probably legion. Theorizing at a high level without benefit of lower-level constraints runs the risk of exploring a part of that space that may be interesting in its own right but remote from where

11

Introduction

NECESSARY

NOT NECESSARY

SUFFICIENT

PURE BOTTOM-UP

either TOP-DOWN or BOTTOM-UP

NOT SUFFICIENT

CO-EVOLUTION STRATEGY

PURE TOP-DOWN

Figure 1.5 Possible research strategies for trying to understand how the brain works as they divide on the question of the importance of cellular and molecular levels in theories of brain function. Some neuroscientists may prefer the pure bottom-up strategy; some psychologists and philosophers prefer the pure top-down strategy; probably no one falls into the upper right box, but we added it to round out the possibility-table; the co-evolutionary strategy {lower left) is the one adopted in this book.
the brain's solutions reside. Thus microlevel constraints and the testing of hypotheses against microlevel facts are of central importance.
On the other hand, research from neuropsychology neuroethology, and psychophysics, as well as experimental psychology generally, provide the detailed characterization of what needs to be explained at lower levels. Without a scientific delineation of cognitive and other psychological capacities, lowerlevel research has only incomplete and confused ideas about the very capacity whose mechanisms are the focus of research. Thus, for example, in studying the neuronal mechanisms of motion perception in visual areas MT and MST, it is wasteful not to have up-to-date information on the psychophysics of visual motion detection. Research on computational principles can profitably coevolve with research in neuroscience and in psychology, for something new is to be learned from the brain about powerful computational methods, and neuroscience in return can absorb abstract discoveries in the theory of computing and practical discoveries in the construction of computing machines.
It is on the network level that we decided to concentrate discussion. We made this decision because network models can be more highly constrained by neurobiology than can high-level psychological models and because network models are more straightforwardly testable against the actual networks modeled. At the same time, we prefer models of capacities that are well studied in psychology and neuropsychology so that we can take advantage of economizing constraints from top-down research. This usually means that the capacities themselves will be rather low level; visual motion detection rather than planning, bending in the leech rather than chess playing in the human. For us, this is an advantage since it is discouraging to put effort into a model of some brain function if the model can be assessed only abstractly or aesthetically. Consequently, the models we select for more intensive discussion in this book

12

Chapter I

will be models that generally exhibit the "accessibility" features: the bending reflex in the leech, a model of the vestibulo-ocular reflex (VOR), and models of visual capacities such as stereopsis. Our particular choices are also guided by the independent virtues of these models. There are, of course, some drawbacks to "neurally dose" models. In a nutshell, how realistic should they be before useful results drown in the detail? How much of the teeming detail is needed to get a reasonably accurate model? These questions will be taken up mainly in chapters 3 and 4.
Although "neurally dose" models of "psychophysically dissectable" capacities are our preference, we hasten to say that we recognize that other scientists have quite different preferences, and that ideas useful for the science of the mind-brain may come from widely diverse locations in the research spectrum. Not only are we cognizant of the value of modeling research outside the compass of our particular prejudices, we would be dismayed if everyone were to share our prejudices, for the co-evolutionary advice regarding methodological efficiency is "let many flowers bloom." And at this stage in the history of neuroscience, the advice is entirely reasonable. First, because it is far too early in the hunt to know where the Big Breakthroughs will come or what they will look like. Second, because ultimately what is wanted is a story, unified from top to bottom-from behavior, through systems to networks to neurons and molecules: a unified science of the mind-brain.
Mathematical models and computer simulations of the single neuron have a distinguished tradition, beginning with Hodgkin and Huxley in 1952 and continuing with highly detailed and revealing models of motor neurons (Rail, 1964), Purkinje cells (Bush and Sejnowski, 1991); hippocampal pyramidal cells (Traub et al., in press), and dendritic processing (Segev et a!., in press, Koch et al. 1990). The single neuron is not the main focus of this book, however, since a good choice of accessible texts with that focus already exists.6 To have put single neuron modeling on center stage would have meant not only redescribing the familiar, but also bypassing some little-known but ought-to-be-known models aimed at the network level. Our discussion by no means excludes single neuron models, however, and several detailed cellular models are introduced in the context of network models in which they might find a place (chapters 5 and 6). Moreover, we emphasize the importance of single neuron models as the bedrock and fundament into which network models must eventually fit. Network models are thus considered not in isolation from single neuron models, but as having a future wherein the two enmesh.
The rationale for a primer is threefold. First, we calculated it would be useful to present and discuss the conceptual framework of the emerging discipline of computational neuroscience, accompanied by a selection of sterling or anyhow seminal examples to flesh out the ideas. Sometimes, for both neophyte and cognoscenti, it can be worthwhile to step back from the crowd of trees and have a look at the shape of the woods.
Second, there are four broad constituencies-neuroscience, psychology, computer science, and philosophy-each voicing a specific and entirely legiti-

13

Introduction

mate demand with respect to neuromodeling, and each harboring a specific complaint about other constituencies. Having encountered these demandcomplaint pairs on innumerable occasions and being convinced of the sometime fruitfulness of boundary-fuzzing, we wanted to have a go at satisfying the demands and addressing the complaints. Made concise for presentation and arrayed to display parallelism, the demand/complaint pairs are articulated below:
The neuroscientist:
1. Show me results of neuromodeling that help explain or predict experimental results.
2. They (the nonneuroscientists) do not know anything much about neuroscience even though they are doing "neural modeling."
The psychologist:
1. Show me results of neuromodeling that help explain or predict psychological functions and behavior. 2. They (the nonpsychologists) do not know anything much about the results from psychophysics and psychology even though they are modeling psychological capacities and performance.
The computer scientists:
1. Show me results of neuromodeling that help understand the nature of computation and representation or that yield new ideas about these things.
2. They (the noncomputer scientists) do not know anything much about electrical circuits, mathematical analyses, or existing theory of computation.
The philosopher:
1. Show me results of neuromodeling that are relevant to philosophical problems concerning the nature of knowledge, the self, and the mind.
2. They (the nonphilosophers) do not understand some of the useful, timesaving, and agony-saving contributions of philosophers in constraining questions about how the mind works.
Since the demand/complaint pairs from the various constituencies are related, it seemed reasonable to try to combine our responses in an integrated text as a sort of conversation with diverse people. Moreover, since the constituencies are diverse, we wanted the book to be broadly accessible. New technical books and vast numbers of technical articles are appearing at a dizzying rate, and we judged that a less technical, more introductory text might be helpful in orienting in the midst of the technical literature. Where there are equations, we have given an English paraphrase, but in any event, they are skippable without substantial loss. References at the chapter ends permit the reader to follow up the discussion. To round out the presentation, we judged it necessary to indude a brief exposure to basic neuroscience and to the foundational issues in computational theory. Thus, chapters 2 and 3 provide some background dis-

14

Chapter 1

cussion on neuroscience and the science of computation. An appendix on neuroscience techniques and a glossary are also included.
The third element in the rationale was more self-oriented. The project forced us to leave intermittently the relative solace of the technical details to see what we could discern of the broader landscape and its contours. In a sense, then, the project has been an excuse to paint in broad strokes as well as an exercise in policing our implicit convictions and our covert enthusiasms. We found ourselves constantly hectoring each other with questions of this form, for many values of X and Y: what is the point of X, what does Y really mean, is X really of any use to anybody? In forcing each other to articulate answers, we often bumped up against the illusion that one's assumptions are generally wellhoned, well-grounded, and entirely coherent with the rest of one's beliefs.
In chapters 4 to 7, we assume a basic background knowledge and proceed to introduce computational models. The first ones discussed are rather abstract models of visual functions that incorporate some neuronal details, but where the neural data are still unavailable, perforce they mirror the want of data. Models introduced in later chapters are increasingly realistic neurobiologically. Plasticity, introduced in chapter 5, has been modeled at many levels, from the very spare models that are virtually innocent of physiology, to higher-fidelity models of dendritic spine behavior whose grain is so fine as to include ion concentration parameters and diffusion times. In chapter 6 on sensory-motor integration, we chart the progress in Lockery's modeling of the bending reflex in the leech from a simple static model to the next increment of complexity, a model with dynamical properties, to plans-if not the finished product-for a model that includes channel properties. Likewise, the modeling of adaptation in the vestibulo-ocular reflex, though incomplete, includes the known dynamical and physiological properties of the circuits. Incorporating more cellular detaiL Grillner's model of swimming in the lamprey has many physiological properties, including time constants for cellular responses and channel properties.
Obviously one intends that a model capture the salient features of reality modeled. AI: the same time, however, this desideratum should not equate a high degree of realism with a high degree of scientific value. Different models are useful for different purposes. At certain levels and for certain questions, abstract, simplifying models are precisely what is needed. Such a model will be more useful than a model slavishly realistic with respect to every leveL even the biochemicaL Excessive realism may entail that the model is too bedizened and rich to analyze or understand or even run on the computers available. For other questions, such as dendritic spine dynamics, the more realism at the biochemical level, for example, the better. But even here, the model will probably not be improved by taking into account quantum properties at the level below, or the cell's circuit cohorts on the level above. There is, of course, no decision procedure for the problem: how realistic should my model of X be, for many values of X? Each case has to be thought out on its own, and solved with imagination and horse sense.

15

Introduction

"Data rich, but theory poor" is a description frequently applied to neuroscience. In one obvious respect, this remains true, inasmuch as we do not yet know how to explain how brains see, learn, and take action. Nevertheless, theory in the form of computational modeling is rapidly catching up with the neurobiological data base. To be sure, there is still a great deal of experimental data that has not yet found a modeling home. Although the store is by no means exhausted, the modeling enterprise is slowed by gaps in our experimental knowledge, and these gaps need to be filled before extensive modeling can proceed. The experiments whose results are needed cannot be done by computer-they can be done only by anatomists, physiologists, biochemists, and geneticists working on real nervous tissue; by neuropsychologists studying patients with brain damage; and by psychologists studying normal humans and other animals. A lesson revealed by the modeling efforts is that there are many places where we are data poor, many questions that simply cannot be addressed in a computer model because the relevant data on which the model must rely are not yet available. Of course, assessments of wealth are essentially relative, both to where one was-in which case neuroscience is data rich and theory rich-and where one wants to be-in which case neuroscience is both data poor and theory poor.
The next few decades will be the formative years for computational neuroscience. Predicting what we shall understand of the brain by 2020 is, needless to say, a mug's game. Nevertheless, the hunch that exciting things are in store is difficult to subdue, and the thrill of discovering what we are and how we work is luring more and more students into the field. They often bring with them novel perspectives from their mother fields, as well as a bold inventiveness, a gift prized by a developing field that needs new ideas-unorthodox and otherwise. WithaL it is a remarkable time in the history of science.

16

Chapter 1

2

Neuroscience Overview

1 INTRODUCTION
If we are to understand how the brain sees, learns, and is aware, we must understand the architecture of the brain itself. The brain's computational style and the principles governing its function are not manifest to a casual inspection. Nor can they be just inferred from behavior, detailed though the behavioral descriptions may be, for the behavior is compatible with a huge number of very different computational hypotheses, only one of which may be true of the brain. Moreover, trying to guess the governing principles by drawing on existing engineering ideas has resulted in surprisingly little progress in understanding the brain, and the unavoidable conclusion is thai: there is no substitute for conjuring the ideas in the context of observations about real nervous systems: from the properties of neurons and the way neurons are interconnecl:ed.
This chapter focuses on the "neuroscience" component of the "computational neuroscience" synergy. Ideally, computer modelers should know as much neuroscience as practising neuroscientists. In fact, however, there is too much neuroscience to be thoroughly mastered even by a single neuroscientist. An anatomist may know a lot about his designated region of the visual cortex, rather less about other cortical areas and subcortical brain structures, less again about central pattern generation in the spinal cord, and even less about plasticity of the vestibulo-ocular reflex. Our aim is to prepare the reader, from whatever constituency, for the general conceptual framework we deploy and the specific neurobiological examples discussed within that framework. Consequently, the material in this chapter is organized to cohere with a computational approach to exploring certain aspects of nervous system function. Because levels tum out to be pivotal in our grand scheme of things, characterizing levels in neurobiology is a mal:l:er of the first importance. The presentation in this chapter is therefore keyed to illustrating anatomical and physiological properties seen at different levels. Although understanding the techniques whereby neurobiological data are gathered is also essential, to keep the wagons moving we elected to provide this in the appendix at the end. Although this chapter is meant to provide some basic neuroscience background,
Substantial portions of this chapter are taken from Sejnowski and Churchland (1989).

in the context of specific neurocomputational models, relevant neuroscience will be introduced.
2 LEVELS IN NERVOUS SYSTEMS
Discussions concerning the nature of psychological phenomena and their neurobiological bases invariably make reference to the notion of "levels." In trying to be a bit more precise about what is meant by "level," we found three different ideas about levels in the literature: levels of analysis, levels of organization, and levels of processing. Roughly speaking, the distinctions are drawn along the following lines: levels of organization are essentially anatomical, and refer to a hierarchy of components and to structures comprising these components. Levels of processing are physiological, and refer to the location of a process relative to the transducers and muscles. Levels of analysis are conceptuaL and refer to different kinds of questions asked about how the brain performs a task: into what subtasks does the brain divide the tasks, what processing steps execute a subl:ask, and what physical structures carry out the steps? In what follows, we elaborate on these distinctions.
Levels of Analysis
A framework for a theory of levels, articulated by Marr (1982), provided an important and influential background for thinking about levels in the context of computation by nervous structures. 1 This framework drew upon the conception of levels in computer science, and accordingly Marr characterized three levels: (I) the computational level of abstract problem analysis, decomposing the task (e.g., determining the 3-D depth of objects from the 2-D pattern on the retina) into its main constituents; (2) the level of the algorithm, specifying a formal procedure to perform the task so that for a given input, the correct output results; and (3) the level of physical implementation, constructing a working device using a particular technology. This division really corresponds to three different sorts of questions that can be raised about a phenomenon: (I) how does the problem decompose into parts?, (2) what principles govern how the parts interact to solve the problem?, and (3) what is the stuff whose causal interactions implement the principles?
An important element in Marr's view was that a higher-level question was largely independent of the levels below it, and hence computational problems of the highest level could be analyzed independently of understanding the algorithm which performs the computation. Similarly, the algorithmic problem of the second level was thought to be solvable independently of understanding its physical implementation. Thus his preferred strategy was top-down rather than bottom-up. At least this was the official doctrine though, in practice, downward glances figured significantly in Marr's attempts to find problem analyses and algorithmic solutions. Ironically, given his advocacy of the top-down strategy, Marr's work was itself highly influenced by neurobiological considerations, and implementation facts constrained his choice of problem

18

Chapter 2

and nurtured his computational and algorithmic insights. Publicly, the advocacy of the top-down strategy did carry the implication, dismaying for some and comforting for others, that neurobiological facts could be more or less ignored, since they were, after all, just at the implementation level.
Unfortunately, two very different issues were confused in the doctrine of independence. One concerns whether, as a matter of discovery, one can figure out the relevant algorithm and the problem analysis independently of facts about implementation. The other concerns whether, as a matter offormal theory, a given algorithm which is already known to perform a task in a given machine (e.g., the brain) can be implemented in some other machine which has a different architecture. So far as the latter is concerned, what computational theory tells us is that an algorithm can be run on different machines, and in that sense and that sense alone, the algorithm is independent of the implementation. The formal point is straightforward: since an algorithm is formaL no specific physical parameters (e.g., vacuum tubes, Ca2+) are part of the algorithm.
That said, it is important to see that the purely formal point cannot speak to the issue of how best to discover the algorithm in fact used by a given machine, nor how best to arrive at the neurobiologically adequate task analysis. Certainly it cannot tell us that the discovery of the algorithms relevant to cognitive functions will be independent of a detailed understanding of the nervous system. Moreover, it does not tell us that any implementation is as good as any other. And it had better not, since different implementations display enormous differences in speed, size, efficiency, elegance, etc. The formal independence of algorithm from architecture is something we can exploit to build computationally equivalent machines once we know how the brain works, but it is no guide to discovery if we do not know how the brain works.
The issue of independence of levels marks a major conceptual difference between Marr (1982) and the current generation of researchers studying neural and connectionist models. In contrast to the doctrine of independence, current research suggests that considerations of implementation play a vital role in the kinds of algorithms that are devised and the kind of computational insights available to the scientist. Knowledge of brain architecture, far from being irrelevant to the project, can be the essential basis and invaluable catalyst for devising likely and powerful algorithms-algorithms that have a reasonable shot at explaining how in fact the neurons do the job.
Levels of Organization
Marr's three-level division treats computation monolithically, as a single kind of level of analysis. Implementation and task-description are likewise each considered as a single level of analysis. Yet when we measure Marr's three levels of analysis against levels of organization in the nervous system, the fit is poor and confusing at best.2 To begin with, there is organized structure at different scales: molecules, synapses, neurons, networks, layers, maps, and systems (figure 2.1). At each structurally specified stratum we can raise the computational question: what does that organization of elements do? What does it

19

Neuroscience Overview

contribute to the wider, computational organization of the brain? In addition, there are physiological levels: ion movement, channel configurations, EPSPs (excitatory postsynaptic potentials), IPSPs (inhibitory postsynaptic potentials), action potentials, evoked response potentials, and probably other intervening levels that we have yet to learn about and that involve effects at higher anatomical levels such as networks or systems.
The range of structural organization implies, therefore, that there are many levels of implementation and that each has its companion task description. But if there are as many types of task descriptions as there are levels of structural organization, this diversity could be reflected in a multiplicity of algorithms

A. BEHAVIOR

neurotransmitter or

/ neuromodulalor "-

Q

Q

~~~~Q~~~Â·

second messenger

0 ion

channel aclivily

G. MEMBRANES, MOLECULES, IONS

impulses I II Ill I I I out
j}

E. MICROCIRCUITS

~ ~

F. SYNAPSE

~

~8-

Figure 2.1 Levels of organization in the nervous system, as characterized by Gordon Shepherd
(1988a).

20

Chapter 2

that characterize how the tasks are accomplished. This in tum means that the notion of the algorithmic level is as over-simplified as the notion of the implementation level.
Note also that the very same level of organization can be viewed computationally (in terms of functional role) or implementationally (in terms of the substrate for the function), depending on what questions you ask. For example, the details of how an action potential is propagated might, from the point of view of communication between distant areas, be considered an implementation, since it is an ali-or-none event and only its timing carries information. However, from a lower structural level-the point of view of ionic distributions-the propagating action potential is a computational construct whose regenerative and repetitive nature is a consequence of several types of nonlinear voltage-dependent ionic channels spatially distributed along an axon.

Figure 2.2 A flattened projection of the cerebral cortex in the right hemisphere of the macaque monkey. Stippling indicates cortical areas implicated in visual processing. (Upper left) Lateral view of macaque brain, showing visual areas. (Lower left) Medial view of macaque brain. (Reprinted with permission from van Essen and Anderson I 990.)

21

Neuroscience Overview

UN! LAMINAR
ORIGIN
IS orll

TERMINAnOH
(F, C, orM)

BIL.AMINAR
ORIGIN
(B )

s â¢ 4~ 4~
S.F

_, __,

F BÂ·F

u 4~

ASCENDING

â¢ â¢â¢ (FORWARD)
I II

SUPRAÂ· ClRAINLAA
UYER4
INFRAÂ· CJW<UI.AA

BÂ·C

LATERAL

DESCENDING (FEEDB ACK)

~M

BÂ·M

Figure 2.3 (Top) Schematic diagram of some of the cortical visual areas and their connections in the macaque monkey. Solid lines indicate projections involving all portions of the visual field representation in an area; dotted lines indicate projections limited to the representation of the peripheral visual field. Heavy arrowheads indicate forward projections; light arrowheads indicate backward projections. (From Desimone and Ungerleider 1989.) (Bottom) Laminar patterns of cortical connectivity used for making hierarchical assignments. Three characteristic patterns of termination are indicated in the central column. These include preferential termination in layer 4 (the F pattern), a columnar (C) pattern involving approximately equal density of termination in all layers, and a multilaminar (M) pattern that preferentially avoids layer 4. There are also three

22

Chapter 2

Levels of Processing
The focus for this levels concept is the link between anatomy and what is represented in the anatomy. As a first pass, it assumes that the greater the distance from cells responding to sensory input, the higher is the degree of information processing. Thus the level-rank assigned is a function of synaptic distance from the periphery. On this measure, cells in the primary visual area of the neocortex that respond to oriented bars of light are at a higher level than cells in the lateral geniculate nucleus (LGN), which in tum are at a higher level than retinal ganglion cells. Because the nature of the representations and the transformations on the representations are still poorly understood, only the relative level-x is higher or lower than y-rather than the ordinal levelfirst, second, etc.-is referred to.
Once the sensory information reaches the cerebral cortex, it fans out through cortico-cortical projections into a multitude of parallel streams of processing. In the primate visual system, 25 areas that are predominantly or exclusively visual have been identified (van Essen et al. 1991; figure 2.2). Many (perhaps all) forward projections are matched by a backward projection, and there are even massive feedback projections from primary visual cortex to the LGN. Given these reciprocal projections, the processing hierarchy is anything but a one-way ladder. Even so, by examining the cortical layer into which fibers project, it is possible to find some order in the information flow. Forward projections generally terminate in the middle layers of cortex, and feedback projections usually terminate in the upper and lower layers.3 So far, however, the function of these feedback pathways is not established, though the idea that they have a role in learning, attention, and perceptual recognition is not unreasonable. If higher areas can affect the flow of information through lower areas, then strictly sequential processing cannot be taken for granted (figure 2.3).
The organization typical of earlier sensory areas is only approximately, roughly, and incompletely hierarchical.4 Beyond the sensory areas, moreover, not even that much hierarchy is manifest. The anatomy of frontal cortex and other areas beyond the primary sensory areas suggests an information organization more like an Athenian democracy than a Ford assembly line. Hierarchies typically have an apex, and following the analogy, one might expect to find a
characteristic patterns for the cells of origin of different pathways. Bilaminar (B) patterns, shown on the right, include approximately equal numbers of cells from superficial and deep layers (no more than a 70%-30% split) and are found to occur with all three types of termination pattern. Unilaminar patterns, shown on the left, include predominantly superficial-layer inputs (S pattern) which correlate with F-type terminations, and predominantly infragranular-layer (1 pattern) inputs which correlate with M-type terminations. Within this general framework, a number of variations on a theme can be encountered. Some pathways terminate primarily in superficial layers, but they are grouped with the M pattern because they avoid layer 4. Other pathways are quasi-columnar, but do not include all layers; they are classified as a C pattern if the labeling in layer 4 is neither heavier nor sparser than in adjoining layers. Filled ovals, cell bodies; angles, axon terminals. (From Felleman and van Essen 1991.)

23

Neuroscience Overview

brain region where all sensory information converges and from which motor commands emerge. It is a striking fad that this is false of the brain. Although there are convergent pathways, the convergence is partial and occurs in many places many times over, and motor control appears to be distributed rather than vested in a command center (Arbib 1989, Altman and Kien 1989; figure 2.4).
The assumption that there is a sensory-processing hierarchy, if only to a first approximation, affords the possibility of probing the processing stages by linking various behavioral measures, such as task-relative reaction time (RT), to events taking place in the processing hierarchy at different times as measured by cellular response. To put it more crudely, temporal ordering helps determine what is cause and what is effect. Accuracy of response under varying conditions can be measured, and both humans and animals may be subjects. This is an important method for triangulating the brain areas involved in executing a certain task and for determining something about the processing stages of the task. For example, on the physiological side, one may measure the
CNS
Brain

3
segmental ganglia

musclesB
l
Action ....... 8 r------'
Figure 2.4 Model for decision-making in the insect nervous system. In the CNS, stations 1, 2, 3 contain local networks 1, 2, 3. These stations approximate the brain, the subesophageal (SOC), and segmental ganglia of the locust. The output of each station results from a consensus between the activity of the inputs and the local networks in that station, so the output of each station is different. The stations are thus linked in several parallel loops, and the output of the whole system is the consensus of the activity in all the loops. (From Altman and Kien 1989.)

24

Chapter 2

delay between the presentation of a moving target and the first response by motion-sensitive cells in visual area MT, and on the behavioral side one may measure the response latency relative to degrees of noise in the stimulus. One surprise is that the latencies for signals reaching the visual areas in the cortex are so long, relative to the behavioral RT. The latency for MT is about 5060 msec, and about 100 msec in inferotemporal cortex. Since human RT to a complex object is on the order of 150-200 msec including assembling the motor response, sending the signal down the spinal cord, and activating the muscles, this suggests that surprisingly few processing steps intervene between detection in MT and preparing the response in the motor cortex, striatum, cerebellum, and spinal cord. Such data help constrain theories about the nature of the processing.
By way of illustration, consider a set of experiments by William Newsome and colleagues (1989) in which they show a correlation between the accuracy of the behavioral response to motion detection and the spiking frequency of single neurons responding to motion stimuli in MT. (Newsome et al. 1989) In the task, tiny dots move randomly on a TV screen. The monkey is trained to respond as soon as it detects coherent motion, to either the right or the left. Across trials, what varies is the number of dots moving coherently and their direction of motion. The monkey detects direction of motion with as few as four dots moving coherently, and his accuracy improves as the number of dots moving together increases. What about the cells in MTI Suppose one records from a cell that prefers right-going motion. The visual display is set up so that it is matched to the cell's receptive field, with the result that the experimenter has control of the minimum stimulus needed to produce the maximum response. So long as fewer than four dots move coherently, the cell does not respond. With increasing numbers of dots moving coherently in the cell's preferred direction, the cell responds more vigorously. Indeed, the accuracy curve displayed in the monkey's behavior and the spiking-frequency curve displayed by the single cell are, to a first approximation, congruent (figure 2.5). This implies, to put it crudely, that the information contained in the cellular responses of single sensory neurons and the information contained in the behavioral response are roughly on par. It should, however, be kept in mind that the monkeys were very highly trained on this task and that the sensory stimulus was chosen to match the optimal response of each neuron. In a naive monkey, there may not be such close correspondence between the response of the single cell and the overt behavior.
The next phase of the experiment tests whether the information carried by directionally selective cells found in MT is really used in generating the response. To do this, Newsome and colleagues presented left-going visual stimuli, and at the proper latencyÂ· they electrically stimulated the column containing cells preferring right-going visual stimuli. How did the animal behave? Would the electrical stimuli demonstrate its effectiveness by overriding, at least sometimes, the visual stimuli? The monkey behaved as though he saw right-going stimuli; more exactly, the electrical stimulus decreased the probability that the animal would respond to the visual stimulus and increased the

25

Neuroscience Overview

A 20
!/)
a;
Â£
0 20
zci
20

correlation R 12.8% correlation w 3.2% correlation - 0.8%

0

100

Spikes per trial

.u.....
(J)
0 0.8
u
c :;0:::
ia5 .
::: 0.6
0..

0.1

1.0

10

100

Correlation (%)

Figure 2.5 (a) Responses of a directionally selective neuron (in visual area MT) at three different motion correlations spanning physiological threshold. Hatched bars represent responses to motion in the neuron's preferred direction; solid bars indicate responses to motion 180c opposite to the preferred direction. Sixty trials were performed in each direction for each of the three correlation levels. Response distributions for a range of correlation levels were used to compute a "neurometric" function that characterized the neuron's sensitivity to the motion signal. and could be compared with the psychometric function computed from the monkey's behavioral response. (b) Comparison of simultaneously recorded psychometric and neurometric functions. Opens circles, psychophysical performance of the monkey; filled circles, performance of the neuron. Psychophysical performance at each correlation is given by the proportion of trials on which the monkey correctly identified the direction of motion. Neuronal performance is calculated from distributions of responses of the directionally sensitive MT neuron. The physiological and psychophysical data form similar curves, but the data for the neuron lie to the left of the data for the monkey, meaning that the neuron was somewhat more sensitive than the monkey. (From Newsome, Britten, and Movshon [1989]. Reprinted by permission from Nature 341: 52-54. Copyright 1989 Macmillan Magazines Ltd.)

26

Chapter 2

probability that it would respond as though presented with a stimulus in the opposite direction. This result implies that the cells' responses-and hence the information carried in those responses-are behaviorally significant (figure 2.6).
During the past hundred years, experimental psychologists have assembled an impressive body of RT information, and it is a valuable data base upon which neuroscientists may draw. Thus consider also a set of studies by Requin and colleagues (Requin et al. 1988, Riehle and Requin 1989). In the first stage, they measured the monkey's RT where the task was to make a wrist flexion in a certain direction and by a certain amount as indicated by a signal. There were basically three conditions: the monkeys were precued or not, and if they were precued, the cue indicated either the direction or the extent of the movement. Precuing was found to have a large effect on the RT but only a slight effect on the movement time, showing that precuing has its major effect on programming and preparing for the movement, rather than on the speed of execution of the movement. Additionally, if the advance cue specified where but not how
much, the RT was shortened more than if the cue specified how much but not
where. This suggests that information about extent of movement cannot be efficiently incorporated until the system knows the direction of the movement.
In the second stage, Riehle and Requin investigated the electrophysiological properties of cells in the primary motor cortex (MI) and the premotor cortex (PM). They found execution-related neurons, which were more common in MI, and preparation-related, directionally selective neurons, which were more common in PM. This coheres with other physiological data, and implies that PM probably involves an earlier stage of processing than does MI, since PM has more to do with preparing for the movement than with executing it. Moreover, within the class of preparation-related cells in PM, they found two subclasses: those related to programming the muscle movements, and those related to preprocessing the general components of the movement program. This is another instance of research that narrows down hypotheses about relative order of processing and the structures involved in a distinct aspect of processing by establishing behavioral reaction times and by correlating those data with specific responses of cells.5
3 STRUCTURE AT VARIOUS LEVELS OF ORGANIZATION
Identification of functionally significant structure at various spatial scales in nervous systems proceeds in partnership with hypotheses about a given structure's role in the nervous system's performance and the manner in which that structure's own subcomponents are organized to constitute the mechanisms to carry out that role. To be sure, functional architecture at various spatial scales is all part of one integrated, unified biological machine. That is, the function of a neuron depends on the synapses that bring it information, and, in tum, the neuron processes information by virtue of its interaction with other neurons in local networks, which themselves play a particular role by virtue of their place in the overall geometry of the brain.

27

Neuroscience Overview

Pref LED
â¢

Receptive rleld
Stlmulus aperture
A

â¢
Null LED

Fixation point ~

Elect. stimulus ~ Target LEDs Eye position

Time B

Tl T2

T3

t second

0
â¢

<l)
0

..--o
0

"0 '

â¢ â¢

â¢
0 "

cen
Â·.eQon

"0 '

Q)

"

c "C

~

Qi 0

Qi

5. <l)

c 0

:0;::

0a. "0'

a0 : 0"

â¢

c

----- stimulation
G- -0 no stimulation

â¢

0

â¢ â¢

â¢ â¢
,'o

,G'

o_, .. --b

0

D

c -----,-----~--...,.-Â·--

----- stimulation
G- -0 no stimulation

-30

-20

-10

0

10

20

30

D

Correlation (%)

Figure 2.6 Microstimulation in cortical area MT biases perceptual judgments of motion. (A) Schematic diagram of the experimental protocol showing the spatial arrangement of the fixation point (FP), receptive field (shaded), stimulus aperture (thick circle), and response light emitting diodes (LEOs). (B) Schematic drawing illustrating the temporal sequence of events during a microstimulation trial. At time T1 the fixation point appeared, and the monkey transferred its gaze to the fixation point, as indicated by the deflection in the eye position trace. At time T2 the visual stimulus appeared, and the train of electrical stimulation pulses began. The monkey was required to maintain fixation for 1 sec until time T3 . The fixation point, the visual stimulus, and the microstimulation pulses were turned off at time T3 , and the target LED turned on. The monkey then indicated its judgment of motion direction by making a saccadic eye movement to one of the two response LEOs. (Right) The effect of microstimulation on performance for two stimulation sites in area MT (C and D). The proportion of decisions in the preferred direction is plotted as a function of the percent correlation in the moving dots during the stimulus presentation (positive correlation values indicate motion in the neuron's preferred direction). In half the trials (closed circles), microstimulation was applied simultaneously with the visual stimulus; the other trials (open circles) contained no microstimulation. The shift in the curves caused by the microstimulation is equivalent to adding 7.7% correlated dots (C) and 20.1% (0). (From Salzman,
Britten, and Newsome 1990. Reprinted by permission from Nature 346: 174-177. Copyright Â©
1989 Macmillan Magazines Ltd.)

28

Chapter 2

Accordingly, which structures really constitute a level of organization in the nervous system is an empirical, not an a priori matter. We cannot tell, in advance of studying the nervous system, how many levels there are, nor what is the nature of the structural and functional features of any given leveL Some techniques used to study various levels will be surveyed in the appendix. In this section, seven general categories of structural organization will be discussed. In fact, however, the count is imprecise, for several reasons. Further research may lead to the subdivision of some categories, such as systems, into finer-grained categories, and some categories may be profoundly misdrawn and may need to be completely reconfigured. As we come to understand more about the brain and how it works, new levels of organization may be postulated. This is especially likely at higher levels where much less is known than at the lower levels.
Systems
To standardize references to brain locations, prominent landmarks, including major gyri, fissures, and the major lobes have been labeled (figures 2.7, 2.8). Using tract-tracing techniques, neuroanatomists have identified many systems in the brain. Some correspond to sensory modalities, such as the visual system; others, for example, the autonomic system, respect general functional characteristics. Yet others, such as the limbic system, are difficult to define, and may tum out not to be one system with an integrated or cohesive function. The components of these systems are not neatly compartmentalized but are distributed widely in the brain and are connected by long fiber tracts. For example, a particular brain system for long-term memory may involve such diverse structures as the hippocampus, the thalamus, the frontal cortex, and basal forebrain nuclei (Mishkin 1982). In this respect brain systems contrast quite vividly, and perhaps discouragingly, with systems designed by an engineer, where components are discrete and functions are compartmentalized.
One of the earliest systems concepts was that of a reflex arc, such as the monosynaptic reflex in !:he knee-jerk response (Sherrington 1906; figure 2.9). The pathways of some reflexes have now been traced in great detail; examples are the vestibulo-ocular reflex, which stabilizes images on the retina when the head is moving (Robinson 1981), and the gill withdrawal reflex in Aplysia, which has been a focus for research into the molecular mechanisms of plasticity (Kandel et aL 1987). The reflex arc is not a useful prototype for brain systems in general-or even, it appears, for most reflexes, such as the stepping reflex in the cat, or the nociceptive reflex (withdrawal of limb from a painful stimulus). Take, for example, the smooth pursuit system for visually tracking moving targets, where one pathway originates in the retina, leads to the lateral geniculate nucleus (LGN), to the cortex and through distinct visual topographic areas, down to the pons, and eventually to the oculomotor nuclei (Lisberger et al. 1987). (See chapter6.) Despite the machine-like quality of smooth pursuit, it is to some extent under voluntary control and depends on expectation as well

29

Neuroscience Overview

'-"
0
g
{'!
n...,r
N
2.7 Major gyri and fissures of the human cerebral cortex. (Top) View from the outside or lateral asped, showing left and hemispheres. (Bottom) View of the inside, or medial, aspect of right and left hemispheres. Note that the hemispheres are not exact mirror images of each other. The precise location of gyri and fissures as well as the degree of asymmetry varies from brain to brain. (Courtesy Hanna Damasio.)

Figure 2.8 Major gyri and fissures of the human cerebral cortex. (Left) View from above (dorsal aspect). (Right) View from below (ventral, or inferior. aspect). (Courtesy Hanna Darnasio.)
as the visual stimulus. Behaviors more sophisticated than simple reflexes probably exploit more complex computational principles.
In this regard, two important features of brain systems should be mentioned. First, there are almost always reciprocal (feedback) connections between brain areas, at least as rich in number as the feedforward connections. For example, the recurrent projections from the visual cortical area VI back to the LGN are about ten times as numerous as those from the LGN to the VI. Second, although the simple models of reflex arcs suggest that a single neuron may be sufficient to activate the neuron on which it synapses, in fad a large number of neurons are almost always involved, and the effect of any single neuron on the next is typically quite small. For example, an important feature in the visual system is that input from a specific neuron in the LGN generally makes relatively weak synaptic contacts on a large population of cortical cells rather than a strong synaptic effect on just one or a few neurons (Martin 1984). This implies that cortical neurons rely on a convergence of many afferents, and correlations between pairs of neurons tends to be relatively weak. 6 There may be interesting exceptions to this; for example, chandel!er cells in cortex make inhibitory connections on the axon hillocks of their targets, and they may, as single cells, have a strong, decisive effect on their target cells. Another exception is the strong influence that single climbing fibers have on single Purkinje cells in the cerebellum.
Topographic Maps
A major principle of organization within many sensory and motor systems is the topographic map. For example, neurons in visual areas of cortex, such as

31

Neuroscience Overview

Ia atferem ftber

I
t! Toconulllllwal s ~oaf cord
I

motOf neurons
Figure 2.9 Schematic diagram of the pathways for the stretch reflex. Stretch receptors in musde spindles react to changes in length of the muscle, and afferent fibers carry this information along the dorsal roots to the spinal cord where they synapse on extensor motoneurons, which extend the knee, and inhibitory intemeurons, which reduce activity in motor neurons that produce contractions of the antagonistic flexor muscles. Both of these actions combine to produce a coordinated expression of the knee-jerk reflex. This information is also conveyed to higher brain centers, which in tum can modify the reflex behavior through descending pathways to the spinal cord. (From Kandell985.)
VI, are arranged topographically, in the sense that adjacent neurons have adjacent visual receptive fields and collectively they constitute a map of the retina. Because neighboring processing units (cell bodies and dendrites) are concerned with similar representations, topographic mapping is an important means whereby the brain manages to save on wire and also to share wire (Mead I989). It is significant that the maps are distorted, in the sense that some regions of the body surface occupy larger regions of cortex than others. The fovea, for example, occupies a relatively large part of VI, and the hands occupy a relatively large area of the somatosensory cortex. In visual area MT of the macaque, which contains many neurons selective for direction of motion, the lower half of the visual field has greater representation than the

32

Chapter 2

A
)
91 Ulnar nerve territory 04

\,,;,
Area 1 '

Median nerve territory

Figure 2.10 Schematic drawing of the multiple representations of the body surface in the primary somatic sensory cortex of the owl monkey. Because the cortex of the owl monkey is relatively flat, most of the body representation is located on the surface rather than in the convolutions found in the species of most other primates. (A) Two representations of the hand are shown in areas 3b and 1. (B) The hand of the owl monkey is innervated by the median and ulnar nerves, which have different territory on the ventral surface (Bl) and are represented in adjacent areas of cortex in each of the two maps (B2). The topographical organization of the cortical map for the ventral surface of the hand is highly ordered (B3) in both areas. Cortex devoted to the ventral surface is indicated in white; that devoted to the dorsal surface, in dark shading. 0 1 to 0 5 , digits; P1 to P4 , palmar pads; I, insular pad; H, hypothenar pads; T, thenar pads. (From Kandel and Schwartz 1985).
upper half. This makes sense because it is the lower half of the visual field where hand skills-searching for termites, picking up lice, and so forthrequire the greatest acuity (Maunsell and van Essen 1987.) 7
In the visual systems of monkeys, physiologists have found about 25 distinct areas, most of which are topographically mapped. 8 A similar hierarchy of multiple topographic maps is found for body location in the somatosensory system (Kaas et al. 1979; figure 2.10), for frequency in the auditory system (Merzenich and Brugge 1973), and for muscle groups in the motor system (Ferrier 1876, Asanuma 1973). One possible exception is the olfactory system, but even odors may be spatially organized at the level of the olfactory bulb (Stewart et al. 1979). To some extent the different sensory maps can be distin-

33

Neuroscience Overview

guished by differences in the fine details in the laminations of neurons (see next section) and their cellular properties, but often these are so subtle that only physiological techniques can distinguish boundaries between different cortical areas.
Some brainstem structures, such as the superior colliculus, also display this organization. The cerebellum appears to have patches of partial maps, though the principles do not seem clear, and these areas may not be maps in any real sense at all. Some areas seem to lack a strong topographic organization, and for other areas the topographic organization is quite complex, for example the basal ganglia (Selemon and Goldman-Rakic 1988). Cortical areas anterior l:o the central sulcus seem sparser in topographic maps, but research may show that what they map are abstract, not sensory, representations, and hence such maps cannot be discovered by methods used to establish response patterns to peripheral stimuli. In bat auditory cortex there are topographic mappings of abstract properties such as frequency differences and time delays between emitted and received sounds, properties that may help the bat to echolocate prey (Suga et al. 1984), and in the bam owl internal spatial maps are synthesized from binaural auditory inputs (Konishi 1986, Knudsen et al. 1987). There are some areas of cortex, such as association areas, parietal cortex, and some parts of frontal cortex, for which it has no!: yet been possible to find properties that form orderly mappings. Nonetheless, projections between these areas remain topographic. For example, Goldman-Rakic (1987) has shown that in the monkey projections from parietal cortex l:o target areas in !:he prefrontal cortex, such as the principal sulcus, preserve the topographic order of the source neurons.
Maps of the surface of the body in the brain are formed during development by projections that become ordered, in part, through competitive inl:eractions between adjacent fibers in the target maps (see chapter 5). Some of the neurons undergo cell death during this period, and with the possible exception of olfactory receptors, no new neurons are formed in the mature mammal (Cowan et al. 1984). However, competitive interactions between neurons continue, to some extent, even in adulthood, since the territory in cortex devoted to a particular part of the body surface can shift as much as 1-2 em, but not much farther, weeks after injury to sensory nerves or after excessive sensory stimulation (Pons et al. 1991). Thus, regions in somatosensory cortex that are silenced following denervation of a sensory nerve will eventually become responsive to nearby regions of the body. It is not yet known how much of this rearrangement is due to plasticity in cerebral cortex, or perhaps in subcortical structures that project to cortical maps. Auditory maps, particularly in the superior colliculus, are also modifiable both in development, and in the adult following partial deafness (King and Moore, 1991). Nonetheless, this evidence, and further evidence for synaptic plasticity summarized below, make it difficult to think of the machinery in the adult brain as "hardwired," or static. Rather, the brain has a remarkable ability to adapt to changes in the environment, at many different structural levels and over a wide range of time scales.

34

Chapter 2

lmm

Figure 2.11 Cross-section through monkey striate cortex using cresyl violet to stain cell bodies. Laminations are clearly visible; the layers are numbered at the left. W, white matter. Deeper layers of the buried fold of cortex are shown in the lower part of the figure. (From Hubel and Wiesel 19 77.)
Layers and Columns
Many brain areas display not only topographic organization, but also laminar organization (figures 2.11, 2.12). Laminae are layers (sheets) of neurons in register with other layers, and a given lamina conforms to a highly regular pattern of where it projects to and from where it receives projections. For example, the superior colliculus receives visual input in superficial layers, and in deeper layers it receives tactile and auditory input. Neurons in an intermediate layer of the superior colliculus represent information about eye movements. In the cerebral cortex, specific sensory input from the thalamus typically projects to layer 4, the middle layer, while output to subcortical motor structures issues from layer 5, and intracortical projections originate chiefly in (superficial) layers 2 and 3. Layer 6 mainly projects back to the thalamus (see figure 2.3). The basal ganglia do not have a laminar organization, but rather a patchwork of islands which can be distinguished by developmental and chemical markers (Graybiel and Hickey I 982).
As well as the horizontal organization seen in laminae, cortical structures also display vertical organization. This organization consists in a high degree of commonality between cells in vertical columns, crossing laminae, and is

35

Neuroscience Overview

(AI AFFERENTS
II& III
JVa , b
JVc
v
VI
fl '11
4~
A laminae C laminae
Lateral geniculate nucleus
(8)

RECEPTIVE FIELD TYPES
"~c
Simple Standard Special complex complex

EFFERENTS

.f ',Â·â¢..â¢

â¢ â¢ ' '

â¢

t

â¢

uuu

Other Superior Lateral

cortical colliculus geniculate

areas

nucleus

II& III

IVa,b lVc
v

VI
- Â·- - - - -- - -

Geniculate axons

Cortex

Cortex

Superior colliculus

Lateral geniculate nucleus

Figure 2.12 Schematic diagram of cortical connections in the cat. (A) Distribution of inputs from layers of the lateral geniculate, showing that geniculate axons project to different cortical laminae as a function of layer of origin in the geniculate. Cortical neurons with similar receptive field properties cluster together in particular lamina. The origin of fibers leaving a region of cortex varies as a function of target. (B) Schematic arborization patterns of the main cell types in laminae 1-VI. (After Gilbert and Wiesel198 1.)

36

Chapter 2

reflected both anatomically in terms of local connections between neurons (Martin I984, Lund I987) and physiologically in terms of similar response properties (Hubel and Wieseli962). For example, a vertical penetration of an electrode in visual cortex reveals cells which share a preference for stimuli with the same orientation (e.g., a bar of light oriented at about 20Â° from the horizontal). Another vertical penetration nearby will show cells which prefer a different orientation. Inputs and outputs are also organized in columns, such as the ocular dominance columns in VI, and inputs into the principal sulcus which alternate between parietal projections from the same side and projections from the principal sulcus in the opposite hemisphere (Goldman-Rakic I987).
Typically, the vertically organized connectivity patterns do not result in columns with sharp boundaries, and the response properties tend to vary continuously across the cortex. Hence the expression "vertical column" may be slightly misleading. Thus for cells in visual area VI, orientation varies over the cortex smoothly, save for some fractures and singularities (Blasdel and Salama I986), and a similar organization can be found in area V2 (Swindale et a!. I987), which receives a topographically mapped projection from VI. There are, however, places where vertical, cross-laminar columns with quite sharp boundaries are seen, for example the ocular dominance columns in layer 4 of area VI and the "barrels" in the rodent somatosensory cortex, where each barrel contains cells preferentially sensitive to stimulation of a particular whisker (Woolsey and van der Loos I970) (figure 2.13). Sharp anatomical boundaries are, however, the exception rather than the rule. Also, the spatial scale of columnar organization can vary from about 0.3 mm for ocular dominance columns to 25 Jim for orientation columns in monkey visual cortex.
Topographic mapping, columnar organization, and laminae are special cases of a more general principle: the exploitation of geometric properties in information processing design. Spatial proximity may be an efficient way for biological systems to assemble in one place information needed to solve a problem. To consider a simple case, suppose it is necessary to compare differences between stimuli at neighboring locations, where comparison requires signals be brought together. Then topographic organization may achieve this efficiently while minimizing the total length of the connections. This is desirable since most of the volume of the brain is filled with axonal processes, and there are limitations on how big the brain can be as well as temporal tolerances that must be met. Lateral inhibitory interactions within the spatial maps are used to make comparisons, enhance contrast at borders, and perform automatic gain control. Mutual inhibition within a population of neurons can be used to identify the neuron with the maximum activity, a type of winner-take-all circuit (Feldman and Ballard I982). (See also chapter 5, last section.)
Local Networks
Within a cubic millimeter of cortical tissue, there are approximately 105 neurons and about I09 synapses, with the vast majority of these synapses arising from cells located within cortex (Douglas and Martin I99I) (figure 2.I4). These

37

Neuroscience Overview

Figure 2.13 (A) Snout of a mouse; the vibrissae (whiskers) are marked by dots. (B) Sections across the somatosensory cortex that receive input from the snout. Each of the rings or "barrels" corresponds to an individual vibrissa, and are spatially organized to preserve the neighborhood relations of the vibrissae (C). (Reprinted with permission from Woolsey and van der Loos 1970.)

38

Chapter 2

Thalamus
Figure 2.14 Schematic diagram of a microcircuit in the cerebral cortex that may be repeated again and again. Three populations of neurons interact with each other: inhibitory (GABA) cells, shown with solid synapses; and excitatory cells (open synapses) representing (i) superficial
(P2 + 3) and (ii) deep (PS + 6) layer pyramidal cells. Each population receives excitatory input
from the thalamus, which is weaker (dashed line) to deep pyramidal cells. (Reprinted with permission from Douglas et a!. 1989.)
local networks have been very difficult to study owing to the complexity of the tangled mass of axons, synapses, and dendrites called the neuropil. Nevertheless, some general features of local networks are beginning to emerge. For example, the orientation tuning of cells in VI must emerge from nonoriented inputs and activity in local networks in ways that we are just beginning to understand (Ferster and Koch 1987).
Most of the data available on local networks are based on single-unit recordings, and to achieve a deeper understanding of the principles governing networks, it will be necessary to monitor a large population of neurons (see Appendix for recording techniques). Even a local network involves many cells, but only small populations can be studied by exhaustive sequential recordings from single cells. Consequently, we run the risk of generalizing from an atypical sample, and of missing circuit properties that can be inferred only from a richer profile. Therefore, to understand the principles of local networks, much more work must be done to determine the dynamical traffic within a larger population of cells over an extended period of time (figure 2.15).
Computer simulations may help to interpret single-unit data by showing how a population of cells could represent properties of objects and perform coordinate transformations. For example, network models of spatial representations have been constructed that help to explain the response properties of single cells in parietal cortex (Andersen and Mountcastle 1983, Zipser and Andersen 1988; figure 2.16). Another network model has been used to explain how the responses of single neurons in visual cortex area V4 could compute color constancy (Zeki 1983, Hurlbert and Poggio 1988). Network simulations can also suggest alternative interpretations for known response properties. For

39

Neuroscience Overview

c
0

'76

Input

E
~

0..
e!

c

0
Â·~

::J
0..

c
0
'16

E
0 (.)

'5

0..
E
c 0
uQ (.)

uc::J

0 (.)

Figure 2.15 (Upper right) Network of pyramidal neurons in mouse cortex, stained by the Golgi method, which stains only about 10% of the population. (Lower left) Schematic of a generalized neuron showing one of its inputs to a dendrite, one to the cell body, and one of its axonal contacts. (Reprinted with permission from Dudai The Neurobiology of Memory: Concepts, Findings, and Trends [1989]. Copyright Oxford University Press.)
example, there are certain oriented cells in VI whose response summates with the length of the slit or edge of light up to the borders of the receptive field, but then the response diminishes as the length increases. This property, called "end-stopping," has recently been related to the extraction of the 1-D curvature of contours (Dobbins et aL 1987) and the 2-D curvature of shapes in shaded images (Lehky and Sejnowski 1988). An example of this approach is given in chapter 4 on visual processing.
Neurons
Ever since Cajal's work in the late nineteenth century, the neuron has been taken as an elementary unit of processing in the nervous system (figure 2.17). In contrast to Golgi, who believed neurons formed a continuous "reticulum," or feltwork, Cajal argued that neurons were distinct, individual cells, separated from each other by a spatial gap, and that mechanisms additional to those operating intracellularly would have to be found to explain how the signal

40

Chapter 1

,,

II

:

II

I

II

:

II
ilII

! ~ I

D

----------------If----------~----------------------------

11

:

i l

D i

I

' ..â¢ ' ... '

.:Â·: :~tÂ·....~.;".~:.:Â·. :":Â· :

.......... .. . â¢ :
â¢:

.. ,"

â¢ II I
' "' "'

.... . ... ' ..... ....â¢ .....

Figure 2.16 Illustration of the single-unit technique used to study the response of neurons in the parietal cortex of an awake, behaving monkey. The animal fixated a smail target light placed at a series of positions on the screen, with its head fixed. The results obtained at two positions are shown here. At each fixation position a square was flashed for I sec at 10Â° above the point of fixation. Recordings from a single neuron are shown below the screen. Each line represents a single triaL and each small nick made on the line represents the discharge of an impulse by the neuron. The impulses were summed in the histograms on the bottom panel. The right side of the figure shows the responses for fixation to the left and down, and the left side shows the responses for fixation to the right and up. This and other experiments show that this class of neurons in parietal cortex has receptive fields that are specific to a retinal location, but the degree of activation of the neuron to a visual stimulus within the receptive field is modulated by the position of the eye. (See also chapter 4, section 10.) (From Andersen and Mountcastle 1983.)

41

Neuroscience Overview

BIPOLAR CELL FROM RETINA
Dendrite

PYRAMIDAL CELL FROM CORTEX
_...-Dendrite

_.-Axon

..--Axon

Figure 2. I 7 Examples of neurons illustrating the variety of shapes in different areas of the brain. (With permission from Kuffler, Nicholls and Martin [1984]. From Neuron to Brain. Sunderland MA: Sinauer Associates.)
passed from neuron to neuron. Physiological studies have borne out Cajal's judgment, though in some areas such as the retina, syncytia of cells that are electrically coupled have been found (Dowling 1987). As it turns out, these are rather more like the structures Golgi predicted because the cells are physically joined by conducting "gap junctions." These electrical synapses are faster and more reliable than chemical transmission, but are more limited in flexibility.
There are many different types of neurons, and different parts of the nervous system have evolved neurons with specialized properties. There are five general types of neurons in the retina, for example, each with a highly distinctive morphology, connectivity pattern, physiological properties, and embryological origin. In recent years, moreover, physiological and chemical differences

42

Chapter 2

A

mV -68

.5.m.s..

~Â·
-72

mV

B

-68

c
Active Active
~~
i-lt ~ ~
(9~ 69

Active Inactive

Figure 2.18 Inhibitory and excitatory synapses on a neuron. (A) The inhibitory postsynaptic potential (IPSP) means that the postsynaptic cell hyperpolarizes (dropping from -70 mV to -72 mV), and the excitatory postsynaptic potential (EPSP) means that the postsynaptic cell depolarizes (from -70 mV to -67 mV). (B) The EPSP was triggered about I , 3, and 5 msec after the onset of the IPSP. (C) The subsynaptic conductance changes occurring when excitatory and inhibitory synapses are activated simultaneously (left) and when only the excitatory synapse is activated (right). (From Schmidt [1978]. Fundamentals of Neurophysiology . Berlin: SpringerVerlag.)

have been found within classes. For example, 23 different types of ganglion cells (whose axons project to the brain through the optic nerve) and 22 different types of amacrine cells (which provide lateral interactions and temporal differentiation) have been identified (Sterling et a!. 1983). There are seven general types of neurons in the cerebellum and about 12 general types in the neocortex, with many subtypes distinguishable by their chemical properties such as the neurotransmitters they contain. The definition of a neuronal type is somewhat arbitrary, since judgments are often made on the basis of subtle morphological differences, which can be graded rather than categorical. As more chemical markers are found, however, it is becoming clear that the diversity of neurons within cerebral cortex has been vastly underestimated. On anatomical and immunocytochemical criteria, therefore, the number of subtypes of cortical neurons is probably between 50 and 500 (Sereno 1988).

43

Neuroscience Overview

Axon hillock

Myelinated axon

Trigger; ollÂ·orÂ·none spike initiated

Conducted all- or- none spike (conduction of spike to next cell)

Figure 2.19 Summary diagram showing the location on a motor neuron of various electrical events. In many neurons, dendrites and cell bodies respond with graded EPSPs or IPSPs; the action potential is triggered in the axon hillock and travels undiminished down the axon. (From Thompson 1967.)

On the basis of their effects, neurons divide in two general classes: excitatory and inhibitory (figures 2.18, 2.19). The effect of an excitatory signal is to increase the probability that the postsynaptic cell fires, and the effect of an inhibitory signal is to decrease that probability (figure 2.19). Some neurons also have modulatory effects on other neurons, principally by releasing peptides or monoamines (see section 4). Another useful classification concerns projections: some cells ramify only within a confined area such as a column, for example stellate cells in cortex; other neurons, such as pyramidal cells, have long-range projections out of an area, where the route goes via the white matter rather than directly through the cortex itself. Research on the properties of neurons shows that they are much more complex processing devices than previously imagined (table 2.1). For example, dendrites of neurons are themselves highly specialized, and some parts can probably act as independent processing units (Shepherd et al. 1985, Koch and Poggio 1987).
Synapses
Chemical synapses are found in nervous systems throughout phylogeny, and they are a basic unit of structure that has been highly conserved during evolution. A synaptic bouton has a surface area of a few square micrometers and forms a highly stereotyped apposition with the postsynaptic membrane, which itself is highly specialized (figure 2.20). Synapses are the primary gateways by which neurons communicate with one another, and they consist of specialized

44

Chapter 2

Table 2.1 Selected biophysical mechanisms, possible neural operations they could implement, and computations they might help perform

Biophysical Mechanism

Neural Operation

Example of Computation

Action potential initiation
Repetitive spiking activity
Action potential conduction Conduction failure at axonal branch points Chemically mediated synaptic transduction
Electrically mediated synaptic transduction
Distributed excitatory synapses in dendritic tree
Interaction between excitatory and (silent) inhibitory conductance inputs Excitatory synapse on dendritic spine with calcium channels Excitatory and inhibitory synapses on dendritric spine Quasi-active membranes
TransmiHer regulation of voltage-dependent channels (M-current inhibition) Calcium sensitivity of cAMP-dependent phosphorylation of potassium channel protein LongÂ·distance action of neurotransmitter

Analog OR/AND one-bit analog-to-digital converter Current-to-frequency transducer Impulse transmission
Temporal/spatial filtering of impulses Nonreciprocal two-port "negative" resistance Sigmoid "threshold" Reciprocal one-port resistance
Linear addition
Analog AND-NOT, veto operation
Postsynaptic modification in functional connectivity

Long -distance communication in axons Opener muscle in crayfish
Coupling of rod photoreceptors to enhance detection of signals
IX, f3 cat retinal ganglion
cells Bipolar cells Directional-selective retinal ganglion cells Disparity-selective cortical cells Short- and long-term information storage

Local AND-NOT "presynaptic inhibition"
Electrical resonant filter analog Differentiation delay Gain control
Functional connectivity

Enabling/ disabling retinal input to geniculate X-cells
Hair cells in lower vertebrates
Midbrain sites controlling gain of retinogeniculate transmission Adaptation and associative storage of information in Aplysia

Modulating and routing transmission of information

From Koch and Poggio (1987).

45

Neuroscience Overview

Dense Dendritic

extracellular

Figure 2.20 Schematic diagram of a synapse on a dendritic spine. Dense projections in the presynaptic membrane are surrounded by vesicles that presumably contain neurotransmitter molecules. This morphology characterizes a type I synapse, which is excitatory. Type II synapses (not shown) have flattened vesicles as viewed in the electron microscope following glutaraldehyde fixation. and they are often inhibitory. (From Gershon et al. 1985.)
presynaptic structures for the release of neurochemicals and postsynaptic structures for receiving and responding to those neurochemicals. Evidence is accumulating that signaling between neurons at synapses can be selectively altered by experience (Alkon 1984). Other, structural components of neurons might also be modified through experience, such as the shape and topology of dendrites as well as the spatial distribution of membrane channels (Purves and Voyvodic 1987).
Our understanding of the nervous system at the subcellular level is changing rapidly, and it is apparent that neurons are dynamic and complex entities whose computational properties cannot be approximated by memoryless response functions, a common idealization. It remains an open scientific question how the integrity of memories that span decades can remain intad if the neural substrate is as fluid as preliminary reports indicate, especially if, as it seems, networks of neurons both process and store information.

46

Chapter 2

""""""'Lipid ....,..,. :::::::. bilayer::

protein
Figure 2.21 Working hypothesis for a voltage-gated channel. The transmembrane protein is shown with a pore that allows sodium ions to flow between the extracellular and intracellular sides of the membrane when the gate is open. (With permission from Hille [1984]. Ionic Channels of Excitable Membranes. Sunderland MA: Sinauer Associates.)
Molecules
The integrity of neurons and synapses depends on the properties of membranes and the internal cytoskeleton of the neuron. The membrane serves as a barrier a few nanometers (10- 9 ) thick separating the intracellular and extracellular aqueous compartments. The membrane itself is a two-dimensional fluid medium in which integral membrane proteins and other molecules form associations. Some integral membrane proteins have an important role in maintaining the ionic milieu inside and outside the cell. For example, membrane proteins that serve as ion channels can be voltage sensitive,9 chemically activated, or both. They may thus permit or prevent the passage of ions across the membrane, which in tum can affect the propagation of a signal down the length of the axon or neurotransmitter release at the presynaptic terminal (figure 2.21). In a sense, the membrane allows the intracellular compartment of a neuron to respond selectively to extracellular signals, and it is this selectivity that endows different neurons with specialized information-processing capabilities. Axon membrane typically contains channels and conductances

47

Neuroscience Overview

that permit it to spike when depolarization reaches a certain threshold. Exactly how dendrite membrane works is much less well understood. Dendrite spiking has been seen in the cerebellum (Llimis and Sugimori 1980), and the conventional wisdom according to which axon membrane is "active" while dendrite membrane is "passive" is undoubtedly a simplification that obscures the subtle, complex, and computationally critical respects in which dendrite membrane is active.
Electrical signaling in neurons is achieved by ionic currents which are regulated by ion channels and ion pumps in the cell membrane. Signaling between neurons is mediated by neurotransmitter receptors in the postsynaptic membrane that respond to particular neurotransmitter molecules by transiently and selectively changing the ionic conductance of the membrane. There are also receptor molecules along the membrane outside of the synaptic site that appear to be functional, but their role is not known (Somogyi et al. 1989). In addition, some receptors can activate one or more second-messenger molecules that can mediate longer-term changes (figure 2.22). Second-messengers in neurons can be activated by more than one receptor. Hence there is a network of interacting chemical systems within a neuron, which can itself be considered a chemical parallel distributed processor.
4 A SHORT LIST OF BRAIN FACTS
A central part of the basic strategy for figuring out how a novel device works is reverse engineering. That is, when a new camera or chip appears on the market, competitors will take it apart to find out how it works. Typically, of course, they already know quite a lot about devices of that general kind, so the problem can be manageable. Although we have to use reverse engineering to study the brain, our starting point is much further back, inasmuch as we know so little about devices of that general kind. From our vantage point, the brain is essentially a bit of alien technology, and hence it is especially difficult to know, among the facts available to us, which are theoretically important and which are theoretically uninteresting. We may actually misunderstand some aspects of brain organization and as a consequence be blocked from having some important insight into mechanisms crucial for cognition. For example, some distinctions made in gross anatomy may tum out to conceal close relationships between distant brain regions, or it may tum out that the functional properties of some synapses in the central nervous system are very different from peripheral synapses in autonomic ganglia and neuromuscular junctions, which have been very well studied.
Since this chapter looks at neuroscience against a background of computational aims, it seems appropriate to raise this question: what are the most basic structural features relevant to neural computal:ion? It goes without saying that many more constraints will be relevant in the context of a specific problem, but we present these 13 as a kind of prolegomenon to problems generally. Short of having formally conducted a proper survey, we conjecture that the following baker's dozen are among those likely to find their way on to a must-know list,

48

Chapter 2

Biosynthesis Transport

E. D'. Insertion

@Conductance Change

+

+

@ Synaptic + -
Potential + _

+

+

+

Figure 2.22 Summary of some of the main biochemical mechanisms that have been identified at chemical synapses. A-E, Long-term steps in synthesis, transport, and storage of neurotransmitters and neuromodulators; insertion of membrane channel proteins and receptors, and neuromodulatory effects. 1-12, these summarize the more rapid steps involved in immediate signaling at the synapse.IP3, inositol triphosphate; CaM II, Ca2+ /calmodulin-dependent protein kinase II; DAG, diacylglycerol; PK, protein kinase; R, receptor; G, G protein; AC adenylate cyclase. (Reprinted with permission from Shepherd 1988.)

49

Neuroscience Overview

DISTANCE

JQ-Hl 10-9

10 6

10-3

100

103

A m,um

,urn

mm

m

km

ANGSTROM

MICROMETER MILLIMETER METER

(MICRON)

TIME

10-6 ,usee

10-3

100

msec

sec

MILLISECOND

Figure 2.23 Logarithmic scales for spatial and temporal magnitudes. Brackets indicate the scales especially relevant to synaptic processing. (Reprinted with permission from Shepherd 1979.)

Object Discrimination

Landmark Discrimination

Figure 2.24 Two behavioral tasks that distinguish between the functions of the inferior temporal (IT) and posterior parietal (PP) cortex. (Left) IT lesions, in black, cause a severe impairment in learning to discriminate between two objects based on their features. but the lesions do not affect spatial capacities. (Right) PP lesions cause an impairment to spatial tasks, such as judging which of two identical plaques is closer to a visual landmark (cylinder), but do not affect object discrimination learning. (From Mishkin, Ungerleider and Macko [198.3]. Object vision and spatial vision: two cortical pathways. Trends in Neurosciences 6: 414-417.)

although we understand very well that opinion can diverge in considerable and surprising ways, and also that a current list will undoubtedly be quickly outdated (for comparable lists, see Crick and Asanuma I 986 and Shepherd I988). (See figure 2.23 for scales of magnitudes.)
I. Specialization of Function There is specialization of function in different regions of nervous systems. This is a ubiquitous and critical feature of nervous system organization, seen in animals from the lowly leech to the human. The specialization enjoyed by regions more distant from the periphery, such as orbitalfrontal cortex of humans, is difficult to determine, though by using a convergence of techniques, including lesions, staining, single-cell recording, evoked potential, and developmental data, the range of likely possibilities can be narrowed (figure 2.24). Specialization so characterized is actually a largegrain feature of an area, based on the statistical distribution of cell response properties and the major input and output pathways. Thus VI, for example, is referred to as a visual area and SI as a somatosensory area. At a finer grain,

50

Chapter 2

however, the specialization of areas is consistent with the existence of atypical cell types and connectivity. Thus while the preponderance of tested cells in VI are indeed visually tuned, there exist some cells coding for nonvisual signals, such as eye movement.
2. Numbers: Neurons and Synapses The estimated number of neurons in the human nervous system is about 1012 ; the number of synapses is about 1015. The rat brain has about 1010 neurons, and about 1013 synapses. In 1 mm3 of cortical tissue there are about 105 neurons and 109 synapses. A handy rule of thumb is 1 synapse/p:m3â¢ A single neuron may have thousands or tens of thousands of synapses. Stevens (1989) has calculated that the number of synapses per neuron for a piece of cortex 1 mm thick from a cat or a monkey is 4.12 x 103 â¢ The main exception to this is the primary visual cortex of primates, where cells are more densely packed and the number of synapses is about 1.17 x 103 for a piece of cortex I mm thick.
3. Numbers: Connectivity (Who Talks to Whom) Not everything is connected to everything else. Each cortical neuron is connected to a roughly constant number of other neurons, irrespective of brain size, namely about 3% of the neurons underlying the surrounding square millimeter of cortex (Stevens 1989). Hence, although the absolute number of input lines to a cortical neuron may be quite large, cortical neurons are actually rather sparsely connected relative to the population of neurons in a cell's neighborhood. Most connections are between, not within, cell classes (Sereno 1988). Forward projections to one area are generally matched by recurrent projections back to the area of origin.
4. Analog Inputs/Discrete Outputs The input to a neuron is analog (continuous values between 0 and I), and a neuron's output is discrete (either it spikes or it does not), though some neurons may have analog outputs. Whether a neuron has an output is governed by a threshold rule; that is, whether the cell spikes depends on whether the integration of the inputs exceeds a certain threshold. The profusion of input lines to a single neuron probably represents sensible computational and engineering design for a network of neurons with these properties (Abu-Mostafa 1989a). 10
5. Timing: General Considerations Getting the timing right is an essential feature of nervous systems and, more particularly, of analog computation (Mead 1989). Whether and how dendritic signals traveling soma-wards will interact depends on the time of their arrival at the common node. The magnitude of signals eventually reaching the axon hillock depends on such interactions. In perception, the time scale of the computation must be matched to the time scale of events in the external world, and in motor control it must be matched to the time it takes for the body parts to move (Mead 1989). When outputs of different computational components need to be integrated, the time scales of the various processors contributing values must also match. In short, the system has to operate in real time. Hence nervous systems must be architecturally rigged so that when a process takes time, it takes the right amount of time.

51

Neuroscience Overview

(A)
spinal nerves

mY
~t=/~

mY (C)

Slow ipsp

:t~

0

20

40

60 msec

0

2

4

6

8 sec

mY (DI

Slow epsp

:~mY

(E)

Late slow epsp

0 OL---L---~--~----L-~

2

3

4

5min

Figure 2.25 Four types of synapses from the sympathetic ganglion of the frog. (A) Innervation of a sympathetic neuron in the ninth ganglion of the paravertebral chain of the bullfrog; the diagram shows the separation of the cholinergic (ACh) and noncholinergic (LHRH) innervation. (B) A single preganglionic stimulus produces a fast EPSP. (C) Repetitive stimulation produces a slow IPSP lasting about 2 sec; the fast EPSP is blocked with a nicotinic blocking agent. (D) Repetitive stimulation also produces a slow EPSP which occurs after the first two responses and lasts about 30 sec. (E) The late slow EPSP, produced by stimulating preganglionic fibers, lasts more than 5 min after repetitive stimulation. (With permission from Kuffier, Nicholls and Martin [1984]. From Neuron lo Brain. Sunderland MA: Sinauer Associates.)

6. Timing: Particular Values An action potential (spike) lasts about 1 msec. Synaptic transmission, including electrotonic conduction in dendrites, takes about 5 msec. Synaptic potentials can last from a millisecond to many minutes (Kuffler 1980) (figure 2.25). Transmission velocity in myelinated axons is about 10-100 meters/sec; in unmyelinated axons it is less than 1 meter/sec. These are general ranges, not precise values.
7. Cell-to-cell Effects The effect of an individual synaptic input on a postsynaptic cell is weak, amounting to 1%-5% of the firing threshold. There may be some important exceptions to this trend, such as the strong effects of an individual synapse of a chandelier cell or a basket cell in the cerebral cortex {Martin 1984).
8. Firing Patterns Different types of neurons have different firing patterns (figure 2.26). Some neurons in the thalamus have multiple intrinsic firing patterns, and the particular pattern displayed on a given occasion is a function of

52

Chapter 2

A

Regular-spiking

..=:J .., '-150 mV 3 nA

50 ms

B

Fast-spiking

50 mV 3 nA
25 ms
Repetitive Bursting
c

J

L

50 ms

Figure 2.26 Differences in intrinsic firing patterns of cortical neurons. (A) When stimulated with a suprathreshold step of depolarizing current, regular-spiking neurons respond with an initial high-frequency spike output that rapidly declines to much lower sustained frequencies. Intracellular voltages are displayed in the top trace, injected current steps in the bottom trace. (B) Under similar conditions, fast-spiking cells generate high frequencies that are sustained for the duration of the stimulus. (C) Repetitive intrinsic bursting to a prolonged stimulus. Mean interburst frequency was about 9Hz. (From Connors and Gutnick [1990]. Intrinsic firing patterns of diverse neocortical neurons. Trends in Neurosciences 13: 98-99.)

the cell's recent depolarization or hyperpolarization history (Llimis and Jahnsen 1982). The ionic conductances of some cells, for example in the brain stem, endow those cells with oscillatory properties. Such a cell may ad as a pacemaker or as a resonator (responding preferentially to certain firing frequencies) (Llimis 1988). Most neurons are spontaneously active, spiking at random intervals in the absence of input. Different neuron types have different characteristic spontaneous rates, ranging from a few spikes per second to about 50 spikes per second.
9. Receptive Fields: Size and Center-Surround Organization Under the classical definition, the receptive field is that region of the sensory field from which an adequate sensory stimulus will elicit a response. In the somatosensory system,

53

Neuroscience Overview

ON-CENTER CELL
stimulus~

OFF CENTER CELl_

strmulus _ r - - - t _ _

II center

111111

I II surround

1111111111111

II center and surround

II II

Â·oÂ·... ... ... ... ...
...... -_-_- ......
++ - ++ ++ ++ +

Figure 2.27 Two types of circular center-surround receptive fields in the retina. When the light is shone on the center of the receptive field, the on-center cell responds vigorously, the offcenter cell is silent. When the light is shone in the annular surround, the opposite effect is achieved. Under diffuse illumination of both the center and the surround, both cells respond weakly. (With permission from Coren Ward [1989]. Sensation and Perception, 3rd ed. Copyright
Â© 1989 Harcourt Brace Jovanovich, Inc.)

the receptive field size varies over the body surface: those for the fingertips are smaller than those for the palm of the hand, and very much smaller than those for the arm. Receptive fields of cells in higher areas of visual cortex tend to be much larger than those in the earlier stages (one sixth of a degree in the foveal region of VI, compared to values ranging from 10 to the whole visual field in inferotemporal cortex). Retinal ganglion cells (cells carrying signals from the retina) have what is called a center-surround organization (figures 2.27, 2.28). This organization comes in two variations: (1) a stimulus in the center of the cell's receptive field excites it, but a stimulus in an area surrounding the receptive field inhibits it. This arrangement is known as "on-center/off-surround." (2) The opposite arrangement, namely, a central stimulus inhibits but a surround stimulus excites the cell, is known as "off-center/on-surround." Off-center cells respond maximally to dark spots, while on-center cells respond maximally to light spots. The information carried by the ganglion cells pertains to the comparison between the amount of light falling on the center of the field and the average amount of light falling on the surround, not absolute values of light intensity at the transducer. A center-surround organization is also evident in

54

Chapter 2

<.n <.n
Light on

Center-surround antagonism

Light on

z

Icenter
A
Center

Surround

B
Center

surround

Surround

1

a"'
)!;

I I

Iii'
:n:l

,..-

\
\

"0 '

I

I

~

I

I

\) r- I

~Â·

Receptor

/
I

\
Receptor

Receptor

I

Receptor

"â¢..,/--,\v.Â·Â·"' - ... I

I

\

\

I

' --"' I

Y.Bipolar

Horizontal

81polar

Ganglion

Ganglion

On-center Optic nerve

On-center Optic nerve

Figure 2.28 In center-surround organization, the response of a ganglion cell to direct light on the center of its receptive field (A) is antagonized by direct light on the surround of its field (B). This antagonistic interaction between neighboring retinal areas is mediated by the inhibitorv action of a horizontal cell. (Reprinted with permission from Kandel and Schwartz 1985.)

TYPE I

HCMT32B

[ Bâ¢'J,','' MCEONVETER DOTS ,â¢ â¢ ' â¢ â¢ ,

BACKGROUND

' :Â·~~~: ' '

.. .. .Â· . .: DOTS
STATIONARY

1', ' : â¢!:":"~: '' â¢â¢

""

100%

CENTER DOTS MOVE IN OPTIMUM DIRECTION
BACKGROUND DIRECTION VARIES 50%
z
0;:
.<...
~
0
.<...
0

- --
-

l.i.J
z(/)
0 Cl.
(/) l.i.J
0:: 50%
0 l.i.J
N::::;
<(
:I! 0::
0 z

z Q....
... -50%
:z:r:

0

-20%

-180' -120' -60'

0"

60' 120'

DIRECTION OF MOVEMENT

OF CENTER DOTS

-100%
-1ao' -12o' -eo' oÂ· 6Cf 120"
DIRECTION OF MOVEMENT
OF BACKGROUND DOTS

figure 2.29 Response of a neuron with antagonistic direction-selective surround. (Left) The cell responds vigorously when the dots in the center of the stimulus move (shown above) in the preferred direction but the dots in the surround are stationary. Negative percentages in the graph indicate inhibition relative to the level of spontaneous activity. (Right) The same cell responds very differently to its preferred direction of motion in the center when the dots in the surround also move in the cell's preferred direction. (From Allman et al. 1985.)

the receptive fields of somatosensory neurons in the thalamus and cortex (Mountcastle 1957) (figures 2.27, 2.28).
10. Receptive Fields: Nonclassical Events outside the classical receptive field of a cell have been found to modulate selectively the responses of the cell (Nelson and Frost 1978, Allman eta!. 1985) (figure 2.29). The effects are selective since they vary as a function of the type of surround stimuli. Nelson and Frost (1985) reported an inhibition as well as a highly specific form of facilitation of the responses of orientation-tuned cells in visual cortex of cats as a nonclassical field effect. Some area 17 cells that were normally responsive to a vertical bar in their receptive fields showed enhanced responses when distant 11 area 17 cells, co-oriented and co-axial to the first, were experimentally stimulated. Zeki (1983) has shown that certain wavelength-dependent neurons in V4 are influenced by the color balance in the surround. The surround effects of cells in
the middle temporal (MT) area, where receptive fields are typically 5o- 10Â°, can
extend 40Â°-80Â° (Allman et a!. 1985). Receptive fields are almost certainly more dynamical than previously assumed. For example, repeated stimulation

56

Chapter 2

A BEHAVIORAL APPARATUS

J}- B NORMAL
)~

Figure 2.30 Alteration of cortical maps after habitual stimulation. (A) The experimental protocoL showing the fingertips stimulated by the rotating disk. (B) Map in the somatosensory cortex of the left hand of the monkey before the stimulation experiment. Stippled area corresponds to 6ngertips 2, 3, and 4. (C) Map of the same region after the stimulation experiment. (From Merzenich eta!. 1990.)
to the fingertips results in an expansion of the regions of the somatosensory codex whose neurons have receptive fields in the fingertips (figure 2.30). Recent experiments in in VI of visual cortex also suggests that receptive fields are labile in that a cell's receptive field may expand when its preferred area on the retina is lesioned (Gibert and WieseL in press).
11. Specific and Nonspecific Systems In addition to the specific system projecting to the neocortex via the thalamus, such as is seen in the visuaL auditory, and somatosensory systems, there are five sources of widely projecting neurons each associated with a specific neurotransmitter, which may play important roles in the sleep-dreaming-waking cycle, in memory, and in awareness and attention. The five are as follows: the locus coeruleus in the brain stem (norepinephrine}, the raphe nucleus in the midbrain (serotonin), the substantia nigra in the midbrain (dopamine}, the nucleus basalis in the basal forebrain (acetylcholine), and special groups of cells in the mammillary region of the hypothalamus (GABA) (figure 2.31).

57

Neuroscience Overview

~

~

I

I

I

~

J

_ ,I I .... /

-- ----

I ;

Figure 2.31 Neurons originating in the locus .coeruleus project very widely all over the brain, including the cerebellum, brain stem, thalamus, and all over the cerebral cortex. The neurotransmitter they release is norepinephrine. (Reprinted with permission from Angevine and Cotman 1981.)

58

Chapter 2

a AUTOCRINE

b PARACRINE

d NEUROTRANSMISSION

e NEUROENDOCRINE

\Â®,.. (:.:t1
Â·Â·/

C ENDOCRINE
...... "' " .
HORMONE MOLECULES\ \

~ ~~~t'1CRINE
~
BLOODSTREAM

~TARGET
\.__:_) CELLS
Figure 2.32 Methods of communication of the hormonal and nervous systems. Although autocrine hormones (a) act on the cell that releases them and paracrine hormones (b) act on adjacent cells, most hormones are in the endocrine system and act on cells or organs anywhere in the body. Endocrine glands (c) release hormone molecules into the bloodstream, where they come in contact with receptors on target cells, which recognize the hormones meant to act on those cells and pull them out of the bloodstream. Neurons (d) communicate by releasing neurotransmitters dose to target cells. In neuroendocrine action (e) a neuron releases substances that act as hormones directly into the blood. (Reprinted with permission from Snyder 1985.)
12. Action-at-a-distance Some neurotransmitters may be released not only at a synaptic site, but may also be dumped into the extracellular space to have an action at a nonsynaptic site some distance from the point of release (Jan et al. 1978) (figure 2.32). Originating in the endocrine system, hormones, such as estradiol, can also reach neurons after traveling through the circulatory system and can alter neural activity.
13. Parallel Architecture The brain appears to be highly parallel in that there are many parallel streams of input for a given function. For example, in the monkey two parallel streams from the retina, starting with different types of ganglion cells, project to two distinct sets of layers of the lateral geniculate nucleus-the parvocellular and magnocellular layers, respectively-which in tum project to distinct sublaminae in layer 4 of cortical area VI of the visual cortex (Hubel and Livingstone 1987, Livingstone and Hubel1987). The streams are not cleanly segregated, however, and there are probably interactions at every stage (Schiller et al. 1990, Logothetis et al. 1990).
Selected Readings
Abeles, M. (1991). Corticonics: Neural Circuits of the Cerebral Cortex. Cambridge: Cambridge University Press.

59

Neuroscience Overview

Changeux, J.-P. (1985). Neuronal Man. Oxford: Oxford University Press. Churchland, P. S. (1986). Neurophi/osophy: Toward a Unified Science of the Mind-Brain. Cambridge, MA: MIT Press.
Dowling, J. E. (1987). The Retina: An Approachable Part of the Brain. Cambridge, MA: Harvard
University Press.
Groves, P.M., and G. V. Rebec (1988). Introduction fo Biological Psychology, 3rd ed. Dubuque, IA: Wm. C. Brown.
Hall, Z. W. (1991). Molecular Neurobiology. Sunderland MA: Sinauer.
Hubel, D. H. (1988). Eye, Vision and Brain. New York: Freeman.
Jeannerod, M. (1985). The Brain Machine. Cambridge, MA: Harvard University Press.
KandeL E., J. Schwartz, and T. M. Jessell, eds. (1991). Principles of Neural Science, 3rd ed. New
York: Elsevier.
Kelner, K., and D. E. Koshland, eds. {1989). Molecules to Models: Advances in Neuroscience. Washington, DC: American Association for the Advancement of Science.
Kuffler, S. W., J. G. Nicolls, and A. R. Martin (1984). From Neuron to Brain: A Cellular Approach fo the Function of the Nervous System, 2nd ed. Sunderland, MA.: Sinauer.
LeVay, S., and S. B. Nelson (1991). Columnar organization of the visual cortex. In The Neural Basis of Visual Function, ed. J. R. Cronly-Dillon. London: Macmillan. Levitan, I. B., and L. K. Kaczmarek (1991). The Neuron: Cell and Molecular Biology. Oxford: Oxford University Press.
Shepherd, G. M. (1987). Neurobiology, 2nd ed. Oxford: Oxford University Press.
Shepherd, G. M. (1990). Synaptic Organization of the Brain, 3rd ed. Oxford: Oxford University Press.
White, E. L. (1989). Cortical Circuits. Boston: Birkhauser.
Selected Journals and Reviews
Current Opinion in Neurobiology. (Current Biology) Review papers on subfields in neuroscience. journal of Cognitive Neuroscience. Quarterly journal (MIT Press}. Articles on systems neuroscience with emphasis on cognitive processing.
Seminars in Neuroscience. Quarterly journal (Saunders). Each issue is on a special topic in neuroscience.
Trends in Neurosciences. Monthly journal (Elsevier). Contains brief but very useful reviews of special topics and is a good source of up-to-date references to the literature.
Concepts in Neuroscience. (World Scientific). Contains discussions of conceptual issues.
Annual Review of Neuroscience. Palo Alto, CA: (Annual Reviews). Comprehensive reviews of the literature.

60

Chapter 2

3

Computational Overview

1 INTRODUCTION
What is computation? In virtue of what is something a computer? Why do we say a slide rule is a computer but an egg beater is no1:7 These are, in a way, the philosophical questions of computer science, inasmuch as they query foundational issues that are typically glossed over as researchers get on with their projects. 1 Like the philosophical questions of other disciplines (What is the nature of life? [Biology] What is the nature of substance and change? [Physics and Chemistry]), the answers become more convincing, meaningful, and interconnected as the empirical discipline matures and gives more ballast to the
theory. In advance of understanding that there are atoms, how atoms link
together, and what their properties are, one simply cannot say a whole lot about the nature of substance and change. It is not, however, that one must say nothing-in that event, one could not get the science started. The point rather is that the theory outlining the elementary ideas of the discipline gradually bootstraps itself up, using empirical discoveries as support, and kicking away old misconceptions in the haul.
The definition of computation is no more given to us than were the definitions of light, temperature, or force field. While some rough-hewn things can, of course, be said, and usefully said, at this stage, precision and completeness cannot be expected. And that is essentially because there is a lot we do not yet know about computation. Notice in particular that once we understand more about what sort of computers nervous systems are, and how they do whatever it is they do, we shall have an enlarged and deeper understanding of what it is to compute and represent. Notice also that we are not starting from ground zero. Earlier work, especially by Turing (1937, 1950), von Neumann (1951, 1952), Rosenblatt (1961), and McCulloch and Pitts (1943), made important advances in the theory and science of computation. The technological development of serial, digital computers and clever software to run on them was accompanied by productive theoretical inquiry into what sort of business computation is. 2
Agreeing that precise definitions are not forthcoming, can we nonetheless give rough and ready answers to the opening questions? First, although we may be guided by the example of a serial digital computer, the notion of "computer" is broader than that. Identifying computers with serial digital com-

puters is neither justified nor edifying, and a more insightful strategy will be to see the conventional digital computer as only a special instance, not as the defining archetype. Second, in the most general sense, we can consider a physical system as a computational system when its physical states can be seen as representing states of some other systems, where transitions between its states can be explained as operations on the representations. The simplest way to think of this is in terms of a mapping between the system's states and the states of whatever is represented. That is, the physical system is a computational system just in case there is an appropriate (revealing) mapping between the system's physical states and the elements of the function computed. This "simple" proposal needs quite a lot of unpacking.
Functions: Computable or Noncomputable, Linear or Nonlinear
Since this hypothesis concerning what makes a physical system a computational system may not be self-evident, let us approach the issue more gradually by first introducing several key but simple mathematical concepts, including "function," and the distinction between computable and noncomputable functions. To begin, what is a function? A function in the mathematical sense is essentially just a mapping, either 1 : 1 or many: 1, between the elements of one set, called the "domain," and the elements of another, usually referred to as the "range"3 (figure 3.1). Consequently, a function is a set of ordered pairs, where the first member of the pair is drawn from the domain, and the second element is drawn from the range. A computable function then is a mapping that can be specified in terms of some rule or other, and is generally characterized in terms of what you have to do to the first element to get the second. For example,
multiply the first by 2, {(1, 2), (2, 4), (3, 6) }, expressible algebraically as y = 2x;
multiply the element from the domain by itself {(6.2, 38.44), (9.6, 92.16)}, expressible algebraically as y = x2, and so on.
What then is a noncomputable function? It is an infinite set of ordered pairs for which no rule can be provided, not only now, but in principle. Hence its specification consists simply and exactly in the list of ordered pairs. For example, if the elements are randomly associated, then no rule exists to specify the mapping between elements of the domain and elements of the range. Outside of mathematics, people quite reasonably tend to equate "function" with "computable function," and hence to consider a nonrule mapping to be no function at all. But this is not in fact how mathematicians use the terms, and for good reason, since it is useful to have the notion of a noncomputable function to describe certain mappings. Moreover, it is useful for the issue at hand because it is an empirical question whether brain activity can really be characterized by a computable function or only to a first approximation, or perhaps whether some of its activities cannot be characterized at all in terms of computable functions (Penrose 1989).
What is a linear function? Intuitively, it is one where the plot of the elements of the ordered pair yields a straight line. A nonlinear function is one where the plot does not yield a straight line (figure 3.2). Thus when brain function is

62

Chapter 3

"average"
~
Decode
~
Output state
1'''1'1111'''1 I ''I' I I I
Physical System Figure 3.1 Mapping between a domain and a range can be accomplished by a variety of physical systems. There are three steps: (I) The input data is coded into a form appropriate for the physical system (electrical signal in an electrical circuit, chemical concentration in neuron, position of a slider in a slide rule). (2) The physical system shifts into a new state. (3) The output state of the physical system is decoded to produce the result of the mapping. The example shown here is the "average" map that takes four values and produces their average. Such a mapping might be useful as part of a visual system. Mappings could also be made from the domain of temporal sequences, and the range could be a sequence of output values.
Figure 3.2 Examples of functions F(x), plotted along the vertical axis, of one variable, x, plotted along the horizontal axis. Function A is a linear function. Function B is a nonlinear function. Function C is a discontinuous function.

63

Computational Overview

described as "nonlinear," what this means is that (a) the activity is characterized by a computable function, and (b) that function is nonlinear. Notice also that the space in which functions are plotted may be a two-dimensional space (the x and y axes), but it may, of course, have more than two dimensions (e.g., an x axis, y axis, and also w, v, z, etc. axes).
Because the notion of a vector simplifies discussion enormously, we introduce it here. A vector is just an ordered set of numbers. For example, the set of incomes for 1990 of three vice-presidents in a corporation can be represented by the vector <$30, $10, $10); the eggs laid per week by five hens as <4, 6, 1, 0, 7); the spiking frequency of four neurons/sec as <10, 55,44, 6 ). By contrast, a scalar is a single value rather than a many-valued set. The order in the set matters when we want to operate on the values in the set according to an order-sensitive rule. Systems, including the nervous system, execute functions that perform vector-to-vector mapping. For example, from the stretch receptors' values to the muscle contraction values, or from the head velocity values to eye velocity values.
A geometric articulation of these concepts compounds their value. Any coordinate system defines a state space, and the number of axes will be a function of the number of dimensions included. A state space is the set of all possible vectors. For example, a patient's body temperature and diastolic blood pressure can be represented as a position in a 2-D state space. Or, if a network has three units, each unit may be considered to define an axis in a 3-D space. The activity of a unit at a time is a point along its axis, so that the global activation of all the units in the net is specified by a point in that 3-D space (figure 3.3). More generally, if a network has n units, then it defines an ndimensional activation space, and an activation vector can be represented as a point in that state space. A sequence of vectors can be represented as a trajectory in the state space.4 Thus the patient's body temperature and blood pres-
Rate
Neuron3

Neuron2
Rate
Figure 3.3 Schematic diagram of the trajectory of a three-neuron system through state space. The state of the system is a 3-D vector whose components are the Bring rates of the three neurons. As the Bring rates change with time. the tip of the vector traces out a trajectory (thick line). For more neurons the state space will have a higher dimension.

64

Chapter 3

sure followed through time results in a trajectory in a 2-space. A function maps a point in one state space to a point in another state space-for example, from a point in stretch-receptor activation space to a point in muscle spindle activation space.
These notions-"vedor" and "state space"-are part of linear algebra, and they are really the core of the mathematics needed to understand model networks. They are mercifully simple conceptually, and they are rather intuitively extendable from easily visualizable 2-D cases to very complex, n-D cases, where n may be thousands or millions. Although volumes more can be written on the topic of linear algebra, this is perhaps enough to ease the entry into the discussion of model neural networks. 5
Computers, Pseudocomputers, and Cryptocomputers
The mathematical interlude was intended to provide a common vocabulary so that we might return to the question of characterizing, albeit roughly, what about a physical system makes it a computer. To pick up the thread left hanging during the mathematical interlude, let us hypothesize that a physical system computes some function f when (1) there is a systematic mapping from states of the system onto the arguments and values off, and (2) the sequence of intermediate states executes an algorithm for the function. 6 Informally, an algorithm is a finite, deterministic procedure, e.g., a recipe for making gingerbread or a rule for finding the square root.
We count something as a computer because, and only when, its inputs and outputs can usefully and systematically be interpreted as representing the ordered pairs of some function that interests us. Thus there are two components to this criterion: (1) the objective matter of what function(s) describe the behavior of the system, and (2) the subjective and practical matter of whether we care what the function is. This means that delimiting the class of computers is not a sheerly empirical matter, and hence that "computer" is not a natural kind, in the way that, for example, "electron" or "protein" or "mammal" is a natural kind. For categories that do delimit natural kinds, experiments are relevant in deciding whether an item really belongs to the category. Moreover, there are generalizations and laws (natural laws) about the items in the categories and there are theories interleaving the laws. Nonnatural kinds differ in all these respects, and typically have an interest-relative dimension.
"Bee," for example, is a natural kind, but "gem" and "weed" are not. Objects are considered gems depeilding on whether some social group puts special value on them, typically as status symbols. Plants are considered weeds depending on whether gardeners (serious gardeners?) in the region happen to like having them in the garden. Some gardeners cultivate baby's breath as a desirable plant; other gardeners fight it as a weed. There is no experiment that will determine whether baby's breath is really a weed or not, because there is no fact of the matter-only social or idiosyncratic conventions.7 Similarly, we suggest, there is no instrinsic property necessary and sufficient for all computers, just the interest-relative property that someone sees value in interpreting

65

Computational Overview

a system's states as representing states of some other system, and the properties of the system support such an interpretation. Desk-top von Neumann machines exist precisely because we are keenly interested in the functions we build and program them to execute, so the interest-relative component is dyed in the wool. For this reason, and because these machines are so common, they are the prototypical computers, just as dandelions are prototypical weeds. These prototypes should not, however, he mistaken for the category itself.
It may he suggested as a criticism of this very general characterization of computation that it is too general. For in this very wide sense, even a sieve or a threshing machine could he considered a computer, since they sort their inputs into types, and if one wanted to spend the time at it, one could discover a function that describes the input-output behavior. While this observation is correct, it is not so much a criticism as an apt appreciation of the breadth of the notion. It is rather like a lawn-growing perfectionist incredulously pointing out that on our understanding of "weed," even dandelions might he nonweeds relative to some clime and some tribe of growers. And so, indeed, they might be some farmer's cash crop. Nor is this idle fancy. Cultivated dandelion greens now appear as a delicacy in the specialty section of the greengrocery.
Conceivably, sieves and threshing machines could be construed as computers if anyone has reason to care about the specific function reflected in their input-output behavior, though it is hard to see what those reasons might be (figure 3.4). Unlike desktop computers that are engineered precisely for their computational prowess, sieves and threshing machines are constructed for other reasons, na,mely their sheerly mechanical prowess in the sorting of objects according to size and shape. Not too much emphasis should be placed
on the link between purposeful design and use as a computer, however, for
a fortuitously shaped rock can be used as a sundial. This is a truly simple computer-trouve, but we do have reason to care about the temporal states that its shadow-casting states can be interpreted as representing.
There is perhaps a correct intuition behind the criticism nonetheless. Finding a device sufficiently interesting to warrant the description "computer" probably also entails that its input-output function is rather complex and inobvious, so that discovering the function reveals something important and perhaps unexpected about the real nature of the device and how it works. Thus finding out what is computed by a sieve is probably not very interesting and will not teach us much we did not already know. How a sieve works is dead simple. In contrast, finding out what is computed by the cerebellum will teach us a lot about the nature of the tissue and how it works.
A computer is a physical device with physical states and causal interactions resulting in transitions between those states. Basically, certain of its physical states are arranged such that they represent something, and its state transitions can be interpreted as computational operations on those representations. A slide rule is taken to compute-for example, (Mult 2, 7) to give 14 as the output-by dint of the fact that its physical regularities are set up in such a way as to honor the abstract regularities in the domain of numbers; the system of Aubrey holes at Stonehenge computes eclipses of the sun by dint of the fact

66

Chapter 3

Figure 3.4 Garrett's improved threshing machine, 1851. The wheat was fed in from above, and the grain was removed by the rubbing action of the beater bars on the drum as it rotated inside the fixed concave. The grain fell onto a sieve below and the chaff was blown away by the fan system on the right. (From The Illustrated Science and Invention Encyclopedia. Westport, CT: H. S. Stuttman, 1983.)
that its physical organization and state transitions are set up so that the sun stone, moon stone, and nodal stone land in the same hole exactly when an eclipse of the sun occurs. Notice that this would be so even in the highly unlikely event that Stonehenge was the fortuitous product of landslides and flooding rather than human contrivance.
Nervous systems are also physical devices with causal interactions that constitute state transitions. Through slow evolution, rather than miraculous chance or intelligent design, they are configured so that their states represent -the external world, the body they inhabit, and in some instances, parts of the nervous system itself-and their physical state transitions execute computations. A circuit in mammalian brain stem evolved to compute the next position of the eyeball based on the angular velocity of the head. Briefly, the neuronal activity originating in the semicircular canals represents head velocity, and the interneurons, motor neurons and eyeball muscles are physically arranged such that for head velocity of a certain amount, the neurons causally interact so that the muscles of eyeball change tension by exactly the amount needed to compensate for the head movement. (For more on this circuit and its computation, see chapter 6). Loosely speaking, this organization evolved "for"

67

Computational Overview

this task; a little more strictly speaking, this circuit came to be the way it is by random mutations and natural selection; in standard epigenetic circumstances and relative to the ancestor's nervous system and to the system's other components, this organization enhances somewhat the organism's Chances of surviving and reproducing.
There is a major contrast between manufactured and biological computers. Since we construct digital computers ourselves, we build the appropriate relationship into their design. Consequently, we tend to take this mapping for granted in computers generally, both manufactured and evolved. But for structures in the nervous system, these relationships have to be discovered. In the case of biological computers, discovery may tum out to be very difficult since we typically do not know what is being computed by a structure, and intuitive folk ideas may be misleading.
By contrast with systems we conventionally call computers, the modus operandi of some devices are such that a purely causal explanation, without reference to anything having been computed or represented, will suffice. A mouse-trap or a sieve, for example, is a simple mechanical device. Purely causal explanations will likely suffice for some aspects of brain activity too, such as the ion pump in neuronal membranes by virtue of which sodium is pumped out of the celL or the manner in which binding of neurochemicals to receptors changes the internal chemistry of the cell. Bear in mind, however, that even at this leveL an ion, such as Na+, could represent a variable like velocity. At this stage, no one is really convinced that this is in fact so, but the possibility is not ruled out simply because ions are very low-level entities. Effects at higher levels of organization appear to require explanations in terms of computations and representations. Here a purely causal story, even if the line is still fairly clean, would give only a very unsatisfying explanation. For example, a purely causal or mechanical explanation of the integration of signals by dendrites is unenlightening with respect to what information the cell is getting and what it does with it. We need to know what this interaction means in terms of what the patterns of activity represent and what the system is computing.
Consider, for example, the neurons in parietal cortex whose behavior can be explained as computing head-centered coordinates, taking positions of the stimulus on the retina and position of the eyeball in the head as input (Zipser and Andersen 1988). Knowing that some neurons have a response profile that causes other neurons to respond in a certain way may be useful, especially in testing the computational hypothesis, but on its own it does not tell us anything much about the role of those neurons in the animal's visual capacity. We need additionally to know what the various states of neurons r-epresent, and how suCh representations can be transformed by neural interactions into other representations. At the network level, there are examples where the details of connectivity and physiology of the neurons in the network still leave many of the whys and wherefores dangling, while a computational approach that incorporates the physiological details may make contact with the broader brainscape of tasks, solutions, environmental niche, and evolutionary history. 8

68

Chapter 3

There is a nonmathematical sense of "function," according to which the job performed by something is said to be its function. In this sense, the heart is said to function as a pump, rather, than say as a noisemaker to soothe babies on their mother's breast. Though making a "ka-thump" sound is something the heart does, and though babies appear to be soothed by it, this surely is not the heart's function, meaning, roughly, its "primary job." Functional assignments can reasonably be made in the context of evolutionary development, what the animal needs to survive and reproduce, its environmental niche, and what would make sense given the assignment of function to related structures. In this "job" sense of function, the function of some part of the nervous system is to compute some function (in the mathematical sense), such as position for the eyeball given head velocity.
There is nothing mystical about characterizing a biological structure as having a specific function, even though neither god nor man designed the structure with a purpose in mind.9 The teleological trappings are only that, and the teleology is eliminable or reducible without remainder in an evolutionary framework. To assign a computational role to a circuit is to specify a job of that circuit-detecting head velocity, for example. Consequently, the considerations that bear on determining the job of an organ such as the liver bear also on the assignment of computational role to neuronal structures. That the nervous system evolved, and that maladaptive structures tend to be weeded out in the evolutionary contest, restricts many functional hypotheses-in both senses of "functional"-that are logically possible but just not biologically reasonable. The crux of the matter is that many biologically irrelevant computational hypotheses can be culled out by a general functional truth about nervous systems, namely that inter alia they serve to help the animal move adaptively in the world. 10
In this chapter we shall characterize a range of computational principles that may be useful when addressing the question of computation in nervous systems. As we shall see, moreover, the computational perspective will allow us to ask questions of biological systems that might not otherwise have been asked. The computational principles introduced here will be applied first to a number of examples chosen for their pedagogical value rather than for immediate biological salience. They allow us to introduce the basic ideas in a simple fashion, and this is their single, overriding virtue. They are not meant to be hypotheses concerning the mechanisms underlying the computational properties of real nervous systems. In chapters 4 to 6 neurobiological realism will be of paramount concern, but an understanding of the basic concepts is the entry ticket to these chapters.
2 LOOKING UP THE ANSWER
Conceptually, the simplest computational principle is "look up the answer." A look-up table is simply some physical arrangement in which answers to specific questions are stored. The engineering trick is to rig the table so that access to answers is fast and efficient, for if it is slow and clumsy, calculating the answers

69

Computational Overview

de novo might be preferable. Inasmuch as look-up tables are really repositories of precomputed answers rather than devices for working out the answer on the spot, it may be suggested that they are not genuine computers at all. For the purist, however, accepting this semantic refinement promotes confusion. A look-up table does after all effect a mapping, it instantiates a rule, and its states represent various things. That, given our groundfloor criteria, qualifies it as a computer. Call it unglamorous, call it humdrum, but a look-up table embedded in a mechanism for delivering answers can as properly be called a computer.
The easiest way to think of a look-up table is simply as an array of boxes each of which says, in effect, "if x is your problem, then y is your answer," for specific x and y. In other words, it does a matching job. For example, the truth table for exclusive "or" looks like this:
P Q XOR
TT F TF T FT T FF F
This mode of representing the truth conditions happens to be very convenient, though many other, less convenient arrangements are easily imagined. And as students are usually told, it requires no significant intelligence to use this look-up table: just ask your question (e.g., what is the value when P is true and Q is false?), go to that row, and scan the answer.
A second but more powerful look-up table is the slide rule. Actually it is a multiplexed look-up table, since it stores answers not only for multiplication tasks, but also for finding sines, cosines and logarithms. When the task is multiplication, one enters the question (what is 3 X 7?) by sliding the center piece and cursor, and scanning the answer at the cursor. Moreover, while the truth table can handle only discrete functions, a slide rule can do continuous functions. To accommodate the variety of arithmetic questions and answers on two pieces of wood, the look-up table is metrically deformed (figure 3.5). As before, there are other ways of physically structuring a look-up table to perform exactly these tasks, but the flat, pocketable slide rule is in fad a wonderfully convenient and efficient way to do so.
Extending the idea a bit further, consider the Tinkertoy look-up table constructed in 1975 by a group of MIT undergraduates to play the game of tic-tac-toe11 (figure 3.6). Making the "table" part of this device consists in storing a set of ordered pairs, where the first element is a possible game position, and the second element is the correct move, given that game position. In operating, the machine looks for a match between its current position and one of the possible positions sitting in storage. Finding the match will automatically divulge what to do next.
The first step in building the Tinkertoy look-up table was to decide on a representation for the state of the board using just the resources of Tinkertoy pieces. The second step was to use rotation and reflection symmetries to reduce the total number of game positions in the table, since the more entries

70

Chapter3

:

Figure 3.5 The object in the center is an oversized slide rule. The authors are on either side.

Tlc-Tac-Toe

Start

#

First Move

- -r-

*Second
Move

;'

,... ,,

*... ~ "l'' ...

#
;' ,... ,,

* * * Figure 3.6 The first three levels of the game tree for tic-tac-toe. The 3 x 3 board at starting
position is in the center, and the first move must be in one of three board positions (arrows) irreducible by mirror and reflection symmetries. The next level gives several possible replies by the opponent.

71

Computational Overview

the larger the storage and the longer the time to search for a match. Thus a board with nothing but an X (or an 0) in the upper right comer can be dealt with in the same way as a board with nothing but an X (or an 0) in any of the other comers, and this consequently reduces the number of stored first positions by six, a substantial savings. These economies are important in reducing the number of entries in the look-up table to a manageable number-in this case from 300,000 to 48. The next step was to design a mechanical system that could match a position on the board with one of the 48 irreducible positions, and to retrieve the correct move (figure 3.7).
Although the tic-tac-toe example may at first seem frivolous, it cleanly illustrates a number of points relevant to computational neuroscience. First, look-up tables can be constructed from unorthodox materials but still come up with the same answer as do conventional electronic circuits. Second, the Tinkertoy computer is not a general-purpose computer; rather, it was built to solve one specific problem. So far as nervous systems are concerned, the analogy is that the genes probably wire up some neural circuits on the look-up table blueprint with the consequence that the animal is prepared at birth to do such things as snuffle around for a warm spot and then suck at whatever soft, mouth-sized thing sticks out Circuits that yield sucking behavior in rats are probably not general-purpose devices, but are dedicated more or less exclusively to sucking.
The theoretical lesson is that if a problem is conceptually reducible to a look-up table problem, then, cost and efficiency aside, it could in principle be implemented by look-up table mechanisms. Cost is rarely irrelevant, however. It is especially pertinent here, since precomputing the answers for each problem requires a substantial, and sometimes exorbitant, investment in the construction of the machine. From an evolutionary point of view, it might be too costly or too difficult to precompute certain tasks, such as semantics or placein-the-social-scheme or dinner-whereabouts, and hence many things must be learned by the infant organism.
How practical really is the look-up table approach? The answer depends on a number of factors, including the complexity of the problem to be solved, the architectural pliancy of the available materials, and the size limits of the lookup table. Chess, unlike tic-tac-toe, appears to be a poor candidate for the look-up table solution. There are approximately 1040 game positions-far more than the capacity of any existing machineY The complexity factor can be reduced considerably using the economy described above; namely, take advantage of the underlying symmetries and position similarities in the problem to reduce the number of entries. Given this possibility, it remains to be seen whether the look-up strategy is indeed utterly unrealistic for chess. For many real-world problems, as in the problem of visually recognizing an object, advantage can be taken of translation, rotation, and scaling invariances, as well as smoothness and continuity constraints, to reduce the number of stored categories. The possibility to consider is that look-up craftsmanship may be seen at various stages in nervous system processing, even if it is not ubiquitous.

72

Chapter 3

Figure 3.7 Tinkertoy computer for playing the game of tic-tac-toe. Each memory spindle encodes a possible game position, along with the optimal response. The read head, loaded with the current game position in the core piece, moves down the memory spindles, one by one, until there is a match. This activates the output duck, which drives the correct move. Compare this special-purpose computer with the general mapping scheme in figure 3.1. (From Dewdney
[1989] Computer recreations: a Tinkertoy computer that plays tic-tac-toe. Copyright Â© 1989
Scientific American.)

73

Computational Overview

If the number of stored question-answer pairs is large, then the search-fora-match time may be prohibitively long. The Tinkertoy look-up machine, for example, compares the current board position to each of the stored board positions, one at a time, until the match is found. This is a rather ponderous business, especially if the search procedure is sequential. Parallel search could provide time economies, as we shall see later. Time is not the only consideration; wiring too must be kept within bounds. Consider, for example, the size of a look-up table needed to handle ordered pairs of the form <edible goodie at coordinates x,y,z moving at velocity vlbody position #).Given the number of independently movable body parts and the number of possible locations and speeds, the look-up table would have to be massive, at a minimum. The wiring cost scotches the idea. For nervous systems, brief times and sparse wiring are generally preferable, other things being equal, so the question is whether there are any neural structures that can be usefully understood as look-up tables.
Until rather recently, the superior colliculus in cats suggested itself as a neural instantiation of a look-up table, at least to a first approximation. The simple story runs like this. The colliculus has a number of layers, or laminae. On its upper layer, the colliculus represents location of visual stimuli on a retinatopic map, while on its bottom layer is a "motor map" representing position of the eyeball muscles suitable for foveating the eye to various locations. Other layers in the structure represent whisker-stimulus location. The maps are deformed with respect to each other so that they are in register. The effect is that a line dropped from the visual map intersects the motor map in a location that causes the eyeballs to move so as to foveate the location of the visual stimulus. The anatomy itself facilitates the look-up of motor answers, just as the "anatomy" of a slide rule facilitates look-up of mathematical answers. The organization enables the system to foveate a peripheral visual stimulus quickly and accurately. It is a kind of two-dimensional slide rule, where the visual and motor surfaces are appropriately aligned so that a position of a peripheral stimulus is mapped onto a where-the-eyeballs-should-be position. According to this conjecture, the anatomy executes a kind of "visual grasp" transformation (figure 3.8).
What is wrong with the colliculus look-up story? As so often in biology, as more data come in, the whole situation begins to look much more complex than the unencumbered, unqualified hypothesis asserts. To begin with, the relation between the visual input and the motor output is not as straightforward as simple "visual grasp"; there are descending fibers from the cortex that affect collicular output, and aHentional processes play an important if poorly understood role in collicular function. Although there do exist ostensible "drop line" connections between mapped layers, it is not yet known exactly what these connections do. In particular, the long time delay between the signal entering the sensory layer and a signal reaching the lower motor layer undermines the hypothesis that these connections straightforwardly execute "visual grasp." So withal, the colliculus cannot be taken as an unproblematic case of a neural look-up table.

74

Chapter 3

a

b

M

?<Â·'Â·Â·::Â·Â·~ } . .
.. .

p
-40Â°
L

Figure 3.8 Organization of the cat superior colliculus. (a) Cross section through the colliculus showing cell bodies of neurons in each lamina (numbered). (b) Map of the eye movements produced by stimulating the deep layers of the colliculus with an electrode. The coordinates refer to the deviation of the eye from its center of gaze that is produced by electrical stimulation. M, medial; L, lateral; A, anterior; P, posterior (Adapted from Schiller 1984.)
The example is instructive nonetheless, for the discrepancies between the pure look-up configuration and the complicated anatomy and physiology of the colliculus suggest that it well behooves evolution to fancy up a true-blue look-up table into something that can do rather more complicated things. Consider first that the colliculus needs also to take head position into account, since the eyes move relative to the head, and hence relative to the ears and the whiskers. In addition to its "in register" drop lines, the system may find it can make good use of connections to other areas of the map, and to the whisker and ear-position maps interleaved in the neural stack, noting that whiskers and ears too can move relative to the head (in some mammals) but with their proprietary degrees of freedom. But if the colliculus takes these other matters into account, pure look-up conforming to the slide-rule style is not what is going on. Were the eyes stuck fast in the head, and were "foveation" to whisker and auditory stimuli absent, the colliculus might approximate more closely a look-up table. As it is, the complexities of the colliculus suggest that even if evolution had fashioned a pure look-up table, it would soon evolve to master these complex and interrelated operations. That is, neurons would have to perform additional computational steps between input and output.
To return to the matter of computer design, one means for reducing storage space consists in allowing the structures doing the transformations to adjust

75

Computational Overview

themselves to existing conditions. Thrift bids the system to store only vectors (game positions) it actually uses rather than all possible vectors, and this requires that the system learn which vectors these are. What is the cost of this flexibility? If the system is to adapt, the adaptation should be in the correct direction. Alas, we cannot very well have a look-up table for the question "is my modification in the right direction?" without going hog-wild over the space limitations. So adaptation pulls the system even further away from the slide-rule paradigm.
But if nervous systems are not using pure look-up tables, what are they doing? The fast answer, to which the rest of the book is an extended elaboration, is this: they are computing with nets. As we shall see later in this chapter, neural nets may have certain properties akin to look-up tables. Consequently, this look-up table prologue is not merely a "first-we-crawl" exercise, but a foundation that will help us understand what actual neural nets are doing. Before moving to a discussion of how nets compute, one further preliminary point must be laid on the board.
Might a close but imperfect match sometimes suffice? For many tasks, especially recognition and categorization tasks, the answer is "yes, close is dose enough." Accordingly, an additional modification to the true-blue look-up table consists in storing not every possible entry, but rather storing prototypes or exemplars of the categories. With this stratagem, we trade off a degree of precision for a saving of space, but the system must now take some computational steps to determine similarity to stored vectors. Eventually we shall want a net that avails 'itself of both economies: it stores prototypes, and it has the plasticity to learn prototypes.
To embody prototypes in a computer, related items are clustered near or less near to an exemplar, according to their degree of similarity. Items stored in this manner define a similarity space in the machine, and distance from the prototype defines a similarity metric. This is known as a nearest-neighbor configuration, and there are many possible architectures for realizing it and many possible ways to style nearest-neighbor algorithms to exploit the organization for computational purposes. If a conventional digital machine is chosen for the implementation, then the machine will have to be spatially prodigal to accomplish complex tasks, for it has to store all the entries, make the distance measures, find the match, and deliver the answer. Clever ways to store data in hierarchical trees have been devised, but even these bog down when the going gets realistic. This gloomy prospectus may be sufficiently discouraging to degrade the look-up idea to nothing more than a charming curiosity, rather like an omithopter13-conceivable perhaps, but practical, probably not. On the other hand, though a digital machine may be impractical in this sphere, there is an architecturally very simple organization that will do the job, and do it cheaply, efficiently, and in satisfyingly few steps. That is a net. In the next several sections, we shall look at a number of types of networks, starting with very simple examples and moving on to networks of greater power and sophistication.

76

Chapter 3

A. outputs

inputs

B.

outputs

c.
outputs inputs

inputs
Figure 3.9 Three types of networks. (A) Feedforward network with one layer of weights conÂ· necting the input units to the output units. (B) Feedforward network with two layers of weights and one layer of hidden units between the input and the output units. (C) Recurrent network with reciprocal connections between units. (Adapted from Hertz et al. 1991.)
3 LINEAR ASSOCIATORS
What is a net? The architecture of the canonical net consists of units, loosely based on neurons, connections (generously speaking, axons) between the units, and weights (generously speaking, synapses) on the connections (figure 3.9). Some units receive external input, some deliver the output, and some may do neither. Because there is more than one input unit and more than one output unit, the ingoing and outcoming representations are vectors, meaning ordered sets of values (e.g., (3.2, 668.9, 0)) rather than single values (scalars, e.g., 668.9). Signals with various magnitudes are passed between units. That is the nub of a net. How can a net compute anything? The abstract explanation is reassuringly simple: the weights on the units are set in such a way that when the input units are given certain values, the output units are activated appropriately. This means that a mapping is achieved and hence that a function is executed. Now to follow the recipe for making a net in a concrete case, we have to decide how to set the weights, whether they are modifiable and if so how, what range of activity values a unit may take and how they are determined, how to represent the input vectors, and the nature of the connectivity between units (network topology). Obviously this means that the canonical description carves out a vast area of computational space, within which specific nets occupy small regions. The canonical description thus stands to a running

77

Computational Overview

y

(3, 1)

X

Figure 3.10 Computing the inner product between two vectors is a fundamental operation in a

feedforward associative network. The example given here is for two-dimensional vectors, but

the same relationships apply to vectors with more components. The inner product is defined as
AÂ· B = A,Bx + A,B, = 1 x, 3 + 3 X 1 = 9, where A. is the x component and A, is the y

component of vedor A. The angle f! between the vectors can be computed from the relationship

J A; cos f! =(AÂ· B)/(iiA!I Â·!!Bill, where II All =

+A: is the magnitude of A. In the network

shown in figure 3.9a, vector A might represent the activity levels of the input units and vector B

could be the weights from the input units to an output unit. The inner product can then be

interpreted as the sum of the weighted inputs to the output unit.

neural network model like a dictionary definition of an airplane stands to a veritable machine itself.
In the 1970s, more or less independently, a number of people were developing associative nets, including Leon Cooper, James Anderson, Teuvo Kohonen, Gunther Palm, Christopher Longuet-Higgins, and David Willshaw. 14 How do associative nets work? In a nutshell, they associate an input vector with an output vector, essentially following the "parallel architecture/similaritymeasure/look-up table" format outlined above. The key mathematical task that networks can perform is computing inner products; that is, taking two vectors and multiplying them component by component and then adding up the products. So if one vector represents the input from a set of units and the other vector is a stored prototype, then the inner product yields a measure of the overlap between them, and hence of their similarity. Geometrically, the inner product is proportional to the cosine of the angle between the vectors, so when there is perfect congruence of the vectors, the angle would be zero (figure 3.10). This is a readily manipulable measure of vector similarity. How are the vectors (prototypes) stored? They are stored in the weights connecting the input units to an in individual output unit. This means that each component in the prototype vector is assigned to one weight, and these weights are attached to a summing unit, which adds up all the products of these two vectors (weight vector and input vector). This summing unit is the output unit.
In the garden variety network, the output of the summing unit is proportional to the sum of the products. The output therefore is a linear transformation of the input, and the network is called a linear associator. For example, for a small network with three input lines and three output units, there are nine possible weights, which can be written as an array of numbers. One such 3 X 3 weight matrix is:

78

Chapter 3

a, ~

b,

....
r"\,1

"\J

â¢ ...

~

~ 1---.
... â¢ '-

"""- â¢

b, '"'

'W

...,

Output lines

~

~

~

~

~

..
.-.--.

...-.
""

,., 'W ....

I
."'
"'
\.J
..

..I
â¢ â¢ ...
"'

.. ~ .
.. j-<J -
â¢ ...
"' '-
... "'

.. ..
....

"" "'

....,
'-1 '-
"" --u --.
' . \..1

Figure 3.11 A Willshaw net showing the input lines (horizontal), the output lines (vertical), and the connections between them. The weights on the connections are binary and can be zero (open circles) or one (closed circles). Thus, the picture is a graphical representation of the weight matrix. (From Willshaw, 1989.)

~]1
0 1

The components of the output vector of the network are given by:

L Y;

wijxj

(1)

j

For the input vector X; = (I, 1, 1), the output vector is given by Y; (1, 2, 1).
For the input vector X; = (1, 2, 3), the output vector is given by Y; (4, 4, 0).
(Multiply the first component of the vector by the first item in the top row [1 X 0], the second component by the second item in the top row [2 X -1], the third by the third [3 X 2]. Add the products I 4]. Repeat for each row.)
As you would expect, there are many variations on the simple linear associator theme, and changes are rung on whether the input and output units take on continuous or binary values, and whether the weights are continuous or binary (figure 3.11). Notice that many inner products can be computed in parallel, one for each summing unit, and the more the summing units, the bigger the net. Additionally, each of the products (weights X inputs) can be computed in parallel. The result is that only one step is required to produce an output vector that is associated with the input vector. What is described here is the paradigmatic net for matching a sample to a stored prototype. This is evidently a classification task: for each category a representative example must be provided, and this is encoded as a vector. As a rule, the less overlap between the prototype vectors the better, since it is preferable that an input unambiguously match a prototype.

79

Computational Overview

