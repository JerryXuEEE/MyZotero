{"indexedPages":8,"totalPages":8,"version":"253","text":"EasyChair Preprint\n№ 4355\nHopfield Neural Network and Anisotropic Ising Model\nDmitry Talalaev\nEasyChair preprints are intended for rapid dissemination of research results and are integrated with the rest of EasyChair.\nOctober 10, 2020\n\nHopﬁeld neural network and anisotropic Ising model\nDmitry V. Talalaev\nMoscow State University, Faculty of Mechanics and Mathematics, 119991 Moscow, Russian Federation, dtalalaev@yandex.ru\nAbstract. The probabilistic Hopﬁeld model known also as the Boltzman machine is a basic example in the zoo of artiﬁcial neural networks. Initially it was designed as a model of associative memory, but played a fundamental role in understanding the statistical nature of the realm of neural networks. The close relation between the Boltzman machine and the Ising model was a challenging observation in [1]. In this note we go further, we establish another type of structural similarity between these models sharing the methods of the Bethe ansatz family of integrable statistical mechanics. We examine the asymmetric model on the triangular lattice with arbitrary weights. We show that the probability of passing a trajectory in time dynamics obeys the Gibbs distribution with a partition function of the Ising model on the cubic lattice with additional weights on diagonals.\n1 Introduction\nThe ﬁeld of artiﬁcial neural networks was born in the 40s, underwent intensive development in the 60s, was actively fueled by the ideas of statistical physics in the 80’s, and in applications made a giant leap in the last 10 years. This leap is largely due to the intensive growth in the performance of modern parallel computing systems. The ﬁeld was born as an attempt to systematize the mechanisms of nervous activity of living beings. It turned out that learning rules like the Hebb rules are so universal that artiﬁcial neural networks, which diﬀer signiﬁcantly from natural ones, began to show high results in generalizing ability. The actual formulation of the problem of evaluating the generalizing ability of the network arose due to the introduction of ideas of statistical mechanics in the ﬁeld of neural networks. The Hopﬁeld network has its proper application as one of the fundamental models of content-addressable associative memory system [1]. This model have demonstrated some very interesting collective properties of the neural network behavior and is used in artiﬁcial neural networks as like as in neurophysiology [3] in research of memory capacities, memory\n\n2\n\nDmitry V. Talalaev\n\nretrieval process, short-term plasticity, working memory properties and many other questions. The main goal of this work is to transfer methods for the exact solution of statistical physics models to neural network models. In this regard, it is necessary to recall what results are considered important in this area. The main model example is the two-dimensional Ising model, which has been solved explicitly by many groups of scientists using diﬀerent methods [4]. It has a critical behavior, that is, in the vicinity of the critical temperature, it has the property of a phase transition of the second kind. The memory capacity of the Hopﬁeld model demonstrated a similar property of the phase transition in[5]. The most common method to solve such models is the Bethe ansatz in termodynamic limit. The principal goal of this paper is to establish a relation between the anisotroppic Hopﬁeld model on the triangular lattice and the Ising on the appropriately chosen lattice. This work extends some ideas of [7, 8] to the anisotropic case. Another purpose of this note is the extension of the ideas of the cluster algebraic structures [12, 13] to the world of neural networks which could produce interesting invariants for the neural networks with respect to the mutaions of the “star-triangle” type.\n\nAcknowledgments\nThe work was partially supported RFBR grant 18-01-00461, this work was carried out within the framework of a development programme for the Regional Scientiﬁc and Educational Mathematical Center of the Yaroslavl State University with ﬁnancial support from the Ministry of Science and Higher Education of the Russian Federation (Agreement No. 075-02-2020-1514/1 additional to the agreement on provision of subsidies from the federal budget No. 075-02-2020-1514). .\n\n1.1 Hopﬁeld model\n\nThe Hopﬁeld model is represented by the complete graph with N vertexes (neurons) with a connectivity matrix Wij characterizing the conductivity of the synapse between i-th and j-th neurons. At each time the system is characterized by its neurons states {xi}, i = 1, . . . , N xi = ±1. Our interest is focused on the network which undergoes the synchronous evolution in discrete time. In the deterministic version the state of the i-th neuron at the next step xi is determined by the formula:\n\n 1 if \n\nj Wij xj > ti\n\nxi = −1 if j Wij xj < ti\n\n(1)\n\n xi else.\n\nHere ti is the threshold level for the activation of the i-th neuron. The probabilistic Hopﬁeld model diﬀers from the deterministic one by the property that the transition is performed with the probability speciﬁed by the Fermi sigmoid function\n\nP (x , x) = (1 + e−βxi( j Wij xj −ti))−1.\ni\n\nHopﬁeld neural network and anisotropic Ising model\n\n3\n\nFor the threshold level ti = 0 this expression can be rewritten as\n\nP\n\n(x\n\n,\n\nx)\n\n=\n\ne−\n\nβ 2\n\n/ ij Wij xixj\n\ne . −\n\nβ 2\n\nij Wij xi xj\n\n(2)\n\nx\n\nThe similarity of such an expression with the partition function of the Ising model was remarked in [1].\nWe need to say some words about the learning stage in the Hopﬁeld model. The Hebbian paradigm for the learning process of the Hopﬁeld network on the set of m patterns { 1, . . . , m} each of which is a vector\n\nk =(\n\nk 1\n\n,\n\n.\n\n.\n\n.\n\n,\n\nk n\n\n)\n\nis achieved instantaneously by ﬁxing the weight matrix in a following way\n\n1m Wij = n\n\nk i\n\nk j\n\n.\n\nk=1\n\nThe work of the network in the recall stage consists of iterative time dynamics deﬁned by the transformation probability (2). The principal interplay of the Hopﬁeld network and the Ising model is due to the energy functional. It turns out that for the symmetric weight function in asynchronous regime the expression\n\n1\n\nE=− 2\n\nWij xixj −\n\ntixi\n\ni,j\n\ni\n\nplays the role of the Lyapunov function, due to the time dynamics it either lower or stays the same. This observation allows to analyze the asymptotic behavior of the Hopﬁeld model. The stable points for its time dynamics are hence the states of local minimum energy for the Ising model on the same lattice. In this paper we explore another relationship between two such models: the Hopﬁeld network on a two-dimensional lattice and the Ising model on the 3-dimensional lattice, making one of the spacial directions relevant to the time evolution.\n\n1.2 Time evolution and the Ising model\nLet us consider the Hopﬁeld model on the triangular lattice (Fig. 1) colored by 3 colors in Z/3Z. The nontrivial weights are deﬁned by rule: the neuron with the color c is inﬂuenced only by the neurons with the colors (c−1) mod 3. This network is not symmetric despite the canonical deﬁnition. This lattice can be viewed as a projection of the cubic lattice to the plane i + j + k = 0. The verteces with diﬀerent colors represent the planes i + j + k = c mod 3. The temporal behavior of this lattice is equivalent to the 3 independent 3-dimensional cubic lattices. We then consider the one of these 3 lattices.\n\n4\n\nDmitry V. Talalaev\n\n- 0 - 1 - 2\n\nFig. 1. Triangular lattice\n\nThe conditional probability that the model passes throw the states with free initialization data is:\n\nb\nP=\n\n1 + exp(xijk(wi1jkxi−1jk + wi2jkxij−1k + wi3jkxijk−1) −1\n\ni+j+k=a\n\n=\n\nb i+j+k=a\n\nexp((xijk(wi1jkxi−1jk + wi2jkxij−1k + wi3jkxijk−1)/2) 2 cosh((xijk(wi1jkxi−1jk + wi2jkxij−1k + wi3jkxijk−1)/2)\n\nLet us deﬁne a matrix\n\n1 1 1 1\n2222\n\n1\n\nA\n\n=\n\n \n\n2\n\n1 2\n\n−\n\n1 2\n\n−\n\n1 2\n\n \n\n\n\n1 2\n\n−\n\n1 2\n\n1 2\n\n−\n\n1 2\n\n \n\n1 2\n\n−\n\n1 2\n\n−\n\n1 2\n\n1 2\n\nwhich is a square root of 1\n\nA2 = 1.\n\nLet us consider a transformation f : w → w f (w1, w2, w3) = (w0, w12, w13, w23)\n\ngiven by the system of equations\n\n w0   log cosh((w1 + w2 + w3)/2) \n\n  \n\nw12 w13\n\n  \n\n=\n\nA\n\n  \n\nlog log\n\ncosh((w1 cosh((w1\n\n+ −\n\nw2 w2\n\n− +\n\nw3)/2) w3)/2)\n\n  \n\n.\n\n(3)\n\nw23\n\nlog cosh((w1 − w2 − w3)/2)\n\nHopﬁeld neural network and anisotropic Ising model\n\n5\n\nLemma 1 Let f (w1, w2, w3) = (w0, w12, w13, w23) than the following equation holds (cosh((w1s1 + w2s2 + w3s3)/2))−1 = exp (w0 + w12s1s2 + w13s1s3 + w23s2s3)/2(4)\n∀si = ±1.\n\nProof Both sides of 4 are invariant with respect to the total change of signs si → −si. Hence it is suﬃcient to prove this statement for 4 combinations of spins with s1 = 1. In this way we get a system of linear equation with the deﬁning matrix A. The fact that it is square root of unity gives the result.\n\nRemark 1 The transformation f restricted to the last 3 variables F : (w1, w2, w3) → (w12, w13, w23) was known in the theory of the Ising model as a “star-triangle” transformation [9]. It appears to be a solution for the Zamolodchikov tetrahedron equation [10].\n\nTheorem 1 The conditional probability 3 coincides with the Ising-type partition function:\n\nb\n\nP=\n\nexp (xijk(wi1jkxi−1jk + wi2jkxij−1k + wi3jkxijk−1)/2 ×\n\ni+j+k=a\n\nb\n\n×\n\nexp (wi1j2kxi−1jkxij−1k + wi1j3kxi−1jkxijk−1 + wi2j3kxij−1kxijk−1)/2\n\ni+j+k=a\n\nwhere (wi1j2k, wi1j3k, wi2j3k) = F (wi1jk, wi2jk, wi3jk).\n\nRemark 2 This model can be interpreted as an Ising model on the reg-\nular cubic lattice with additional diagonal edges with weights deﬁned by (wi1j2k, wi1j3k, wi2j3k). We illustrate the weight distribution on Fig. 2.\n\n1.3 Conclusion\nThe principal aim of this work concerns the possibility of the Bethe ansatz method application to the description of the critical behavior of the Hopﬁeld neural network. This could be fruitful in such a technique as the simulated annealing in neural networks [11]. In this note we generalized a relation between the Hopﬁeld model and the Ising one in the case of the completely anysotropic models. This is interesting for us also for the reason that this case allows to apply the methods of cluster algebraic structures [12] and [13] for the case of neural network models.\n\n6\n\nDmitry V. Talalaev\n\ni j k-1 w3\nijk\nijk\n\nw23 ijk i j-1 k\n\nw2 ijk w13 ijk\n\nw12 ijk\n\nw1 ijk\n\ni-1 j k\n\nFig. 2. Cubic lattice\nReferences\n1. W. A. Little.The Existence of Persistent States in the Brain, Math. Biosciences 19,101-120 (1974) 101 J. J. Hopﬁeld. Neural networks and physical systems with emergent collective computational abilities. in Pk. Nat. Academy Sci., USA, vol. 19, 1982, pp. 2554-2558.\n2. Haykin, Simon, Neural networks and learning machines / Simon Haykin. - 3rd ed\n3. S. Recanatesi, M. Katkov, S. Romani and M. Tsodyks. Neural network model of memory retrieval. Front Comput Neurosci. 2015 9:149. S. Romani and M. Tsodyks. Short-term plasticity based network model of place cells dynamics. Hippocampus 2015 25:94-105.\n4. Onsager, Lars (1944), Crystal statistics. I. A two-dimensional model with an order-disorder transition, Physical Review, Series II, 65 (34): 117-149\n5. Daniel J. Amit, Hanoch Gutfreund, H. Sompolinsky, Statistical Mechanics of Neural Networks near Saturation Annals of Physics 173. 3S67 (1987)\n6. A.A. Belavin, A.G. Kulakov, R.A. Usmanov, Lectures on theoretical physics. MCCME, 2001\n7. Dmitry V. Talalaev, Towards integrable structure in 3d Ising model, Journal of Geometry and Physics, Volume 148, February 2020, 103545\n8. Dmitry V. Talalaev, Asymmetric Hopﬁeld neural network and twisted tetrahedron equation, arXiv:1806.06680\n9. R. J. Baxter, ”Exactly Solved Models in Statistical Mechanics” Academic Press, 1982\n10. Zamolodchikov A.B., Tetrahedra equations and integrable systems in three-dimensional space, Soviet Phys. JETP 52 (1980), 325-336.\n11. Da, Y.; Xiurun, G. An improved PSO-based ANN with simulated annealing technique. Neurocomputing 63 (2005) 527-533\n\nHopﬁeld neural network and anisotropic Ising model\n\n7\n\n12. Arkady Berenstein Sergey Fomin Andrei Zelevinsky, Parametrizations of Canonical Bases and Totally Positive Matrices, Advances in mathematics 122, 49-149 (1996).\n13. Vassily Gorbounov, Dmitry Talalaev, Electrical varieties as vertex integrable statistical models, arXiv:1905.03522\n\n"}