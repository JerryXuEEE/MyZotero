Understanding the Metropolis-Hastings Algorithm Siddhartha Chib; Edward Greenberg The American Statistician, Vol. 49, No. 4. (Nov., 1995), pp. 327-335.
Stable URL: http://links.jstor.org/sici?sici=0003-1305%28199511%2949%3A4%3C327%3AUTMA%3E2.0.CO%3B2-2 The American Statistician is currently published by American Statistical Association.
Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://www.jstor.org/about/terms.html. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at http://www.jstor.org/journals/astata.html. Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission.
The JSTOR Archive is a trusted digital repository providing for long-term preservation and access to leading academic journals and scholarly literature from around the world. The Archive is supported by libraries, scholarly societies, publishers, and foundations. It is an initiative of JSTOR, a not-for-profit organization with a mission to help the scholarly community take advantage of advances in technology. For more information regarding JSTOR, please contact support@jstor.org.
http://www.jstor.org Tue Feb 5 20:32:54 2008

Understanding the Metropolis-Hastings Algorithm 

Siddhartha CHIBand Edward GREENBERG

We provide a detailed, introductory exposition of the Metropolis-Hastings algorithm, a powerful Markov chain method to simulate multivariate distributions. A simple, intuitive derivation of this method is given along with guidance on implementation. Also discussed are two applications of the algoritl~m,one for implementing acceptance-rejection sampling when a blanketing function is not available and the other for implementing the algorithm with block-at-a-time scans. In the latter situation, many different algorithms, including the Gibbs sampler, are shown to be special cases of the Metropolis-Hastings algoritl~m.The methods are illustrated with examples.
KEY WORDS: Gibbs sampling; Markov chain Monte Carlo; Multivariate density simulation; Reversible Markov chains.
1. INTRODUCTION
In recent years statisticians have been increasingly drawn to Markov chain Monte Carlo (MCMC) methods to simulate complex, nonstandard multivariate distributions. The Gibbs sampling algorithm is one of the best known of these methods, and its impact on Bayesian statistics, following the work of Tanner and Wong (1987) and Gelfand and Smith (1990), has been immense as detailed in many articles, for example, Smith and Roberts (1993), Tanner (1993), and Chib and Greenberg (1993). A considerable amount of attention is now being devoted to the Metropolis-Hastings (M-H) algorithm, which was developed by Metropolis, Rosenbluth, Rosenbluth, Teller, and Teller (1953) and subsequently generalized by Hastings (1970). This algorithm is extremely versatile and gives rise to the Gibbs sampler as a special case, as pointed out by Gelman (1992). The M-H algorithm has been used extensively in physics, yet despite the paper by Hastings, it was little known to statisticians until recently. Papers by Miiller (1993) and Tierney (1994) were instrumental in exposing the value of this algorithm and stimulating interest among statisticians in its use.
Because of the usefulness of the M-H alogrithm, applications are appearing steadily in the current literature (see Muller (1993), Chib and Greenberg (1994), and Phillips and Smith (1994) for recent examples). Despite its obvious importance, however, no simple or intuitive exposition of the M-H algorithm, comparable to that of Casella and George (1992) for the Gibbs sampler, is available. This article is an attempt to fill this gap. We provide a tutorial introduction to the algorithm, deriving the algorithm from first principles. The article is self-contained
Siddhartha Chib is at the John M.Olin School of Business. Wash-
ington University, St. Louis, MO 63130. Edward Greenberg is at the Department of Economics. Washington University, St. Louis, MO 63 130. The authors express their thanks to the editor of the journal, the associate editor, and four referees for many useful comments on the paper, and tohlichael Ogilvie and pin-Huang Chou for helpful discussions.
@ 1995 Alilel-ica11Statistical Associatio~z

since it includes the relevant Markov chain theory, issues related to implementation and tuning, and empirical illustrations. We also discuss applications of the method, one for implementing acceptance-rejection sampling when a blanketing function is not available, developed by Tierney (1994), and the other for applying the algorithm one "block at a time." For the latter situation, we present an important principle that we call the groduct of kernels principle and explain how it is the basis of many other algorithms, including the Gibbs sampler. In each case we emphasize the intuition for the method and present proofs of the main results. For mathematical convenience, our entire discussion is phrased in the context of simulating an absolutely continuous target density, but the same ideas apply to discrete and mixed continuousdiscrete distributions.
The rest of the article is organized as follows. In Section 2 we briefly review the acceptance-rejection (A-R) method of simulation. Although not an MCMC method, it uses some concepts that also appear in the Metropolis-Hastings algorithm and is a useful introduction to the topic. Section 3 introduces the relevant Markov chain theory for continuous state spaces, along with the general philosophy behind MCMC methods. In Section 4 we derive the M-H algorithm by exploiting the notion of reversibility defined in Section 3, and discuss some important features of the algorithm and the mild regularity conditions that justify its use. Section 5 contains issues related to the choice of the candidate-generating density and guidance on implementation. Section 6 discusses how the algorithm can be used in an acceptance-rejection scheme when a dominating density is not available. This section also explains how the algorithm can be applied when the variables to be simulated are divided into blocks. The final section contains two numerical examples, the first involving the simulation of a bivariate normal distribution, and the second the Bayesian analysis of an autoregressive model.
2. ACCEPTANCE-REJECTION SAMPLING
In contrast to the MCMC methods described below, classical simulation techniques generate non-Markov (usually independent) samples, that is, the successive observations are statistically independent unless correlation is artificially introduced as a varlance reduction device. An important method in this class is the A-R method, which can be described as follows:
The objective is to generate samples from the absolutely continuous target clensitl). ~ ( x =) f (x)/K, where x E 2".f ( x ) is the unnorrnalized density, and K is the (possibly unknown) normalizing constant. Let Iz(x) be a density that can be simulated by some known method, and
suppose there is a known constant c sucll that f (x) 5 clz(x)
for all n. Then, to obtain a random variate from T(.),
a (*) Generate a candidate Z from lz(.) and a value u from U(0; l), the uniform distribution on (0,1).
The A~verlca~Sztatisticzalz, Novell~hel-1995, Vol. 49, No. 4 327

If 11 5 f (Z)/clz(Z)
-return Z = ),. Else
-goto (*).
It is easily shown that the accepted value y is a random variate from T(.). For this method to be efficient, c must be carefully selected. Because the expected number of iterations of steps 1 and 2 to obtain a draw is given by c-l, the rejection method is optimized by setting
C = s u.f(px) .
n 17 (x) Even this choice, however. may result In an undesirably large number of rejections.
The notion of a generating density also appears in the M-H algorithm, but before considering the differences and similarities, we turn to the rationale behind MCMC methods.

3. MARKOV CHAIN MONTE CARL0 SIMULATION
The usual approach to Markov chain theory on a continuous state space is to start with a transition kernel P(x,A) for x E 2" and A E B , where B is the Bore1 rr-field on 2". The transition kernel is a conditional distribution function that represents the probability of moving froin x to a point in the set A . By virtue of its being a distribution function, P(x, 2") = 1, where it is permitted that the chain can make a transition from the point x to x, that is, P(x, {x)) is not necessarily zero.
A major concern of Markov chain theor; [see Nulnmelin (1984), Billingsley (1986), Bhattacharya and Waymire (1990), and, especially, Meyn and Tweedie (1993)l is to determine conditions under which there exists an invariant distribution T* and conditions under which iterations of the transition kernel converge to the invariant distribution. The invariant distribution satisfies

a ' ( d ~ , )= P ( i , ~ > ) T ( Xdx)

(1)

where T is the density with respect to Lebesgue measure of a:"tthus ~ * ( d y=) ~ ( yd)y). The ~ t ihterate is given by Pc")(xA, ) = JT,P,("-')(n, dy)P(y, A ) , where P(')(x,dy) = P(x, dy). under conditions discussed in the following, it can be shown that the 12thiterate converges to the invariant distribution as n + oo.
MCMC methods turn the theory around: the invariant density is known (perhaps up to a constant multiple)-it is T(.), the target density from which samples are desiredbut the transition kernei is unknown. To generate sainpies from T(.), the methods find and utilize a transition kernel P(x, dy) whose nth iterate converges to a(.) for large 11. The process is started at an arbitrary x and iterated a large number of times. After this large number, the distribution of the observations generated from the simulation is approximately the target distribution.
The problem then is to find an appropriate P(x, dl'). What might appear to be a search for the proverbial needle in a haystack is somewhat simplified by the following considerations. Suppose that the transition kernel, for some function p(x, y), is expressed as

328 The Aiiler-icarz Statistician, Noveli7Der-1995, Vol. 49, No. 4

wherep(x, x) = 0,S,:(cly) = 1if x t dy and 0 otherwise, and r(x) = 1 - J2,/p(x, y) cl~ils the probability that the chain re-
inains at x. From the possibility that r(x) # 0, it should be
clear that the integral of y(.x, v) over >, is not necessarily 1. Now, if the functionp(x,J.) in (2) satisfies the reversibil-
ity condition (also called "detailed balance," "microscopic reversibility," and "time reversibility")
then T(.) is the invariant density of P(x, .) (Tierney 1994). To verify this we evaluate the right-hand side of (1):

1+ r(x)T(x)dx

= /(I

- Y ( ~ ) ) T ( JdI )~+'

r
1

r ( x ) ~ ( xd) . ~

/T. A
= n(y) 6,.

J A

(4)

Intuitively, the left-hand side of the reversibility condition is the unconditional probability of moving froin x to y, where x is generated from T(.), and the right-hand side is the unconditional probability of moving from y t o n , where y is also generated from a ( . ) . The reversibility condition says that the two sides are equal. and the above result shows that T*(.) is the invariant distribution for P ( . , .).
This result gives us a sufficient condition (reversibility) that must be satisfied by p(x,y). We now show how the Metropolis-Hastings algorithm finds a p(x, y) with this property.

4. THE METROPOLIS-HASTINGS ALGORITHM
As in the A-R method, suppose we have a density that can generate candidates. Since we are dealing with Markov chains, however, we permit that density to depend on the current state of the process. Accordingiy, the carldidcrte-getlelntirlgdensity is denoted q(x,y), where
J q(x,J.) dy = 1. This density is to be interpreted as saying
that when a process is at the point x, the density generates a value y from q(x,3,). If it happens that q(x,y) itself satisfies the reversibility condition (3) for all x, y, our search is over. But most likely it will not. We might find, for example, that for some x , y,

In this case, speaking somewhat loosely, the process ~ n o v e sfrom x to J' too often and from J, to x too rarely. A convenient way to correct this condition is to reduce the
number of moves from x to y by introducing a probability

a(x. y) < 1 that the move 1s made. We refer to a(.u. y) as
the probabilit). of r?zove. If the move is not made, the process again returns x as a value from the target distribution. (Note the contrast with the A-R neth hod in which, when a y is rejected, a new pair (y, u) is drawn independently of the previous value of y.) Thus transitions from .x to y (y f x) are made according to

where a(n. y) is yet to be determined. Consider again inequality (5). It tells us that the move-
ment from y to x is not made often enough. We should therefore define a ( y , x ) to be as large as possible, and since it is a probability, its upper limit is 1. But now the probability of move a(x,y) is determined by requiring that phIH(xy, ) satisfies the reversibility condition, because then

T(x)q(x, ),)a(x;y) = .Ti(y)q(y;x)a(y, x)

= .Ti(y)q(y;x).

(6)

We now see that a(x, y) = ~ ( y ) q ( jx,)/.~i(x)q(xy, ). Of course, if the inequality in (5) is reversed, we set a(x,y) = 1 and derive a ( y , x) as above. The probabilities a(x,y) and a ( y , x) are thus introduced to ensure that the two sides of (5) are in balance or, in other words, that pbIH(xy, ) satisfies reversibility. Thus we have shown that in order for yhfH(xy, ) to be reversible, the probability of move must be set to

a(x,y) = min

, if T ( x ) ~ ( xy,) > 0

= 1,

otherwise.

To complete the definition of the transition kernel for the Metropolis-Hastings chain, we must consider the possibly nonzero probability that the process remains at x. As defined above, this probability is

Consequently, the transition kernel of the M-H chain, de-
noted by PMH(xb, ),is given by

a particular case of (2). BecausepbIH(xy, ) is reversible by construction, it follows from the argument in (4) that the M-H kernel has ~ ( xa)s its invariant density.
Several remarks about this algorithm are in order. First, the M-H algorithm is specified by its candidate-generating density q(x,y) whose selection we take up in the next section. Second, if a candidate value is rejected, the current value is taken as the next item in the sequence. Third, the calculation of a(x,y) does not require knowledge of the normalizillg constant of T(.b)ecause it appears both in the numerator and denominator. Fourth, if the candidategenerating density is symmetric, an important special case, q(x,y) = q(y, x) and the probability of move reduces to
> T ( ~ ) / T ( x ) ;hence, if ~ ( y ) ~ ( x )t,he chain moves to y;
otherwise, it moves with probability given by T(~)/T(x). In other words, if the jump goes "uphill," it is always accepted; if "downhill," it is accepted with a nonzero probability. [See Fig. 1 where, from the current point x, a move

Figure 1. Calculating Probabilities of Move With Symmetric Candidate-Generating Function (see text).
to candidate y1 is made with certainty, while a move to candidate y2 is made with probability ~ ( y ~ ) / ~ ( xT)h.i]s is the algorithm proposed by Metropolis et al. (1953). Interestingly, it also forms the basis for several optimization algorithms, notably the method of simulated annealing.
We now summarize the M-PI algorithm in algorithmic form initialized with the (arbitrary) value x(O):
Repeat for j = 1 , 2 ,. . . ,N.
0 Generate y from q(x(i),.) and u from U(0, 1). If u 5 a(x(i),y) -set x(i+l) = Y. Else -set x(i+l)= x(i)
Return the values ( ~ ( ' 1 ,x ( ~ ). ., . ,~ ( ~ ' 1 ) .
As in ally MCMC method, the draws are regarded as a sample from the target density ~ ( xo)nly after the chain has passed the transient stage and the effect of the fixed starting value has become so small that it can be ignored. In fact, this convergence to the illvariallt distribution occurs under mild regularity conditions. The regularity collditiolls required are irreducibility and aperiodicity [see Smith and Roberts (1993)l. What these mean is that, if x and y are in
the domain of T(,)it,must be possible to move frolnx to dy
in a finite number of iterations with llollzero probability, and the number of moves required to move from x to (14' is not required to be a multiple of some integer. These conditions are usually satisfied if q(x,y) has a positive density on the same support as that of T(.). It is usually also satisfied by a q(x,y) with a restricted support (e.g., a uniform distribution around the current point with finite width).
These conditions, however, do not determine the rate of convergence [see Roberts and Tweedie (1994)], so there is an empirical question of how large an initial sample of size no (say) should be discarded and how long the sampling should be run. One possibility, due to Gelman and Rubin (1992), is to start multiple chains from dispersed initial values and compare the within and between variation of the sampled draws. A simple heuristic that works in some situations is to make no and N increasing functions of the first-order serial correlation in the output.

This entire area, however, is quite unsettled and is being actively researched. For more details the reader should consult Gelman and Rubin (1992) and the accompanying discussion.
5. IMPLEMENTATION ISSUES: 
 CHOICE OF q ( x ,y) 

To implement the M-H algorithm, it is necessary that a suitable candidate-generating density be specified. Typically, this density is selected from a family of distributiolls that requires the specification of such tuning parameters as the location and scale. Collsiderable recent work is being devoted to the question of how these choices should be made and, although the theory is far from complete, enough is known to conduct most practical simulatioll studies.
One family of candidate-generating densities, that appears in the work of Metropolis et al. (19531, is given by q(x,y) = q l ( y - x), where ql(.) is a multivariate density [see Miiller (1993)l. The candidate y is thus drawn
according to the process y = x + z, where z is called the
increment random variable and follows the distribution q l . Because the candidate is equal to the current value plus noise, this case is called a mndorlz ~.t,alkchain. Possible choices for ql include the multivariate normal density and the multivariate-t with the parameters specified according to the prillciples described below. Note that when q, is symmetric, the usual circumstance, ql(z) = ql(-2); the probability of move then reduces to
As mentionedearlier, the same reduction occurs if q(x,y) = q(y, x).
A second family of candidate-generating densities is given by the form q(x,y) = qz(y) [see Hastings (1970)l. In contrast to the random walk chain, the candidates are drawn independently of the current location x-an irzdeper~derzcecizairz in Tierney's (1994) terminology. As in the first case, we can let q2 be a multivariate normal or multivariate-t density, but now it is necessary to specify the location of the generating density as well as the spread.
A third choice, which seems to be an efficient solution when available, is to exploit the known form of T(.) to specify a candidate-generating density [see Chib and Greenberg (1994)l. For example, if ~ ( tc)an be written as ~ ( t 3)~ +(t)h(t), where h(t) is a density that can be sampled and w(t) is uniformly bounded, then set q(x,y) = h(y) (as in the independence chain) to draw candidates. In this case, the probability of move requires only the computation of the 13 function (not T or lz) and is given by
A fourth method of drawing candidates is to use the A-R method with a pseridodominating density. This method was developed in Tierney (1994), and because it is of independent interest as an M-H acceptance-rejection method, we explain it in Section 6.1.
A fifth family, also suggested by Tierney (1994), is represented by a vector autoregressive process of order 1. These alltoregressive cl~ainasre produced by letting
330 Tlze Ali~ericarzStatisticiarz, i\'o~oi;erizber1995, Vnl. 49, No. 4

y = a + B(x - a) + z, where a is a vector and B is a matrix
(both conformable with x) and z has q as its density. Then,
q(x, y) = q(y - a - B(x - a)). Setting B = -I produces chains that are reflected about the point a and is a simple way to induce negative correlation between successive elements of the chain.
We now return to the critical question of choosing the spread, or scale, of the candidate-generating density. This is an important matter that has implications for the efficiency of the algorithm. The spread of the candidategenerating density affects the behavior of the chain in at least two dimensions: one is the "acceptance rate" (the percentage of times a move to a new point is made), and the other is the region of the sample space that is covered by the chain. To see why, consider the situation in which the chain has converged and the density is being sampled around the mode. Then, if the spread is extremely large, some of the generated candidates will be far from the current value, and will therefore have a low probability of being accepted (because the ordinate of the candidate is small relative to the ordinate near the mode). Reducing the spread will correct this problem, but if the spread is chosen too small, the chain will take longer to traverse the support of the density, and low probability regions will be undersampled. Both of these situations are likely to be reflected in high autocorrelatiolls across sample values.
Recent work by Roberts, Gelman, and Gilks (1994) discussed this issue in the context of q, (the random walk proposal density). They show that if the target and proposal densities are normal, then the scale of the latter should be tuned so that the acceptance rate is approximately .45 in one-dimensional problems and approximately .23 as the number of dimensions approaches infinity, with the optimal acceptance rate being around .25 in as low as six dimensions. This is similar to the recommelldation of Miiller (1993), who argues that the acceptance rate should be around .5 for the random walk chain.
The choice of spread of the proposal density in the case of q2 (the independence proposal density) has also come under recent scrutiny. Chib and Geweke [work in progress] show that it is important to ensure that the tails of the proposal density dominate those of the target density, which is similar to a requirement on the importance sampling function in Monte Carlo integration with importance sampling [see Geweke (1989)l. It is important to mention the caveat that a chain with the "optimal" acceptance rate may still display high autocorrelations. In such circumstances it is usually necessary to try a different family of candidate-generating densities.
6. APPLICATIONS OF THE M-H ALGORITHM
We hope that the reader is now convinced that the M-H algorithm is auseful and straightforward device with which to sample an arbitrary multivariate distribution. In this sectio11 we explain two uses of the algorithm, one involving the A-R method, and the other for implementing the algorithm with block-at-a-time scans. In the latter situation many different algorithms, including the Gibbs sampler, are shown to arise as special cases of the M-H algorithm.

6.1 An M-H Acceptance-Rejection Algorithm
Recall that in the A-R method described earlier, a constant c and a density h(x) are needed such that c h ( ~ ) dominates or blankets the (possibly) unnormalized target density f(x). Finding a c that does the trick may be difficult in some applications; moreover, if f(x) depends on parameters that are revised during an iterative cycle, finding a new value of c for each new set of the parameters may significantly slow the computations. For these reasons it is worthwhile to have an A-R method that does not require a blanketing function. Tienley's (1994) remarkable algorithm does this by using an A-R step to generate candidates for an M-H algorithm. This algorithm, which seems complicated at first, can be derived rather easily using the intuition we have developed for the M-H algorithm.
To fix the context again: we are interested in sampling the target density ~ ( x )~, ( x =) f(x)/K, where K may be unknown, and apdf /I(.)is available for sampling. Suppose
c > 0 is a known constant, but that f(x) is not necessarily
less than ch(x) for all x; that is, clz(x) does not necessarily dominatef (x). It is convenient to define the set C where domination occurs:
C = {x: f (x) < ch(x)).
In this algorithm, given x(") = x-, the next value x("+')is
obtained as follows: First, a candidate value z is obtained,
independent of the current vallre x, by applying the A-R algorithm with ell(.) as the "dominating" density. The AR step is implemented through steps 1 and 2 in Section 2.
What is the density of the rv y that comes through this step? Following Rubinstein (1981, pp. 45-46), we have

But because P(U If(Z)/ch(Z) Z = y) = min{f(y)/
ch(y), 11, it follows that

q(y)

=

min{f

(y)/ch(y). d

1)

x

h(y)

,

< where d = Pr(U f(Z)/clz(Z)). By simplifying the nu-

merator of this denslty we obtain a more useful represen-

tation for the candidate-generating density:

Figure 2. Acceptance-Rejection Sampling With Pseudodominating Density ch(x).
To proceed, we derive a(x,y) in each of the four possible cases given above. As in (2), we consider ~ ( x ) q ( y )and r(y)q(x) [or, equivalently, f (x)q(y) and f (y)q(x)] to see how the probability of moves should be defined to ensure reversibility. That is, we need to find a(x; y) and a()';x) such that
in each of the cases (a)-(d), where q(y) is chosen from (7). Case (a): x E C, y E C. In this case it is easy to verify
that f (x)q(y) =f (x)f (y)/cd is equal to f (y)q(x). Accord-
ingly, setting a(x; y) = a ( y ;x) = 1 satisfies reversibility.
Cases (b) and (c): x $ C. y E C o r x E C. y $ C. In the first case f(x) > c~I(x)o, r h(x) < f(x)/c, which implies (on multiplying both sides by f (y)/d) that
or, from (7), f (y)q(x) < f (x)q(y). We now see that there
are relatively too few transitions from y to x and too many in the opposite direction. By setting ~ ( yx), = 1 the first problem is alleviated, and then a(x;y) is determined from

(Note that there is no need to write q(x,y) for this density because the candidate y is drawn independently of x.)
Because clz(y) does not dominate the target density in C"(by definition), it follows that the target density is not adequately sampled there. See Figure 2 for an illustration of a nondominating density and the C region. This can be corrected with an M-H step applied to the y values that come through the A-R step. Since x and y can each be in C or in Cc;there are four possible cases: (a) x E C, y E C;
(b) and (c) x @ C, y E C or x E C, y $ C; and (d) x $ C,
Y $C. The objective now is to find the M-H moving proba-
bility a(x,y) such that q(y)cu(x, y) satisfies reversibility.

which gives ~ ( xy), = ch(x)/f (x). If x E C. y @ C, reverse the roles of x and y above to find that a(x, y) = 1 and d ) ' , ~=) clzO')/f(y).
Case (d): x $ C. y $ C. In this case we have
f(x)q(y) = f (x)h(.Y)ld and f (y)q(x) = f Ol)h(x)ld, and
there are two possibilities. There are too few transitions from y to x to satisfy reversibility if
In that case set a ( y , x) = 1 and determine a(x, y) from
The Arnericarz Sfnfisficinrz,Novernbel- 1995, Vol. 49, No. 4 331

which implies
If there are too few transitions from x to J,,just interchange x and )' in the above discussion.
We thus see that in two of the cases, those where x E C, the probability of move t o y is 1, regardless of where lies. To summarize, we have derived the following probability of move to the candidates y that are produced from the A-R step:
Let C1 = {f(x) < clz(X)}; and C2 = {f(y) < ch(y)}.
Generate LL from U(0. 1) and -if C l = 1, then let cu = 1; -if C l = 0 and C2 = 1 , then let cu = (ch(x)/f (x)); -if C1 = 0 and C2 = 0 , then let cu = min{(f (y)h(x)/ f ( ~ ) h ( ~11) 1. .
Ifu<cu -return y.
Else -return x.
6.2 Block-at-a-TimeAlgorithms
Another interesting situation arises when the M-H algorithm is applied in turn to subblocks of the vector x, rather than simultaneously to all elements of the vector. This "block-at-a-time" or "variable-at-a-time" possibility, which is discussed in Hastings (1970, sec. 2.4), often simplifies the search for a suitable candidate-generating density and gives rise to several interesting hybrid algorithms obtained by combining M-H updates.
The central idea behind these algorithms may be il-
lustrated with two blocks, x = (xl:x2), where x, E R"'.
Suppose that there exists a conditional transition kernel
PI(xl,dyl 1 x2) with the property that, for a fixed value
of x2, riiTI2(. x2) is its invariant distribution (with density 7iI2(. x:)), that is,
Also, suppose the existence of a conditional transition kernel P2(x2,dy2 XI) with the property that, for a given xl,
7i;,(. 1 XI) is its invariant distribution, analogous to (8).
For example, PI could be the transition kernel generated by a Metropolis-Hastings chain applied to the block xl with x2 fixed for all iterations. Now, somewhat surprisingly, it turns out that the product of the transition kernels has T(XI,x2) as its invariant density. The practical significance of this principle (which we call the prodiict of kernelsprinciple) is enormous because it allows us to take draws in succession from each of the kernels, instead of having to run each of the kernels to convergence for every value of the conditioning variable. In addition, as suggested above, this principle is extremely useful because it is usually far easier to find several conditional kernels that converge to their respective conditioilal densities than to find one kernel that converges to the joint.
To establish the product of kernels principle it is necessary to specify the nature of the "scan" through the eiements of x (Hastings mentions several possibilities).
Suppose the transition kernel PI(.; . 1 x2)produces yl given

xl and x2; and the trailsition kernel P2(., . 1 y1) generates
y2 given x2 and yl . Then the kernel formed by multiplying the conditional kerneis has r * ( . ,.) as its invariant distribution:
S/ P I ( X,I& I x2)P2(x2; dy2 1 YI)T(*I, R ) ~ XdxI:
= 7iT(d~1)7i2*~1(1dY~2I )
= T * ( ~ YdIy,2):
where the third line follows from (8), the fourth from Bayes theorem, the sixth from assumed invariance of P2,and the last from the iaw of total probability.
With this result in hand, several important special cases of the M-H algorithm can be mentioned. The first special case is the so-called "Gibbs sampler." This algorithm is
obtained by letting the transition kernel P I(xl : dyl 1 x2) = 7i;12(d~lI ~ 2 ) a;nd P 2 ( ~ 2 ; d ~I 2~ 1 =) ~2*ll(dyI2Yl), that
is, the samples are generated directly from the "full conditional distributions." Note that this method requires that it be possible to generate independent samples from each of the full conditional densities. The calculations above demonstrate that this algorithm is a special case of the M-H algorithm. Alternatively, it may be checked that the M-H acceptance probability a ( x , y) = 1 for all x; y.
Another special case of the M-H algorithm is the socalled "M-H within Gibbs" algorithm (but see our comments on terminology below), in which an intractable full conditional density [say 7il12(y1 x2)] is sampled with the general form of the M-H algorithm described in Section 4 and the others are sampled directly from their full conditional distributions. Many other algorithms can be similarly developed that arise from multiplying conditional kernels.
We conclude this section with a brief digression on terminology. It should be clear from the discussion in this subsection that the M-H algorithm can take many different forms, one of which is the Gibbs sampler. Because much of the literature has overlooked Hastings's discussion of M-H algorithms that scan one block at a time, some unfortunate usage ("M-H within Gibbs," for example) has arisen that should be abandoned. In addition, it may be desirable to define the Gibbs sampler rather narrowly, as we have done above, as the case in which all full conditional kernels are sampled by independent algorithms in a fixed order. Although a special case of the M-H algorithm, it is an extremely important special case.
7. EXAMPLES
We next present two examples of the use of the M-H algorithm. In the first we simulate the bivariate normal to illustrate the effects of various choices of q(x, y); the

332 The A~nerica~Sztatisticialz, November 1995, Vol. 49, No. 4

second example illustrates the value of setting up blocks of variables in the Bayesian posterior analysis of a secondorder autoregressive time series model.
7.1 Simulating a Bivariate Normal
To illustrate the M-H algorithm we consider the simu-
lation of the bivariate normal distribution &(p, I),where
p = (1, 2)' is the mean vector and Z = (D,,):2 x 2 is the covariance matrix given by
Because of the high correlation the contours of this distrib~ltionare "cigar-shaped," that is, thin and positively inclined. Although this distribution can be simulated di-
rectly in the Choleski approach by letting y = p + P'u,
where 11 -- X2(0:12) and P satisfies P ' P = C . this
weil-known problem is usefill for illustrating the M-H algorithm.
From the expression for the multivariate normal density, the probability of move (for a symmetric candidategenerating density) is
LL(Xy.) = mln exp [-;(?I - p ) ' C - ' ( ) ~- p)] . 1 ) .
- ,u)lC-l(w - b~)]
We use the following candidate-generating densities, for which the parameters are adjusted by experimentation to achieve an acceptance rate of 40% to 50%:
+ 1. Random walk generating density (J, = x z), where
the increment random variable z is distributed as bivariate uniform, that is, the it11 component of z is uniform on the
interval (-hi, hi). Note that h1 controls the spread along the first coordinate axis and h2 the spread along the second. To avoid excessive moves we let = .75 and 52 = 1.
2. Random walk generating density (y = x + z ) with
z distributed as independent normal N2(0,D), where D =
diagonal(.6, .4). 3. Pseudorejection sampling generating density with
"dominating function" clz(x) = c(27;)-'jD - I / ' exp[- 2 (x - p)'D(x - p ) ] , where D = diagonal(2,2) and c = .9. The trial draws, which are passed through the A-R step, are thus obtained from a bivariate, independent normal distribution.
4. The autoregressive generating density y = p - (x -
p) + z, where z is independent uniform with 6')= I = S2.
Thus values of y are obtained by reflecting the current point around p and then adding the increment.
Note that the probability of move in cases 1, 2, and 4 is given by (9). In addition, the first two generating densities do not make use of the known value of ~ i a,lthough the values of the 6, are related to C. In the third generating density we have set the value of the constant c to be smaller than that which leads to true domination. For domination it is necessary to let all diagonal entries of D be equal to
1.9 (the largest eigenvalue of C) and to set c = q'm
[see Dagpunar (1988, p. 159)]. Each of these four candidate-generating densities repro-
duces the shape of the bivariate normal distribution being

simulated, although overall the best result is obtained from the fourth generating density. To illustrate the characteristics of the output, the top panel of Figure 3 contains
the scatter plot of i V = 4,000 simulated values from the
Choleski approach and the bottom panel the scatter plot of N = 6,000 simulated values using the fourth candidategenerating density. More observations are taken from the M-H algorithm to make the two plots comparable. The plots of the output with the other candidate-generating densities are similar to this and are therefore omitted. At the suggestion of a referee, points that repeat in the M-H chain are "jittered" to improve clarity. The figure clearly reveals that the sampler does a striking job of visiting the entire support of the distribution. This is confirmed by the estimated tail probabilities computed from the M-H output for which the estimates are extremely ciose to the true values. Details are not reported to save space.
For the third generating density we found that reductions in the elements of D led to an erosion in the number of times the sampler visited the tails of the distribution. In addition, we found that the first-order serial correlation of the sampled values with the first and second candidategenerating densities is of the order .9, and with the other two it is .30 and .16, respectively. The high serial correlation with the random walk generating densities is not unexpected and stems from the long memory in the candidate draws. Finally, by reflecting the candidates we see that it is possible to obtain a beneficial reduction in the serial correlation of the output with little cost.
7.2 Simulating a Bayesian Posterior
We now illustrate the use of the M-H algorlthln to sample an intractable distribution that arises in a stationary second-order autoregressive [AR(2)] time series model. Our presentation is based on Chib and Greenberg (1994), which contains a more detailed discussion and results for the general ARMA( p. q) model.
For our illustration, we simulated 100observations from the model
- + ?', = Ql!,-l + d2yr-2 ti, t = 1 , 2 ,. . . , 100, (10)
where 4) = 1, 0 2 = -.5, and E , N(0, I). The values
of Q = d 2 ) lie in the region S c 2' that satisfies the
stationarity restrictions
Following Box and Jenkins (1976), we express the (exact or ur~condltionnl)likelihood function for this model given
the rz = 100 data values Y,, = (1.).y2, . . . ,y,,)' as
/(o, g2) = Q ( d , 0') x (g2)-("-2)/2

where w, = ( Y , _ ~, t.- 2 ) ' ,

Q(~,D"=(D~)-'V-I~~/~~X~

(12)

is the density of Y2 = ( y l .y2)',

Tile Arlzericnrl Statisticiarz, Noi:e~lzber1995, Vol. 49, No. 4 333

Figure 3. Scatter Plots of Simulated Draws. Top panel: Generated by Choleski approach. Bottom panel: Generated by M-H with reflection candidate-generating density

and the third term in ( I 1) is proportional to the density of the observations ( j 3 ,. . . .y,,) given Y2.
If the only prior information available is that the process is stationary, then the posterior distribution of the parameters is
~ ( 4cr2;/ Y,,) cx l ( 4 ;02)1[4E S ] ;
where I [ 4 â‚¬ S] is 1 if 4 t S and 0 otherwise.
How can this posterior density be simulated? The answer lies in recognizing two facts. First, the blocking
strategy is useful for this problem by taking 4 and o2as
blocks. Second, from the regression ANOVA decomposition, the exponential term of ( 1 1 ) is proportional to
334 Tlze Ari~ericarzStntzsticinn, h1over~lDe1r995, Vol. 49, No. 4

3 where = G-I Cy=,(w,y,) and G = Cy=3(\ljw:). This is
the kernel of the normal density with mean 4 and covari-
ance matrix 02G-'. These observations immediately lead
to the following full conditional densities for o 2 and 4:
1. The density of o2given 4 and Y,, is inverted gamma with parameters r2/2 and y2V-'Y2 + C:i3(y-t ~ ( 4 ) ~ .
2. The density of 4 given o 2 and Y,, is
o. . r ( i Y,,,0') WQ.02!x { f n o r (Ii 0 2 ~ - ' ) 1 [to S ] ) .
( 13) where f,,, 1s the normal denslty function.
A sample of draws 4 from .ir(02, I Y,) can now be obtained by successively sampling 4 from ~ ( ( Y4,,,c2)a,nd given this value of 4 , simulating o2 from .ir(02 1 Y n ,4 ) .
The latter simulation is straightforward. For the former,

Table I. Summaries of the Posterior Distribution for Simulated AR(2) Model
Posterior
Param. Mean Num. SE SD Median Lower Upper Corr.
because it can be shown that IV-'/'/' is bounded for all
values of 4 in the stationary region, we generate candi-
dates from the density in curly braces of (13), following the idea described in Section 5. Then, the value of 4 is simulated as: At the jth iteration (given the current value m2(j)).draw a candidate c5(it1)from a normal density with
mean & and covariance m2(i'G-I; if it satisfies stationarity.
Inove to this point with probability
and otherwise set di+'=)6 ( j 1 , where Q(., .) is defined in
(12). The A-R method of'section 2 can also be applied to this problem by drawing candidates b(i+')from the nor-
< mal density in (13) until U Q ( d i t l ) ,m2(jj). Many draws
~f ch may be necessary, howe\~er,before one is accepted because Q ( Q :m2) can become extremely small. Thus the direct A-R method, although available, 1s not a competitive substitute for the M-H scheme described above.
In the sampling process we ignore the first no = 500 draws and collect the next N = 5,000. These are used to approximate the posterior distributio~lsof b and 0'. It is worth mentioning that the entire sampling process took just 2 minutes on a 50 MHz PC. For comparison we obtained samples from the A-R method, which took about 4 times as long as the M-H algorithm.
The posterior distributions are summarized in Table 1, where we report the posterior mean (the average of the simulated values), the ~luinericasl tandard error of the posterior mean (computed by the batch ineans method), the posterior standard deviations (the standard deviation of the simulated values). the posterior median, the lower 2.5 and upper 97.5 percentiles of the simulated values, and the sample first-order serial correlation in the simulated values (which is low and not of concern). From these results it is clear that the M-H algorithm has quickly and accurately produced a posterior distribution concentrated on the values that generated the data.
8. CONCLUDING REMARKS
Our goal in this article is to provide a tutorial exposition of the Metropolis-Hastings algorithm, a versatile. efficient, and powerful simulation technique. It borrows from the well-known A-R method the idea of generating candidates that are either accepted or rejected, but then retains the current value when rejection takes place. The Markov chain thus generated can be shown to have the target distrib~itionas its limiting distribution. Simulating from the target distribution is then accomplished by

running the chain a large number of rimes. We provide a simple, intuitive justification for the form taken by the probability of Inove in the M-H algorithm by showing ~ t s relation to reversibility. We also discuss implementation issues and two applications, the M-H acceptance rejection algorithm and the use of the algorithm in block-at-a-time setting. Finally, the procedures are illustrated with two examples.
[Received April 1994. Revised Jcir~rrnry1995.1
REFERENCES
Bhattacharya, R. N., and Waymire. E. C. (1900). Srochosric Proc~.c.ses it.itil A[~[~/icntiolNlse.w 'l'ork: John Wiley.
Billingsley, P. (1986). Probability orid .kfro.c.lirr (2nd ed.), New York: John LViley.
Box, G. E. P., and Jenkins, G . M . (1976), Tirr~eSei.ies Ailnlysrs: Foiacasriilg nild Corltrol (re\,. ed.). San Francisco: Holden-Day.
Casella, G.. and George, E. (1992), "Explaining the Gibbs Sampler," The A?tlericn?lSmri.rticiaiz. 46, 167-1 74.
Chib, S.. and Greenberg, E. (1993). "Markov Chain Monte Carlo Sirnulation Methods in Econometrics," Eco?loil~etricTheor:,'. in press. (1994), "Bayes Inference for Regression Models with ARhlA(p, 9 ) Erors," Jolcrrlnl of Ecorloi~~etric.6r4. , 183-206.
Dagpunar, J. (1988), Prbici11Ir.r o f Roi!doir~Vrrr.iorr Genrrotioil, New York: Oxford University Press.
Gelfand. A. E., and Smith. A. F. 121. (1990). "Sampling-Based Approaches to Calculating Marginal Densities," .io~irilnoi fiiie Aii~ericnrl Smtr.c.rrcaiAssocrritioii,85, 398-409.
Geiman, A. (i992). "iterative and Non-Iterative Sirnuiation Aigorithms," in Coi??plltiiigScie?zceo i i d S r a t i ~ t i c(I~i~rer-fncPelvceedii?gs). 24,433438.
Gelman. A,,and Rubin, D. B. (1992). "Inference from Iterative Sirnulation Using Multiple Sequences" (with discussion). Sratisticiil Screr~ce, 7,457-5 11
Ge~veke.J. (1989). "Bayesian inference in econometric models using Monte Carlo integration." Ecorlori~errica,57. 1317-1 340.
Hastings, W. K. (1970),"MonteCarloSamplingMethods UsingMarkov Chains and Their Applications," Bioii~rtrikci5, 7, 97-109.
Metropolis, N.,Rosenbluth, A. W.. Rosenbluth, M. N., Teller. A. H.. and
Teller, E. (1953). "Equations of State Calculations by Fast Computing Machines," Jollrrlol of Cherniccil Physics, 21, 1087-1 092. Meyn, S . P., and T~veedieR, . L. (1993). ~MorkoC~l,~iiirzsarid Smci~cistic StciDili/y.London: Springer-Veriag. hliiller, P. (1993). "A Generic Approach to Posterior Integration and Gibbs Sampling," manuscript. Nummelin. E. (1984). Geileral Irrrcl~iciblrA4orkov Ci~ilirlsnr~d1Yor11Negntive 011erciiors.Cambridge: Cambridge University Press. Phillips, D. B., and Smith, A. F. &I. (1994), "Bayesian Faces via Hierarchical Template Modeling," Jolirilol of tile Ari~ericniS~ioristiccil Associcirior~,89. 1151-1 163. Roberts, G. 0 . . Gelman, A,, and Gilks, LI'. R. (1994), "Weak Convergence and Opt~rnaSl caling of Random Walk bletropolis Algorithms," Technical Report, University of Cambridge. Roberts. G. O., and T~veedieR, . L. (I 994), "Geometric Convergence and Central Limit Theorems for Multidimensional Hastings and Metropolis Algorithms," Technical Report, University of Cambridge. Rubinstein. R. Y. (198 I), Sirrzrrlotioil eind file Morzte Carlo Merllod. New York: John Lt'iley. Smith, A. F. M., and Roberts, G . 0.(1993). "Bayesian Computation via the Gibbs Sampler and Related Markov Chain Monte Carlo Methods," Jolrr~rlnlof the Royal Sriiiisriceil Sociefy. Ser. B , 55, 3-24. Tanner. M. A. (1993), %ols,for. Statisriciil h1ferer1c.e(2nd ed.), New York: Springer-Verlag. Tanner, M. A,, and Wong, MI. H. (1987), "The Calculation of Posterior Distributions by Data Augmentation," .lourrlal of the Ar,!ericai~ Sroti.c.ricn1Associatioil, 82, 528-549. Tierney, L. (1994). "Marko\, Chains for Exploring Posterior Distributions" (~vlthdiscussion), Aiii~alsof Srntistics, 22. 1701-1762.

171e Ai~zerlcarzSmtlctzcrnr~P, 17over~iherj995, \hi.49, h'o 4 335

http://www.jstor.org
LINKED CITATIONS
- Page 1 of 2 -
You have printed the following article: Understanding the Metropolis-Hastings Algorithm Siddhartha Chib; Edward Greenberg The American Statistician, Vol. 49, No. 4. (Nov., 1995), pp. 327-335.
Stable URL: http://links.jstor.org/sici?sici=0003-1305%28199511%2949%3A4%3C327%3AUTMA%3E2.0.CO%3B2-2
This article references the following linked citations. If you are trying to access articles from an off-campus location, you may be required to first logon via your library web site to access JSTOR. Please visit your library's website or contact a librarian to learn about options for remote access to JSTOR.
References
Explaining the Gibbs Sampler George Casella; Edward I. George The American Statistician, Vol. 46, No. 3. (Aug., 1992), pp. 167-174.
Stable URL: http://links.jstor.org/sici?sici=0003-1305%28199208%2946%3A3%3C167%3AETGS%3E2.0.CO%3B2-R
Sampling-Based Approaches to Calculating Marginal Densities Alan E. Gelfand; Adrian F. M. Smith Journal of the American Statistical Association, Vol. 85, No. 410. (Jun., 1990), pp. 398-409.
Stable URL: http://links.jstor.org/sici?sici=0162-1459%28199006%2985%3A410%3C398%3ASATCMD%3E2.0.CO%3B2-3
Inference from Iterative Simulation Using Multiple Sequences Andrew Gelman; Donald B. Rubin Statistical Science, Vol. 7, No. 4. (Nov., 1992), pp. 457-472.
Stable URL: http://links.jstor.org/sici?sici=0883-4237%28199211%297%3A4%3C457%3AIFISUM%3E2.0.CO%3B2-Q
Bayesian Inference in Econometric Models Using Monte Carlo Integration John Geweke Econometrica, Vol. 57, No. 6. (Nov., 1989), pp. 1317-1339.
Stable URL: http://links.jstor.org/sici?sici=0012-9682%28198911%2957%3A6%3C1317%3ABIIEMU%3E2.0.CO%3B2-5

http://www.jstor.org
LINKED CITATIONS
- Page 2 of 2 -
Monte Carlo Sampling Methods Using Markov Chains and Their Applications W. K. Hastings Biometrika, Vol. 57, No. 1. (Apr., 1970), pp. 97-109.
Stable URL: http://links.jstor.org/sici?sici=0006-3444%28197004%2957%3A1%3C97%3AMCSMUM%3E2.0.CO%3B2-C
Bayesian Faces via Hierarchical Template Modeling D. B. Phillips; A. F. M. Smith Journal of the American Statistical Association, Vol. 89, No. 428. (Dec., 1994), pp. 1151-1163.
Stable URL: http://links.jstor.org/sici?sici=0162-1459%28199412%2989%3A428%3C1151%3ABFVHTM%3E2.0.CO%3B2-V
The Calculation of Posterior Distributions by Data Augmentation Martin A. Tanner; Wing Hung Wong Journal of the American Statistical Association, Vol. 82, No. 398. (Jun., 1987), pp. 528-540.
Stable URL: http://links.jstor.org/sici?sici=0162-1459%28198706%2982%3A398%3C528%3ATCOPDB%3E2.0.CO%3B2-M
Markov Chains for Exploring Posterior Distributions Luke Tierney The Annals of Statistics, Vol. 22, No. 4. (Dec., 1994), pp. 1701-1728.
Stable URL: http://links.jstor.org/sici?sici=0090-5364%28199412%2922%3A4%3C1701%3AMCFEPD%3E2.0.CO%3B2-6

