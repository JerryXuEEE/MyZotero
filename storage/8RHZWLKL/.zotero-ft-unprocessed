{"indexedPages":15,"totalPages":15,"version":"245","text":"Extracting computational mechanisms from neural data using low-rank RNNs\n\n⇤\nAdrian Valente École Normale Supérieure PSL Research University\n\nJonathan W. Pillow Princeton Neuroscience Institute\nPrinceton University\n\nSrdjan Ostojic École Normale Supérieure PSL Research University\n\nAbstract\nAn inﬂuential framework within systems neuroscience posits that neural computations can be understood in terms of low-dimensional dynamics in recurrent circuits. A number of methods have thus been developed to extract latent dynamical systems from neural recordings, but inferring models that are both predictive and interpretable remains a difﬁcult challenge. Here we propose a new method called Low-rank Inference from Neural Trajectories (LINT), based on a class of low-rank recurrent neural networks (lrRNNs) for which a link between connectivity and dynamics has been previously demonstrated. By ﬁtting such networks to trajectories of neural activity, LINT yields a mechanistic model of latent dynamics, as well as a set of axes for dimensionality reduction and veriﬁable predictions for inactivations of speciﬁc populations of neurons. Here, we ﬁrst demonstrate the consistency of our method and then apply it to two use cases: (i) we reverse-engineer “black-box” vanilla RNNs trained to perform cognitive tasks, and (ii) we infer latent dynamics and neural contributions from electrophysiological recordings of nonhuman primates performing a similar task.\n1 Introduction\nAs large-scale neural recordings in behaving animals are becoming commonplace, a pressing question is how computational principles can be extracted from the electrical activity of thousands of cells. An inﬂuential framework posits that neural computations rely on latent low-dimensional dynamics [54, 60] distributed across populations of neurons [66, 43]. In line with this proposal, a number of dataanalysis methods have been developed to infer latent dynamics from neural recordings [65, 25, 36, 32, 12, 31, 24, 33, 10, 19, 16, 40, 41, 45, 22] (reviewed in [11]). While these statistical approaches often provide compelling descriptions of the recorded data, they generally lack a mechanistic interpretation that would, for instance, allow them to make predictions for responses to novel interventions on the underlying neural circuits. How to identify mechanistic, predictive models from neural data thus currently remains an important challenge.\nRecurrent neural networks (RNNs) have emerged as key models for studying neural computations [64]. Indeed, RNNs can be trained to solve various cognitive tasks, and lead to dynamics surprisingly similar to those observed in neural recordings [29, 56, 5, 61, 39], and have also successfully been trained to reproduce the activity of neurons recorded in vivo [38, 6, 14, 35]. While potentially predictive, the obtained networks are however typically challenging to understand mechanistically [55, 28, 27, 26, 63, 57], and ongoing research directions aim at reducing them to simpliﬁed, interpretable models, for example through the use of linearized dynamical systems [12, 19, 50] or through \"network distillation\" methods [44, 22].\n⇤Correspondence to adrian.valente@ens.psl.eu. Code available at https://github.com/adrian-valente/lowrank_inference/\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\nHere, we exploit a particular class of interpretable RNNs, namely low-rank RNNs (lrRNNs) [30, 3, 9, 46, 47, 58], to develop a new method that extracts mechanistic and predictive low-dimensional models from observed neural activity, and can be applied to both artiﬁcial and biological neural networks. Previous work has performed theoretical analyses of low-rank RNNs, either designed or trained on speciﬁc tasks [30, 9], but has not introduced methods for ﬁtting them to neural activity. Our method, Low-rank Inference from Neural Trajectories (LINT) infers a minimal rank lrRNN from a dataset and then exploits the theory of low-rank networks to relate the obtained connectivity with low-dimensional dynamics and computations. It produces three outputs: a set of axes for dimensionality reduction of neural population activity, an effective connectivity that implements a latent dynamical system, and predictions for inactivations on speciﬁc subsets of recorded neurons (ﬁg. 1a).\nOur contributions can be summarized in three steps: ﬁrst, we verify the consistency of LINT by applying it to data simulated from lrRNNs, and show that it recovers the effective part of the connectivity that reproduces the dynamics and computations. Second, we apply LINT to data generated from full-rank RNNs trained on cognitive tasks, and demonstrate that the resulting lrRNNs can capture their dynamics and offer mechanistic insights into how they function. Notably, we identify a novel population-based mechanism enabling context-dependent switching in a vanilla RNN, and verify it by targeted inactivation experiments. Finally, we apply LINT to electrophysiological recordings in nonhuman primates performing a context-dependent decision making task [29]; this reveals that a rank-1 network can capture most aspects of the dynamics present in the data, and that computations in this neural circuit appear to be supported by a small proportion of all recorded neurons.\n\n2 Approach\n\nHere we describe our method for extracting an interpretable low-dimensional projection and dynamical model from neural trajectories by ﬁtting a low-rank recurrent network to data.\n\nLow-rank RNNs (lrRNNs). We start from rate-based recurrent neural networks of N units, each characterized by an activation variable xi which follows the dynamics:\n\n⌧ dxi = dt\n\nX N xi + Jij\n\n(xj ) + X Nin Ii(s)us(t) + ⌘i(t).\n\n(1)\n\nj=1\n\ns=1\n\nHere J represents the network connectivity matrix and is a nonlinear transfer function deﬁning the neural ﬁring rate (xi), which we take here to be tanh. Each network also receives Nin input signals us(t) via a set of weights I(s) that we refer to as input vectors.\nLow-rank RNNs represent a subclass of models in which the connectivity matrix is constrained to be of ﬁnite rank R ⌧ N [30, 3, 9]. In this case, J can be written as a sum of outer products of connectivity vectors n(r) and m(r):\n\nJij\n\n=\n\n1 N\n\nX R m(ir)n(jr).\n\n(2)\n\nr=1\n\nTo deﬁne a unique representation, we take as m(r) the left singular vectors of the connectivity matrix, and as n(r) the right singular vectors with an appropriate normalization.\n\nNetwork inference. We infer lrRNNs from data by training the connectivity parameters to reproduce recorded neural trajectories (either single-trial or condition-averaged trajectories). Speciﬁcally, we use back-propagation on connectivity vector parameters n(ir) and m(ir) and input parameters Ii(s) to minimize the squared difference between target and reproduced trajectories:\n\nX C X N X T\n\nL=\n\n(\n\n(x(ic)(t))\n\n(xˆ(ic)(t)))2\n\n(3)\n\nc=1 i=1 t=1\n\nwhere x(ic)(t)) represents the target trajectory in condition c, for neuron i and timestep t and xˆ(ic)(t) the corresponding trajectory produced by the model.\n\n2\n\na.\n\nb.\n\nc.\n\nNeural data Low-rank RNN\n\nr3\n\nLow-d\n\nr2\n\ndynamics\n\nInfer\n\nr1\n\nJˆij\nJˆiejf f\n\nJ = mnT\n\nE ective connectivity\n\n& mechanism\n\nJij\n\nJiejf f\n\nFigure 1: a. LINT pipeline: Low-rank RNNs are ﬁtted to either simulated or recorded neural trajectories. The obtained lrRNNs provide interpretable low-dimensional dynamics as well as\na computational mechanism based on an effective connectivity structure. b-c. LINT tested on trajectories simulated from a rank-1 lrRNN trained to perform the CDM task (see Table 1 for results across tasks). b. For each pair of neurons, relationship between the original (Jij) and inferred (Jˆij) synaptic connectivity (5122 pairs, r = 0.57). c. For the same networks, relationship between the original effective connectivity (Jiejff , see text) and the inferred one (Jˆiejff , r = 0.99)\n\nDimensionality reduction. One property of lrRNNs is that, in the absence of noise, they constrain the activity vector x(t) to evolve in a low-dimensional subspace spanned by the R connectivity vectors m(r) and the Nin input vectors I(s) [3]. The trial-averaged trajectories therefore explore at most R + Nin dimensions and can be parametrized as:\n\nX R\n\nX Nin\n\nx(t) = r(t)m(r) + vs(t)I(s),\n\n(4)\n\nr=1\n\ns=1\n\nwhere vs(t) are the low-pass ﬁltered input signals us(t) (see Appendix A), and r are a set of latent variables generated by recurrent activity. Thus, lrRNNs provide by design a reduction of\nN -dimensional neural activity to at most R + Nin dimensions that can be directly interpreted in terms of components on a recurrent subspace spanned by the connectivity vectors m(r) and on an input subspace spanned by inputs vectors I(s) [61].\n\nLatent dynamics. In lrRNNs, the latent variables r(t) form a non-linear low-dimensional dynamical system, described by:\n\nd\n\n(t) = F ((t), u(t))\n\n(5)\n\ndt\n\nwhere (t) = {r(t)}r=1..R is an R-dimensional vector representing the activity on the recurrent subspace, u(t) represents the input signals, and F is a non-linear function that can be directly determined from network connectivity parameters [3, 9] (Appendix A). Moreover, it can be shown that rank-R networks are universal approximators of R-dimensional dynamical systems [3], and thus that the function F can approximate any non-linear mapping in R dimensions.\n\nTask-optimized RNNs. We applied our method ﬁrst to trajectories produced by task-optimized RNNs, which produce an output signal z(t) from the recurrent dynamics (1) via a linear readout:\n\nX N\n\nz(t) = wi (xi(t))\n\n(6)\n\ni=1\n\nIn task-optimized networks, the parameters are trained with backpropagation through time to minimize the squared error between the output z(t) and a target z⇤(t). In this work, we consider four cognitive tasks: Decision Making (DM), Working Memory (WM), Context-Dependent Decision Making (CDM, ﬁg. 2a) and Delayed Match-to-Sample (DMS) (see Appendix B for task deﬁnitions). To analyze the computations underlying these tasks, we generated neural trajectories from both low-rank (in section 3.1) and full-rank (see section 3.2) task-optimized RNNs. All networks were implemented in pytorch [34]; training details can be found in Appendix C.\n\n3\n\nTable 1: Synthetic data validation results for LINT (CC: connectivity correlation, ECC: effective connectivity correlation)\n\nTask\n\nRank Trajectory R2 CC ECC\n\nDecision Making (DM) 1\n\n0.97\n\nWorking Memory\n\n2\n\n0.96\n\nContext-dependent DM 1\n\n0.91\n\nDelayed Match-to-Sample 2\n\n0.98\n\n0.50 0.99 0.43 0.93 0.57 0.99 0.39 0.63\n\n3 Results\n3.1 Validation with synthetic data and effective aspects of connectivity\nWe ﬁrst validated the capacity of the LINT method to recover low-rank connectivity features and reproduce neural trajectories on data simulated from lrRNNs trained on cognitive tasks. For this, we ﬁrst trained lrRNNs with 512 neurons to produce a correct behavioral output on four systems neuroscience tasks, each time retaining the minimal rank solution [9] (see Table 1). For each taskoptimized lrRNN, we then generated simulated trajectories corresponding to trials in the trained task, and applied our method to ﬁt these trajectories using equivalent models with the same rank and number of neurons. We found that the inferred networks were able to reliably reproduce the original trajectories when fed with the same inputs, as quantiﬁed by the R2 scores between original and ﬁtted trajectories. Although the inferred networks were not explicitly trained on behavioral outputs, they were able to perform their task accurately when plugged to the original readout vector, implying that they also captured behavioral aspects of the original networks.\nThe output of LINT is an inferred low-rank connectivity that we compared with the original one. We noted that the inferred connectivity weights are not identical to the original ones, although a certain degree of correlation is present (ﬁg. 1b and Table 1, column CC for connectivity correlation). However, from a theoretical point of view, a range of different connectivity matrices can lead to identical dynamics [37, 13, 3]. The low-rank framework allows for a precise characterization of the relevant part of the connectivity, and in particular allows us to deﬁne an effective connectivity matrix J eff that captures the minimal features required to obtain certain dynamics. Indeed, for this class of networks, the low-dimensional dynamics described by equations (4)-(5) depend on the exact entries on the m(r) and I(s) vectors, but not on the individual entries on the n(r) vectors. Instead, n(r) vectors inﬂuence the dynamics only through their global projections on the m(r) and I(s) vectors (see details in Appendix A). Thus, the components of n(r) orthogonal to the space spanned by m(r) and I(s) are irrelevant for the dynamics, and removing them leads to an effective connectivity matrix J eff which captures the minimal features required for obtaining particular dynamics. Our results on the synthetic data showed indeed a very high degree of similarity between original and inferred effective connectivities, demonstrating that LINT recovered the relevant aspects of the neural connectivity (Table 1, ECC for Effective Connectivity Correlation, and ﬁg. 1c).\nIn conclusion, LINT is able to accurately ﬁt neural trajectories generated by low-rank RNNs, to infer networks that give rise to similar trajectories and behavioral performance, and to recover the essential features of the connectivity.\n3.2 Application to reverse-engineering full-rank RNNs\nWe next ask to which extent our method can infer computational mechanisms from activity generated by unconstrained, full-rank networks. Indeed, RNNs trained on simple tasks without a rank constraint often exhibit low-dimensional dynamics that can be related to the way computations unfold in the network [55, 28, 27, 26, 57]. It has been found that such low-dimensional dynamics can be reproduced by low-rank networks [3], yet the best approach for inferring the corresponding low-rank connectivity remains to be determined. Here we show that our method vastly outperforms a direct low-rank approximation of the connectivity matrix based on truncating the SVD [47]. We then demonstrate how the inferred low-rank models can be used to interpret the computational mechanisms in the original full-rank networks.\n4\n\na.\n\nb.\n\nc.\n\nuctxA(t) uA(t) uB (t)\nuctxB (t)\n\ntask output\nneural trajectories\n\nd.\n\ne.\n\nf.\n\nFigure 2: LINT applied to full-rank task-optimized RNNs. a. Description of the context-dependent decision making (CDM) task inputs and outputs. Neural trajectories correspond to the neural ﬁring rates, task output refers to a linear readout as in eq. 6. b. For the CDM task, similarity between trajectories produced by original full-rank networks and a series of ﬁtted networks of increasing rank (red), or low-rank networks obtained by truncating the original connectivity matrix (grey). Each line corresponds to a randomly initialized full-rank network. c. Task accuracy of the same networks when associated with the original task readout. d-e. Same as b-c. for the delayed match-to-sample (DMS) task. f. For the CDM task, similarities between trajectories of the original and ﬁtted networks of rank one (blue) or full-rank (orange), when networks are ﬁtted only to a random subset of neurons of the original network. Error bars: mean ±1std over 10 random subsamples for each ratio value (see also sup. ﬁg. 8).\n3.2.1 Extracting low-dimensional dynamics through low-rank connectivity\nWe considered full-rank, vanilla RNNs trained on two systems neuroscience tasks, respectively context-dependent decision-making[29] (CDM) and delayed match-to-sample[5] (DMS). The fullrank RNNs reached a 100% accuracy on each task, and—as expected—exhibited low-dimensional dynamics. From these RNNs, we generated trajectories corresponding to trials in the trained tasks, and then used LINT to infer lrRNNs of increasing rank.\nIn the CDM task, a network received two signal inputs which varied randomly around a positive or negative mean, as well as two binary context cues, one of which was set to 1 in each trial. The network was trained to output the average sign of the signal indicated by the active context, while ignoring the other signal (ﬁg. 2a, see Appendix B for task details). Applying LINT, we found that rank-1 networks are sufﬁcient to accurately reproduce the trajectories of the trained full-rank networks. Increasing the rank improved the goodness-of-ﬁt only marginally (ﬁg. 2b). In contrast, low-rank networks obtained by truncating the SVD of the trained full-rank connectivity matrix required a rank of 4 or 5 to reach a comparable accuracy (ﬁg. 2b), and much higher for different hyperparameters (sup. ﬁg. 1). This shows that LINT captures a simpliﬁed connectivity structure that could not be trivially extracted from the original connectivity. Moreover, even though not explicitly trained on the behavioral outputs, the inferred rank-1 networks performed the task correctly when plugged onto the original readout, implying that they faithfully captured the task-related dynamics (ﬁg. 2c).\nFor the DMS task, we similarly found that rank-2 networks were able both to capture the dynamics of the trained full-rank network and to accurately perform the task, with no notable improvement when the rank is increased (ﬁg. 2d-e). As for the CDM task, LINT vastly outperformed direct truncation of the full-rank connectivity matrix. For both these tasks, reproducing the experiment over a larger number of full-rank RNNs, trained with diverse hyperparameters and random seeds led to similar results (sup. ﬁg. 1).\n5\n\nFinally, it is important to note that the CDM and DMS tasks are particularly prone to low-dimensional implementations, which is not necessarily the case for all computations of interest in systems neuroscience [18]. To probe the capacities of LINT with higher-dimensional tasks, we applied it to full-rank networks trained on the K-back task, where a network receives a random number P of stimulations in { 1, 1} and has to output the K-th last stimulation received, a task directly inspired from sequence working memory paradigms [62]. This task requires the implementation of an internal working memory with a capacity of at least K bits, and should thus require a rank at least K. We indeed observed that the rank identiﬁed by LINT increased with K, but only linearly (see details in appendix F and sup. ﬁg. 9). These results show that LINT is also helpful in probing the dimensionality required for more complex tasks.\nSubsampling. In experimental settings, one cannot expect to have access to all units of a network. It is thus important to assess whether we can still recover the relevant low-dimensional dynamics and information when only a handful of neurons from a network are recorded. We considered a full-rank network trained on the CDM task, and ﬁtted networks to its trajectories, either without constraining their rank or by ﬁxing it to one. Without subsampling, the inferred full-rank networks slightly outperformed rank-1 networks in reproducing the original trajectories. Yet when considering subsamples of neurons, rank-1 networks appear to be more robust than unconstrained ones, maintaining a good performance until subsampling ratios as low as 1% of original neurons (ﬁg. 2f). This experiment was reproduced on the DMS task setting, showing a similar advantage for low-rank ﬁtted networks (sup. ﬁg. 8).\n\n3.2.2 Extracting computational mechanisms from inferred low-rank connectivity\n\nLow-rank models of behavioral tasks open the door to mechanistic interpretations of the underlying dynamics [30, 9]. Here we show how LINT allows us to extract computational mechanisms from full-rank networks performing the CDM task.\n\nA ﬁrst output of LINT is a set of interpretable\n\naxes deﬁning a task-related subspace for dimensionality reduction. The inferred rank-1 model\n\nContext A\n\nContext B\n\nsignal A coh. signal B coh.\n\nof the CDM task speciﬁcally yields ﬁve axes that correspond to the two stimulus inputs, the two\n\n+\n\ncontext inputs and an internal latent variable,\n\ngenerated by recurrent connectivity, which rep-\n\nresents integrated evidence and therefore choice.\n\n-\n\nProjecting the activity of the full-rank network\n\nalong these axes shows how low-dimensional\n\n+\n\ndynamics transform inputs into the choice out-\n\nput (ﬁg. 3). In this case, the axes determined\n\nby LINT are closely related to those obtained by standard targeted dimensionality reduction\n\n-\n\n(TDR, sup. ﬁg. 2) [29]. However, in contrast\n\nwith TDR, the axes inferred by LINT corre- Figure 3: Low-dimensional projections produced\n\nspond to connectivity features causing the low- by LINT. Trial-averaged trajectories for differ-\n\ndimensional dynamics, and provide a method ent task conditions, in the original full-rank net-\n\nfor an unsupervised discovery of recurrently- work (full lines) and the inferred low-rank network\n\ngenerated latent variables. In particular, we did (dashed lines), projected on axes obtained from the\n\nnot a priori specify that the activity along m ﬁtted lrRNN connectivity: input A and B axes, and\n\nshould encode choice, but directly observed that the output axis of the rank-1 recurrent connectivity\n\nchoice was generated along that axis through m, which encodes choice. All trajectories start at\n\nrecurrent dynamics.\n\ncenter.\n\nGoing beyond dimensionality reduction, LINT extracts an effective low-dimensional model of the task. For the CDM task, the inferred model is a one-dimensional, non-linear latent dynamical system, where the latent variable integrates the relevant evidence. To understand how context-dependent selection of the evidence was performed, we analyzed the connectivity parameters of the inferred rank-1 RNN. Indeed, recent work has introduced a clustering method for analyzing low-rank connectivity, and has showed that in trained lrRNNs, contextdependent integration relies on gain-modulation mechanism mediated by two distinct populations of neurons [9]. Applying a similar clustering approach to rank-1 networks inferred by LINT led to a\n\n6\n\na.\n\nb.\n\nIictxB\n\nIictxA\n\nCtx A\n\nCtx B\n\nc.\nunperturbed No pop. A No pop. B\n\nRank-one (inferred)\n\nFull-rank (result)\n\nIictxB\nIictxA\npop. A pop. B\nFigure 4: Extracting mechanisms and testing predictions from a low-rank network (top) inferred from a full-rank network trained on the CDM task (bottom). a. Three populations of neurons (grey, green and purple) are identiﬁed by clustering connectivity parameters in the ﬁtted rank-1 network (top). Here, we plot the same populations in joint distributions of contextual input vectors in the rank-1 (top) vs full-rank (bottom) networks. The populations are not directly identiﬁable from input parameters in the original network (bottom). b. Neural transfer function (tanh) and smoothed distributions of mean neural activity for populations A and B in the inferred rank-1 network (top) and the full-rank network (bottom). c. Inactivation experiments: task performance on each context for unperturbed networks and after inactivating populations A and B in the ﬁtted rank-1 network (top) and inactivating the same neurons in the original unconstrained network (bottom). Red dots: chance level.\ncomparable mechanism. Speciﬁcally, this method identiﬁed two populations distinguished by strong values of the ﬁtted weights for the context-cue inputs (ﬁg. 4a top and sup. ﬁg. 2). In each context, these context-cue inputs place the neurons belonging to the two populations at different positions on the non-linear transform function (x) (ﬁg. 4b top), thereby modulating in opposite ways their gains (deﬁned for each neuron as the local slope 0(x)). Combined with different statistics of the connectivity parameters on each of the populations, this modulation is sufﬁcient to implement the desired context-switching behavior of the network (see details in Appendix D). The same pattern of gain modulation was found in the original full-rank network, indicating that it is using a similar mechanism (ﬁg. 4b bottom).\nThe computational mechanism extracted using LINT produces predictions for inactivations that can be directly tested in the original full-rank model. The analysis described above assigns neurons to different populations that have speciﬁc computational roles. Speciﬁcally, in the inferred rank-1 network, inactivating separately each population leads to a speciﬁc loss of performance in a single context, but not the other one (ﬁg. 4c top and sup. ﬁg. 2 for detailed error patterns). Since individual neurons in the original networks directly correspond to neurons in the rank-1 network on a oneto-one basis, the identiﬁed populations can be directly mapped back onto the original network to reproduce the inactivation experiment. The predicted effects of inactivations on context-dependent performance were directly replicated in the full-rank network, demonstrating that it relies on the identiﬁed computational mechanism (ﬁg. 4c bottom and sup. ﬁg. 2). One can note that although the two identiﬁed populations clearly stand out in the inferred rank-1 connectivity, they are not directly apparent in the original full-rank connectivity (ﬁg. 4a bottom), showing that although the mechanism extracted by LINT applies to the full-rank network, using an lrRNN was a necessary step to uncover it.\n3.3 Application to neural recordings\nWe next applied our method to in vivo recordings. We considered here electrophysiological recordings from non-human primates performing a context-dependent decision-making task similar to that studied in previous sections [29]. More speciﬁcally, two macaques (designed by A and F) were presented with random dots stimuli that varied along two dimensions: the overall motion direction of the dots, and their overall color, ranging from two extremes with a set of intermediary coherences in between. At the beginning of each trial, a cue indicated a context for the trial, ordering the\n7\n\nmotion coh.\n\na.\n\nc. Context A\n\nContext B\n\n+\n\nb.\n+\n\ncolor coh.\n\n-\nFigure 5: LINT applied to neural recordings from a non-human primate performing a contextdependent task (monkey A) [29]. a. Global R2 of model trajectories with respect to original ones, and task accuracy of inferred networks of increasing ranks. b. For the rank-1 inferred network, R2 of each recorded neuron plotted against its average data ﬁring rate, median R2 in red, with marginal box-plot of R2 values. 14 neurons with an R2 < 1 are not shown, all with a mean ﬁring rate < 2.2 (see sup. ﬁg. 4). c. Trial-averaged trajectories in the recorded data (full lines) and for the rank-1 model (dashed lines), for different task conditions (only one coherence value for each coherence sign is plotted here), projected on axes identiﬁed by LINT (motion and color input axes and the output axis m of the rank-1 recurrent matrix). Trajectories start at the center.\nsubject to report either the average motion or color with an eye saccade, while ignoring the irrelevant evidence. Non-simultaneous recordings were performed in the frontal eye ﬁeld (FEF) an area of the macaque prefrontal cortex, with respectively 727 neurons for monkey A and 574 neurons for monkey F recorded in all 72 conditions the task presented (ignoring error trials).\nFor the analysis, we started by binning (5 ms) and smoothing (50 ms std Gaussian window) spike train data, and then computed the trial-averaged response of every neuron in each of the task conditions, forming a pseudo-population tensor of size N ⇥ T ⇥ 72 with N the number of neurons in each monkey and T = 150 the number of discrete time steps. We denoised this tensor by projecting its ﬁrst axis on the subspace spanned by the top 12 principal components of the pseudo-population activity, and then projecting back to the high-dimensional space spanned by all neurons [29]. Finally, in order to consider only task-related neural activity, we subtracted from each neuron’s trajectory its condition-averaged mean. We denote the entries of the ﬁnal obtained tensor by x˜i,t,c.\nWe applied LINT to the obtained trajectories, training the activation of each unit in the low-rank RNN (noted xi,t,c) to match the corresponding pre-processed activity. Importantly, the inferred networks received inputs following the same structure than in the CDM task of previous sections, that is two noisy signal inputs and two contextual cue inputs (ﬁg. 2a), and were left unconstrained for the ﬁrst 350 ms of each trial while receiving only contextual cues, to account for the original task procedure. In particular, we made the implicit hypothesis that choice signals are generated by the recurrent activity of the network from received inputs. The quality of ﬁts was quantiﬁed by leaving out a random subset of 8 conditions during network inference, and evaluating the R2 of ﬁtted networks on these left-out conditions.\nWe found that for both monkeys the neural activity was well reproduced by a rank-1 network, with minimal improvements when the rank was increased (ﬁg. 5a and sup. ﬁg. 5, R2 = 0.66 for monkey A, and R2 = 0.57 for monkey F). Moreover, when simulated independently, the inferred rank-1 networks were able to perform the task by adding a linear readout (accuracy curves in ﬁg. 5a, sup. ﬁg. 5). This was the case even though the recurrent and input connections were not trained on task performance, demonstrating that the reproduced trajectories contained information about the choice made by the monkey. The activity of both original and reproduced trajectories can moreover be projected on the axes identiﬁed by LINT, providing a geometrical picture of task execution (ﬁg. 5c). These axes were\n8\n\nclosely related to those found by targeted dimensionality reduction: notably the context, motion, and color axes identiﬁed by TDR closely match the corresponding input axes in the inferred network, while the choice TDR axis could be identiﬁed to the m axis of the rank-1 recurrent connectivity (sup. ﬁgs. 4,5). The projections of the original activity on the connectivity axes (full lines on ﬁg. 5b) therefore showed how inferred connectivity explains the geometry of recorded data.\nA closer look at the distribution of inferred connectivity weights provided more information on the neural mechanisms at play (sup. ﬁgs. 6, 7). In contrast to the ﬁts of task-optimized networks, we observed for both monkeys a clearly non-normal, heavy-tailed distribution of inferred connectivity parameters (Fisher’s kurtosis between 4.9 and 62.7 for different connectivity parameters), echoing some past observations on biological synaptic weights [51, 4]. Separating with a clustering algorithm (GMM) large and small-weight neurons showed that the latter could be inactivated without affecting the task performance of the network in a signiﬁcant way (sup. ﬁgs. 6, 7), even though they represented a majority of neurons in the circuits (570/727 neurons in monkey A, 389/574 in monkey F). This suggests that a minority of neurons with large connectivity weights supports the computation performed by these networks.\n4 Discussion\nIn this paper, we introduced LINT, a new method for inferring a latent dynamical system from neural activity based on the theory of low-rank RNNs. This method yields a mechanistic, interpretable and predictive model of neural trajectories, and bridges different levels of analysis, from state-space geometry to neural connectivity. After verifying the consistency of our method, we demonstrated its potential to extract the mechanisms used by \"black-box\" vanilla RNNs trained to perform cognitive tasks. In particular, we found that low-rank RNNs reproduced well the dynamics of full-rank RNNs trained on a variety of tasks and across a range of hyperparameters. Moreover, the obtained lrRNNs provided interpretable low-dimensional projections of the ﬁtted activity, as well as predictions for the effects of inactivations of speciﬁc populations of neurons. These predictions were veriﬁed in the original network, validating computational mechanisms derived from inferred low-rank connectivity weights. Finally, we applied LINT to neural activity recorded in the prefrontal cortex of nonhuman primates during a context-dependent decision-making task. LINT was able to infer rank-1 networks that reproduced both neural activity and task performance, from which low-dimensional projections and interpretable connectivity could be extracted.\nOur results pave the way for a better understanding of how RNNs perform computations. In particular, LINT is complementary to other approaches seeking to reverse-engineer RNNs [55, 29, 50] in that it infers an effective connectivity that links low-dimensional dynamics with interactions at the single neuron level, opening possibilities for goal-oriented interventions and a better control on the behavior of trained networks. Moreover, LINT leads to compressed, distilled versions of large networks that can be of practical interest to engineers, as suggested by recent research in low-rank training of deep neural networks [20]. Extensions to reverse-engineering more complex architectures like GRUs or LSTMs, which have already been shown to rely on low-dimensional trajectories [28, 26, 21] would be an interesting direction for future research.\nIt is worth noting that other statistical methods such as mTDR [1] can provide a better ﬁt to the same data, but do not identify a dynamical system that generates the trajectories. LINT is related to switching latent dynamical systems [10, 50, 24, 19, 12, 58] in that it bridges descriptive latentspace analyses like mTDR with methods that identify a predictive model of the dynamics, such as LFADS [33]. A distinctive feature of LINT is however that (unlike LFADS) it retains a mechanistic description at the level of neural connectivity of how the dynamics emerge from interactions between neurons.\nAs a number of other data analysis methods for neuroscience, our approach relies on the ubiquitous observation that neural dynamics appear to be conﬁned to low-dimensional manifolds within the activity state-space [7, 17, 43]. Recent works have proposed that this low-dimensionality might be an artifact stemming from the simplicity of the examined tasks [18], or have reported higher-dimensional activity patterns [53, 23, 59]. This raises the question of how far our method could account for activity in more complex tasks. Our results on the K-back task suggest that LINT scales well with task dimensionality, and hint to a possible compositional implementation of internal computations\n9\n\nthat could be unpacked by breaking them into simpler primitives [63, 8]. Further confronting our and related methods to higher-dimensional task designs however remains an important endeavour. Distinguishing signals generated by dynamics within a given area from external inputs it receives is a major challenge for any interpretation of neural recordings, and a focus of recent work [16, 45, 42]. Most methods for inferring latent dynamics from activity data rely on ad hoc assumptions about the inputs received by a circuit. Following previous work [29], when analyzing recordings from the prefrontal cortex we hypothesized that choice was recurrently generated within the recorded area. LINT however potentially provides a new approach for testing hypotheses on input structure, by comparing models ﬁts obtained from single-trial recordings, or by building multi-area models [2] from large-scale recordings that are increasingly available nowadays [49, 48, 52, 35].\nAcknowledgments and Disclosure of Funding\nThe authors would like to thank Valerio Mante for kindly providing access to the monkey data, and João Barbosa for relevant discussions. AV and SO were supported by the NIH Brain Initiative project U01-NS122123 and the program “Ecoles Universitaires de Recherche” launched by the French Government and implemented by the ANR, with the reference ANR-17-EURE-0017. JWP was supported by grants from the Simons Collaboration on the Global Brain (SCGB AWD543027), the NIH BRAIN initiative (R01EB026946), and a U19 NIH-NINDS BRAIN Initiative Award (5U19NS104648).\n10\n\nReferences\n[1] Mikio C Aoi, Valerio Mante, and Jonathan W Pillow. Prefrontal cortex exhibits multidimensional dynamic encoding during decision-making. Nature neuroscience, 23(11):1410–1420, 2020.\n[2] Joao Barbosa, Remi Proville, Chris C Rodgers, Srdjan Ostojic, and Yves Boubenec. Flexible selection of task-relevant features through across-area population gating. bioRxiv, 2022.\n[3] Manuel Beiran, Alexis Dubreuil, Adrian Valente, Francesca Mastrogiuseppe, and Srdjan Ostojic. Shaping dynamics with multiple populations in low-rank recurrent networks. Neural computation, 33(6):1572–1615, 2021.\n[4] György Buzsáki and Kenji Mizuseki. The log-dynamic brain: how skewed distributions affect network operations. Nature Reviews Neuroscience, 15(4):264–278, 2014.\n[5] Warasinee Chaisangmongkon, Sruthi K Swaminathan, David J Freedman, and Xiao-Jing Wang. Computing by robust transience: how the fronto-parietal network performs sequential, categorybased decisions. Neuron, 93(6):1504–1517, 2017.\n[6] Zach Cohen, Brian DePasquale, Mikio C Aoi, and Jonathan W Pillow. Recurrent dynamics of prefrontal cortex during context-dependent decision-making. bioRxiv, 2020.\n[7] John P Cunningham and M Yu Byron. Dimensionality reduction for large-scale neural recordings. Nature neuroscience, 17(11):1500–1509, 2014.\n[8] Laura Driscoll, Krishna Shenoy, and David Sussillo. Flexible multitask computation in recurrent networks utilizes shared dynamical motifs. bioRxiv, 2022.\n[9] Alexis Dubreuil, Adrian Valente, Manuel Beiran, Francesca Mastrogiuseppe, and Srdjan Ostojic. The role of population structure in computations through neural dynamics. Nature Neuroscience, 25:783–794, 2022.\n[10] Lea Duncker, Gergo Bohner, Julien Boussard, and Maneesh Sahani. Learning interpretable continuous-time models of latent stochastic dynamical systems. In International Conference on Machine Learning, pages 1726–1734. PMLR, 2019.\n[11] Lea Duncker and Maneesh Sahani. Dynamics on the manifold: Identifying computational dynamical activity from neural population recordings. Current opinion in neurobiology, 70:163– 170, 2021.\n[12] Daniel Durstewitz. A state space approach for piecewise-linear recurrent neural networks for identifying computational dynamics from neural measurements. PLoS computational biology, 13(6):e1005542, 2017.\n[13] Chris Eliasmith and Charles H Anderson. Neural engineering: Computation, representation, and dynamics in neurobiological systems. MIT press, 2003.\n[14] Arseny Finkelstein, Lorenzo Fontolan, Michael N Economo, Nuo Li, Sandro Romani, and Karel Svoboda. Attractor dynamics gate cortical information ﬂow during decision-making. Nature Neuroscience, 24(6):843–850, 2021.\n[15] Timo Flesch, Keno Juechems, Tsvetomira Dumbalska, Andrew Saxe, and Christopher Summerﬁeld. Orthogonal representations for robust context-dependent task performance in brains and neural networks. Neuron, 2022.\n[16] Aniruddh R Galgali, Maneesh Sahani, and Valerio Mante. Residual dynamics resolves recurrent contributions to neural computation. bioRxiv, 2021.\n[17] Juan A Gallego, Matthew G Perich, Lee E Miller, and Sara A Solla. Neural manifolds for the control of movement. Neuron, 94(5):978–984, 2017.\n[18] Peiran Gao, Eric Trautmann, Byron Yu, Gopal Santhanam, Stephen Ryu, Krishna Shenoy, and Surya Ganguli. A theory of multineuronal dimensionality, dynamics and measurement. BioRxiv, page 214262, 2017.\n11\n\n[19] Joshua Glaser, Matthew Whiteway, John P Cunningham, Liam Paninski, and Scott Linderman. Recurrent switching dynamical systems models for multiple interacting neural populations. Advances in neural information processing systems, 33:14867–14878, 2020.\n[20] Siddhartha Rao Kamalakara, Acyr Locatelli, Bharat Venkitesh, Jimmy Ba, Yarin Gal, and Aidan N Gomez. Exploring low rank training of deep neural networks. arXiv preprint arXiv:2209.13569, 2022.\n[21] Kamesh Krishnamurthy, Tankut Can, and David J Schwab. Theory of gating in recurrent neural networks. Physical Review X, 12(1):011011, 2022.\n[22] Christopher Langdon and Tatiana A Engel. Latent circuit inference from heterogeneous neural responses during cognitive tasks. bioRxiv, 2022.\n[23] Frederic Lanore, N Alex Cayco-Gajic, Harsha Gurnani, Diccon Coyle, and R Angus Silver. Cerebellar granule cell axons support high-dimensional representations. Nature Neuroscience, 24(8):1142–1150, 2021.\n[24] Scott Linderman, Matthew Johnson, Andrew Miller, Ryan Adams, David Blei, and Liam Paninski. Bayesian learning and inference in recurrent switching linear dynamical systems. In Artiﬁcial Intelligence and Statistics, pages 914–922. PMLR, 2017.\n[25] Jakob H Macke, Lars Buesing, John P Cunningham, Byron M Yu, Krishna V Shenoy, and Maneesh Sahani. Empirical models of spiking in neural populations. Advances in neural information processing systems, 24, 2011.\n[26] Niru Maheswaranathan and David Sussillo. How recurrent networks implement contextual processing in sentiment analysis. In Proceedings of the 37th International Conference on Machine Learning, pages 6608–6619, 2020.\n[27] Niru Maheswaranathan, Alex Williams, Matthew Golub, Surya Ganguli, and David Sussillo. Reverse engineering recurrent networks for sentiment classiﬁcation reveals line attractor dynamics. Advances in neural information processing systems, 32, 2019.\n[28] Niru Maheswaranathan, Alex Williams, Matthew Golub, Surya Ganguli, and David Sussillo. Universality and individuality in neural dynamics across large populations of recurrent networks. Advances in neural information processing systems, 32, 2019.\n[29] Valerio Mante, David Sussillo, Krishna V Shenoy, and William T Newsome. Context-dependent computation by recurrent dynamics in prefrontal cortex. Nature, 503(7474):78–84, 2013.\n[30] Francesca Mastrogiuseppe and Srdjan Ostojic. Linking connectivity, dynamics, and computations in low-rank recurrent neural networks. Neuron, 99(3):609–623, 2018.\n[31] Marcel Nonnenmacher, Srinivas C Turaga, and Jakob H Macke. Extracting low-dimensional dynamics from multiple large-scale neural population recordings by learning to predict correlations. Advances in Neural Information Processing Systems, 30, 2017.\n[32] Marius Pachitariu, Biljana Petreska, and Maneesh Sahani. Recurrent linear models of simultaneously-recorded neural populations. Advances in neural information processing systems, 26, 2013.\n[33] Chethan Pandarinath, Daniel J O’Shea, Jasmine Collins, Rafal Jozefowicz, Sergey D Stavisky, Jonathan C Kao, Eric M Trautmann, Matthew T Kaufman, Stephen I Ryu, Leigh R Hochberg, et al. Inferring single-trial neural population dynamics using sequential auto-encoders. Nature methods, 15(10):805–815, 2018.\n[34] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\n[35] Matthew G Perich, Charlotte Arlt, Soﬁa Soares, Megan E Young, Clayton P Mosher, Juri Minxha, Eugene Carter, Ueli Rutishauser, Peter H Rudebeck, Christopher D Harvey, et al. Inferring brain-wide interactions using data-constrained recurrent neural network models. bioRxiv, pages 2020–12, 2021.\n12\n\n[36] Biljana Petreska, Byron M Yu, John P Cunningham, Gopal Santhanam, Stephen Ryu, Krishna V Shenoy, and Maneesh Sahani. Dynamical segmentation of single trials from population neural data. Advances in neural information processing systems, 24, 2011.\n[37] Eli Pollock and Mehrdad Jazayeri. Engineering recurrent neural networks from task-relevant manifolds and dynamics. PLoS computational biology, 16(8):e1008128, 2020.\n[38] Kanaka Rajan, Christopher D Harvey, and David W Tank. Recurrent network models of sequence generation and memory. Neuron, 90(1):128–142, 2016.\n[39] Evan D Remington, Devika Narain, Eghbal A Hosseini, and Mehrdad Jazayeri. Flexible sensorimotor computations through rapid reconﬁguration of cortical dynamics. Neuron, 98(5):1005– 1019, 2018.\n[40] Omid G Sani, Hamidreza Abbaspourazad, Yan T Wong, Bijan Pesaran, and Maryam M Shanechi. Modeling behaviorally relevant neural dynamics enabled by preferential subspace identiﬁcation. Nature Neuroscience, 24(1):140–149, 2021.\n[41] Omid G Sani, Bijan Pesaran, and Maryam M Shanechi. Where is all the nonlinearity: ﬂexible nonlinear modeling of behaviorally relevant neural dynamics using recurrent neural networks. bioRxiv, 2021.\n[42] Britton A Sauerbrei, Jian-Zhong Guo, Jeremy D Cohen, Matteo Mischiati, Wendy Guo, Mayank Kabra, Nakul Verma, Brett Mensh, Kristin Branson, and Adam W Hantman. Cortical pattern generation during dexterous movement is input-driven. Nature, 577(7790):386–391, 2020.\n[43] Shreya Saxena and John P Cunningham. Towards the neural population doctrine. Current opinion in neurobiology, 55:103–111, 2019.\n[44] Rylan Schaeffer, Mikail Khona, Leenoy Meshulam, Ila Fiete, et al. Reverse-engineering recurrent neural network solutions to a hierarchical inference task for mice. Advances in Neural Information Processing Systems, 33:4584–4596, 2020.\n[45] Marine Schimel, Ta-Chu Kao, Kristopher T Jensen, and Guillaume Hennequin. ilqr-vae: control-based learning of input-driven dynamics with applications to neural data. bioRxiv, 2021.\n[46] Friedrich Schuessler, Alexis Dubreuil, Francesca Mastrogiuseppe, Srdjan Ostojic, and Omri Barak. Dynamics of random recurrent networks with correlated low-rank structure. Physical Review Research, 2(1):013111, 2020.\n[47] Friedrich Schuessler, Francesca Mastrogiuseppe, Alexis Dubreuil, Srdjan Ostojic, and Omri Barak. The interplay between randomness and structure during learning in rnns. Advances in neural information processing systems, 33:13352–13362, 2020.\n[48] João D Semedo, Amin Zandvakili, Christian K Machens, M Yu Byron, and Adam Kohn. Cortical areas interact through a communication subspace. Neuron, 102(1):249–259, 2019.\n[49] Markus Siegel, Timothy J Buschman, and Earl K Miller. Cortical information ﬂow during ﬂexible sensorimotor decisions. Science, 348(6241):1352–1355, 2015.\n[50] Jimmy Smith, Scott Linderman, and David Sussillo. Reverse engineering recurrent neural networks with jacobian switching linear dynamical systems. Advances in Neural Information Processing Systems, 34, 2021.\n[51] Sen Song, Per Jesper Sjöström, Markus Reigl, Sacha Nelson, and Dmitri B Chklovskii. Highly nonrandom features of synaptic connectivity in local cortical circuits. PLoS biology, 3(3):e68, 2005.\n[52] Nicholas A Steinmetz, Peter Zatka-Haas, Matteo Carandini, and Kenneth D Harris. Distributed coding of choice, action and engagement across the mouse brain. Nature, 576(7786):266–273, 2019.\n[53] Carsen Stringer, Marius Pachitariu, Nicholas Steinmetz, Matteo Carandini, and Kenneth D Harris. High-dimensional geometry of population responses in visual cortex. Nature, 571(7765):361– 365, 2019.\n13\n\n[54] David Sussillo. Neural circuits as computational dynamical systems. Current opinion in neurobiology, 25:156–163, 2014.\n[55] David Sussillo and Omri Barak. Opening the black box: low-dimensional dynamics in highdimensional recurrent neural networks. Neural computation, 25(3):626–649, 2013.\n[56] David Sussillo, Mark M Churchland, Matthew T Kaufman, and Krishna V Shenoy. A neural network that ﬁnds a naturalistic solution for the production of muscle activity. Nature neuroscience, 18(7):1025–1033, 2015.\n[57] Elia Turner, Kabir V Dabholkar, and Omri Barak. Charting and navigating the space of solutions for recurrent neural networks. Advances in Neural Information Processing Systems, 34:25320–25333, 2021.\n[58] Adrian Valente, Srdjan Ostojic, and Jonathan W Pillow. Probing the relationship between latent linear dynamical systems and low-rank recurrent neural network models. Neural computation, 34(9):1871–1892, 2022.\n[59] Ivan Voitov and Thomas D Mrsic-Flogel. Cortical feedback loops bind distributed representations of working memory. Nature, 608(7922):381–389, 2022.\n[60] Saurabh Vyas, Matthew D Golub, David Sussillo, and Krishna V Shenoy. Computation through neural population dynamics. Annual Review of Neuroscience, 43:249–275, 2020.\n[61] Jing Wang, Devika Narain, Eghbal A Hosseini, and Mehrdad Jazayeri. Flexible timing by temporal scaling of cortical responses. Nature neuroscience, 21(1):102–110, 2018.\n[62] Yang Xie, Peiyao Hu, Junru Li, Jingwen Chen, Weibin Song, Xiao-Jing Wang, Tianming Yang, Stanislas Dehaene, Shiming Tang, Bin Min, et al. Geometry of sequence working memory in macaque prefrontal cortex. Science, 375(6581):632–639, 2022.\n[63] Guangyu Robert Yang, Madhura R Joglekar, H Francis Song, William T Newsome, and XiaoJing Wang. Task representations in neural networks trained to perform many cognitive tasks. Nature neuroscience, 22(2):297–306, 2019.\n[64] Guangyu Robert Yang and Xiao-Jing Wang. Artiﬁcial neural networks for neuroscientists: A primer. Neuron, 107(6):1048–1070, 2020.\n[65] Byron M Yu, Afsheen Afshar, Gopal Santhanam, Stephen Ryu, Krishna V Shenoy, and Maneesh Sahani. Extracting dynamical structure embedded in neural activity. Advances in neural information processing systems, 18, 2005.\n[66] Rafael Yuste. From the neuron doctrine to neural networks. Nature reviews neuroscience, 16(8):487–497, 2015.\n14\n\nChecklist\n1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] The three contributions exposed in the introduction are detailed in the results section. The speciﬁc scope is detailed in the introduction and the discussion. (b) Did you describe the limitations of your work? [Yes] Limitations are exposed in the discussion, notably with respect to hypotheses made about the inputs. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See discussion. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\n2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] No novel theoretical results are exposed here, the theoretical results on low-rank RNNs already exist in the literature, notably in [3] (b) Did you include complete proofs of all theoretical results? [N/A] Same answer.\n3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [No] Code will be made available upon publication. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] In the supplemental material, Appendix C (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] Figure 2 contains whole distributions for the ﬁtting experiments, standard deviation for the subsampling experiment, and the material exposed in section 3.3 and ﬁgure 5 has been fully reproduced on data from a second macaque subject. Moreover, additional hyperparameters are explored in sup. ﬁg. 1. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] In the supplemental material, Appendix C\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] Providers of the dataset [29] cited in section 3.3 (b) Did you mention the license of the assets? [Yes] (c) Did you include any new assets either in the supplemental material or as a URL? [No] Trained network weights will be provided upon publication. (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] No human participants. (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] No human participants not offensive content.\n5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] No human participants. (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] No human participants. (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] No human participants.\n15\n\n"}