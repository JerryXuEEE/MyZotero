Sloppy Systems
Ka Man (Ambrose) Yim
St Johnâ€™s College University of Oxford Dissertation submitted for Honour School of Mathematical and Theoretical Physics Part C
Trinity 2017

Acknowledgements
It has been an illuminating experience to work on this fascinating project with Professor Ard Louis and Chico Camargo and I am most grateful to them for their advice and guidance. I am also indebted to my friends Joe Kidson, Matias Janvin and Alex Thorne for their counsel and support. None of my work would have been possible if not for the unwavering love of my family.

Abstract
A sloppy system is one which displays a logarithmic hierarchy of sensitivity to certain parameter combinations. To study the eï¬€ect of sloppiness on evolution in biological systems driven by random mutations in parameter space, measures of evolvability and robustness are derived by seeking inspiration from analogous measures in discrete genotype-phenotype maps. Under this set of deï¬nitions, sloppiness is found to decrease state evolvability. However, it is not a suï¬ƒcient condition for determining state robustness, parameter evolvability and parameter robustness.

Contents

1 Introduction

1

1.1 Information Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . 1

1.2 Parameter Sensitivity . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

1.3 Sloppiness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

1.4 Evolvability and Robustness . . . . . . . . . . . . . . . . . . . . . . . 5

1.5 The Null Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

2 Evolvability and Robustness in Discrete Genotype-Phenotype Maps 7 2.1 Evolvability and Robustness of Genotypes . . . . . . . . . . . . . . . 8 2.2 Evolvability and Robustness of Phenotypes . . . . . . . . . . . . . . . 9 2.3 Sloppiness and Neutral Spaces . . . . . . . . . . . . . . . . . . . . . . 10

3 Robustness and Evolvability of Parameters

12

3.1 Parameter Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . 12

3.2 Parameter evolvability . . . . . . . . . . . . . . . . . . . . . . . . . . 15

4 Robustness and Evolvability of States

18

4.1 States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

4.2 State Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

4.3 State Evolvability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

4.4 Globally Non-identiï¬able but Locally Identiï¬able Models . . . . . . . 22

5 Sloppiness, Robustness and Evolvability

24

6 Summary and Outlook

26

Bibliography

28

i

List of Figures
1.1 Illustration of the relationship between parameter space Î˜, a 3 dimensional phase space and the space of observables Z. â„¦ maps parameters to trajectories F (t, Î¸) in phase space. Ï• samples the trajectory at times (t1, t2, t3) and maps the position of F at those times to a space of observables Z. This deï¬nes a composite map f = Ï• â—¦ â„¦. . . . . . 2
1.2 Figure from â€˜Universally sloppy parameter sensitivities in systems biology modelsâ€™ by Gutenkunst et al. [5]. A shows a surface of constant Ï‡2 (Î´s2) projected onto parameter space Î˜; such surfaces are ellipsoids whose semi-major axes are characterised by (Î»i)âˆ’1/2. B shows the eigenvalue spectra of various systems biology models describing cell cycles, mitosis, circadian rhythm, growth-factor signaling, regulatory networks, metabolism and etc. In C, Ii/Pi is the ratio between the intersection of the Ï‡2 ellipsoid with the parameter axes of Î¸i and the projection of the ellipsoid onto the axes. The smaller the ratios, the smaller the alignment between the bare parameter axes and the eigenvectors of the Hessian. . . . . . . . . . . . . . . . . . . . . . . . 5
2.1 Representation of a genotype-phenotype map in genotype space. Genotype A has very few neighbours of the same kind, yet its neighbourhood contains a diverse set of alternative phenotypes. Most of the neighbours of genotype B have the same phenotype, leaving little room for a diverse set of alternative phenotypes in its neighbourhood. Observe how the mapping from genotype to phenotype is many-to-one. The most frequent phenotype, the circle, forms a large neutral network that extends across parameter space, allowing it to mutate into triangles and squares. The triangle and square which are less frequent can only mutate into the circle, but not into each other. . . . . . . . . . . . . 8
3.1 (a) Projection of -tolerance sphere from M onto Î˜. Parameter robustness (by volume subtraction) can be understood as the fraction of perturbations |Î´Î¸| < âˆ† that satisfy Î´s < , indicated by the volume shaded dark grey. (b) Projection of âˆ†-perturbation sphere from Î˜ onto M. Evolvability is the average distance between two points A and B on the surface |Î´Î¸| = âˆ† in M. . . . . . . . . . . . . . . . . . . . . . 13
ii

4.1 (a): State Î¨ in M and its corresponding projection Î¨Ëœ in Î˜. Î˜ and M is tiled by Î¨ and Î¨Ëœ respectively. (b) Left: diï¬€usion of mutating agents out of Î¨Ëœ in a model with parameter space anisotropy; Right: diï¬€usion out of Î¨Ëœ âˆ— in the null model. Observe how mutating agents leave each side of Î¨Ëœ âˆ— with equal probability, yet they are biased towards leaving the longer sides of Î¨Ëœ . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
iii

Chapter 1
Introduction
Models in system biology describe the complex biochemical processes in living things. Very often, these complex nonlinear models display a property known as â€˜sloppinessâ€™, which may be a general property of a very wide class of systems described by many parameters [5]. A sloppy system is one that displays a logarithmic hierarchy of sensitivity to certain speciï¬c combinations of parameters [2] [8] [9]. The phenomenon of sloppiness is best understood in the context of â€˜information geometryâ€™ which formalises the relationship between parameters and the observable states of the system.
1.1 Information Geometry
Models of dynamical systems describe how a collection of quantities changes over time. In systems biology, the time evolution of a set of quantities y = (y1, . . . , ym) is often modelled by a set of coupled nonlinear ordinary diï¬€erential equations (ODEs) involving a set of parameters Î¸ = (Î¸1, . . . , Î¸D). Given a set of parameters Î¸, the model predicts the evolution of the system y(t) = F (t, Î¸) = (F1(t, Î¸), . . . , Fm(t, Î¸)) (see ï¬gure 1.1). This deï¬nes a trajectory parametrised by t in phase space Î“, the m-dimensional space containing all possible values of y. In an abstract sense, a model maps parameters from parameter space Î˜ âŠ† RD to trajectories in phase space: â„¦ : Î˜ â†’ Î“. Yet in reality it is uncommon that the evolution of the system is observed at all times. Experimentalists often sample a trajectory at discrete points in time (t1, . . . , tn). Thus the behaviour observed by an experimentalist is the set of sampled positions on the trajectory Ï•(F ) = (F (t1), . . . , F (tn)). The act of observation, Ï• : Î“ â†’ Z, maps the trajectories to an N = n Ã— m dimensional space of observables Z âŠ† RN . This process deï¬nes a composite map f = Ï• â—¦ â„¦ : Î˜ â†’ Z. The image of f in Z, i.e. the collection of all possible observations that are described by the model, is called the â€˜model manifoldâ€™ M [9]. Since Î˜ âŠ† RD and Z âŠ† RN are both diï¬€erentiable manifolds, assuming N > D and f is a smooth (inï¬nitely diï¬€erentiable) map, then f is an immersion of Î˜ in Z; if f is also injective, f is an embedding of Î˜ in Z. Thus M can be thought of as a manifold parametrised by coordinates Î¸ âˆˆ Î˜.
A natural object to consider on manifolds is its metric which encodes its geometry. The metrics of Z and Î˜ are often assumed to be Euclidean [4]. Since M is immersed
1

Parameter Space
Î˜
Î¸

Space of Observables
Z M = Im( f )
f f (Î¸)

Î©

ğœ‘

Phase space

Î“

F (t, Î¸)

ğœ‘(F) = ( F (t1) , F (t2) , F (t3) )

t

t1

t3

t2

Figure 1.1: Illustration of the relationship between parameter space Î˜, a 3 dimensional phase space and the space of observables Z. â„¦ maps parameters to trajectories F (t, Î¸) in phase space. Ï• samples the trajectory at times (t1, t2, t3) and maps the position of F at those times to a space of observables Z. This deï¬nes a composite map f = Ï• â—¦ â„¦.

by f in a Euclidean space Z , the metric induced on M is

âˆ‚fa âˆ‚fb gÂµÎ½ = Î´ab âˆ‚Î¸Âµ âˆ‚Î¸Î½

(1.1)

Thus the inï¬nitesimal distance in M due to an inï¬nitesimal diï¬€erence in parameter

coordinates dÎ¸ is

ds2

=

gÂµÎ½ dÎ¸ÂµdÎ¸Î½

=

âˆ‚fa Î´ab âˆ‚Î¸Âµ

âˆ‚ âˆ‚

fb Î¸Î½

dÎ¸Âµ

dÎ¸Î½

(1.2)

2

Unlike Î˜, the distance in M is not Euclidean. The geometry of the model manifold is dependent on the â„¦ which maps parameters to trajectories in phase space; and Ï•, the experimental design which samples the trajectory for observable behaviours.
Remark 1.1. The current study limits its scope to models that are at least locally identiï¬able [2] [4]. A globally identiï¬able model f is one that is injective everywhere on the domain Î˜. An example of a globally non-identiï¬able model is y(t) = F (t, Î¸1, Î¸2) = eâˆ’t/Î¸1 + eâˆ’t/Î¸2, since exchanging the values of (Î¸1, Î¸2) maps to the same trajectory in phase space. Though F (t, Î¸1, Î¸2) is not globally identiï¬able, it is locally identiï¬able, in the sense that for almost any point in Î¸ = (Î¸1, Î¸2) âˆˆ Î˜ there exists a neighbourhood U (Î¸) such that f = Ï• â—¦ â„¦ is injective in the domain U (Î¸).
Remark 1.2. For many models described by ODEs (such as those in systems biology), an exact solution for phase space trajectories cannot be obtained analytically. However, the ODEs can be solved numerically for each parameter; by sampling across parameter space a â€˜numerical model manifoldâ€™ can be constructed [9] [4].

1.2 Parameter Sensitivity

Given a variation in parameter Î¸ â†’ Î¸ + Î´Î¸, what is the magnitude of the change in behaviour? For any suï¬ƒciently small Î´Î¸, this can be well approximated by (1.2):

Î´s2 â‰ˆ gÂµÎ½(Î¸)Î´Î¸ÂµÎ´Î¸Î½

(1.3)

Since gÂµÎ½ is a symmetric matrix, it has real eigenvalues {Î»a} and a basis of orthonor-

mal eigenvectors {nË† Î±}; moreover since ds2 â‰¥ 0 âˆ€dÎ¸, gÂµÎ½ is positive (semi-)deï¬nite

and its eigenvalues are non-negative. Because {nË† Î±} is a basis of Î˜, any Î´Î¸ can be

written as

Î´Î¸ = Î´Î¸ Î±nË† Î± i.e. Î´Î¸ = P Î´Î¸

(1.4)

where the columns of P are nË† Î±. Recognising that (P T P )ij = nË† i Â· nË† j = Î´ij, the coordinate transformation (1.4) between Î´Î¸ = (Î´Î¸1, . . . , Î´Î¸D) and Î´Î¸ = (Î´Î¸ 1, . . . , Î´Î¸ D)
preserves the norm of Î´Î¸ in Î˜:

Î´Î¸ T Î´Î¸ = Î´Î¸T P P T Î´Î¸ = Î´Î¸T Î´Î¸

(1.5)

By spectrally decomposing the metric into g = P Î›P T (where Î› = diag(Î»1, . . . , Î»D))

Î´s2 = Î´Î¸T P Î›P T Î¸ = Î´Î¸ T Î›Î´Î¸ = Î»1(Î´Î¸ 1)2 + Â· Â· Â· + Î»D(Î´Î¸ D)2

(1.6)

One can immediately read oï¬€ the entries of the metric in the eigen-coordinates:

gÂµÎ½ = Î»ÂµÎ´ÂµÎ½

(1.7)

âˆš

This admits a geometric interpretation of the eigenvalues: Î»Î± it is the change

in behaviour (distance on M) per unit distance of parameter variation Î´Î¸ Î± in the

direction nË† Î± in Î˜. The eigenvalue spectrum of the metric characterises the local

sensitivity of the model behaviour to variations in parameter space. If {Î»Î±} are

3

all equal and constant everywhere, then M is ï¬‚at; if {Î»Î±} changes from Î¸ to Î¸, M is â€˜inhomogeneousâ€™ with respect to Î˜; if {Î»Î±} takes a hierarchy of values, M is â€˜anisotropicâ€™ with respect to Î˜.
Remark 1.3. When data analysts ï¬t models to data, they often quantify the goodness of ï¬t with the â€˜cost functionâ€™ or Ï‡2, which is the Euclidean distance in Z between the model prediction f (Î¸) and data zËœ . The best ï¬t parameters Î¸âˆ— are those that minimise Ï‡2. Practitioners often characterise the parameter sensitivity of the model at Î¸âˆ— using the Hessian of Ï‡2, which is mathematically identical to gÂµÎ½(Î¸âˆ—) [9].
1.3 Sloppiness
In systems biology models, the dimension of parameter space is often quite large. For example, a model for the growth-factor-signaling network in PC12 cells is dependent on 48 parameters [1]. Due to the complexity of such systems, it is diï¬ƒcult a priori to identify the relevant mechanisms that play a dominant role and distinguish them from the irrelevant mechanisms. To identify the relevant mechanisms, a complex model that incorporates a large variety of mechanisms into account are ï¬tted to data [11]. If a mechanism is relevant, then the system should be sensitive to the parameters which tune the mechanism. For example, if a relevant mechanism of a model is tuned by parameters (Î¸1, Î¸2, Î¸3), Î´Î¸ in the subspace of (Î¸1, Î¸2, Î¸3) should induce a larger Î´s2 in M, compared to variations of parameters which characterise mechanisms of lesser relevance.
However, it is rather uncommon in systems biology models that mechanisms can be distinctively classiï¬ed as relevant or irrelevant. In their study of PC12 cells Brown et al. found that the system shows great sensitivity to all 48 parameters, suggesting that all mechanisms engage a concerted eï¬€ort in inï¬‚uencing the gross dynamics of the system [1]. In a study of 17 cell cycle models, Gutenkunst et al. found that very few parameters induce a behavioural response of great signiï¬cance or insigniï¬cance [5] (see ï¬gure 1.2). Given the complex nonlinear coupling between mechanisms in systems biology models, it should not be surprising that contributions from individual mechanisms to the overall dynamics of the system cannot be separated from each other.
Yet this is not to say parameter space Î˜ is isotropic. In all of the cell cycles investigated by Gutenkunst et al., the eigenvalues of the Hessian of these models are found to be approximately evenly spaced in their logarithms, the smallest eigenvalue being several orders of magnitudes smaller than the leading eigenvalue [5] (see ï¬gure 1.2). This phenomenon is known as â€˜sloppinessâ€™ [2] [8] [9]. Such a â€˜sloppy systemâ€™ exhibits a hierarchy of sensitivity to perturbations in the eigen-directions of the metric {nË† Î±}. Note that sloppiness makes no statement about the absolute size of the eigenvalues, only their relative magnitudes. Why sloppiness occurs is still a matter of active research. While studies show that experimental design Ï• could play a role in increasing the separation between eigenvalues [2] [8], the fact that sloppiness has only emerged in some nonlinear models suggests that these models possess certain intrinsic properties which provide the necessary conditions for sloppiness [5].
4

Figure 1.2: Figure from â€˜Universally sloppy parameter sensitivities in systems biology modelsâ€™ by Gutenkunst et al. [5]. A shows a surface of constant Ï‡2 (Î´s2) projected onto parameter space Î˜; such surfaces are ellipsoids whose semi-major axes are characterised by (Î»i)âˆ’1/2. B shows the eigenvalue spectra of various systems biology models describing cell cycles, mitosis, circadian rhythm, growth-factor signaling, regulatory networks, metabolism and etc. In C, Ii/Pi is the ratio between the intersection of the Ï‡2 ellipsoid with the parameter axes of Î¸i and the projection of the ellipsoid onto the axes. The smaller the ratios, the smaller the alignment between the bare parameter axes and the eigenvectors of the Hessian.
1.4 Evolvability and Robustness
The question of how easily organisms encounter new traits is of great importance to understanding the process of adaptation in evolutionary biology. In a study of L = 15 RNA sequences and their secondary structures, it was discovered that 50% of all possibles sequences fold into only 6% of the structures that appear [6]. The folding process - an instance of what is called a â€˜genotype-phenotypeâ€™ (GP) map - shows a huge bias towards a small number of â€˜frequentâ€™ secondary structures. Thus a â€˜mutating agentâ€™ randomly searching the space of sequences (genotypes) is far more likely to encounter a very small subset of structures (phenotypes) that occur frequently. Such a bias limits the variety of phenotypes that is available for natural selection.
The example of RNA folding illustrates how a map between a space of parameters (genotypes) and a space of behaviours (phenotypes) aï¬€ects the likelihood of encountering diï¬€erent behaviours under random mutation in parameter space. One would similarly expect parameter space inhomogeneity and anisotropy in continuous models
5

to inï¬‚uence the likelihood of ï¬nding new behaviour in M under random mutations in Î˜. Since the processes that systems biology models describe (e.g. metabolism) must adapt their behaviour to changes in their circumstances, the question of how easily these systems ï¬nd new traits via parameter perturbations is just as pertinent.
This dissertation focuses two ideas that are relevant to this question. One is robustness, which describes the likelihood of a system to change its behaviour under mutation; the other is evolvability, which describes the diversity of behaviour accessible under mutation. Both are desirable characteristics of a biological system. A system should be able to resist mutation and retain favourable phenotypes, yet also have the ï¬‚exibility to adapt to its circumstances. Notions of evolvability and robustness are precisely quantiï¬ed in discrete GP maps by a set of deï¬nitions developed by Wagner [10] and an attempt has been made to generalise the discrete deï¬nitions to continuous models by Sethna et al. [3]. Sethna et al. made the extraordinary claim that sloppiness enables mutations in parameter space to explore a diverse range of behaviour. This dissertation sets out to assess the validity of this claim and examine their deï¬nitions of evolvability and robustness. In chapter 2, deï¬nitions of evolvability and robustness in discrete GP maps proposed by Wagner are reviewed; using the machinery of information geometry, chapters 3 and 4 put forward modiï¬ed or new measures of evolvability and robustness that are more consistent with Wagnerâ€™s deï¬nitions; working with a fresh set of deï¬nitions, the role of sloppiness on evolvability and robustness is investigated in chapter 5.

1.5 The Null Model

It will be useful for subsequent discussions to introduce an object called â€˜the null modelâ€™ at this juncture. The null model Mâˆ— is an artiï¬cial object constructed for the
analysis of a model M. It is nothing more than a ï¬‚at manifold (homogeneous and
isotropic with respect to Î˜) that takes up the same volume as M in Z. Given that it is ï¬‚at, the metric on Mâˆ— takes the form

Î·ÂµÎ½ = Î›Î´ÂµÎ½ where Î› is a constant scale. The volume of the model manifold over Î˜ is

(1.8)

V = dÎ¸ det(g)

(1.9)

The volume of the null manifold is V âˆ— = dÎ¸ det(Î·) = Î›D/2 dÎ¸

(1.10)

The volume of parameter space is simply Vp = dÎ¸, so forcing V = V âˆ— ï¬xes Î› to be

2
VD Î›=
Vp

(1.11)

6

Chapter 2
Evolvability and Robustness in Discrete Genotype-Phenotype Maps
Following on from the discussion in section 1.4, it would seem that robustness and evolvability are competing characteristics of a system. Consider ï¬gure 2.1 which represents a space of discrete genotypes. Each site on the lattice is a genotype, and the genotypes that can mutate into each other through one mutation are connected by an edge. Comparing genotypes A and B, one can observe how genotypes with more neighbours of the same phenotype (neutral neighbours) are connected to fewer alternative phenotypes that are diï¬€erent from each other, since there are fewer nonneutral neighbouring genotypes left over to support a diverse collection of alternative phenotypes. Hence genotypes that are more robust (more neutral neighbours and less likely to change phenotype after mutation) are less evolvable (fewer alternative phenotypes to mutate into), a correlation that Wagner has discovered in RNA secondary structure GP maps [10]. Wagner argued that evolvability and robustness can also be assessed from the perspective of phenotypes [10]. Consider ï¬gure 2.1 as an example again. A more frequent phenotype occupies a larger extent of genotype space and increases the number of alternative phenotypes within the reach of the set of genotypes that map to the phenotype - as such the phenotype is more evolvable. Yet Wagner also empirically observed in RNA secondary structure GP maps that genotypes belonging to the same phenotype are connected to each other more often than what one would expect by chance. Hence the robustness and evolvability of secondary structures (phenotypes) are positively correlated with each other. If this correlation holds generally for other GP systems, it would have great implications on how evolutionary biology considers the emergence of evolutionary novelty. The rest of this chapter provides more precise deï¬nitions and distinctions between genotype evolvability, genotype robustness, phenotype evolvability and phenotype robustness, and makes initial contact with sloppiness.
7

Genotype space (network)
A
B
Phenotypes:
Figure 2.1: Representation of a genotype-phenotype map in genotype space. Genotype A has very few neighbours of the same kind, yet its neighbourhood contains a diverse set of alternative phenotypes. Most of the neighbours of genotype B have the same phenotype, leaving little room for a diverse set of alternative phenotypes in its neighbourhood. Observe how the mapping from genotype to phenotype is manyto-one. The most frequent phenotype, the circle, forms a large neutral network that extends across parameter space, allowing it to mutate into triangles and squares. The triangle and square which are less frequent can only mutate into the circle, but not into each other.
2.1 Evolvability and Robustness of Genotypes
In a discrete GP system, genotypes can mutate into one another by one-step mutations; this mutational relationship between genotypes can be described by a network, in which genotypes are the nodes and edges between nodes represent allowed mutations. An example of a genotype space is the space of RNA sequences of length n. The genotype - an RNA sequence - can be thought of abstractly as a string of n letters where each letter in the sequence can be either A, G, C or U. Changing one letter in the string mutates one sequence into another. It is useful to deï¬ne the notion of a 1-neighbourhood of a genotype: Deï¬nition 2.1 (1-neighbourhood of a genotype). The 1-neighbourhood of a genotype g are the genotypes that can be accessed by g in one mutation.
Stepping through the network of genotypes, a mutating individual can potentially encounter new phenotypes at each node. It has been observed that the map between
8

genotypes and phenotypes are very often many-to-one [6]. In other words, there tends to be a redundancy of genotypes for each phenotype. Moreover, as Wagner has observed, the genotypes that map to the same phenotype are often connected to each other by one-step mutations in the genotype network [10]. Given this structure in GP systems, it is convenient to introduce three related concepts:
Deï¬nition 2.2 (Neutral neighbour of a genotype). A neutral neighbour of a genotype g is a genotype in the 1-neighbourhood of g which maps to the same phenotype as g.
Deï¬nition 2.3 (Neutral set). The neutral set of phenotype p is the maximal subset of genotypes which map to p.
Deï¬nition 2.4 (Neutral network). In a genotype network, connected components of genotypes that map to the same phenotype p are the neutral networks of p.
The existence of neutral networks has signiï¬cant implications on the mutational robustness of genotypes. If the majority of a genotypeâ€™s 1-neighbourhood are neutral neighbours, it is more likely to mutate into a genotype that maps to the same phenotype. In otherwords, the phenotype is likely to persist after mutation. A deï¬nition of â€˜genotype robustnessâ€™ can be used to quantify the likelihood of persistence [10]:
Deï¬nition 2.5 (Genotype robustness). The robustness of a genotype is the fraction of neutral neighbours of a genotype g.
The evolvability of a genotype can be deï¬ned in a similar way. A more evolvable genotype should be able to explore a more diverse set of phenotypes after mutation. Wagner quantiï¬es this by enumerating the number of diï¬€erent phenotypes in the 1-neighbourhood of a genotype:
Deï¬nition 2.6 (Genotype evolvability). The evolvability of a genotype is the number of diï¬€erent phenotypes that are accessible in the 1-neighbourhood of g.
Remark 2.1. It is important to point out a ï¬‚aw in Wagnerâ€™s deï¬nition in quantifying genotype evolvability. Suppose the 1-neighbourhood of a genotype g contains nine neighbours mapping to three diï¬€erent phenotypes A, B and C. Consider two cases: (i) A, B and C splits the nine neighbours evenly amongst themselves, i.e. A, B and C each corresponds to three neighbours; (ii) 7 neighbours map to A while B and C only correspond to 1 each. Wagnerâ€™s deï¬nition does not distinguish between cases (i) and (ii) where (i) obviously creates a greater diversity of outcomes for the genotype. In (ii) it is far more likely that the genotype mutates into neighbours which map to A.
2.2 Evolvability and Robustness of Phenotypes
The story could also be told from the perspective of phenotypes. Given the redundancy in GP maps, the likelihood of a phenotype p to persist after mutation should take into account all the genotypes that map to p. Consider a scenario in which a
9

phenotype p yields the optimum adaptation to selection pressure. Averaging over an ensemble of the systemâ€™s possible evolutionary histories, the population of individuals with phenotype p should be evenly distributed in the neutral set of p, as evolutionary pressure does not distinguish between the genotypes that map to p. The robustness of the phenotype can be quantiï¬ed by the fraction of individuals that retain the same phenotype p after mutation. This can be computed by adding up the probabilities of each individual retaining the same phenotype after mutation. The likelihood of an individual with genotype g retaining the same phenotype is simply the robustness of g. Since the population is evenly distributed in the neutral set, the fraction of the population that retains the same phenotype is computed by summing up the genotype robustness of the neutral set. This argument justiï¬es Wagnerâ€™s deï¬nition of phenotype robustness [10]:
Deï¬nition 2.7 (Phenotype robustness). The robustness of a phenotype p is the average genotype robustness of its neutral set.
The evolvability of a phenotype can also be motivated by the same argument used for genotype evolvability. A phenotype is able to â€˜mutateâ€™ into another phenotype if and only if there exists at least one genotype in their respective neutral sets that are connected to one another in the genotype network. Wagner quantiï¬es the evolvability of a phenotype by the number of diï¬€erent phenotypes accessible by a population of individuals dispersed in its neutral set [10]; in other words,
Deï¬nition 2.8 (Phenotype evolvability). The evolvability of a phenotype p is the number of unique phenotypes that are accessible by genotypes in the neutral set of p.
Remark 2.2. Wagnerâ€™s deï¬nition of phenotype evolvability suï¬€ers from the same issue plaguing genotype evolvability which was discussed in remark 2.1: it does not distinguish between the relative likelihood of outcomes after mutations and so fails to give a full measure of the diversity of mutational outcomes.
2.3 Sloppiness and Neutral Spaces
Most of the idioms of GP maps are not immediately suitable for describing continuous models [3]. While there are only a ï¬nite number of genotypes and phenotypes in the domain and image of discrete GP maps, parameters and behavioural states live in spaces of real numbers which are uncountable sets; hence the counting schemes employed in the deï¬nitions above need to be carefully modiï¬ed for those ideas to make sense in continuous models. However, an important theme is carried through: the distinction between the evolution of a single individual and that of a population of individuals remains relevant. Parameter evolvability/robustness, the continuous analogue of genotype evolvability/robustness, characterises the mutational outcome of an individual occupying a single point in parameter space; state (behaviour) evolvability/robustness, the continuous analogue of phenotype evolvability/robustness, describes an ensemble of individuals that belong to the same state on the model manifold. This idea is a very useful starting point for formulating deï¬nitions of evolvability and robustness in continuous models.
10

Sethna et al. makes an analogy between sloppiness in continuous models and neutral sets in GP maps [3]. Since sloppiness reduces a systemâ€™s sensitivity to perturbations in some directions of parameter space, Sethna et al. argued that sloppiness would create â€˜neutral subspacesâ€™ - parameter insensitive subspaces of Î˜ - analogous to neutral networks in GP maps. Wagner proposed that large neutral networks in discrete GP systems enable individuals on the neutral network to explore a diverse set of phenotypes [10]. In response to this, Sethna et al. claims that large neutral subspaces, similar to large neutral networks in discrete GP maps, allow mutating agents with similar behaviour to explore a larger extent of parameter space and increase their likelihood of encountering new behaviour in M [3]. This claim is examined in chapter 5.
11

Chapter 3
Robustness and Evolvability of Parameters

3.1 Parameter Robustness

Inspired by Wagnerâ€™s deï¬nition of genotype robustness, Sethna et al. deï¬ned the
robustnesss of a parameter Rp(Î¸), or what they refer to as the â€˜chemotypeâ€™, as the fraction of mutations Î´Î¸ in Î˜ satisfying |Î´Î¸|2 < âˆ† that do not change the distance in M beyond a tolerance Î´s2 < 2, where âˆ†, â†’ 0 with ï¬nite âˆ†/ [3]. This deï¬nition
is best interpreted geometrically (see ï¬gure 3.1). Consider an agent at Î¸0 under the inï¬‚uence of a random isotropic perturbation of size less than âˆ†. The agentâ€™s possible
positions in Î˜ after the perturbation is the volume enclosed by a D-dimensional hypersphere of radius âˆ†. Expressing this in the norm-preserving coordinates Î´Î¸i that
(locally) diagonalise the metric of the model manifold gÂµÎ½(Î¸0) (equation 1.4)

âˆ†2 = (Î´Î¸1)2 + Â· Â· Â· + (Î´Î¸D)2

(3.1)

This is the equation of a sphere in Î˜. Now change perspective to M and consider the distance in M in Î´Î¸i coordinates:

Î´s2 = Î»ÂµÎ´ÂµÎ½Î´Î¸ÂµÎ´Î¸Î½

(3.2)

Deï¬ne a new set of local â€˜normalâ€™ coordinates âˆš
Î´Î¸Ë†i = Î´Î¸i Î»i

(3.3)

such that in these coordinates the metric is ï¬‚at:

Î´s2 = gÂµÎ½(Î¸Ë†)Î´Î¸Ë†ÂµÎ´Î¸Ë†Î½ = Î´ÂµÎ½Î´Î¸Ë†ÂµÎ´Î¸Ë†Î½

(3.4)

For a perturbation satisfying the tolerance Î´s2 < 2, it must lie within a D-dimensional hypersphere of radius on M; in Î´Î¸Ë†i coordinates,

2 = (Î´Î¸Ë†1)2 + . . . + (Î´Î¸Ë†D)2

(3.5)

12

(a) Parameter robustness by volume subtraction

Îµ /âˆšÎ»2

Î˜M

Î¸ Îµ /âˆšÎ»1
|Î´Î¸| = Î”
Î´s = Îµ

âŸ¹

f (Î¸)
Î´s = Îµ

(b) Parameter Evolvability

B
Î¸
A |Î´Î¸| = Î”

Î˜M

âˆšÎ»2 Î”

B
âˆšÎ»1 Î”

âŸ¹

f (Î¸)

A |Î´Î¸| = Î”

Figure 3.1: (a) Projection of -tolerance sphere from M onto Î˜. Parameter robustness (by volume subtraction) can be understood as the fraction of perturbations |Î´Î¸| < âˆ† that satisfy Î´s < , indicated by the volume shaded dark grey. (b) Projection of âˆ†-perturbation sphere from Î˜ onto M. Evolvability is the average distance between two points A and B on the surface |Î´Î¸| = âˆ† in M.

This ellipse can be mapped onto Î˜ by recasting the equation in Î´Î¸ coordinates,

2 = Î»1(Î´Î¸1)2 + Â· Â· Â· + Î»D(Î´Î¸D)2

(3.6)

âˆš The semi-major axes of the ellipse in the Î¸i direction is / Î»i. Returning to the

deï¬nition of Rp(Î¸), ï¬nding the fraction of perturbations satisfying |Î´Î¸|2 < âˆ† and

Î´s2 < 2 is equivalent to calculating the intersecting volume between the sphere and

ellipse deï¬ned by equations (3.1) and (3.6) respectively as a fraction of the volume

of the sphere. In other words, robustness is the fraction of the perturbation sphere

13

remaining once the points that lie outside the tolerance ellipse is subtracted. However, as Sethna et al. points out, this is a diï¬ƒcult calculation [3]!
To get around this diï¬ƒculty, Sethna et al. softened the hard boundaries of the spheres and ellipses and represented them with probability distributions that decay to inï¬nity [3]. Suppose the agent mutating away from Î¸0 travels a distance |Î´Î¸| that is normally distributed with a width scale âˆ†

1

P (|Î´Î¸|, âˆ†) = âˆš

exp

( 2Ï€âˆ†)D

1

=âˆš

exp

( 2Ï€âˆ†)D

|Î´Î¸|2 âˆ’ 2âˆ†2

1 âˆ’

D
(Î´Î¸i)2

2âˆ†2

i=1

(3.7) (3.8)

To take away the perturbations that lie outside the ellipse, imagine a scenario where the agent is removed with some probability that increases with distance Î´s. Sethna et al. chose the probability of survival Q to be a Gaussian with a typical length scale Î´s âˆ¼ :

Q(Î´s, ) = exp = exp

Î´s2

âˆ’ 2

2

1 âˆ’
22

D

Î»i(Î´Î¸i)2

i=1

(3.9) (3.10)

where the normalisation of Q(Î´s, ) ensures the probability of survival for not mutating is Q(Î´s = 0, ) = 1. If an ensemble of mutating agents start oï¬€ at Î¸0 and diï¬€use away from Î¸0 with a scale |Î´Î¸| âˆ¼ âˆ† a la equation (3.7), the fraction of agents that survive removal can be used as a measure of the robustness of Î¸0. In terms of P (|Î´Î¸|, âˆ†) and Q(Î´s, ),

Rp(Î¸, âˆ†, ) = dÎ´Î¸P (|Î´Î¸|, âˆ†)Q(Î´s, )

(3.11)

1

=âˆš

dÎ´Î¸ exp

( 2Ï€âˆ†)D

D
=
i=1

1

1

+

Î»i Î»c

1 âˆ’ 2âˆ†2

D
(Î´Î¸i)2

i=1

âˆ†2 1 + Î»i 2

(3.12)

where

Î»c

=

.2
âˆ†2

This deï¬nition of parameter robustness has a few undesirable properties. First of

all, it is dependent on an arbitrary scale Î»c whose value is left to whoeverâ€™s applying the deï¬nition to decide. In their analysis of a model of an EGF/NGF signaling

pathway, Sethna et al. chose Î»c to be the fourth-largest eigenvalue of the local Hessian, which changes from Î¸ to Î¸ [3]. This is a problematic choice when the robustness of

diï¬€erent parameters are being compared.

14

Fortunately, the model manifold M has a natural scale in relation to parameter space Î˜, Î›. Insight into why this is an appropriate scale for Î»c can be gained by going back to the geometric interpretation of hard âˆ† perturbation spheres and toleâˆšrance ellipses. In the null model, a perturbation |Î´Î¸| = âˆ† in Î˜ corresponds to Î´s = Î›âˆ†. For the âˆ† perturbation sphere to be fully enclosed âˆšby the tolerâˆšance ellipse (a sphere in the null model), must be chosen such that â‰¥ Î›âˆ†. If < Î›âˆ†, the tolerance ellipse is smaller than the âˆ† perturbation sphere for any parameter in the null model. Hence any parameter will be designated as not âˆšrobust under this scheme, which is not desriable for the null model. A choice of = Î›âˆ†, i.e. Î»c = Î› would ensure that all parameters in the null model have a neutral robustness. Given this choice,

D
Rp(Î¸) =
i=1

1

1

+

Î»i Î›

(3.13)

However if this deï¬nition of robustness is computed for the null model Mâˆ— with

Î»i = Î›,

Rpâˆ—

=

1 2D/2

(3.14)

One would expect the measure of robustness for the null model to evaluate to unity,

since the choice of Î»c = Î› ensures neutral robustness in the null model, i.e. the âˆ†sphere and -ellipse overlap exactly. The D-dependence is also somewhat troubling. The incongruity of Rpâˆ— is due to the inï¬nite extents of the Gaussian distributions and the agent removal process. Any agent that is perturbed ever so slightly has a ï¬nite

probability of being removed, and there are always agents perturbed suï¬ƒciently far

away that are deï¬nitely removed. Since there is always a loss of agents, Rp must be smaller than 1, even in the null model. A more appropriate measure of robustness

would be to compare the survival rate of agents in a particular model to that of the

null model. Hence one can deï¬ne a new robustness measure Ïp(Î¸):

Ïp(Î¸)

=

Rp(Î¸) Rpâˆ—

=

D i=1

2

1

+

Î»i Î›

(3.15)

3.2 Parameter evolvability
The evolvability of a genotype g is the number of diï¬€erent phenotypes found in the one-neighbourhood of g (deï¬nition 2.6). While diï¬€erences between phenotypes are discontinuous and easy to enumerate in discrete GP systems, the diï¬€erence between behaviour in a continuous model is measured by the distance on the model manifold M. How do we â€˜enumerateâ€™ the number of â€˜diï¬€erentâ€™ behaviours that are accessible in the mutational neighbourhood of Î¸0?
The response of Sethna et al. to this challenge is to bypass this argument completely. They deï¬ned the evolvability of a â€˜chemotypeâ€™ (parameter) to be the maximum change in â€˜ï¬tnessâ€™ averaged over a spherically distributed â€˜forceâ€™ F in Z, where ï¬tness is deï¬ned to be the inner product between the change in behaviour Î´f and F

15

on M [3]. This is a rather unsatisfactory deï¬nition since it introduces the unnecessary complication of selection pressure (F ) and ï¬tness which is intrinsically independent of a systemâ€™s innate ability to discover new behaviour via parameter space ï¬‚uctuations. Moreover, it is unclear how this relates to the diversity of behaviour in the neighbourhood of a parameter.
While the alternative deï¬nition proposed here does not follow Wagners deï¬nition to the letter, it respects the idea behind Wagnerâ€™s deï¬nition. The evolvability of a parameter Î¸0 should be a measure of the potential of a mutating agent at Î¸0 to explore a â€˜diverseâ€™ set of outcomes. Since the diï¬€erence between behaviours is measured by their distance on M, the diversity of the mutational neighbourhood can be quantiï¬ed by picking two random points on the âˆ† sphere and computing their mean squared distance Î´s2 on M (see ï¬gure 3.1). If Î´s2 is â€˜largeâ€™, the behavioural outcomes in the mutational neighbourhood are well separated in M; conversely if Î´s2 is â€˜smallâ€™, there is little diversity to be found in behaviour.
The ï¬rst step in computing Î´s2 is to project the âˆ† perturbation sphere in Î˜ onto M. Adopting spherical coordinates in Î¸, the âˆ† perturbation sphere is parametised by angular coordinates (Ï†1, . . . , Ï†Dâˆ’1):

Î´Î¸1 = âˆ† cos(Ï†1) Î´Î¸2 = âˆ† sin(Ï†1) cos(Ï†2) Î´Î¸3 = âˆ† sin(Ï†1) sin(Ï†2) cos(Ï†3)
... Î´Î¸Dâˆ’1 = âˆ† sin(Ï†1) . . . sin(Ï†Dâˆ’2) cos(Ï†Dâˆ’1)
Î´Î¸D = âˆ† sin(Ï†1) . . . sin(Ï†Dâˆ’2) sin(Ï†Dâˆ’1)

(3.16)

Using the normal coordinates deï¬ned in equation (3.3), the âˆ† sphere in Î¸Ë† is

Î´Î¸Ë†1 = Î»1âˆ† cos(Ï†1) Î´Î¸Ë†2 = Î»2âˆ† sin(Ï†1) cos(Ï†2)
... etc.

(3.17)

Consider Î¸A = (Ï†1A, . . . , Ï†ADâˆ’1) and Î¸B = (Ï†1B, . . . , Ï†DBâˆ’1) on the âˆ† sphere. The distance between Î¸A and Î¸B in M is simply

Î´s2AB = (Î´Î¸Ë†A1 âˆ’ Î´Î¸Ë†B1 )2 + . . . + (Î´Î¸Ë†AD âˆ’ Î´Î¸Ë†BD)2

(3.18)

Averaging the expression over the solid angles dâ„¦A and dâ„¦B of Î¸A and Î¸B respectively,

Î´s2 =

dâ„¦A â„¦

dâ„¦B â„¦

Î´s2AB

= Î»1âˆ†2

dâ„¦A â„¦

dâ„¦B â„¦

(cos(Ï†1A)

âˆ’

cos(Ï†1B

))2

+ Î»2âˆ†2

dâ„¦A â„¦

dâ„¦B â„¦

(sin(Ï†1A)

cos(Ï†2A)

âˆ’

sin(Ï†1B

)

cos(Ï†2B

))2

(3.19) (3.20)

+ etc.

16

While the integrals multiplying Î»i in the expression appear diï¬€erent, they in fact evaluate to the same value. Î´s2 should be invariant to permutations of (Î´Î¸1, . . . , Î´Î¸D) on the left hand side of equations (3.16), since the permutation of (Î´Î¸1, . . . , Î´Î¸D) chosen in equations (3.16) was arbitrary in the ï¬rst place (ignoring the handedness of the coordinate system). Such permutations would replace the integral multiplying Î»i with another one, yet the contribution of Î»i to the sum cannot change, so the two integrals that have exchanged places must evaluate to the same value. Thus after going through all such permutations for all Î»i, one is forced to conclude that all the integrals in this sum evaluate to the same value. Hence

D
Î´s2 = N âˆ†2 Î»i
i=1
where N is the value of the integrals. For the null model,

(3.21)

Î´s2 âˆ— = N âˆ†2DÎ›

(3.22)

Î´s2 is a measure of behavioural diversity in the mutational neighbourhood and hence provides a measure of evolvability. Taking the null model as the baseline reference, the evolvability of a parameter is thus deï¬ned as

Î·p =

Î´s2

1

=

Î´s2 âˆ— D

D

Î»i Î›

i=1

(3.23)

If Î·p > 1, posed by

it is more Sethna et

evolvable than the null model. Coincidentally, the al. is actually proportional to âˆšÎ·p; yet not only is

deï¬nition prothis derivation

closer to Wagnerâ€™s intentions, it also does away with the need of appealing to selection

pressure. It is noteworthy that the mean squared distance of a mutating agent subject

to a random isotropic perturbation in Î˜ is also proportional to Î·p. Hence Î·p can not only be interpreted as a measure of diversity in a parameterâ€™s mutational neighbour-

hood, it can also be understood as a measure of the average change in behaviour in

response to perturbations in Î˜.

17

Chapter 4
Robustness and Evolvability of States

4.1 States

A straightforward phenotype analogue in continuous models is the behaviour v = f (Î¸). While a behaviour is strictly a point on M, it is more useful to consider an inï¬nitesimally small volume of behaviours in the neighbourhood of v. The set of behaviours enclosed by this volume is referred to as the state Î¨(v) (a.k.a â€˜dynatypeâ€™ in [3]). By relaxing a point into an inï¬nitesimally small volume, deï¬nitions of robustness and evolvability could be more readily conceived for the phenotype analogue.
While Sethna et al. pictures the state (dynatype) as a D-dimensional hypersphere in M, it is far more convenient to consider an inï¬nitesimally small D-dimensonal hypercube with sides of length as measured in M (see ï¬gure 4.1). Thus, in this construction, Î¨(v) is the set of points enclosed in the hypercube with v at the cubeâ€™s centre. Each hypercube touches 2D other neighbouring hypercubes. The hypercube can be chosen to be oriented along the normal coordinates Î´Î¸Ë†i as deï¬ned in equation (3.3). Using equation (3.3), Î¨(v) can be transformed into Î´Î¸i coordinates and projected back to Î˜. The projection Î¨Ëœ (Î¸) is a hypercuboid with sides |Î´Î¸i| = âˆš .
Î»i
Remark 4.1. The process of relating Î¨(v) to Î¨Ëœ (Î¸) has implicitly assumed that the model f is injective on some subset of Î˜ i.e. f is locally identiï¬able (see remark 1.1). If f is locally identiï¬able but not globally identiï¬able, this one-to-one correspondence between parameters and states can only make sense by restricting Î˜ to a subset on which f is injective. Remark 4.2. The volume of Î¨Ëœ (Î¸) is

V = D D âˆš1 i=1 Î»i

and the surface area is

A = 2 Dâˆ’1 D âˆš1 D i=1 Î»i i=1

VD Î»i = 2
i=1

Î»i

(4.1) (4.2)

18

Î´Î¸2 = Îµ / Î»âˆš2 Î´ âˆ§Î¸2 = Îµ

(a)
Î´Î¸1 = Îµ /âˆšÎ»1
Î¨~
Î˜ Parameter Space

âŸ¹

Î´Î¸âˆ§1 = Îµ
Î¨
M Model Manifold

(b)

Parameter Space with a homogeneous

density of mutating agents

Model with parameter space anisotropy

Null Model

Figure 4.1: (a): State Î¨ in M and its corresponding projection Î¨Ëœ in Î˜. Î˜ and M is tiled by Î¨ and Î¨Ëœ respectively. (b) Left: diï¬€usion of mutating agents out of Î¨Ëœ in a model with parameter space anisotropy; Right: diï¬€usion out of Î¨Ëœ âˆ— in the null model. Observe how mutating agents leave each side of Î¨Ëœ âˆ— with equal probability, yet they are biased towards leaving the longer sides of Î¨Ëœ .

19

Remark 4.3. Since the sponds to a hypercube

null model is Î¨Ëœ âˆ— in Î˜ with

isotropic, sides |Î´Î¸i|

Î¨âˆ— =

wâˆšith sides / Î›. The

|Î´Î¸Ë†i| = volume

in of a

M correnull state

is

Vâˆ— =

D

âˆšD /Î›

(4.3)

The surface area of a null state is

Aâˆ—

=

2D

Dâˆ’1

âˆš Dâˆ’1 /Î›

(4.4)

4.2 State Robustness

Sethna et al. did not attempt to deï¬ne the robustness of a â€˜dynatypeâ€™ [3]. Since the
robustness and evolvability of a phenotype p describe the evolvability and robustness
of a population of individuals with phenotype p, it is meaningful to consider the
evolvability and robustness of a state Î¨ in terms of the outcome of an ensemble of agents that are enclosed in Î¨Ëœ at a particular time. Consider a thought experiment in
which Î˜ is ï¬lled with a homogeneous density of agents in brownian motion. These agents walk around Î˜ randomly and explore diï¬€erent states Î¨Ëœ as they do so. At a particular time t0, the permeable boundary of Î¨Ëœ is suddenly made impermeable to agents exterior to Î¨Ëœ . However, agents in Î¨Ëœ are allowed to cross the boundary and leave Î¨Ëœ .
Conjecture: the robustness of the state is quantiï¬ed by the length of time taken for the ensemble of agents to escape Î¨Ëœ (Î¸). Such a time scale Ï„ is

N Ï„=
Î¦

(4.5)

where N is the number of agents in Î¨Ëœ (Î¸) and Î¦ is the ï¬‚ux of agents out of Î¨Ëœ (Î¸)

averaged over time. Since Î˜ is ï¬lled initially with a constant density of agents, the number of agents trapped in Î¨Ëœ (Î¸) is proportional to the volume V of Î¨Ëœ (Î¸). If the total ï¬‚ux Î¦ across the surface of Î¨Ëœ (Î¸) is assumed to be proportional to the surface

area A,

V

Ï„ (Î¸) = Ï‡ = Ï‡ A2

D i=1

âˆš Î»i

(4.6)

where the factor Ï‡ accounts for the conditions such as the mobility and density of

agents which is unrelated to the modelâ€™s geometry. The time scale for a state in the

null model is

Ï„âˆ—

=

Vâˆ— Ï‡

=

Ï‡

âˆš

Aâˆ— 2D Î›

(4.7)

The robustness of a state Î¨ is deï¬ned as the time scale of escape relative to that

of the null model, therefore

Ï„ (Î¸)

1D

ÏÎ¨(Î¸) = Ï„ âˆ— = D

i=1

âˆ’1
Î»i
Î›

(4.8)

In loose terms, a state is in its most robust form if its surface area to volume ratio

is minimised.

20

4.3 State Evolvability

Sethna et al. deï¬ned the evolvability of a â€˜dynatypeâ€™ (state) as the optimum response within a population of agents at Î¸ to a force F in Z [3]. Just like their deï¬nition of â€˜chemotypeâ€™ (parameter) evolvability, selection forces are involved for no good reason and it is not clear how it relates to Wagnerâ€™s deï¬nition. Wagner quantiï¬es the evolvability of a phenotype p by the number of unique phenotypes that are accessible by genotypes in the neutral set of p (deï¬nition 2.8) [10]. For states in continuous models, a D-dimensional box always has 2D number of faces, so the number of neighbouring boxes of Î¨Ëœ is always 2D. If Wagnerâ€™s deï¬nition for phenotypes is naively applied to states, all states would have the same evolvability, regardless of thir geometries. However this way of counting is clearly problematic. Consider an ensemble of agents diï¬€using out of a rectangle in ï¬gure 4.1 (b). Such agents are more likely to cross the longer sides of the rectangles. Hence most of the agents end up in the two states that share the longer side with Î¨Ëœ . The ensemble of agents access an â€˜eï¬€ectiveâ€™ number of neighbours that is closer to two than four.
The thought experiment in subsection 4.2 can be employed to formalise this argument. If the ï¬‚ux through the kth face of the box, Î¦k, is proportional the faceâ€™s area, Ak, the probability that any agent escapes through the kth face of the box pk is given by

pk =

Î¦k = k Î¦k

Ak k Ak

(4.9)

The expected value of the surface area of a face crossed by any randomly chosen agent

escaping Î¨(Î¸) is

A = pkAk =
k

k A2k k Ak

(4.10)

We deï¬ne the eï¬€ective number of faces, or eï¬€ective number of neighbours, N to be the total area divided by A

N=

kA = ( A

k Ak)2 k A2k

(4.11)

In a null model, the state is a simple cube in Î˜. With all faces equal in area the

eï¬€ective number of neighbours is simply N âˆ— = 2D. Deï¬ning the evolvability of a

state Î·Î¨ as the eï¬€ective number of neighbours relative to that of a null state,

Î·Î¨ =

N 1( =
N âˆ— 2D

k Ak)2

2D k=1

A2k

(4.12)

This could be rewritten as

( Î·Î¨ =

2D k=1

Ak

/2D)2

2D k=1

A2k

/2D

=

A2 A2

(4.13)

21

The explicit expression of Î·Î¨(Î¸) in terms of the eigenvalues of the metric tensor at Î¸ can be computed. Consideâˆšr the face of the cube in the plane perpendicular to the eigenvector with eigenvalue Î»i. The area of that face is

Aj = Dâˆ’1

D1 âˆš
i=1 Î»i

/

1 âˆš
Î»i

=

Î»jV /

(4.14)

Each cube has two such faces for each axes. Substituting (4.14) into equations (4.11)

and (4.12),

( Î·Î¨(Î¸) =

D i=1

âˆš Î»i/D)2

D i=1

Î»i/D

(4.15)

Remarkably, ÏÎ¨, Î·Î¨ and Î·p can be summarised into a rather concise relation

Ï2Î¨

=

1 Î·Î¨Î·p

(4.16)

Remark 4.4. Loosely speaking, the two evolvabilities capture diï¬€erent aspects of the

local geometry at Î¸: Î·p is a measure of the local length scale and Î·Î¨ is a function of the angular distribution around Î¸. ÏÎ¨ depends on both aspects of the geometry that are encapsulated in the two evolvabilities respectively.

4.4 Globally Non-identiï¬able but Locally Identiï¬able Models

It was noted in remark 4.1 that the geometric construction above relies on a oneto-one correspondence between Î¨Ëœ (Î¸) and Î¨(v) on a subset of Î˜, i.e. the model f

being locally identiï¬able. If the model f globally non-identiï¬able (see remark 1.1), a

state Î¨ on M corresponds to more than one point in Î˜. Fortunately, restricting f to

be locally identiï¬able, one can ï¬nd disjoint neighbourhoods for each of these points.

Consider the maximal set of points S = {Î¸1, . . . , Î¸m} âˆˆ Î˜ that map to the same point

v in M by a locally identiï¬able model f : v = f (Î¸1) = f (Î¸2) = Â· Â· Â· = f (Î¸m). For each

Î¸k âˆˆ S, one can compute the eigenvalues of the metric gÂµÎ½(Î¸k). The state Î¨(v) can be

projected onto the individual disjoint neighbourhoods Uk of Î¸k. Let the projections be {Î¨Ëœ k}. Casting this in the context of the thought experiment in subsection 4.2, the state Î¨ now encloses populations of mutating agents in disjoint regions Î¨Ëœ k âŠ‚ Uk of Î˜.

Recall that ÏÎ¨ is simply a normalised escape time-scale of agents out of Î¨; averaging over all agents escaping out of every Î¨Ëœ k, the mean escape time Ï„ out of Î¨(v) can be

computed by

1 =

k Î¦k =

( Nk ) Î¦k =

Î½k

Ï„

k Nk

k

k Nk Nk

k Ï„k

(4.17)

where Î½k = Nk/ k Nk = Vk/ k Vk and Ï„k = Nk/Î¦k is the escape time-scale out of Î¨Ëœ k. Normalising the lifetime with Ï„ âˆ— (4.7), the robustness of the state Î¨(v), ÏÎ¨(v) is

1

Ï„âˆ—

Î½(Î¸)

==

ÏÎ¨(v) Ï„ Î¸âˆˆS ÏÎ¨(Î¸)

(4.18)

22

All the quantities on the right hand side can be computed explicitly in terms of the
eigenvalues at Î¸k using equations (4.1) and (4.8). Since the evolvability of a state is simply the normalised eï¬€ective number of neigh-
bours, one can simply sum over all the individual evolvabilities Î·Î¨(Î¸k) of Î¨Ëœ k to obtain the normalised total number of eï¬€ective neighbours for Î¨(v):

Î·Î¨(v) = Î·Î¨(Î¸)
Î¸âˆˆS

(4.19)

23

Chapter 5
Sloppiness, Robustness and Evolvability

In their paper â€˜Sloppiness, Evolvability and Robustness in Systems Biologyâ€™, Sethna

et al. argued that sloppiness - parameter indeterminacy in certain dimensions of

parameter space - induces â€˜neutral subspacesâ€™ in Î˜. Such neutral subspaces allow

mutating agents of similar behaviour to explore larger extents of parameter space

and increase their likelihood of encountering new behaviour (section 2.3) [3]. This ar-

gument is rooted in Wagnerâ€™s demonstration that larger neutral networks in genotype

space (analogous to neutral subspaces in Î˜) increases the evolvability of phenotypes

[10]. If the argument proposed by Sethna et al. holds water, it should be reï¬‚ected

in the evolvability Î·Î¨ of states, the analogue of phenotypes in continuous models. If the eigenvalue spectrum of gÂµÎ½(Î¸) is sloppy, Î¨(f (Î¸)) should be an evolvable state (assuming global identiï¬ability), i.e. Î·Î¨ > 1.
As it turns out, parameter space anisotropy is a necessary and suï¬ƒcient condition

for Î·Î¨ < 1. Consider

N

A2

Î·Î¨ =

= N âˆ— A2

(4.13 revisited)

where A is the area of faces of Î¨Ëœ . Recognising

A2 âˆ’ A2 = (Ak âˆ’ A)2 â‰¥ 0
k

(5.1)

It is apparent that

N

Î·Î¨ =

â‰¤1 Nâˆ—

(5.2)

In other words, the eï¬€ective number of neighbours of any state cannot be greater

than that of the null model, where there is no parameter space anisotropy at all.

More insight can be gained by applying Î·Î¨ to a phenomenological toy â€˜meta-modelâ€™ of a sloppy eigenvalue spectrum. A sloppy system is characterised by a roughly even

spread of eigenvalues over a logarithmic scale. It would be natural to consider the

spectrum

Î»n = Î»0eâˆ’2nÂµ

(5.3)

24

where n = 0, . . . , D âˆ’ 1 and 2Âµ = log(Î»n) âˆ’ log(Î»n+1) is the constant log-separation between eigenvalues. This model reduces a sloppy spectrum to two â€˜meta-parametersâ€™:

the scaled leading order eigenvalue Î² = Î»0/Î› and the log-separation Âµ. Substituting this into equation (4.15),

1 tanh (DÂµ/2) Î·Î¨ = D tanh (Âµ/2)

(5.4)

Î·Î¨ is always smaller than Î·Î¨(Âµ = 0) = 1 and is a monotonically decreasing function of Âµ for Âµ > 0. As the separation between the eigenvalues, Âµ, increases, the local

parameter space becomes more anisotropic. This shows that, to ï¬rst approximation,

sloppiness decreases the evolability of a state. It seems that under a strict adherence to

Wagnerâ€™s original deï¬nitions, Sethnaâ€™s claim that neutral subspaces allow individuals

in it to reach a broader range of behavioural changes [3] [9] is thoroughly debunked.

This dissertation makes two conjectures as to why the neutral subspace argument

fails. Firstly, the diversity of behaviour is limited by the hypercube geometry of Î¨ in

M: it has a ï¬xed number of neighbours, a fact that is independent of parametrisation.

No amount of deformation in parameter space can increase that. In contrast, this

limit is not imposed on phenotypes in discrete GP maps as the mutational relation-

ships between phenotypes are established by the topology of the genotype network

rather than any a priori measures of similarity between phenotypes. Hypothetically,

a neutral set can grow in size to increase the number of accessible phenotypes up to

the number of phenotypes allowed in the system. Secondly, sloppiness discourages

individual agents within the neutral subspace to explore certain directions in Î˜. As

a result, mutating agents can only explore a low dimensional subspace of Î˜ and is

unable to access a maximally diverse set of behaviours.

It is manifest from 4.15 that Î·Î¨ is invariant under rescaling of the eigenvalues:

( Î·Î¨ =

D i=1

âˆš Î»i

/D)2

D i=1

Î»i/D

(4.15 revisited)

Hence Î·Î¨ is only dependent on the sizes of the eigenvalues relative to each other. The invariance under rescaling is not true of Î·p, Ïp and ÏÎ¨; in their case the absolute value of Î» (in units of Î›) matters. Hence one is cautioned against developing reasonings about Î·p, Ïp and ÏÎ¨ on the basis of sloppiness only - sloppiness is merely a description of the relative sizes of eigenvalues, not their absolute magnitude. Sethna et al. noted a negative correlation between parameter evolvability and robustness in a sloppy model of an EGF/NGF in PC12 cells [3]1, which mirrors the negative correlation between genotype evolvability and robustness in RNA second structures in Wagnerâ€™s ï¬ndings [10]. Though Sethna et al. did not make an explicit connection between this phenomenon and sloppiness, it is worth emphasising that there is insuï¬ƒcient evidence to determine the signiï¬cance of sloppiness in correlating these two quantities. If this correlation appears in other sloppy systems biology models, it is prudent to examine other possible reasons for its occurence rather than hastily attributing it to sloppiness.

1This correlation should hold under the deï¬nitions of parameter evolvability and robustness in this dissertation as they are only slightly diï¬€erent to Sethnaâ€™s deï¬nitions.

25

Chapter 6
Summary and Outlook
Evolvability and robustness are desirable qualities in biological systems: favourable traits need to persist and resist random mutation, and a diverse set of behaviour accessible by mutation helps the system adapt to changes in circumstances. What is the role of sloppiness in inï¬‚uencing the evolvability and robustness of a system? This dissertation has demonstrated that parameter space anisotropy is the only factor in determining the evolvability of a state. In particular, increasing sloppiness decreases a stateâ€™s evolvability. Yet sloppiness is not a suï¬ƒcient condition to determine state robustness, parameter evolvability and paramter robustness. While sloppiness is a relevant to the discussion on adaptation, it cannot provide an elegant and uniï¬ed account of evolvability and robustness in system biology models.
This dissertation has developed a set of measures of evolvability and robustness which can be applied to any continuous model which is locally identiï¬able. Local metric eigenvalues of models can be numerically computed eï¬ƒciently using tools such as Sloppy Cells [7]. Since the measures developed are purely functions of metric eigenvalues (in fact, apart from Ïp, all of them can be computed from the trace of the metric or metric square rooted), Î·p vs Ïp and Î·Î¨ vs ÏÎ¨ correlations can be evaluated for continuous models. This provides an apparatus for further research to investigate whether systems biology models can be simultaneously evolvable and robust.
26

Bibliography
[1] Kevin S Brown, Colin C Hill, Guillermo A Calero, Christopher R Myers, Kelvin H Lee, James P Sethna, and Richard A Cerione. The statistical mechanics of complex signaling networks: nerve growth factor signaling. Physical biology, 1(3):184, 2004.
[2] Oana-Teodora Chis, Alejandro F Villaverde, Julio R Banga, and Eva BalsaCanto. On the relationship between sloppiness and identiï¬ability. Mathematical Biosciences, 282:147â€“161, 2016.
[3] Bryan C Daniels, Yan-Jiun Chen, James P Sethna, Ryan N Gutenkunst, and Christopher R Myers. Sloppiness, robustness, and evolvability in systems biology. Current opinion in biotechnology, 19(4):389â€“395, 2008.
[4] Emilie Dufresne, Heather A Harrington, and Dhruva V Raman. The geometry of sloppiness. arXiv preprint arXiv:1608.05679, 2016.
[5] Ryan N Gutenkunst, Joshua J Waterfall, Fergal P Casey, Kevin S Brown, Christopher R Myers, and James P Sethna. Universally sloppy parameter sensitivities in systems biology models. PLoS Comput Biol, 3(10):e189, 2007.
[6] Ard A Louis. Contingency, convergence and hyper-astronomical numbers in biological evolution. Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences, 58:107â€“116, 2016.
[7] Christopher R Myers, Ryan N Gutenkunst, and James P Sethna. Python unleashed on systems biology. Computing in Science & Engineering, 9(3):34â€“37, 2007.
[8] Christian ToÂ¨nsing, Jens Timmer, and Clemens Kreutz. Cause and cure of sloppiness in ordinary diï¬€erential equation models. Physical Review E, 90(2):023303, 2014.
[9] Mark K Transtrum, Benjamin B Machta, Kevin S Brown, Bryan C Daniels, Christopher R Myers, and James P Sethna. Perspective: Sloppiness and emergent theories in physics, biology, and beyond. The Journal of chemical physics, 143(1):07B201 1, 2015.
27

[10] Andreas Wagner. Robustness and evolvability: a paradox resolved. Proceedings of the Royal Society of London B: Biological Sciences, 275(1630):91â€“100, 2008.
[11] Andrew White and Mark Transtrum. Limitations of model-based experimental design in systems biology. Bulletin of the American Physical Society, 59, 2014.
28

