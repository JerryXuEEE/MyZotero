{"indexedPages":45,"totalPages":45,"version":"243","text":"Article\n\nLinking Connectivity, Dynamics, and Computations in Low-Rank Recurrent Neural Networks\n\nHighlights\nd We study network models characterized by minimal connectivity structures\n\nAuthors\nFrancesca Mastrogiuseppe, Srdjan Ostojic\n\nd For such models, low-dimensional dynamics can be directly inferred from connectivity\nd Computations emerge from distributed and mixed representations\nd Implementing speciﬁc tasks yields predictions linking connectivity and computations\n\nCorrespondence\nsrdjan.ostojic@ens.fr\nIn Brief\nNeural recordings show that cortical computations rely on low-dimensional dynamics over distributed representations. How are these generated by the underlying connectivity? Mastrogiuseppe et al. use a theoretical approach to infer lowdimensional dynamics and computations from connectivity and produce predictions linking connectivity and functional properties of neurons.\n\nMastrogiuseppe & Ostojic, 2018, Neuron 99, 609–623 August 8, 2018 ª 2018 Elsevier Inc. https://doi.org/10.1016/j.neuron.2018.07.003\n\nNeuron\nArticle\n\nLinking Connectivity, Dynamics, and Computations in Low-Rank Recurrent Neural Networks\nFrancesca Mastrogiuseppe1,2 and Srdjan Ostojic1,3,* 1Laboratoire de Neurosciences Cognitives, INSERM U960, E´ cole Normale Supe´ rieure - PSL Research University, 75005 Paris, France 2Laboratoire de Physique Statistique, CNRS UMR 8550, E´ cole Normale Supe´ rieure - PSL Research University, 75005 Paris, France 3Lead Contact *Correspondence: srdjan.ostojic@ens.fr https://doi.org/10.1016/j.neuron.2018.07.003\n\nSUMMARY\nLarge-scale neural recordings have established that the transformation of sensory stimuli into motor outputs relies on low-dimensional dynamics at the population level, while individual neurons exhibit complex selectivity. Understanding how low-dimensional computations on mixed, distributed representations emerge from the structure of the recurrent connectivity and inputs to cortical networks is a major challenge. Here, we study a class of recurrent network models in which the connectivity is a sum of a random part and a minimal, low-dimensional structure. We show that, in such networks, the dynamics are low dimensional and can be directly inferred from connectivity using a geometrical approach. We exploit this understanding to determine minimal connectivity required to implement speciﬁc computations and ﬁnd that the dynamical range and computational capacity quickly increase with the dimensionality of the connectivity structure. This framework produces testable experimental predictions for the relationship between connectivity, low-dimensional dynamics, and computational features of recorded neurons.\nINTRODUCTION\nUnderstanding the relationship between synaptic connectivity, neural activity, and behavior is a central endeavor of neuroscience. Networks of neurons encode incoming stimuli in terms of electrical activity and transform this information into decisions and motor actions through synaptic interactions, thus implementing computations that underlie behavior. Reaching a simple, mechanistic grasp of the relation between connectivity, activity, and behavior is, however, highly challenging. Cortical networks, which are believed to constitute the fundamental computational units in the mammalian brain, consist of thousands of neurons that are highly inter-connected\n\nthrough recurrent synapses. Even if one were able to experimentally record the activity of every neuron and the strength of each synapse in a behaving animal, understanding the causal relationships between these quantities would remain a daunting challenge because an appropriate conceptual framework is currently lacking (Gao and Ganguli, 2015). Simpliﬁed, computational models of neural networks provide a test bed for developing such a framework. In computational models and trained artiﬁcial neural networks, the strengths of all synapses and the activity of all neurons are known, yet an understanding of the relation between connectivity, dynamics, and input-output computations has been achieved only in very speciﬁc cases (e.g., Hopﬁeld (1982); Ben-Yishai et al. (1995); Wang (2002)).\nOne of the most popular and best-studied classes of network models is based on fully random recurrent connectivity (Sompolinsky et al., 1988; Brunel, 2000; van Vreeswijk and Sompolinsky, 1996). Such networks display internally generated irregular activity that closely resembles spontaneous cortical patterns recorded in vivo (Shadlen and Newsome, 1998). However, randomly connected recurrent networks display only very stereotyped responses to external inputs (Rajan et al., 2010), can implement only a limited range of input-output computations, and their spontaneous dynamics are typically high dimensional (Williamson et al., 2016). To implement more elaborate computations and low-dimensional dynamics, classical network models rely instead on highly structured connectivity, in which every neuron belongs to a distinct cluster and is selective to only one feature of the task (e.g., Wang (2002); Amit and Brunel (1997); Litwin-Kumar and Doiron (2012)). Actual cortical connectivity appears to be neither fully random nor fully structured (Harris and Mrsic-Flogel, 2013), and the activity of individual neurons displays a similar mixture of stereotypy and disorder (Rigotti et al., 2013; Mante et al., 2013; Churchland and Shenoy, 2007). To take these observations into account and implement generalpurpose computations, a large variety of functional approaches have been developed for training recurrent networks and designing appropriate connectivity matrices (Hopﬁeld, 1982; Jaeger and Haas, 2004; Maass et al., 2007; Sussillo and Abbott, 2009; Eliasmith and Anderson, 2004; Boerlin et al., 2013; Pascanu et al., 2013; Martens and Sutskever, 2011). A uniﬁed conceptual picture of how connectivity determines dynamics and\n\nNeuron 99, 609–623, August 8, 2018 ª 2018 Elsevier Inc. 609\n\ncomputations is, however, currently missing (Barak, 2017; Sussillo, 2014).\nRemarkably, albeit developed independently and motivated by different goals, several of the functional approaches for designing connectivity appear to have reached similar solutions (Hopﬁeld, 1982; Jaeger and Haas, 2004; Sussillo and Abbott, 2009; Eliasmith and Anderson, 2004; Boerlin et al., 2013), in which the implemented computations do not determine every single entry in the connectivity matrix but instead rely on a speciﬁc type of minimal, low-dimensional structure, so that in mathematical terms the obtained connectivity matrices are low rank. In classical Hopﬁeld networks (Hopﬁeld, 1982; Amit et al., 1985), a rank-one term is added to the connectivity matrix for every item to be memorized, and each of these terms ﬁxes a single dimension, i.e., row/column combination, of the connectivity matrix. In echo state (Jaeger and Haas, 2004; Maass et al., 2007) and FORCE learning (Sussillo and Abbott, 2009), and similarly within the Neural Engineering Framework (Eliasmith and Anderson, 2004), computations are implemented through feedback loops from readout units to the bulk of the network. Each feedback loop is mathematically equivalent to adding a rank-one component and ﬁxing a single row/column combination of the otherwise random connectivity matrix. In the predictive spiking theory (Boerlin et al., 2013), the requirement that information is represented efﬁciently leads again to a connectivity matrix with similar low-rank form. Taken together, the results of these studies suggest that a minimal, low-rank structure added on top of random recurrent connectivity may provide a general and unifying framework for implementing computations in recurrent networks.\nBased on this observation, here we study a class of recurrent networks in which the connectivity is a sum of a structured, low-rank part and a random part. We show that in such networks, both spontaneous and stimulus-evoked activity are low-dimensional and can be predicted from the geometrical relationship between a small number of high-dimensional vectors that represent the connectivity structure and the feedforward inputs. This understanding of the relationship between connectivity and network dynamics allows us to directly design minimal, low-rank connectivity structures that implement speciﬁc computations. We focus on four tasks of increasing complexity, starting with basic binary discrimination and ending with context-dependent evidence integration (Mante et al., 2013). We ﬁnd that the dynamical repertoire of the network increases quickly with the dimensionality of the connectivity structure, so that rank-two connectivity structures are already sufﬁcient to implement complex, context-dependent tasks (Mante et al., 2013; Saez et al., 2015). For each task, we illustrate the relationship between connectivity, low-dimensional dynamics, and the performed computation. In particular, our framework naturally captures the ubiquitous observation that single-neuron responses are highly heterogeneous and mixed (Rigotti et al., 2013; Mante et al., 2013; Churchland and Shenoy, 2007; Machens et al., 2010), while the dimensionality of the dynamics underlying computations is low and increases with task complexity (Gao and Ganguli, 2015). Crucially, for each task, our framework produces experimentally testable predictions that directly relate connectivity, the dominant di-\n\nmensions of the dynamics, and the computational features of individual neurons.\n\nRESULTS\n\nWe studied a class of models that we call low-rank recurrent networks. In these networks, the connectivity matrix was given by a sum of an uncontrolled, random matrix and a structured, controlled matrix P. The structured matrix P was low-rank, i.e., it consisted only of a small number of independent rows and columns, and its entries were assumed to be weak (of order 1=N, where N is the number of units in the network). We considered P moreover to be ﬁxed and known, and uncorrelated with the random part gc, which was considered unknown except for its statistics (mean 0, variance g2=N). As in classical models, the networks consisted of N ﬁring rate units with a sigmoid inputoutput transfer function (Sompolinsky et al., 1988; Sussillo and Abbott, 2009):\n\nXN x_iðtÞ = À xiðtÞ + JijfðxjðtÞÞ + Ii;\nj=1\n\n(Equation 1)\n\nwhere xiðtÞ is the total input current to unit i, Jij = gcij + Pij is the connectivity matrix, fðxÞ = tanhðxÞ is the current-to-rate transfer function, and Ii is the external, feedforward input to unit i.\nTo connect with the previous literature and introduce the methods that underlie our results, we start by describing the spontaneous dynamics ðIi = 0Þ in a network with a unit-rank structure P. We then turn to the response to external inputs, the core of our results that we exploit to demonstrate how low-rank networks can implement four tasks of increasing complexity.\n\nOne-Dimensional Spontaneous Activity in Networks with Unit-Rank Structure We started with the simplest possible type of low-dimensional connectivity, a matrix P with unit rank (Figure 1A). Such a matrix is speciﬁed by two N-dimensional vectors m = fmig and n = fnjg, which fully determine all its entries. Every column in this matrix is a multiple of the vector m, and every row is a multiple of the vector n, so that the individual entries are given by\n\nPij\n\n=\n\nmi nj N\n\n:\n\n(Equation 2)\n\nWe will call m and n, respectively, the right- and left-connectivity vectors (as they correspond to the right and left eigenvectors of the matrix P, see STAR Methods), and we consider them arbitrary but ﬁxed and uncorrelated with the random part of the connectivity. As we will show, the spontaneous network dynamics can be directly understood from the geometrical arrangement of the vectors m and n.\nIn absence of structured connectivity, the dynamics are determined by the strength g of the random connectivity: for g < 1, the activity in absence of inputs decays to zero, while for g > 1 it displays strong, chaotic ﬂuctuations (Sompolinsky et al., 1988). Our ﬁrst aim was to understand how the interplay between the ﬁxed,\n\n610 Neuron 99, 609–623, August 8, 2018\n\nA\n\nStationary\n\nChaotic\n\nC\n\nB\n\nStruct. stationary\n\nStruct. chaot.\n\nD\n\nHom.\n\n0\n\nchaotic\n\nFigure 1. Spontaneous Activity in Random Networks with Unit-Rank Connectivity Structure (A) The recurrent network model, whose connectivity matrix consists of the sum of a random (gray) and of a structured unit-rank (colored) component. (B) Left: dynamical regimes of the network activity as function of the structure connectivity strength mT n=N and the random strength g. Gray areas: bistable activity; red: chaotic activity. Side panels: samples of dynamics from ﬁnite networks simulations (parameters indicated by colored dots in the phase diagram). (C and D) Activity statistics as the random strength g is increased, and the structure strength is ﬁxed to 2.2 (dashed line in B). (C) Activity along the vector m, as quantiﬁed by k (Equation 3). Blue (resp. red) lines: theoretical prediction for stationary (resp. chaotic) dynamics. (D) Activity variance due to random connectivity. Blue and pink lines: static heterogeneity; red: temporal variance that quantiﬁes chaotic activity. Dots: simulations of ﬁnite-size networks. See STAR Methods for details.\n\nlow-rank part and the random part of the connectivity shapes the spontaneous activity in the network.\nOur analysis of network dynamics relies on an effective, statistical description that can be mathematically derived if the network is large and the low-dimensional part of the connectivity is weak (i.e., if Pij scales inversely with the number of units N in the network as in Equation 2). Under those assumptions, the activity of each unit can be described in terms of the mean and variance of the total input it receives. Dynamical equations for these quantities can be derived by extending the classical dynamical mean-ﬁeld theory (Sompolinsky et al., 1988). This theory effectively leads to a low-dimensional description of network dynamics in terms of equations for a couple of macroscopic quantities. Full details of the analysis are provided in the STAR Methods; here, we focus only on the main results.\nThe central ingredient of the theory is an equation for the average equilibrium input mi to unit i:\n\n1 XN Â Ã mi = kmi; where k = N j = 1 nj fj :\n\n(Equation 3)\n\nThe scalar quantity k represents the overlap between the left-\nconnectivity vector n and the N-dimensional vector ½f = f½fjg that describes the mean ﬁring activity of the network (½fj is the ﬁring rate of unit j averaged over different realizations of the\n\nrandom component of the connectivity, and depends implicitly\n\non k). The overlap k therefore quantiﬁes the degree of structure\n\nalong the vector n in the activity of the network. If k > 0, the equi-\n\nlibrium activity of each neuron is correlated with the correspond-\n\ning component of the vector n, while k = 0 implies no such struc-\n\nture is present. The overlap k is the key macroscopic quantity\n\ndescribing the network dynamics, and our theory provides equa-\n\ntions specifying its dependence on network parameters.\n\nIf one represents the network activity as a point in the NÀ\n\ndimensional state space where every dimension corresponds\n\nto the activity of a single unit, Equation 3 shows that the\n\nstructured part of the connectivity induces a one-dimensional\n\norganization of the spontaneous activity along the vector m.\n\nThis one-dimensional organization, however, emerges only if\n\nthe overlap k does not vanish. As the activity of the network is\n\norganized along the vector m, and k quantiﬁes the projection\n\nof the activity onto the vector n, non-vanishing values of k require\n\na non-vanishing given by mT n=N\n\novePrlap between vectors m and = jmjnj=N, directly quantiﬁes\n\nn. This overlap, the strength of\n\nthe structure in the connectivity. The connectivity structure\n\nstrength mT n=N and the activity structure strength k are there-\n\nfore directly related, but in a highly non-linear manner. If the\n\nconnectivity structure is weak, the network only exhibits\n\nhomogeneous, unstructured activity corresponding to k = 0 (Fig-\n\nure 1B, blue). If the connectivity structure is strong, structured\n\nNeuron 99, 609–623, August 8, 2018 611\n\nheterogeneous activity emerges ðk > 0Þ, and the activity of the network at equilibrium is organized in one dimension along the vector m (Figures 1B, green, and 1C), while the random connectivity induces additional heterogeneity along the remaining N À 1 directions. Note that because of the symmetry in the speciﬁc input-output function we use, when a heterogeneous equilibrium state exists, the conﬁguration with the opposite sign is an equilibrium state too, so that the network activity is bistable (for more general asymmetric transfer functions, this bistability is still present, although the symmetry is lost, see Figure S7).\nThe random part of the connectivity disrupts the organization of the activity induced by the connectivity structure through two different effects. The ﬁrst effect is that as the random strength g is increased, for any given realization of the random part of the connectivity, the total input to unit i will deviate more strongly from the expected mean mi (Figure 1D). As a consequence, the activity along the N À 1 directions that are orthogonal to m increases, resulting in a noisy input to individual neurons that smoothens the gain of the non-linearity. This effectively leads to a reduction of the overall structure in the activity as quantiﬁed by k (Figure 1C). A second, distinct effect is that increasing the random strength eventually leads to chaotic activity as in purely random networks. Depending on the strength of the structured connectivity, two different types of chaotic dynamics can emerge. If the disorder in the connectivity is much stronger than structure, the overlap k is zero (Figure 1C). As a result, the mean activity of all units vanishes and the dynamics consist of unstructured, NÀ dimensional temporal ﬂuctuations (Figure 1D), as in the classical chaotic state of fully random networks (Figure 1B, red). In contrast, if the strengths of the random and structured connectivity are comparable, a structured type of chaotic activity emerges, in which k > 0 so that the mean activity of different units is organized in one dimension along the direction m as shown by Equation 3, but the activity of different units now ﬂuctuates in time (Figure 1B, orange). As for structured static activity, in this situation the system is bistable as states with opposite signs of k always exist.\nThe phase diagram in Figure 1B summarizes the different types of spontaneous dynamics that can emerge as a function of the strength of structured and random components of the connectivity matrix. Altogether, the structured component of connectivity favors a one-dimensional organization of network activity, while the random component favors high-dimensional, chaotic ﬂuctuations. Particularly interesting activity emerges when the structure and disorder are comparable, in which case the dynamics show one-dimensional structure combined with high-dimensional temporal ﬂuctuations that can give rise to dynamics with very slow timescales (see Figure S6).\n\nTwo-Dimensional Activity in Response to an External Input We now turn to the response to an external, feedforward input (Figure 2A). At equilibrium, the total average input to unit i is the sum of a recurrent input kmi and the feedforward input Ii:\n\nmi = kmi + Ii;\n\nwhere\n\nk\n\n=\n\n1 N\n\nXN Â Ã nj fj :\nj=1\n\n(Equation 4)\n\nTransient, temporal dynamics close to this equilibrium are obtained by including temporal dependencies in k and Ii (see STAR Methods; Equation 102).\nFigure 2B illustrates the response of the network to a step input. The response of individual units is highly heterogeneous, different units showing increasing, decreasing, or multi-phasic responses. While every unit responds differently, the theory predicts that, at the level of the N-dimensional state space representing the activity of the whole population, the trajectory of the activity lies on average on the two-dimensional plane spanned by the right-connectivity vector m and the vector I = fIig that corresponds to the pattern of external inputs (Figure 2B). Applying to the simulated activity a dimensionality reduction technique (see Cunningham and Yu [2014] for a recent review) such as principal-component analysis conﬁrms that the two dominant dimensions of the activity indeed lie in the m À I plane (Figure 2C), while the random part of connectivity leads to additional activity in the remaining N À 2 directions that grows quickly with the strength of random connectivity g (see Figure S3). This approach therefore directly links the connectivity in the network to the emerging low-dimensional dynamics and shows that the dominant dimensions of activity are determined by a combination of feedforward inputs and connectivity (Wang et al., 2018).\nThe contribution of the connectivity vector m to the twodimensional trajectory of activity is quantiﬁed by the overlap k between the network activity ½f and the left-connectivity vector n (Equation 4). If k = 0, the activity trajectory is one dimensional and simply propagates the pattern of feedforward inputs. This is in particular the case for fully random networks. If ks0, the network response is instead a non-trivial two-dimensional combination of the input and connectivity structure patterns. In general, the value of k, and therefore the organization of network activity, depends on the geometric arrangement of the input vector I with respect to the connectivity vectors m and n, as well as on the strength of the random component of the connectivity g.\nAs the neural activity lies predominantly in the m À I plane, a non-vanishing k, together with non-trivial two-dimensional activity, is obtained when the vector n has a non-zero component in the m À I plane. Two qualitatively different input-output regimes can be distinguished. The ﬁrst one is obtained when the connectivity vectors m and n are orthogonal to each other (Figure 2D, left and center). In that case, the overlap between them is zero, and the spontaneous activity in the network bears no sign of the underlying connectivity structure. Adding an external input can, however, reveal this connectivity structure and generate nontrivial two-dimensional activity if the input vector I has a nonzero overlap with the left-connectivity vector n. In such a situation, the vector n picks up the component of the activity along the feedforward input direction I. This leads to a nonzero overlap k, which in turn implies that the network activity will have a component along the right-connectivity vector m. Increasing the external input along the direction of n will therefore progressively increase the response along m (Figure 2D, center), leading to a two-dimensional output.\nA second, qualitatively different input-output regime is obtained when the connectivity vectors m and n have a strong enough overlap along a common direction (Figure 2D, right).\n\n612 Neuron 99, 609–623, August 8, 2018\n\nA\n\nB\n\nC\n\nD\n\nFigure 2. External Inputs Generate Two-Dimensional Activity in Random Networks with Unit-Rank Structure (A) The pattern of external inputs can be represented by an N-dimensional vector I = fIig, where Ii is the input to unit i. (B) Transient dynamics in response to a step input along I in a sample network of N = 3500 units. Left: activity traces for ﬁve units. Right: projections of the population trajectory onto the plane deﬁned by the right-connectivity vector m and the input vector I. Light trace: theoretical prediction. Dark traces: simulations. (C) Principal-component analysis (PCA) of the average activity trajectory. Bottom: fraction of SD explained by successive PCs. Top: correlation between PCs and the vectors m and I. The direction of the projections onto the m À I plane of the two top PCs e1 and e2 are represented in (B). See also Figure S3. (D) The activity k along m is determined by the geometrical arrangement of the vector I and the connectivity vectors m and n. Three different cases are illustrated: (left) I, m, and n mutually orthogonal; (center) m and n mutually orthogonal, but I has a non-zero overlap with n; (right) m and n have non-zero overlap, leading to bistable activity in absence of inputs. Increasing the external input along n suppresses one of the two stable states. Continuous lines: theoretical predictions. Dots: simulations. See STAR Methods for details.\n\nAs already shown in Figure 1, an overlap larger than unity between m and n induces bistable, structured spontaneous activity along the dimension m. Adding an external input along the vector n increases the activity along m but also eventually suppresses one of the bistable states. Large external inputs along the n direction therefore reliably set the network into a state in which the activity is a two-dimensional combination of the input direction and the connectivity direction m. This can lead to a strongly non-linear input-output transformation if the network was initially set in the state that lies on the opposite branch (Figure 2D, right).\nAn additional effect of an external input is that it generally tends to suppress chaotic activity present when the random part of connectivity is strong (Figures S3 and S4). This suppression occurs irrespectively of the speciﬁc geometrical conﬁguration between the input I and connectivity vectors m and n and therefore independently of the two input-output regimes described above. Altogether, external inputs suppress both chaotic and bistable dynamics (Figure S4) and therefore always decrease the amount of variability in the dynamics (Churchland et al., 2010; Rajan et al., 2010).\nIn summary, external, feedforward inputs to a network with unit-rank connectivity structure in general lead to two-dimen-\n\nsional trajectories of activity. The elicited trajectory depends on the geometrical arrangement of the pattern of inputs with respect to the connectivity vectors m and n, which play different roles. The right-connectivity vector m determines the output pattern of network activity, while the left-connectivity vector n instead selects the inputs that give rise to outputs along m. An output structured along m can be obtained when n selects recurrent inputs (non-zero overlap between n and m) or when it selects external inputs (non-zero overlap between n and I).\n\nHigher-Rank Structure Leads to a Rich Dynamical Repertoire This far we focused on unit-rank connectivity structure, but our framework can be directly extended to higher-rank structure. A more general structured component of rank r ( N can be written as a superposition of r independent unit-rank terms\n\nPij\n\n=\n\nmði 1Þnðj 1Þ N\n\n+\n\n.\n\n+\n\nmði rÞnðj rÞ; N\n\n(Equation 5)\n\nand is in principle characterized by 2r vectors mðkÞ and nðkÞ. In such a network, the average dynamics lie in the ðr + 1Þ-dimensional\n\nNeuron 99, 609–623, August 8, 2018 613\n\nA\n\nB\n\nC\n\nF\n\nD\n\nE\n\nG\n\nFigure 3. Implementing a Simple Go-Nogo Discrimination Task with a Unit-Rank Connectivity Structure (A) A linear readout is added to the network, with randomly chosen weights wi. The stimuli are represented by random input patterns IA and IB. The task consists in producing an output in response to stimulus A, but not B. The simplest unit-rank structure that implements the task is given by m = w and n = IA. (B) Response of a sample network to the Go (blue) and Nogo (green) inputs. Activity traces for ﬁve units.\n(C) Projections of the population trajectories onto the planes predicted to contain the dominant part of the dynamics. Gray: predicted trajectory. Colored traces:\nsimulations.\n(D) Linear regression coefﬁcients for the Go and the Nogo stimuli. Every dot corresponds to a network unit.\n(E) Readout dynamics for the Go (blue) and the Nogo (green) stimulus.\n(F) Average connectivity strength as a function of the product between the coefﬁcients of the ﬁrst PC. Every dot corresponds to a pair of units. (G) Generalization properties of the network. We select two Go stimuli IA1 and IA2 , and we set n = IA1 + IA2 . We build the input pattern as a normalized mixture of the two preferred patterns, and we gradually increase the component along IA1 . Continuous lines: theoretical predictions. Dots: simulations. See STAR Methods for details.\n\nsubspace spanned by the r right-connectivity vectors mðkÞ; k = 1; .; r and the input vector I, while the left connectivity vectors nðkÞ select the inputs ampliﬁed along the corresponding dimension mðkÞ. The details of the dynamics will in general depend on the geometrical arrangement of these 2r vectors among themselves and with respect to the input pattern. The number of possible conﬁgurations increases quickly with the structure rank, leading to a wide repertoire of dynamical states that includes continuous attractors (Figure S5) and sustained oscillatory activity (Figure S8). In the remainder of this manuscript, we will explore only the rank-two case.\nImplementing a Simple Discrimination Task Having developed an intuitive, geometric understanding of how a given unit-rank connectivity structure determines the lowdimensional dynamics in a network, we now reverse our approach to ask how a given computation can be implemented by choosing appropriately the structured part of the connectivity. We start with the computation underlying one of the most basic and most common behavioral tasks, Go-Nogo stimulus discrimination. In this task, an animal has to produce a speciﬁc motor\n\noutput, e.g., press a lever or lick a spout, in response to a stimulus IA (the Go stimulus), and ignore another stimuli IB (Nogo stimuli). This computation can be implemented in a straightforward way in a recurrent network with a unit-rank connectivity structure. While such a simple computation does not in principle require a recurrent network, the implementation we describe here illustrates in a transparent manner the relationship between connectivity, dynamics, and computations in low-rank networks and leads to non-trivial and directly testable experimental predictions. It also provides the basic building block for more complex tasks, which we turn to in the next sections.\nWe model the sensory stimuli as random patterns of external inputs to the network, so that the two stimuli are represented by two ﬁxed, randomly chosen N-dimensional vectors IA and IB. To model the motor response, we supplement the network with an output unit, which produces a linear readout\n1P zðtÞ = N iwi4ðxiðtÞÞ of network activity (Figure 3A). The readout weights wi are chosen randomly and form also a ﬁxed N-dimensional vector w. The task of the network is to produce an output that is selective to the Go stimulus: the readout z at the end of\n\n614 Neuron 99, 609–623, August 8, 2018\n\nstimulus presentation needs to be non-zero for the input pattern IA that corresponds to the Go stimulus, and zero for the other input IB.\nThe two N-dimensional vectors m and n that generate the appropriate unit-rank connectivity structure to implement the task can be directly determined from our description of network dynamics. As shown in Equation 4 and Figure 2, the response of the network to the input pattern I is in general two-dimensional and lies in the plane spanned by the vectors m and I. The output unit will therefore produce a non-zero readout only if the readout vector w has a non-vanishing overlap with either m or I. As w is assumed to be uncorrelated, and therefore orthogonal, to all input patterns, this implies that the connectivity vector m needs to have a non-zero overlap with the readout vector w for the network to produce a non-trivial output. This output will depend on the amount of activity along m, quantiﬁed by the overlap k. As shown in Figure 2, the overlap k will be non-zero only if n has a non-vanishing overlap with the input pattern. Altogether, implementing the Go-Nogo task therefore requires that the right-connectivity vector m is correlated with the readout vector w, and that the left-connectivity vector n is correlated with the Go stimulus IA.\nChoosing m = w and n = IA therefore provides the simplest unit-rank connectivity that implements the desired computation. Figure 3 illustrates the activity in the corresponding network. At the level of individual units, by construction both stimuli elicit large and heterogeneous responses (Figure 3B) that display mixed selectivity (Figure 3D). As predicted by the theory, the response to stimulus B is dominantly one-dimensional and organized along the input direction IB, while the response to stimulus A is two-dimensional and lies in the plane deﬁned by the rightconnectivity vector m and the input direction IA (Figure 3C). The readout from the network corresponds to the projection of the activity onto the m direction and is non-zero only in response to stimulus A (Figure 3E), so that the network indeed implements the desired Go-Nogo task. Our framework therefore allows us to directly link the connectivity, the low-dimensional dynamics, and the computation performed by the network and leads to two experimentally testable predictions. The ﬁrst one is that performing a dimensionality reduction separately on responses to the two stimuli should lead to larger dimensionality of the trajectories in response to the Go stimulus. The second prediction is that for the Go stimulus, the dominant directions of activity depend on the recurrent connectivity in the network, while for the Nogo stimulus they do not. More speciﬁcally, for the activity elicited by the Go stimulus, the dominant principal components are combinations of the input vector IA and right-connectivity vector m. Therefore, if two neurons have large principal-component weights, they are expected to also have large m weights and therefore stronger mutual connections than average (Figure 3F, top). In contrast, for the activity elicited by the Nogo stimulus, the dominant principal components are determined solely by the feedforward input, so that no correlation between dominant PC weights and recurrent connectivity is expected (Figure 3F, bottom). This prediction can in principle be directly tested in experiments analogous to Ko et al. (2011), where calcium imaging in behaving animals is combined with measurements of connectivity in a subset of recorded neurons. Note that in this setup very\n\nweak structured connectivity is sufﬁcient to implement computations, so that the expected correlations may be weak if the random part of the connectivity is strong (see Figure S5).\nThe unit-rank connectivity structure forms the fundamental scaffold for the desired input-output transform. The random part of the connectivity adds variability around the target output and can induce additional chaotic ﬂuctuations. Summing the activity of individual units through the readout unit, however, averages out this heterogenpeﬃiﬃtﬃy, so that the readout error decreases with network size as 1= N (Figure S5). The present implementation is therefore robust to noise and has desirable computational properties in terms of generalization to novel stimuli. In particular, it can be extended in a straightforward way to the detection of a category of Go stimuli, rather than a single stimulus (Figure 3G).\nDetection of a Noisy Stimulus We now turn to a slightly more complex task: integration of a continuous, noisy stimulus. In contrast to the previous discrimination task, where the stimuli were completely different (i.e., orthogonal), here we consider a continuum of stimuli that differ only along the intensity of a single feature, such as the coherence of a random-dot kinetogram (Newsome et al., 1989). In a given stimulus presentation, this feature moreover ﬂuctuates in time. We therefore represent each stimulus as cðtÞI, where I is a ﬁxed, randomly chosen input vector that encodes the relevant stimulus feature, and cðtÞ is the amplitude of that feature. We consider a Go-Nogo version of this task, in which the network has to produce an output only if the average value of c is larger than a threshold (Figure 4A).\nAs for the basic discrimination task, the central requirements for a unit-rank network to implement this task are that the right-connectivity vector m is correlated with the readout vector w, and the left-connectivity vector n is correlated with the input pattern I. A key novel requirement in the present task is, however, that the response needs to be non-linear to produce the Go output when the strength of the input along I is larger than the threshold. As shown in Figure 2D, such a non-linearity can be obtained when the left- and right-connectivity vectors n and m have a strong enough overlap. We therefore add a shared component to m and n along a direction orthogonal to both w and I. In that setup, if the stimulus intensity c is low, the network will be in a bistable regime, in which the activity along the direction m can take two distinct values for the same input (Figure 2D, right). Assuming that the lower state represents a Nogo output, and that the network is initialized in this state at the beginning of the trial, increasing the stimulus intensity c above a threshold will lead to a sudden jump, and therefore a non-linear detection of the stimulus. Because the input amplitude ﬂuctuates noisily in time, whether such a jump occurs depends on the integrated estimate of the stimulus intensity. The timescale over which this estimate is integrated is determined by the time constant of the effective exponential ﬁlter describing the network dynamics. In our unit-rank network, this time constant is set by the connectivity strength, i.e., the overlap between the left- and right-connectivity vectors m and n, which also determines the value of the threshold. Arbitrarily large timescales can be obtained by adjusting this overlap close to the bifurcation value, in which case the threshold becomes arbitrarily small (Figure 4F). In this section,\n\nNeuron 99, 609–623, August 8, 2018 615\n\nA F\nD B\nG\n\nC\n\nE\n\nH\n\nFigure 4. Implementing a Noisy Detection Task with a Unit-Rank Connectivity Structure (A) The network is given a noisy input cðtÞ along a ﬁxed, random pattern of inputs I. The task consists in producing an output if the average input c is larger than a threshold q. (B) Dynamics in a sample network. Top: noisy input and threshold. Bottom: activity traces for four units and two different noise realizations in the stimulus, leading to a Go (dark blue) and a Nogo (light blue) output. (C) Readout dynamics for the two stimuli. (D) Projections of the population trajectory onto the plane deﬁned by the right-connectivity vector m and the input vector I. Left: single-trial trajectories corresponding to (B). Right: trial-averaged trajectories, for Go (top) and Nogo (bottom) outputs, and different values of the mean input c. Stars indicate correct responses. (E) Left: linear regression coefﬁcients for the input amplitude and the decision outcome. Every dot corresponds to a network unit. Right: correlation coefﬁcients between the vectors m and I and the input and choice regression axes (see STAR Methods). Projection directions of the two input and choice regression axes onto the m À I plane are shown in (D). (F) Detection threshold (dashed) and timescale of the effective exponential ﬁlter (full line) for increasing values of the structure strength. (G) Psychometric curve. The shaded area indicates the bistable region. (H) Average connectivity strength as a function of the product of the linear regression coefﬁcients for the choice variable. Every dot corresponds to a pair of network units. See STAR Methods for details.\n\nwe ﬁx the structure strength so that the threshold is set to 0.5, which corresponds to an integration timescale of the order of the time constant of individual units.\nFigure 4 illustrates the activity in an example implementation of this network. In a given trial, as the stimulus is noisy, the activity of the individual units ﬂuctuates strongly (Figure 4B). Our theory predicts that the population trajectory on average lies in the plane deﬁned by the connectivity vector m and the input pattern I (Figure 4D). Activity along the m direction is picked up by the readout, and its value at the end of stimulus\n\npresentation determines the output (Figure 4C). Because of the bistable dynamics in the network, whether the m direction is explored, and an output produced, depends on the speciﬁc noisy realization of the stimulus. Stimuli with an identical average strength can therefore lead to either two-dimensional trajectories of activity and Go responses or one-dimensional trajectories of activity corresponding to Nogo responses (Figure 4D). The probability of generating an output as function of stimulus strength follows a sigmoidal psychometric curve that reﬂects the underlying bistability (Figure 4G). Note that the\n\n616 Neuron 99, 609–623, August 8, 2018\n\nA\n\nC\n\nE\n\nF B\n\nD\n\nG\n\nFigure 5. Implementing a Context-Dependent Go-Nogo Discrimination Task with a Rank-Two Connectivity Structure (A) As in Figure 3, two stimuli A and B are presented to the network. The task consists in producing an output in response to the Go stimulus, which is determined by the contextual cue (A in context A, B in context B), modeled as inputs along random directions IctxA and IctxB. (B) Inputs along the overlap direction between the left- and the right-connectivity vectors modulate the response threshold of the network (see also Figure S5). (C) Dynamics in a sample network in response to the stimulus A. Top: stimulus and contextual input. Bottom: activity for ﬁve units in context A (crimson) and B (pink). (D) Readout dynamics in the two contexts. (E) Projections of the average population trajectories onto the planes spanned by vectors w, IA and IB. (F) Network performance in the two contexts. (G) Average connectivity strength between pairs of units as a function of the product between the regression coefﬁcients for context. Every dot corresponds to a pair of network units. See STAR Methods for details.\n\nbistability is not clearly apparent on the level of individual units. In particular, the activity of individual units is always far from saturation, as their inputs are distributed along a zero-centered Gaussian (Equation 4).\nThe responses of individual units are strongly heterogeneous and exhibit mixed selectivity to stimulus strength and output choice (Figure 4E). A popular manner to interpret such activity at the population level is a targeted dimensional reduction approach, in which input and choice dimensions are determined through regression analyses (Mante et al., 2013). As expected from our theoretical analysis, the two dimensions obtained through regression are closely related to m and I; in particular, the choice dimension is highly correlated with the right-connectivity vector m (Figure 4E). As a result, the plane in which network activity dominantly lies corresponds to the plane deﬁned by the choice and the input dimensions (Figure 4D). Our framework therefore directly links recurrent connectivity and effective output choice direction through the low-dimensional dynamics.\n\nA resulting experimentally testable prediction is that neurons with strong choice regressors have stronger mutual connections (Figure 4H).\nA Context-Dependent Discrimination Task We next consider a context-dependent discrimination task, in which the relevant response to a stimulus depends on an additional, explicit contextual cue. Speciﬁcally, we focus on the task studied in Saez et al. (2015) where in one context (referred to as context A), the stimulus A requires a Go output, and the stimulus B a Nogo, while in the other context (referred to as context B), the associations are reversed (Figure 5A). This task is a direct extension of the basic binary discrimination task introduced in Figure 3; yet it is signiﬁcantly more complex as it represents a hallmark of cognitive ﬂexibility: a non-linearly separable, XOR-like computation that a single-layer feedforward network cannot solve (Rigotti et al., 2010; Fusi et al., 2016). We will show that this task can be implemented in a rank-two recurrent\n\nNeuron 99, 609–623, August 8, 2018 617\n\nnetwork that is a direct extension of the unit-rank network used for the discrimination task in Figure 4.\nThis context-dependent task can be seen as a combination of two basic, opposite Go-Nogo discriminations, each of which can be independently implemented by a unit-rank structure with the right-connectivity vector m correlated to the readout, and the left-connectivity vector correlated to the Go input (IA for context A, IB for context B). Combining two such unit-rank structures, with left-connectivity vectors nð1Þ and nð2Þ correlated respectively with IA and IB, leads to a rank-two connectivity structure that serves as a scaffold for the present task. The cues for context A and B are represented by additional inputs along random vectors IctxA and IctxB, presented for the full length of the trial (Remington et al., 2018) (Figure 5C). These inputs are the only contextual information incorporated in the network. In particular, the readout vector w is ﬁxed and independent of the context (Mante et al., 2013). Crucially, since the readout w needs to produce an output for both input stimuli, both right-connectivity vectors mð1Þ and mð2Þ need to be correlated with it.\nThe key requirement for implementing context-dependent discrimination is that each contextual input effectively switches off the irrelevant association. To implement this requirement, we rely on the same non-linearity as for the noisy discrimination task, based on the overlap between the left- and right-connectivity vectors (Figure 2D). We however exploit an additional property, which is that the threshold of the non-linearity (i.e., the position of the transition from a bistable to a mono-stable region in Figure 2D) can be controlled by an additional modulatory input along the overlap direction between m and n (Figures 5B and S4). Such a modulatory input acts as an effective offset for the bistability at the macroscopic, population level (see Equation 153 in STAR Methods). A stimulus of a given strength (e.g., unit strength in Figure 5B) may therefore induce a transition from the lower to the upper state (Figure 5B, top), or no transition (Figure 5B bottom) depending on the strength of the modulatory input that sets the threshold value. While in the noisy discrimination task, the overlap between m and n was chosen in an arbitrary direction, in the present setting, we take the overlaps between each pair of left- and right-connectivity vectors to lie along the direction of the corresponding contextual input (i.e., mð1Þ and nð1Þ overlap along IctxA, mð2Þ and nð2Þ along IctxB), so that contextual inputs directly modulate the threshold of the non-linearity. The ﬁnal rank-two setup is described in detail in the STAR Methods.\nFigure 5 illustrates the activity in an example of the resulting network implementation. The contextual cue is present from the very beginning of the trial and effectively sets the network in a context-dependent initial state (Figure 5C) that corresponds to the lower of the two bistable states. The low-dimensional response of the network to the following stimulus is determined by this initial state and the sustained contextual input. If the cue for context A is present, stimulus A leads to the crossing of the non-linearity, a transition from the lower to the upper state, and therefore a two-dimensional response in the plane determined by IA and w (Figure 5E, top left), generating a Go output (Figure 5D). In contrast, if the cue for context B is present, the threshold of the underlying non-linearity is increased in the direction of input IA (Figure 5B, bottom), so that the presentation of stimulus A does not induce a transition between the lower and\n\nupper states but leads only to a one-dimensional trajectory orthogonal to the readout, and therefore a Nogo response (Figure 5E, top right). The situation is totally symmetric in response to stimulus B (Figure 5E, bottom), so that contextual cues fully reverse the stimulus-response associations (Figure 5F). Overall, this context-dependent discrimination relies on strongly nonlinear interactions between the stimulus and contextual inputs, that on the connectivity level are implemented by overlaps between the connectivity vectors along the contextual inputs. A central, experimentally testable prediction of our framework is therefore that, if a network is implementing this computation, units with strong contextual selectivity have on average stronger mutual connections (Figure 5G).\nA Context-Dependent Evidence Integration Task We ﬁnally examine a task inspired by Mante et al. (2013) that combines context-dependent output and ﬂuctuating, noisy inputs. The stimuli now consist of superpositions of two different features A and B, and the strengths of both features ﬂuctuate in time during a given trial. In Mante et al. (2013), the stimuli were random dot kinetograms, and the features A and B corresponded to the direction of motion and color of these stimuli. The task consists in classifying the stimuli according to one of those features, the relevant one being indicated by an explicit contextual cue (Figure 6A).\nWe implemented a Go-Nogo version of the task, in which the output is required to be non-zero when the relevant feature is stronger than a prescribed threshold (arbitrarily set to 0.5). The present task is therefore a direct combination of the detection task introduced in Figure 4 and the context-dependent discrimination task of Figure 5, but the individual stimuli are now two dimensional, as they consist of two independently varied features A and B. In this task, a signiﬁcant additional difﬁculty is that on every trial the irrelevant feature needs to be ignored, even if it is stronger than the relevant feature (e.g., color coherence stronger than motion coherence on a motion-context trial).\nThis context-dependent evidence integration task can be implemented with exactly the same rank-two conﬁguration as the basic context-dependent discrimination in Figure 5, with contextual gating relying on the same non-linear mechanism as in Figure 5B. The contextual cue is presented throughout the trial (Figure 6B) and determines which of the features of the twodimensional stimulus leads to non-linear dynamics along the direction of connectivity vectors mð1Þ and mð2Þ (Figure 6D). These directions share a common component along the readout vector w, and the readout unit picks up the activity along that dimension. As a consequence, depending on the contextual cue, the same stimulus can lead to opposite outputs (Figure 6C). Altogether, in context A, the output is independent of the values of feature B, and conversely in context B (Figure 6E). The output therefore behaves as if it were based on two orthogonal readout directions, yet the readout direction is unique and ﬁxed, and the output relies instead on a context-dependent selection of the relevant input feature (Mante et al., 2013).\nAn important additional requirement in the present task with respect to the basic context-dependent integration is that the network needs to perform temporal integration to average out temporal ﬂuctuations in the stimulus. As illustrated in Figures\n\n618 Neuron 99, 609–623, August 8, 2018\n\nA\n\nB\n\nD\n\nE F\nC\n\nFigure 6. Implementing a Context-Dependent Evidence Accumulation Task Using Rank-Two Connectivity Structure (A) The stimuli consist of a superposition of two features cA and cB, which ﬂuctuate in time around mean values cA and cB. In every trial, a pair of contextual inputs determines the relevant input feature. The task consists in producing an output if the average strength of the relevant feature is larger than a threshold. (B) Dynamics in a sample network. Top: stimulus and contextual inputs. Bottom: activity of four units in contexts A (crimson) and B (pink). (C) Readout dynamics in the two contexts. (D) Average population trajectories projected onto the planes spanned by vectors w, IA and IB. Blue (resp. green) trajectories have been sorted according to the value of the strength of stimulus A (resp. B), and averaged across stimulus B (resp. A). (E) Network performance. Top row: probability of response as function of input strengths cA and cB (simulated data). Bottom: probability of response averaged over cB. Continuous line: theoretical prediction; dots: simulations. (F) Projection of the population activity onto the plane deﬁned by the orthogonal components of the vectors mA and mB and comparison with the underlying circular attractor (see STAR Methods). Trajectories are sorted by the strength of the relevant stimulus and averaged across the non-relevant one. The direction of the projections of the regression axes for choice and context are indicated in gray. See STAR Methods for details.\n\n6B and 6C, the network dynamics in response to stimuli indeed exhibit a slow timescale and progressively integrate the input. Strikingly, such slow dynamics do not require additional constraints on network connectivity; they are a direct consequence of the rank-two connectivity structure used for contextual gating (in fact the dynamics are already slow in the basic contextual discrimination task, see Figures 5C and 5D). More speciﬁcally, the symmetry between the two contexts implies that two sets of left- and right-connectivity vectors have identical overlaps (i.e. mð1ÞT nð1Þ = mð2ÞT nð2Þ). Without further constraints on the connectivity, such a symmetric conﬁguration leads to an emergence of a continuous line attractor, with the shape of a two-dimensional ring in the plane deﬁned by mð1Þ and mð2Þ (see STAR Methods and Figure S5). In the implementation of the present task, on top of symmetric overlaps, the four connectivity vectors include a common direction along the readout vector. This additional constraint eliminates the ring attractor and stabilizes only two equilibrium states that correspond to Go and Nogo outputs. Yet, the ring attractor is close in parameter space, and this prox-\n\nimity induces a slow manifold in the dynamics, so that the trajectories leading to a Go output slowly evolve along two different sides of the underlying ring depending on the context (Figure 6F). As a result, the two directions in the plane mð1Þ À mð2Þ correspond to choice and context axis as found by regression analysis (Figure 6F). A similar mechanism for context-dependent evidence integration based on a line attractor was previously identiﬁed by reverse-engineering a trained recurrent network (Mante et al., 2013). Whether the underlying dynamical structure was a ring as in our case or two line attractors for the two contexts depended on the details of the network training protocol (V. Mante, unpublished data). Here, we show that such a mechanism based on a ring attractor can be implemented in a minimal network with rank-two connectivity structure, but other solutions can certainly be found. Note that this rank-two network can also serve as an alternative implementation for context-independent evidence integration in which the integration timescale and the threshold value are fully independent in contrast to the unit-rank implementation (Figure 4).\n\nNeuron 99, 609–623, August 8, 2018 619\n\nDISCUSSION\nMotivated by the observation that a variety of approaches for implementing computations in recurrent networks rely on a common type of connectivity structure, we studied a class of models in which the connectivity matrix consists of a sum of a ﬁxed, lowrank term and a random part. Our central result is that the lowrank connectivity structure induces low-dimensional dynamics in the network, a hallmark of population activity recorded in behaving animals (Gao and Ganguli, 2015). While low-dimensional activity is usually detected numerically using dimensional-reduction techniques (Cunningham and Yu, 2014), we showed that a mean-ﬁeld theory allows us to directly predict the low-dimensional dynamics based on the connectivity and input structure. This approach led us to a simple, geometrical understanding of the relationship between connectivity and dynamics and enabled us to design minimal-connectivity implementations of speciﬁc computations. In particular, we found that the dynamical repertoire of the network increases quickly with the rank of the connectivity structure, so that rank-two networks can already implement a variety of computations. In this study, we have not explicitly considered structures with rank higher than two, but our theoretical framework is in principle valid for arbitrary rank r ( N, where N is the size of the network.\nWhile other works have examined dynamics in networks with a mixture of structured and random connectivity (e.g., Roudi and Latham [2007]; Ahmadian et al. [2015]), the most classical approach for implementing computations in recurrent networks has been to endow them with a clustered (Wang, 2002; Amit and Brunel, 1997; Litwin-Kumar and Doiron, 2012) or distancedependent connectivity (Ben-Yishai et al., 1995). Such networks inherently display low-dimensional dynamics similar to our framework (Doiron and Litwin-Kumar, 2014; Williamson et al., 2016), as clustered connectivity is in fact a special case of lowrank connectivity. Clustered connectivity, however, is highly ordered: each neuron belongs to a single cluster and therefore is selective to a single task feature (e.g., a given stimulus, or a given output). Neurons in clustered networks are therefore highly specialized and display pure selectivity (Rigotti et al., 2013). Here, instead, we have considered random low-rank structures, which generate activity organized along heterogeneous directions in state space. As a consequence, stimuli and outputs are represented in a random, highly distributed manner and individual neurons are typically responsive to several stimuli, outputs, or combinations of the two. Such mixed selectivity is a ubiquitous property of cortical neurons (Rigotti et al., 2013; Mante et al., 2013; Churchland and Shenoy, 2007) and confers additional computational properties to our networks (Kanerva, 2009). In particular, it allowed us to easily extend to a contextdependent situation (Mante et al., 2013; Saez et al., 2015), a network implementation of a basic discrimination task. This is typically difﬁcult to do in clustered, purely selective networks (Rigotti et al., 2010).\nThe type of connectivity used in our study is closely related to the classical framework of Hopﬁeld networks (Hopﬁeld, 1982; Amit et al., 1985). The aim of Hopﬁeld networks is to store in memory speciﬁc patterns of activity by creating for each pattern a corresponding ﬁxed point in the network dynamics. This is\n\nachieved by adding a unit-rank term for each item, and one approach for investigating the capacity of such a setup has relied on the mean-ﬁeld theory of a network with a connectivity that consists of a sum of a rank-one term and a random matrix (Tirozzi and Tsodyks, 1991; Shiino and Fukai, 1993; Roudi and Latham, 2007). While this approach is clearly close to the one adopted in the present study, there are important differences. Within Hopﬁeld networks, the unit-rank terms are symmetric, so that the corresponding left- and right-connectivity vectors are identical for each pattern. Moreover, the unit-rank terms that correspond to different patterns are generally uncorrelated. In contrast, here we have considered the more general case where the left- and right-eigenvectors are different and potentially correlated between different rank-one terms. Most importantly, our main focus was on responses to external inputs and input-output computations, rather than memorizing items. In particular, we showed that left- and right-connectivity vectors play different roles with respect to processing inputs, with the left-connectivity vector implementing input- selection, and the right-connectivity vector determining the output of the network.\nOur study is also directly related to echo-state networks (ESNs) (Jaeger and Haas, 2004) and FORCE learning (Sussillo and Abbott, 2009). In those frameworks, randomly connected recurrent networks are trained to produce speciﬁed outputs using a feedback loop from a readout unit to the network, which is mathematically equivalent to adding a rank-one term to the random connectivity matrix (Maass et al., 2007). In their most basic implementation, both ESN and FORCE learning train only the readout weights. The training is performed for a ﬁxed, speciﬁed realization of the random connectivity, so that the ﬁnal rank-one structure is correlated with the random part of the connectivity and may be strong with respect to it. In contrast, the results presented here rely on the assumption that the low-rank structure is weak and independent from the random part. Although ESN and FORCE networks do not necessarily fulﬁll this assumption, in ongoing work we found that our approach describes well networks trained using ESN or FORCE to produce a constant output (Rivkind and Barak, 2017). Note that in our framework, the computations rely solely on the structured part of the connectivity, but ongoing work suggests that the random part of the connectivity may play an important role during training.\nThe speciﬁc network model used here is identical to most studies based on trained recurrent networks (Sussillo and Abbott, 2009; Mante et al., 2013; Sussillo, 2014). It is highly simpliﬁed and lacks many biophysical constraints, the most basic ones being positive ﬁring rates, the segregation between excitation and inhibition and interactions through spikes. Recent works have investigated extensions of the abstract model used here to networks with biophysical constraints (Ostojic, 2014; Kadmon and Sompolinsky, 2015; Harish and Hansel, 2015; Mastrogiuseppe and Ostojic, 2017; Thalmeier et al., 2016). Additional work will be needed to implement the present framework in networks of spiking neurons.\nOur results imply novel, directly testable experimental predictions relating connectivity, low-dimensional dynamics and computational properties of individual neurons. Our main result is that the dominant components of low-dimensional dynamics are a combination of feedforward input patterns, and vectors\n\n620 Neuron 99, 609–623, August 8, 2018\n\nspecifying the low-rank recurrent connectivity (Figure 2C). A direct implication is that, if the low-dimensional dynamics in the network are generated by low-rank recurrent connectivity, two neurons that have large loadings in the dominant principal components will tend to have mutual connections stronger than average (Figure 3F, top). In contrast, if the low-dimensional dynamics are not generated by recurrent interactions but instead are driven by feedforward inputs alone, no correlation between principal components and connectivity is expected (Figure 3F, bottom). Since the low-dimensional dynamics based on recurrent connectivity form the scaffold for computations in our model, this basic prediction can be extended to various taskdependent properties of individual neurons. For instance, if the recurrent connectivity implements evidence integration, two units with strong choice regressors are predicted to have mutual connections stronger than average (Figure 4H). Analogously, if recurrent connections implement context-dependent associations, two units with strong context regressors are expected to share connections stronger than average (Figure 5G). Such predictions can in principle be directly tested in experiments that combine calcium imaging of neural activity in behaving animals with measurements of connectivity between a subset of recorded neurons (Ko et al., 2011). It should be noted, however, that very weak structured connectivity is sufﬁcient to implement computations, so that the expected correlations between connectivity and various selectivity indices may be weak.\nThe class of recurrent networks we considered here is based on connectivity matrices that consist of an explicit sum of a lowrank and a random part. While this may seem as a limited class of models, in fact, any arbitrary matrix can be approximated with a low-rank one, e.g., by keeping a small number of dominant singular values and singular vectors (Markovsky, 2012)—this is the basic principle underlying dimensionality reduction. A recurrent network with any arbitrary connectivity matrix can therefore in principle be approximated by a low-rank recurrent network. From this point of view, our theory suggests a simple conjecture: the low-dimensional structure in connectivity determines lowdimensional dynamics and computational properties of recurrent networks. While more work is needed to establish under which precise conditions a low-rank network provides a good computational approximation of a full recurrent network, this conjecture provides a simple and practically useful working hypothesis for reverse-engineering trained neural networks (Sussillo and Barak, 2013), and relating connectivity, dynamics, and computations in neural recordings.\n\nB Population-averaged equations for stationary solutions\nB Transient dynamics and stability of stationary solutions B Homogeneous stationary solutions B Heterogeneous stationary solutions B Mean-ﬁeld analysis of transient dynamics and stability\nof stationary solutions B Dynamical Mean Field equations for chaotic solutions B Spontaneous dynamics: structures overlapping on the\nunitary direction B Stationary solutions B Chaotic solutions B Spontaneous dynamics: structures overlapping on an\narbitrary direction B Response to external inputs B Asymmetric solutions B Transient dynamics B Rank-two connectivity structures B Rank-two structures with null overlap B Rank-two structures with internal pairwise overlap B Rank-two structures for oscillations d IMPLEMENTATION OF COMPUTATIONAL TASKS B Go-Nogo discrimination B Detection of a continuous noisy stimulus B Contextual modulation of threshold value B Rank-two structures for context-dependent compu-\ntations d METHOD DETAILS FOR MAIN FIGURES\nB Figure 1 B Figure 2 B Figure 3 B Figure 4 B Figure 5 B Figure 6 d QUANTIFICATION AND STATISTICAL ANALYSIS B Dimensionality reduction B Linear regression d DATA AND SOFTWARE AVAILABILITY\nSUPPLEMENTAL INFORMATION\nSupplemental Information includes eight ﬁgures and can be found with this article online at https://doi.org/10.1016/j.neuron.2018.07.003.\nACKNOWLEDGMENTS\n\nSTAR+METHODS\nDetailed methods are provided in the online version of this paper and include the following:\nd KEY RESOURCES TABLE d CONTACT FOR REAGENT AND RESOURCE SHARING d METHOD DETAILS\nB The network model B Overview of Dynamical Mean-Field Theory d DETAILS OF DYNAMICAL MEAN-FIELD THEORY B Single-unit equations for spontaneous dynamics\n\nWe are grateful to Alexis Dubreuil, Vincent Hakim, and Kishore Kuchibhotla for discussions and feedback on the manuscript. This work was funded by the Programme Emergences of City of Paris, Agence Nationale de la Recherche grants ANR-16-CE37-0016-01 and ANR-17-ERC2-0005-01, and the program ‘‘Investissements d’Avenir’’ launched by the French Government and implemented by the ANR, with the references ANR-10-LABX-0087 IEC and ANR11-IDEX-0001-02 PSL* Research University. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.\nAUTHOR CONTRIBUTIONS\nF.M. and S.O. designed the study and wrote the manuscript. F.M. performed model analyses and simulations.\n\nNeuron 99, 609–623, August 8, 2018 621\n\nDECLARATION OF INTERESTS\nThe authors declare no competing interests.\nReceived: December 4, 2017 Revised: April 27, 2018 Accepted: July 2, 2018 Published: July 26, 2018\nREFERENCES\nAhmadian, Y., Fumarola, F., and Miller, K.D. (2015). Properties of networks with partially structured and partially random connectivity. Phys. Rev. E Stat. Nonlin. Soft Matter Phys. 91, 012820. Aljadeff, J., Stern, M., and Sharpee, T. (2015b). Transition to chaos in random networks with cell-type-speciﬁc connectivity. Phys. Rev. Lett. 114, 088101. Amit, D.J., and Brunel, N. (1997). Model of global spontaneous activity and local structured activity during delay periods in the cerebral cortex. Cereb. Cortex 7, 237–252. Amit, D.J., Gutfreund, H., and Sompolinsky, H. (1985). Storing inﬁnite numbers of patterns in a spin-glass model of neural networks. Phys. Rev. Lett. 55, 1530–1533. Barak, O. (2017). Recurrent neural networks as versatile tools of neuroscience research. Curr. Opin. Neurobiol. 46, 1–6. Ben-Yishai, R., Bar-Or, R.L., and Sompolinsky, H. (1995). Theory of orientation tuning in visual cortex. Proc. Natl. Acad. Sci. USA 92, 3844–3848. Boerlin, M., Machens, C.K., and Dene` ve, S. (2013). Predictive coding of dynamical variables in balanced spiking networks. PLoS Comput. Biol. 9, e1003258. Brunel, N. (2000). Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons. J. Comput. Neurosci. 8, 183–208. Churchland, M.M., and Shenoy, K.V. (2007). Temporal complexity and heterogeneity of single-neuron activity in premotor and motor cortex. J. Neurophysiol. 97, 4235–4257. Churchland, M.M., Yu, B.M., Cunningham, J.P., Sugrue, L.P., Cohen, M.R., Corrado, G.S., Newsome, W.T., Clark, A.M., Hosseini, P., Scott, B.B., et al. (2010). Stimulus onset quenches neural variability: A widespread cortical phenomenon. Nat. Neurosci. 13, 369–378. Cunningham, J.P., and Yu, B.M. (2014). Dimensionality reduction for largescale neural recordings. Nat. Neurosci. 17, 1500–1509. Doiron, B., and Litwin-Kumar, A. (2014). Balanced neural architecture and the idling brain. Front. Comput. Neurosci. 8, 56. Eliasmith, C., and Anderson, C. (2004). Neural Engineering - Computation, Representation, and Dynamics in Neurobiological Systems (MIT Press). Fusi, S., Miller, E.K., and Rigotti, M. (2016). Why neurons mix: High dimensionality for higher cognition. Curr. Opin. Neurobiol. 37, 66–74. Gao, P., and Ganguli, S. (2015). On simplicity and complexity in the brave new world of large-scale neuroscience. Curr. Opin. Neurobiol. 32, 148–155. Girko, V.L. (1985). Circular law. Theory Probab. Appl. 29, 694–706. Harish, O., and Hansel, D. (2015). Asynchronous rate chaos in spiking neuronal circuits. PLoS Comput. Biol. 11, e1004266. Harris, K.D., and Mrsic-Flogel, T.D. (2013). Cortical connectivity and sensory coding. Nature 503, 51–58. Hopﬁeld, J.J. (1982). Neural networks and physical systems with emergent collective computational abilities. Proc. Natl. Acad. Sci. USA 79, 2554–2558. Jaeger, H., and Haas, H. (2004). Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication. Science 304, 78–80. Kadmon, J., and Sompolinsky, H. (2015). Transition to chaos in random neuronal networks. Phys. Rev. X 5, 041030.\n\nKanerva, P. (2009). Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Cognit. Comput. 1, 139–159.\nKo, H., Hofer, S.B., Pichler, B., Buchanan, K.A., Sjo¨ stro¨ m, P.J., and MrsicFlogel, T.D. (2011). Functional speciﬁcity of local synaptic connections in neocortical networks. Nature 473, 87–91.\nLitwin-Kumar, A., and Doiron, B. (2012). Slow dynamics and high variability in balanced cortical networks with clustered connections. Nat. Neurosci. 15, 1498–1505.\nMaass, W., Joshi, P., and Sontag, E.D. (2007). Computational aspects of feedback in neural circuits. PLoS Comput. Biol. 3, e165.\nMachens, C.K., Romo, R., and Brody, C.D. (2010). Functional, but not anatomical, separation of ‘‘what’’ and ‘‘when’’ in prefrontal cortex. J. Neurosci. 30, 350–360.\nMante, V., Sussillo, D., Shenoy, K.V., and Newsome, W.T. (2013). Contextdependent computation by recurrent dynamics in prefrontal cortex. Nature 503, 78–84.\nMarkovsky, I. (2012). Low Rank Approximation - Algorithms, Implementations, Applications (Springer).\nMartens, J., and Sutskever, I. (2011). Learning recurrent neural networks with hessian-free optimization. In ICML’11 Proceedings of the 28th International Conference on International Conference on Machine Learning. (ICML), pp. 1033–1040.\nMastrogiuseppe, F., and Ostojic, S. (2017). Intrinsically-generated ﬂuctuating activity in excitatory-inhibitory networks. PLoS Comput. Biol. 13, e1005498.\nNewsome, W.T., Britten, K.H., and Movshon, J.A. (1989). Neuronal correlates of a perceptual decision. Nature 341, 52–54.\nOstojic, S. (2014). Two types of asynchronous activity in networks of excitatory and inhibitory spiking neurons. Nat. Neurosci. 17, 594–600.\nPascanu, R., Mikolov, T., and Bengio, Y. (2013). On the difﬁculty of training recurrent neural networks. In ICML’13 Proceedings of the 30th International Conference on International Conference on Machine Learning (ICML), pp. III–1310–III–1318.\nRajan, K., and Abbott, L.F. (2006). Eigenvalue spectra of random matrices for neural networks. Phys. Rev. Lett. 97, 188104.\nRajan, K., Abbott, L.F., and Sompolinsky, H. (2010). Stimulus-dependent suppression of chaos in recurrent neural networks. Phys. Rev. E Stat. Nonlin. Soft Matter Phys. 82, 011903.\nRemington, E.D., Narain, D., Hosseini, E.A., and Jazayeri, M. (2018). Flexible sensorimotor computations through rapid reconﬁguration of cortical dynamics. Neuron 98, 1005–1019.e5.\nRigotti, M., Ben Dayan Rubin, D., Wang, X.-J., and Fusi, S. (2010). Internal representation of task rules by recurrent dynamics: The importance of the diversity of neural responses. Front. Comput. Neurosci. 4, 24.\nRigotti, M., Barak, O., Warden, M.R., Wang, X.-J., Daw, N.D., Miller, E.K., and Fusi, S. (2013). The importance of mixed selectivity in complex cognitive tasks. Nature 497, 585–590.\nRivkind, A., and Barak, O. (2017). Local dynamics in trained recurrent neural networks. Phys. Rev. Lett. 118, 258101.\nRoudi, Y., and Latham, P.E. (2007). A balanced memory network. PLoS Comput. Biol. 3, 1679–1700.\nSaez, A., Rigotti, M., Ostojic, S., Fusi, S., and Salzman, C.D. (2015). Abstract context representations in primate amygdala and prefrontal cortex. Neuron 87, 869–881.\nShadlen, M.N., and Newsome, W.T. (1998). The variable discharge of cortical neurons: Implications for connectivity, computation, and information coding. J. Neurosci. 18, 3870–3896.\nShiino, M., and Fukai, T. (1993). Self-consistent signal-to-noise analysis of the statistical behavior of analog neural networks and enhancement of the storage capacity. Phys. Rev. E Stat. Phys. Plasmas Fluids Relat. Interdiscip. Topics 48, 867–897.\n\n622 Neuron 99, 609–623, August 8, 2018\n\nSompolinsky, H., Crisanti, A., and Sommers, H.J. (1988). Chaos in random neural networks. Phys. Rev. Lett. 61, 259–262.\nSussillo, D. (2014). Neural circuits as computational dynamical systems. Curr. Opin. Neurobiol. 25, 156–163.\nSussillo, D., and Abbott, L.F. (2009). Generating coherent patterns of activity from chaotic neural networks. Neuron 63, 544–557.\nSussillo, D., and Barak, O. (2013). Opening the black box: Low-dimensional dynamics in high-dimensional recurrent neural networks. Neural Comput. 25, 626–649.\nTao, T. (2013). Outliers in the spectrum of iid matrices with bounded rank perturbations. Probab. Theory Relat. Fields 155, 231–263.\nThalmeier, D., Uhlmann, M., Kappen, H.J., and Memmesheimer, R.M. (2016). Learning universal computations with spikes. PLoS Comput. Biol. 12, e1004895.\n\nTirozzi, B., and Tsodyks, M. (1991). Chaos in highly diluted neural networks. EPL 14, 727.\nvan Vreeswijk, C., and Sompolinsky, H. (1996). Chaos in neuronal networks with balanced excitatory and inhibitory activity. Science 274, 1724–1726.\nWang, X.-J. (2002). Probabilistic decision making by slow reverberation in cortical circuits. Neuron 36, 955–968.\nWang, J., Narain, D., Hosseini, E.A., and Jazayeri, M. (2018). Flexible timing by temporal scaling of cortical responses. Nat. Neurosci. 21, 102–110.\nWilliamson, R.C., Cowley, B.R., Litwin-Kumar, A., Doiron, B., Kohn, A., Smith, M.A., and Yu, B.M. (2016). Scaling properties of dimensionality reduction for neural populations and network models. PLoS Comput. Biol. 12, e1005141.\n\nNeuron 99, 609–623, August 8, 2018 623\n\nSTAR+METHODS\nKEY RESOURCES TABLE\nREAGENT or RESOURCE Software and Algorithms Algorithms for solving Dynamical Mean-Field equations\n\nSOURCE this paper\n\nIDENTIFIER https://github.com/fmastrogiuseppe/lowrank/\n\nCONTACT FOR REAGENT AND RESOURCE SHARING\n\nFurther requests for resources should be directed to and will be fulﬁlled by the Lead Contact, Srdjan Ostojic (srdjan.ostojic@ens.fr).\n\nMETHOD DETAILS\n\nThe network model We study large recurrent networks of rate units. Every unit in the network is characterized by a continuous variable xiðtÞ, commonly interpreted as the total input current. More generically, we also refer to xiðtÞ as the activation variable. The output of each unit is a nonlinear function of its inputs modeled as a sigmoidal function fðxÞ. In line with previous works (Sompolinsky et al., 1988; Sussillo and Abbott, 2009; Rivkind and Barak, 2017), we focus on fðxÞ = tanhðxÞ, but we show that qualitatively similar dynamical regimes appear in network models with more realistic, positively deﬁned activation functions (Figure S7). The transformed variable fðxiðtÞÞ is interpreted as the ﬁring rate of unit i, and is also referred to as the activity variable.\nThe time evolution is speciﬁed by the following dynamics:\n\nXN\n\nx_iðtÞ = À xiðtÞ + JijfðxjðtÞÞ + Ii:\n\n(6)\n\nj=1\n\nWe considered a particular class of connectivity matrices, which can be written as a sum of two terms:\n\nJij = gcij + Pij:\n\n(7)\n\nSimilarly to (Sompolinsky et al., 1988), cij is a Gaussian all-to-all random matrix, where every element is drawn from a centered normal distribution with variance 1=N. The parameter g scales the strength of random connections in the network, and we refer to it also as the random strength. The second term Pij is a low-rank matrix. In this study, we consider the low-rank part of the connectivity ﬁxed, while the random part varies between different realizations of the connectivity. Our results rely on two simplifying assumptions. The ﬁrst one is that the low-rank term and the random term are statistically uncorrelated. The second one is that, as stated in Equation 8p, thﬃﬃﬃe structured connectivity is weak in the large N limit, i.e., it scales as 1=N, while the random connectivity components cij scale as 1= N.\nWe ﬁrst consider the simplest case where Pij is a rank-one matrix, which can generally be written as the external product between two one-dimensional vectors m and n:\n\nPij\n\n=\n\nmi nj N\n\n:\n\n(8)\n\nAccording to our ﬁrst assumption, the entries of vectors m and n are independent of the random bulk of the connectivity cij. Note that the only non-zero eigenvalue of P is given by the scalar product mT n=N, and the corresponding right and left eigenvectors are, respectively, vectors m and n. In the following, we will refer to the eigenvalue mT n=N as the strength of the connectivity structure, and\nto m and n as the right- and left-connectivity vectors. Here we focus on vectors obtained by generating the components from a joint\nGaussian distribution.\nMore general connectivity structures of rank r ( N can be written as a sum of unit-rank terms\n\nPij\n\n=\n\nmði 1Þnðj 1Þ N\n\n+\n\n.\n\n+\n\nmði rÞnðj rÞ; N\n\n(9)\n\nand are therefore speciﬁed by r pairs of vectors mðkÞ and nðkÞ, where different m vectors are linearly independent, and similarly for n vectors.\n\ne1 Neuron 99, 609–623.e1–e29, August 8, 2018\n\nOverview of Dynamical Mean-Field Theory Our results rely on a mathematical analysis of network dynamics based on Dynamical Mean-Field (DMF) theory (Sompolinsky et al., 1988; Rajan et al., 2010; Kadmon and Sompolinsky, 2015). To help navigate the analysis, here we provide ﬁrst a succint overview of the approach. Full details are given further down in the section Details of Dynamical Mean-Field Theory.\nDMF theory allows one to derive an effective description of the dynamics by averaging over the disorder originating from the random part of the connectivity. Across different realizations of the random connectivity matrix cij, the sum of inputs to unit i is approximated by a Gaussian stochastic process hiðtÞ\n\nXN\n\nJijfðxjðtÞÞ + IizhiðtÞ;\n\n(10)\n\nj=1\n\nso that each unit obeys a Langevin-like equation:\n\nx_iðtÞ = À xiðtÞ + hiðtÞ:\n\n(11)\n\nThe Gaussian processes hi can in principle have different ﬁrst and second-order statistics for each unit, but are otherwise statistically independent across different units. As a consequence, the activations xi of different units are also independent Gaussian sto-\nchastic processes, coupled only through their ﬁrst and second-order statistics. The core of DMF theory consists of self-consistent equations for the mean mi and auto-correlation function DIiðtÞ.\nAt equilibrium (i.e., in absence of transient dynamics) the equation for the mean mi of xi is obtained by directly averaging Equation 6 over the random part of the connectivity. For a unit-rank connectivity, it reads\n\nmi = kmi + Ii;\n\n(12)\n\nwhere\n\nk\n\n=\n\n1 N\n\nXN\nj=1\n\nÂÃ nj fj :\n\n(13)\n\nIn the last equation, we adopted the short-hand notation fiðtÞ : = fðxiðtÞÞ. Here ½fj is the average ﬁring rate of unit j, i.e., fðxjÞ\n\naveraged over the Gaussian variable xj. In a geometrical interpretation, the quantity k represents the overlap between the left-con-\n\nnectivity vector n and the vector of average ﬁring rates. Equivalently, it is given by a population average of nj½fj, which can also be\n\nexpressed as\n\nZ\n\nZ \n\nqﬃﬃﬃﬃﬃ  \n\nk = dm dn dI pðm; n; IÞn Dzf mk + I + DI0 z\n\n(14)\n\nwR here Dz =\n\npRð+mN;\nÀN\n\nn; IÞ eÀz2\n\nis pthﬃﬃﬃeﬃﬃﬃ joint =2= 2pdz.\n\ndistribution\n\nof\n\ncomponents\n\nof\n\nvectors\n\nm,\n\nn\n\nand\n\nI.\n\nDI0\n\nis\n\nthe\n\nvariance\n\nof\n\nxi\n\n(see\n\nbelow),\n\nand\n\nThe auto-correlation function DIiðtÞ quantiﬁes the ﬂuctuations of the activation xi around the expected mean. Computing this auto-\n\ncorrelation function shows that it is identical for all units in the network, i.e., independent of i (see Equation 27). It can be decomposed\n\ninto a static variance, which quantiﬁes the ﬂuctuations of the equilibrium values of xi across different realizations of the random component of the connectivity, and an additional temporal variance which is present when the network is in a temporally ﬂuctuating, chaotic state. In a stationary state, the variance DI0hDIðt = 0Þ can be expressed as\n\nDI0\n\n=\n\ng2 1 N\n\nXN\nj=1\n\nÂÃ f2i :\n\n(15)\n\nwhere ½f2i  is the average of f2i ðxÞ over the Gaussian variable xi. The right-hand-sides of Equations 13 and 15 show that both the mean mi and variance DI0 depend on population-averaged, macro-\nscopic quantities. To fully close the DMF description, the equations for single-unit statistics need to be averaged over the population. For static equilibrium dynamics, this leads to two coupled equations for two macroscopic quantities, the overlap k and the static, population-averaged variance D0:\n\nk = Fðk; D0Þ D0 = Gðk; D0Þ:\n\n(16)\n\nHere F and G are two non-linear functions, the speciﬁc form of which depends on the geometrical arrangement of the connectivity vectors m and n and the input vector I. For temporally ﬂuctuating, chaotic dynamics an additional macroscopic quantity (corresponding to the temporal variance) needs to be taken into account. In that case, the full DMF description is given by a system of three nonlinear equations for three unknowns. The equilibrium states of the network dynamics are therefore obtained by solving these systems of equations using standard non-linear methods.\n\nNeuron 99, 609–623.e1–e29, August 8, 2018 e2\n\nTo describe the transient dynamics and assess the stability of the obtained equilibrium states, we determined the spectrum of eigenvalues at the obtained equilibrium ﬁxed points. This spectrum consists of two components: a continuous, random component distributed within a circle in the complex plane, and a single outlier induced by the structured part of the connectivity (Figures S1A and S1D). The radius of the continuous component and the value of the outlier depend on the connectivity parameters. Although the two quantities in general are non-trivially coupled, the value of the radius is mostly controlled by the strength of the disorder, while the value of the outlier increases with the strength mT n=N of the rank-one structure (Figure S1F). The equilibrium is stable as long as the real part of all eigenvalues is less than unity. For large connectivity structure strengths, the outlier crosses unity, generating an instability that leads to the appearance of one-dimensional structured activity. Increasing the disorder strength on the other hand leads to another instability, corresponding to the radius of the continuous component crossing unity. This instability gives rise to chaotic, ﬂuctuating activity.\nWhen a linear readout with weights wi is added to the network, its average output is given by\n\nzðtÞ\n\n=\n\n1 N\n\nXN\ni=1\n\nwi ½fi ðtÞ;\n\n(17)\n\ni.e., by the projection of the average network ﬁring rate on the readout vector w. This quantity is analogous to k, except that the vector\n\nn is replaced by the vector w, so that similarly to Equation 14, the average readout can also be expressed as\n\nZ\n\nZ\n\n \n\nqﬃﬃﬃﬃﬃ  \n\nz = dm dw dI pðm; w; IÞ w Dyf mz + I + DI0 y\n\n(18)\n\nand therefore directly depends on the joint distribution pðm; w; IÞ which characterizes the geometric arrangement of vectors m, w and I.\nThe DMF theory can be directly extended to connectivity structures of rank r greater than one. The equilibrium mean input to unit i is then given by\n\nXr\n\nmi =\n\nkðkÞmði kÞ + Ii:\n\n(19)\n\nk=1\n\nThe activity therefore lives in an ðr + 1Þ-dimensional space determined by the r right-connectivity vectors mðkÞ and the input vector I. It is characterized by r overlaps kðkÞ, each of which quantiﬁes the amount of activity along the corresponding direction mðkÞ. Averaging over the population, the DMF theory then leads to a system of r + 1 nonlinear coupled equations for describing stationary dynamics.\n\nDETAILS OF DYNAMICAL MEAN-FIELD THEORY\n\nHere we provide the full details of the mathematical analysis. We start by examining the activity of a network with a rank-one structure in absence of external inputs (Ii = 0 ci in Equation 6).\n\nSingle-unit equations for spontaneous dynamics We start by determining the statistics of the effective noise hi to unit i, deﬁned by\n\nhi\n\nðtÞ\n\n=\n\ng\n\nXN\nj=1\n\ncij\n\nfðxj\n\nðtÞÞ\n\n+\n\nmi N\n\nXN nj fðxj ðtÞÞ:\nj=1\n\n(20)\n\nThe DMF theory relies on the hypothesis that a disordered component in the coupling structure, here represented by cij, efﬁciently decorrelates single neuron activity when the network is sufﬁciently large. We will show that this hypothesis of decorrelated activity is\nself-consistent for the speciﬁc network architecture we study.\nAs in standard DMF derivations, we characterize self-consistently the distribution of hi by averaging over different realizations of the random matrix cij (Sompolinsky et al., 1988; Rajan et al., 2010). In the following, ½: indicates an average over the realizations of the random matrix cij, while h:i stands for an average over different units of the network. Note that the network activity can be equivalently characterized in terms of input current variables xiðtÞ or their non-linear transforms fðxiðtÞÞ. As these two quantities are not independent, the statistics of the distribution of the latter can be written in terms of the statistics of the former.\nThe mean of the effective noise received by unit i is given by:\n\n½hi\n\nðtÞ\n\n=\n\ng\n\nXN Â cij\nj=1\n\nfðxj\n\nÃ ðtÞÞ\n\n+\n\nmi N\n\nXN nj ½fðxj ðtÞÞ:\nj=1\n\n(21)\n\ne3 Neuron 99, 609–623.e1–e29, August 8, 2018\n\nUnder the hypothesis that in large networks, neural activity decorrelates (more speciﬁcally, that activity fðxjðtÞÞ is independent of its outgoing weights), we have:\n\n½hi\n\nðtÞ\n\n=\n\ng\n\nXN Â cij\nj=1\n\nÃ ½fðxj\n\nðtÞÞ\n\n+\n\nmi N\n\nXN nj½fðxjðtÞÞ = mik\nj=1\n\n(22)\n\nas ½cij = 0. Here we introduced\n\n1 XN\n\n  Â Ã \n\nk: = N\n\nnj½fðxjðtÞÞ =\nj=1\n\nnj\n\nfj ðtÞ\n\n;\n\n(23)\n\nwhich quantiﬁes the overlap between the mean population activity vector and the left-connectivity vector n. Similarly, the noise correlation function is given by\n\nÂ hi\n\nðtÞhj\n\nðt\n\n+\n\nÃ tÞ\n\n=\n\ng2\n\nXN\n\nk=1\n\nXN Â cik\nl=1\n\ncjl\n\nÃ ½fðxk\n\nðtÞÞfðxl\n\nðt\n\n+\n\ntÞÞ\n\n+\n\nmi mj N2\n\nXN\nk=1\n\nXN nknl½fðxkðtÞÞfðxlðt + tÞÞ:\nl=1\n\n(24)\n\nNote that every cross-term in the product vanishes since ½cij = 0. Similarly to standard DMF derivations (Sompolinsky et al., 1988), the ﬁrst term on the r.h.s. vanishes for cross-correlations ðisjÞ while it survives in the auto-correlation function ði = jÞ, as ½cikcjl = dijdkl=N. We get:\n\nÂ hi\n\nðtÞhj\n\nðt\n\n+\n\nÃ tÞ\n\n=\n\ndij\n\ng2h½fi\n\nðtÞfi\n\nðt\n\n+\n\ntÞi\n\n+\n\nmi mj N2\n\nXN\nk=1\n\nXN nknl½fðxkðtÞÞfðxlðt + tÞÞ:\nl=1\n\n(25)\n\nWe focus now on the second term in the right-hand side. The corresponding sum contains N terms where k = l. This contribution\n\nvanishes in the large N limit because of the 1=N2 scaling. According to our starting hypothesis, when ksl, activity decorrelates:\n\n½fkðtÞflðt + tÞ = ½fkðtÞ½flðt + tÞ. To the leading order in N, we get:\n\nÂ hi ðtÞhj ðt\n\n+\n\nÃ tÞ\n\n=\n\ndij g2 h½fi ðtÞfi ðt\n\n+\n\ntÞi\n\n+\n\nmi mj N2\n\nX nk\nk\n\n½fðxk\n\nX ðtÞÞ nl½fðxlðt\nlsk\n\n+\n\ntÞÞ\n\n(26)\n\n= dijg2h½fiðtÞfiðt + tÞi + mimjk2\n\nso that:\n\nÂ hi\n\nðtÞhj\n\nðt\n\n+\n\nÃ tÞ\n\nÀ\n\n½hi\n\nÂ ðtÞ hj\n\nÃ ðtÞ\n\n=\n\ndij\n\ng2\n\nh½fi\n\nðtÞfi\n\nðt\n\n+\n\ntÞi:\n\n(27)\n\nWe therefore ﬁnd that the statistics of the effective input are uncorrelated across different units, so that our initial hypothesis is selfconsistent.\nTo conclude, for every unit i, we computed the ﬁrst- and the second-order statistics of the effective input hiðtÞ. The expressions we obtained show that the individual noise statistics depend on the statistics of the full network activity. In particular, the mean of the effective input depends on the average overlap k, but varies from unit to unit through the components of the right-connectivity vector m. On the other hand, the auto-correlation of the effective input is identical for all units, and determined by the population-averaged ﬁring rate auto-correlation h½fiðtÞfiðt + tÞi.\nOnce the statistics of hiðtÞ have been determined, a self-consistent solution for the activation variable xiðtÞ can be derived by solving the Langevin-like stochastic process from Equation 11. As a ﬁrst step, we look at its stationary solutions, which correspond to the ﬁxed points of the original network dynamics.\n\nPopulation-averaged equations for stationary solutions\nFor any solution that does not depend on time, the mean mi and the variance DI0 of the variable xi with respect to different realizations of the random connectivity coincide with the statistics of the effective noise hi. From Equations 22 and 27, the mean mi and variance DI0 of the input to unit i therefore read\n\nmi : DI0 :\n\n==½xÂxi i2=Ã\n\nmi À\n\nk ½xi\n\n2\n\n=\n\ng2\n\n Âf2i\n\nÃ \n\n(28)\n\nwhile any other cross-variance ½xixj À ½xi½xj vanishes. We conclude that, on average, the structured connectivity Pij shapes the network activity along the direction speciﬁed by its right eigenvector m. Such a heterogeneous stationary state critically relies on\na non-vanishing overlap k between the left eigenvector n and the average population activity vector ½f. Across different realizations\nof the random connectivity, the input currents xi ﬂuctuate around these mean values. The typical size of ﬂuctuations is determined by the individual variance DI0, equal for every unit in the network.\n\nNeuron 99, 609–623.e1–e29, August 8, 2018 e4\n\nThe r.h.s. of Equation 28 contains two population averaged quantities, the overlap k and the second moment of the activity h½f2i i. To close the equations, these quantities need to be expressed self-consistently. Averaging Equation 28 over the population, we get\nexpressions for the population-averaged mean m and variance D0 of the input:\n\nm: D0\n\n= h½ xÂii =Ã hmi : = xi2 À\n\nik h½xi\n\ni2\n\n=\n\n Â Ã  g2 f2i\n\n+\n\n     m2i\n\nÀ\n\n  hmii2 k2:\n\n(29)\n\nNote that the total population variance D0 is a sum of two terms: the ﬁrst term, proportional to the strength of the random part of connectivity, coincides with the individual variability DI0 which emerges from different realizations of cij; the second term, proportional to the variance of the right-connectivity vector m, coincides with the variance induced at the population level by the spread of the\n\nmean values mifmi. When the vector m is homogeneous ðmi = mÞ, input currents xi are centered around the same mean value m, and the second variance term vanishes.\n\nWe next derive appropriate expression for the r.h.s. terms k and h½f2i i. To start with, we rewrite ½fi by substituting the average over the random connectivity with the equivalent Gaussian integral:\n\nZ   qﬃﬃﬃﬃﬃ  \n\n½fi = Dzf mi + DI0 z\n\n(30)\n\nwhere\n\nwe\n\nused\n\nthe\n\nshort-hand\n\nnotation\n\nR\n\nDz\n\n=\n\nR +N\nÀN\n\neÀz2\n\n=2\n\npﬃﬃﬃﬃﬃﬃ = 2pdz.\n\nTo\n\nobtain\n\nk,\n\n½fi \n\nneeds\n\nto\n\nbe\n\nmultiplied\n\nby\n\nni\n\nand\n\naveraged\n\nover\n\nthe\n\npopulation. This average can be expressed by representing the ﬁxed vectors m and n through the joint distribution of their elements\n\nover the components:\n\npðm;\n\nnÞ\n\n=\n\n1 N\n\nXN\nj=1\n\ndðm\n\nÀ\n\nmj Þdðn\n\nÀ\n\nnj Þ:\n\n(31)\n\nThis leads to\n\n( Z   qﬃﬃﬃﬃﬃ  )\n\nk = ni Dzf mi + DI0 z\n\nZZ\n\nZ   qﬃﬃﬃﬃﬃ  \n\n(32)\n\n= dm dn pðm; nÞ n Dzf mk + DI0 z :\n\nSimilarly, a suitable expression for the second-order momentum of the ﬁring rate is given by:\n\n Âf2i Ã  = Z\n\nZ dm pðmÞ\n\n  qﬃﬃﬃﬃﬃ   Dzf2 mk + DI0 z :\n\n(33)\n\nEquations 32 and 33, combined with Equation 29, provide a closed set of equations for determining k and D0 once the vectors m\n\nand n have been speciﬁed.\n\nTo further simplify the problem, we reduce the full distribution pðm; nÞ of elements mi and ni to their ﬁrst- and second-order\n\nmomenta. That is equivalent to substituting the probability density pðm; nÞ with a bivariate Gaussian distribution. We therefore write:\n\npﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ m = Mm + Spm ﬃﬃﬃﬃ1ﬃﬃﬃﬃÀﬃﬃﬃ r x1 n = Mn + Sn 1 À r x2 +\n\n+SnSpmﬃrpﬃ yﬃrﬃ\n\ny\n\n(34)\n\nwhere x1, x2 and y are three normal Gaussian processes. Here, Mm (resp. Mn) and Sm (resp. Sn) correspond to the mean and the stan-\n\ndard deviation of m (resp. n), pretation, Mm and Mn are the\n\nwhile the covariance between m and projections of N–dimensional vectors\n\nnmisangdivennobnytohtmheinuipinÀﬃiﬃtﬃﬃaﬃMﬃrﬃﬃyﬃmﬃﬃvMecn t=orSupm=ﬃSﬃﬃﬃnðﬃﬃ1rﬃﬃﬃ.;ﬃﬃ1W;.ith1inÞ=aNg, eSommpeﬃrﬃtraicnadl\n\nSinntperﬃr-ﬃ\n\nare the projections onto a direction orthogonal to u and common to m and n, and Sm 1 À r and Sn 1 À r scale the parts of m and n\n\nthat are mutually orthogonal.\n\nThe expression for k becomes:\n\nZ k=\n\nZ Dy\n\nDx2\n\n  Mn\n\n+\n\npﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Sn 1 À r\n\nx2\n\n+\n\nSn\n\npﬃﬃ r\n\n  y\n\nZ\n\nZ Dz\n\nDx1\n\n   f k Mm\n\n+\n\npﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Sm 1 À r\n\nx1\n\n+\n\nSm\n\npﬃﬃ r\n\n  y\n\n+\n\nqﬃﬃﬃﬃﬃ DI0\n\n  z\n\n(35)\n\nwhich\n\ngives\n\nrise\n\nto\n\nthree\n\nterms\n\nwhen\n\nexpanding\n\nthe\n\nsum\n\nMn\n\n+\n\npﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Sn 1 À rx2\n\n+\n\nSn pﬃrﬃ y.\n\nThe\n\nﬁrst\n\nterm\n\ncan\n\nbe\n\nrewritten\n\nas:\n\nZ\n\n \n\nqﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ  \n\nMn Dz f Mmk + DI0 + S2mk2 z\n\nZ\n\n  pﬃﬃﬃﬃﬃ  \n\n(36)\n\n= Mn Dz f m + D0 z\n\n= Mnh½fii;\n\ne5 Neuron 99, 609–623.e1–e29, August 8, 2018\n\nwhich coincides with the overlap between vectors n and ½f along the unitary direction u = ð1; 1;.1Þ=N. In the last step, we rewrote\n\nour expression for k in terms of the population averaged statistics m and D0 (Equation 29).\n\nThe second term vanishes, while the third one gives:\n\nSnpﬃrﬃ Z\n\nZ Dy y\n\nZ Dz\n\nDx1\n\n   f k Mm\n\n+\n\npﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Sm 1 À r\n\nx1\n\n+\n\nSm\n\npﬃﬃ r\n\n  y\n\n+\n\nqﬃﬃﬃﬃﬃ DI0\n\n  z\n\n= krSmSn Âf0i Ã \n\n(37)\n\nwhich coincides with the overlap between n and ½f in a direction orthogonal to u. Here we used the equality:\n\nZ\n\nZ Dz zfðzÞ =\n\nDz\n\ndf ðzÞ dz\n\n(38)\n\nwhich is obtained by integrating by parts.\n\nThrough a similar reasoning we obtain:\n\n Âf2i Ã  = Z\n\n  pﬃﬃﬃﬃﬃ   Dz f2 m + D0 z\n\n(39)\n\nas in standard DMF derivations. To conclude, the mean-ﬁeld description of stationary solutions reduces to the system of three implicit equations for m, k and D0:\n\nm = Mmk Â Ã \n\nD0 = g2 f2i k = Mmh½fii\n\n+\n\n+ S2mk2 krSmSn\n\n Âf0i\n\nÃ  :\n\n(40)\n\nBoth averages h½:i are performed with respect to a Gaussian distribution of mean m and variance D0. Once m, D0 and k have been determined, the single unit mean mi and the individual variance DI0 are obtained from Equation 28.\nThe dynamical mean-ﬁeld equations given in Equation 40 can be fully solved to determine stationary solutions. Detailed descrip-\ntions of these solutions are provided further down for two particular cases: (i) overlap between m and n only along the unitary direction\nu (Mms0, Mns0, r = 0); (ii) overlap between m and n only in a direction orthogonal to u (Mm = Mn = 0, rs0).\n\nTransient dynamics and stability of stationary solutions\n\nWe now turn to transient dynamics around ﬁxed points, and to the related problem of evaluating whether the stationary solutions\n\nfound within DMF are stable with respect to the original network dynamics (Equation 6).\n\nFor any given realization of the connectivity matrix, the network we consider is completely deterministic. We can then study the\n\nlocal, transient dynamics by linearizing the dynamics around any stationary solution. We therefore look at the time evolution of a small\n\ndisplacement away from the ﬁxed point: xðtÞ = xi0 + xi1ðtÞ. For any generic stationary solution fxi0g the linearized dynamics are given\n\nby the stability matrix Sij which reads:\n\nSij\n\n=\n\nf0\n\n     xj0 gcij\n\n+\n\nmi\n\nnj\n\n  :\n\nN\n\n(41)\n\nIf the real part of every eigenvalue of Sij is smaller than unity, the perturbation decays in time and thus the stationary solution is stable.\n\nHomogeneous stationary solutions We ﬁrst consider homogeneous stationary solutions, for which xi0 = x for all units. A particular homogeneous solution is the trivial solution x = 0, which the network admits for all parameter values when the transfer function is fðxÞ = tanhðxÞ. Other homogeneous\nsolutions can be obtained when the vector m is homogeneous, i.e., mi = Mm for all i. For homogeneous solutions, the stability matrix reduces to a scaled version of the connectivity matrix:\n\nSij = f0ðxÞJij:\n\n(42)\n\nWe are thus left with the problem of evaluating the eigenspectrum of the global connectivity matrix Jij. The matrix Jij consists of a full-rank component cij, the entries of which are drawn at random, and of a structured component of small dimensionality with ﬁxed entries. We focus on the limit of large networks; in that limit, an analytical prediction for the spectrum of its eigenvalues can be\nderived. Because of the 1=N scaling, the matrix norm of Pij is bounded as N increases. We can then apply results from random matrix theory\n(Tao, 2013) which predict that, in the large N limit, the eigenspectra of the random and the structured parts do not interact, but sum\ntogether. The eigenspectrum of Jij therefore consists of two separated components, inherited respectively from the random and the structured terms (Figure S1A). Similarly to (Girko, 1985), the random term cij returns a set of N À 1 eigenvalues which lie on the complex plane in a compact circular region of radius g. In addition to this component, the eigenspectrum of Jij contains the non-zero\n\nNeuron 99, 609–623.e1–e29, August 8, 2018 e6\n\nP eigenvalues of Pij: in the case of a rank-one matrix, one single outlier eigenvalue is centered at the position imini=N = hminii. In Figure S1B we measure both the outlier position and the radius of the compact circular component. We show that deviations from the theoretical predictions are in general small and decay to zero as the system size is increased.\nGoing back to the stability matrix Sij = f0 ðxÞJij, we conclude that a homogeneous stationary solution can lose stability in two different ways, when either mT n=N or g become larger than 1=f0 ðxÞ. We expect different kinds of instabilities to occur in the two cases. When g crosses the instability line, a large number of random directions become unstable at the same time. As in (Sompolinsky et al., 1988), this instability is expected to lead to the onset of irregular temporal activity. When the instability is lead by the outlier, instead, the trivial ﬁxed point becomes unstable in one unique direction given by the corresponding eigenvector. When g = 0, this eigenvector coincides exactly with m. For ﬁnite values of the disorder g, the outlier eigenvector ﬂuctuates depending on the random part of the connectivity, but remains strongly correlated with m (Figure S1C), which therefore determines the average direction of the instability. Above the instability, as the network dynamics is completely symmetric with respect to a change of sign of the input variables, we expect the non-linear boundaries to generate two symmetric stationary solutions.\n\nHeterogeneous stationary solutions\n\nA second type of possible stationary solutions are heterogeneous ﬁxed points, in which different units reach different equilibrium\n\nvalues. For such ﬁxed points, the linearized stability matrix Sij is obtained by multiplying each column of the connectivity matrix Jij\n\nby a different gain value (see Equation 41), so that the eigenspectrum of Sij is not trivially related to the spectrum of Jij.\n\nNumerical investigations reveal that, as for Jij, the eigenspectrum of Sij consists of two discrete components: one compact set of N À 1 eigenvalues contained in a circle on the complex plane, and a single isolated outlier eigenvalue (Figure S1D).\n\nAs previously noticed in (Harish and Hansel, 2015), the radius of the circular compact set r can be computed as in (Rajan and Ab-\n\nbott, 2006; Aljadeff et al., 2015b) by summing the variances of the distributions in every column of Sij. To the leading order in N:\n\nr = gv u u tﬃN1ﬃﬃﬃﬃﬃX jﬃﬃ=Nﬃﬃ1ﬃﬃﬃfﬃﬃﬃ0ﬃ2ﬃﬃ ﬃﬃﬃxﬃﬃﬃ0jﬃﬃ ﬃﬃ\n\n(43)\n\nwhich, in large networks, can be approximated by the mean-ﬁeld average:\n\nr = gqﬃ ﬃﬃÂﬃﬃfﬃﬃﬃ0i2ﬃﬃÃﬃﬃ ﬃﬃ:\n\n(44)\n\nNote that, because of the weak scaling in Pij, the structured connectivity term does not appear explicitly in the expression for the\nradius. As the structured part of the connectivity determines the heterogeneous ﬁxed point, the value of r however depends implicitly on the structured connectivity term through h½fi02i, which is computed as a Gaussian integral over a distribution with mean m and variance D0 given by Equation 40. In Figures S1D–S1F, we show that Equation 44 approximates well the radius of ﬁnite-size, numer-\nically computed eigenspectra. Whenever the mean-ﬁeld theory predicts instabilities led by r, we expect the network dynamics to\nconverge to irregular non-stationary solutions. Consistently, at the critical point, where r = 1, the DMF equations predict the onset\nof temporally ﬂuctuating solutions (see later on in STAR Methods).\nWe now turn to the problem of evaluating the position of the outlier eigenvalue. In the case of heterogeneous ﬁxed points, the structured and the random components of the matrix Sij are strongly correlated, as they both scale with the multiplicative factor f0ðxj0Þ, which correlates with the particular realization of the random part of the connectivity cij. As a consequence, cij cannot be considered as a truly random matrix with respect to mif0ðxj0Þnj=N, and in contrast to the case of homogeneous ﬁxed points, results from (Girko, 1985) do not hold.\nWe determined numerically the position of the outlier in ﬁnite-size eigenspectra (Figures S1D–S1F). We found that its value indeed signiﬁcantly deviates from the only non-zero eigenvalue of the rank-one structure mif0 ðxj0Þnj=N, which can be computed in the meanﬁeld framework (when r = 0, it corresponds to MmMnh½f0ii + MnkS2mh½f0i0i). On the other hand, the value of the outlier coincides exactly with the eigenvalue of mif0 ðxj0Þnj=N whenever the random component cij is shufﬂed (black dots in Figure S1F). This observation conﬁrms that the position of the outlier critically depends on the correlations existing between the rank-one structure mif0 ðxj0Þnj=N and the speciﬁc realization of the random bulk cij.\n\nMean-ﬁeld analysis of transient dynamics and stability of stationary solutions\n\nAs for heterogeneous ﬁxed points we were not able to assess the position of the outlying eigenvalue using random matrix theory, we turned to a mean-ﬁeld analysis to determine transient activity. This analysis allowed us to determine accurately the position of the outlier, and therefore the stability of heterogeneous ﬁxed points. The approach exploited here is based on (Kadmon and Sompolinsky, 2015).\n\nWe consider the stability of the single unit activation xi when averaged across different realizations of the random connectivity and its random eigenmodes. Directly averaging across realizations the network dynamics deﬁned in Equation 6 yields the time evolution of the mean activation mi of unit i:\n\nm_ iðtÞ = À miðtÞ + mikðtÞ:\n\n(45)\n\ne7 Neuron 99, 609–623.e1–e29, August 8, 2018\n\nWe observe that we can write: miðtÞ = mi~kðtÞ, where ~k is the low-pass ﬁltered version of k: ð1 + d=dtÞ~kðtÞ = kðtÞ. Small perturbations around the ﬁxed point solution read: miðtÞ = m0i + m1i ðtÞ. The equilibrium values m0i correspond to the DMF stationary solution computed from Equation 28 and 40: m0i = mik0. The ﬁrst-order perturbations thus obey:\n\nm_ 1i ðtÞ = À m1i ðtÞ + mik1ðtÞ;\n\n(46)\n\nindicating that the decay timescale of the mean activity is inherited by the decay time constant of k1. An additional equation for the\n\ntime evolution of k1 thus needs to be derived.\n\nWhen activity is perturbed, the ﬁring activity fi of unit i can be evaluated at the ﬁrst order: f0i /f0i + f1i ðtÞ = fðxi0Þ + f0 ðxi0Þxi1ðtÞ. As a consequence, the ﬁrst-order in k reads:\n\nk1\n\nðtÞ\n\n=\n\n  ni\n\nÂf0\n\nÀxi0\n\nÁxi1\n\nÃ  ðtÞ :\n\n(47)\n\nSumming Equation 47 to its time-derivative, we get:\n\nk_ 1ðtÞ =\n\nÀ\n\nk1\n\nðtÞ\n\n+\n\n  1\n\n+\n\nd    dt ni\n\nÂf0\n\nÀxi0\n\nÁxi1\n\nÃ  ðtÞ :\n\n(48)\n\nIn order to simplify the r.h.s., we start by considering the average with respect to the random part of the connectivity for a single unit\n\ni. In order to compute ½f0 ðxi0Þxi1, we explicitly build xi0 and xit : = xiðtÞ as Gaussian variables centered respectively in m0i and mti . We will call DI00 and DI0t the variances of the two variables, and DI;t:0 their two-times correlation deﬁned by DI;t:0 = ½xitxi0 À ½xit½xi0. We can then\n\nwrite the two variables as\n\nqﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pﬃﬃﬃﬃﬃﬃﬃﬃﬃ\n\nxi0\n\n=\n\nm0i\n\n+qﬃﬃDﬃﬃﬃI0ﬃ0ﬃﬃﬃÀﬃﬃﬃﬃﬃDﬃﬃﬃIﬃ;ﬃtﬃ:0\n\nx1\n\n+ DI;t:0 pﬃﬃﬃﬃﬃﬃﬃﬃﬃ\n\ny\n\n(49)\n\nxit = mti + DI0t À DI;t:0x2 + DI;t:0y\n\nThe ﬁrst-order response of xi is given by the difference between xit and xi0, and reads: qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\n\nxi1 = m1i + DI0t À DI;t:0x2 À DI00 À DI;t:0x1:\n\n(50)\n\nAs in classical DMF derivations (Sompolinsky et al., 1988; Rajan et al., 2010; Kadmon and Sompolinsky, 2015), x1, x2and y are stan-\n\ndard normal variables. By integrating over their distributions we can write:\n\nÂf0 Àxi0Áxi1Ã = Z\n\nZ Dx1\n\n  qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ   Z Dx2 m1i + DI0t À DI;t:0 x2 À DI00 À DI;t:0 x1\n\n  qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pﬃﬃﬃﬃﬃﬃﬃﬃﬃ   Dyf0 m0i + DI00 À DI;t:0 x1 + DI;t:0 y :\n\n(51)\n\nIntegrating by parts as in Equation 38 we get:\n\nÂf0 Àxi0Áxi1Ã = m1i Âf0i Ã + ÀDI;t:0 À DI00ÁÂf0i0 Ã\n\n(52)\n\nwhere the Gaussian integrals ½f0i  and ½f0i0  are evaluated using the ﬁxed point statistics.\n\nNote that, at the ﬁxed point, DI;t:0 = DI00. As a consequence, DI;t:0 À DI00 gives a ﬁrst-order response:\n\nDI;1:0: = DI;t:0 À DI00 = Âxi1xi0Ã À Âxi1ÃÂxi0Ã = Âxi1xi0Ã À m0i m1i\n\n(53)\n\nwhich can be rewritten as a function of the global second-order statistics D1:0 = h½xi1xi0i À h½xi1ih½xi0i as:\n\nDI;1:0 = D1:0 À È m1i m0i   À  m1i   m0i  É = D1:0 À S2m~k0~k1:\n\n(54)\n\nEquation 54 can be rewritten in terms of the ﬁrst-order perturbation for the global equal-time variance: D10 = Dt0 À D00. We consider\n\nthat, by deﬁnition:\n\nD1:0\n\n=\n\nXN\nj=1\n\nxj1\n\nvDt:0 vxjt\n\nD10\n\n=\n\nXN\nj=1\n\nxj1\n\nvDt0 vxjt\n\n      0\n\n      :\n\n0\n\n(55)\n\nNeuron 99, 609–623.e1–e29, August 8, 2018 e8\n\nWe then observe that, when the derivatives are evaluated at the ﬁxed point, we have:\n\nvDt:0 vxjt\n\n     \n\n0\n\n=\n\n1 2\n\nvDt0 vxjt\n\n     \n\n0\n\n;\n\n(56)\n\nand we conclude that:\n\nD1:0 = 12D10\n\n(57)\n\nEquation 52 thus becomes:\n\nÂf0 Àxi0Áxi1Ã\n\n=\n\nmi\n\n~k1Âf0i Ã\n\n+\n\n D10 2\n\nÀ\n\nS2m\n\n~k0\n\n~k1\n\n Âf0i0\n\nÃ :\n\n(58)\n\nIn a second step, we perform the average across different units of the population, by writing m and n as in Equation 34. After some algebra, we get:\n\n  ni\n\nÂf0\n\nÀxi0\n\nÁxi1\n\nÃ  ðtÞ\n\n=\n\n~k1\n\nÂ ðMm\n\nMn\n\n+\n\nrSm Sn\n\nÞ Âf0i\n\nÃ \n\n+\n\nrk0\n\nMm Sm Sn  Âf0i0\n\nÃ Ã\n\n+\n\nD10 2\n\nÂ Mn\n\n Âf0i0\n\nÃ \n\n+\n\nrk0 Sm Sn  Âf0i00\n\nÃ Ã\n\n(59)\n\n: = ~k1a + D10b\n\nwhere constants a and b were deﬁned as:\n\na = ðMmMn + rSmSnÞ Âf0i Ã  + rk0MmSmSn Âf0i0 Ã \n\nb\n\n=\n\n1 2\n\nÈ Mn\n\n Âf0i0\n\nÃ \n\n+\n\nrk0\n\nSm\n\nSn\n\n Âf0i00\n\nÃ É :\n\n(60)\n\nThe time evolution of k can be ﬁnally rewritten as:\n\nk_ 1ðtÞ =\n\nÀ\n\nk1\n\nðtÞ\n\n+\n\n  1\n\n+\n\nd  È~k1 dt\n\na\n\n+\n\nD10\n\nÉ b;\n\n(61)\n\nso that the time evolution of the perturbed variance must be considered as well. In order to isolate the evolution law of D0, we rewrite the activation variable xiðtÞ by separating the uniform and the heterogeneous\ncomponents: xiðtÞ = mðtÞ + dxiðtÞ. The time evolution for the residual dxiðtÞ is given by:\n\nd_xiðtÞ =\n\nÀ\n\nXN dxiðtÞ + g cijfðxjðtÞÞ + ðmi\n\nÀ\n\nMmÞkðtÞ\n\n(62)\n\nj=1\n\nso that, squaring:\n\n  ddxi\n\nðtÞ 2\n\ndt\n\n+\n\n2dxi ðtÞ\n\nddxi ðtÞ dt\n\n+\n\ndxi ðtÞ2\n\n=\n\ng2\n\nXN\nj=1\n\nXN\nk=1\n\ncij cik fðxj ðtÞÞfðxk ðtÞÞ\n\n+\n\nðmi\n\nÀ\n\nMm Þ2 kðtÞ2\n\n+\n\ngðmi\n\nÀ\n\nMmÞkðtÞ\n\nXN\nk=1\n\ncij fðxk ðtÞÞ:\n\n(63)\n\nAveraging over i and the realizations of the disorder yields:\n\ndD0ðtÞ = dt\n\nÀ\n\nD0ðtÞ\n\n+\n\ng2\n\n Âf2i\n\nÃ  ðtÞ\n\n+\n\nS2m\n\nkðtÞ2\n\nÀ\n\n*\"  ddxi\n\nðtÞ 2\n\n#+\n\ndt\n\n:\n\n=\n\nÀ\n\nD0\n\nðtÞ\n\n+\n\nGðm;\n\nD0\n\n;\n\nkÞ\n\nÀ\n\n*\"  ddxi\n\nðtÞ 2\n\ndt\n\n#+\n\n(64)\n\nas by deﬁnition we have: h½dxi2ðtÞi = D0ðtÞ.\n\nExpanding the dynamics of D0 to the ﬁrst order, we get:\n\nD_ 10ðtÞ =\n\nÀ\n\nD10\n\nðtÞ\n\n+\n\nm1\n\nvG vm\n\n    \n\n0\n\n+\n\nD10\n\nvG vD0\n\n    \n\n0\n\n+\n\nk1\n\nvG vk\n\n    \n\n0\n\n:\n\n(65)\n\ne9 Neuron 99, 609–623.e1–e29, August 8, 2018\n\nNote that we could neglect the contributions originating from the last term of Equation 64 because they do not enter at the leading\n\norder. Indeed we have:\n\nv vm\n\n*\"  ddxi\n\nðtÞ 2\n\ndt\n\n#+\n\n     \n\n( =2\n\n0\n\nddxi ðtÞ dt\n\nv vm\n\n!) ddxi ðtÞ\ndt\n\n    \n\n0\n\n=\n\n0\n\n(66)\n\nsince temporal derivatives for every i vanish when evaluated at the ﬁxed point.\n\nA little algebra returns the last three linear coefﬁcients:\n\nvG vm\n\n    \n\nvG\n\nvD0\n\nvG vk\n\n    \n\n=\n\n2g2\n\n Â fi\n\nf0i\n\nÃ \n\n    0 = g2È Âfi02Ã  +\n\n0\n\n= 2S2mk0:\n0\n\n Â fi\n\nf00 i\n\nÃ É\n\n(67)\n\nCollecting all the results together in Equation 61 we obtain:\n\nk_ 1ðtÞ =\n\nÀ\n\nk1\n\nðtÞ\n\n+\n\nak1\n\nðtÞ\n\n+\n\n& b m1\n\nvG vm\n\n    \n\n0\n\n+ D10vvDG0\n\n    \n\n0\n\n+\n\nk1\n\nvG vk\n\n    \n\n0\n\n' :\n\n(68)\n\nBy averaging Equation 45 we furthermore obtain:\n\nm_ 1ðtÞ = À m1ðtÞ + Mmk1:\n\n(69)\n\nWe ﬁnally obtained that the perturbation timescale is determined by the population-averaged dynamics:\n\n01 01 01\n\nd dt\n\nm1 @ D10\nk1\n\nA=\n\nÀ\n\nm1 @ D10\nk1\n\nm1 A + M@ D10\nk1\n\nA\n\n(70)\n\nwhere the evolution matrix M is deﬁned as:\n\n0\n\n1\n\nM = @ 22bgg22  Â0Âffifif0i Ã0i Ã  \n\nbgg22ÈÈ  ÂÂffi02i02ÃÃ  0++  ÂÂffifif}i }iÃÃ  ÉÉ\n\nMm 2S2mk0 A:\nb2S2mk0 + a\n\n(71)\n\nNote that one eigenvalue of matrix M, which corresponds to the low-pass ﬁltering between k and m, is always ﬁxed to zero. Equations 70 and 71 reveal that, during the relaxation to equilibrium, the transient dynamics of the ﬁrst- and second-order statistics of the activity are tightly coupled. Diagonalizing M allows to retrieve the largest decay timescale of the network, which indicates the average, structural stability of stationary states. When an outlier eigenvalue is present in the eigenspectrum of the stability matrix Sij, the largest decay timescale from M predicts its position. The corresponding eigenvector eb contains indeed a structured component along m, which is not washed out by averaging across different realizations of cij. The second non-zero eigenvalue of M, which vanishes at g = 0, measures a second and smaller effective timescale, which derives from averaging across the remaining N À 1 random modes. Varying g, we computed the largest eigenvalue of M for corresponding stationary solutions of mean-ﬁeld equations. In Figure S1F we show that, when the stability eigenspectrum includes an outlier eigenvalue, its position is correctly predicted by the largest eigenvalue of M. The mismatch between the two values is small and can be understood as a ﬁnite-size effect (Figure S1E, gray). To conclude, we found that the stability of arbitrary stationary solutions can be assessed by evaluating, with the help of mean-ﬁeld theory, both the values of the radius (Equation 44) and the outlier (Equation 71) of the stability eigenspectrum. Instabilities led by the two different components are expected to reshape activity into two qualitatively different classes of dynamical regimes, which are discussed in detail, further in STAR Methods, for two speciﬁc classes of structures.\n\nDynamical Mean Field equations for chaotic solutions When a stationary state loses stability due to the compact component of the stability eigenspectrum, the network activity starts developing irregular temporal ﬂuctuations. Such temporally ﬂuctuating states can be described within the DMF theory by taking into account the full temporal auto-correlation function of the effective noise hi (Sompolinsky et al., 1988). For the sake of simplicity, here we derive directly the mean-ﬁeld equations for population-averaged statistics, and we eventually link them back to single unit quantities.\n\nNeuron 99, 609–623.e1–e29, August 8, 2018 e10\n\nBy differentiating twice Equation 11, and by substituting the appropriate expression for the statistics of the noise hi, we derive that the auto-correlation function DðtÞ = h½xiðt + tÞxiðtÞi À h½xiðtÞi2 obeys the second-order differential equation:\n\nD€ ðtÞ = DðtÞ À g2h½fiðtÞfiðt + tÞi À S2mk2:\n\n(72)\n\nIn this context, the activation variance D0 coincides with the peak of the full auto-correlation function: D0 = Dðt = 0Þ. We expect the total variance to include a temporal term, coinciding with the amplitude of chaotic ﬂuctuations, and a quenched one, representing the\nspread across the population due to the disorder in cij and the structure imposed by the right-connectivity vector m. In order to compute the full rate auto-correlation function h½fiðtÞfiðt + tÞi, we need to explicitly build two correlated Gaussian vari-\nables xðtÞ and xðt + tÞ, such that:\n\n h½Âxxi ði2tðÞtÞiÃ= \n\nh½xiðt + tÞi À h½xiðtÞi2\n\n= =\n\n mÂxi2\n\nðt\n\n+\n\nÃ  tÞ\n\nÀ\n\nh½xi\n\nðtÞi2\n\n=\n\nD0\n\n(73)\n\nh½xiðt + tÞxiðtÞi À h½xiðtÞi2 = DðtÞ:\n\nFollowing previous studies (Sompolinsky et al., 1988; Rajan et al., 2010), we obtain:\n\nZ\n\nZ\n\n  pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pﬃﬃﬃ  !2\n\nh½fiðtÞfiðt + tÞi = Dz Dxf m + D0 À D x + D z\n\n(74)\n\nwhere we used the short-hand notation D : = DðtÞ and we assumed for simplicity D > 0. As we show later, this requirement is satisﬁed by our ﬁnal solution.\nIn order to visualize the dynamics of the solutions of Equation 72, we study the equivalent problem of a classical particle moving in a one-dimensional potential (Sompolinsky et al., 1988; Rajan et al., 2010):\n\nD€ðtÞ =\n\nÀ\n\nvV vD\n\n(75)\n\nwhere the potential V is given by an integration over D:\n\nVðD; D0Þ =\n\nÀ\n\nD2 2\n\n+ g2h½FiðtÞFiðt + tÞi + S2mk2D\n\n(76)\n\nand\n\nFðxÞ\n\n=\n\nRx\nÀN\n\nfðx0\n\nÞdx0\n\n.\n\nAs\n\nthe\n\npotential\n\nV\n\ndepends\n\nself-consistently\n\non\n\nthe\n\ninitial\n\ncondition\n\nD0,\n\nthe\n\nshape of\n\nthe\n\nauto-correlation\n\nfunction DðtÞ depends parametrically on the value of D0. Similarly to previous works, we isolate the solutions that decay monoton-\n\nically from D0 to an asymptotic value Dðt/NÞ : = DN, where DN is determined by dV=dD j D = DN = 0. This translates into a ﬁrst condition to be imposed. A second equation comes from the energy conservation condition: VðD0;D0Þ = VðDN;D0Þ. Combined with the\n\nusual equation for the mean m and the overlap k, the system of equations to be solved becomes:\n\nm = Mmk\n\nk = Mnh½fii + rk Âf0i Ã \n\nD20\n\nÀ\n\nD2N\n\n=\n\n& g2\n\nZ\n\n2\n\n  pﬃﬃﬃﬃﬃ   Z DzF2 m + D0 z À\n\nDz\n\nZ\n\n  pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pﬃﬃﬃﬃﬃﬃﬃ  !2'\n\nDxF m + D0 À DN x + DN z\n\n+ S2mk2ðD0 À DNÞ\n\n(77)\n\nZ\n\nZ\n\n  pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pﬃﬃﬃﬃﬃﬃﬃ  !2\n\nDN = g2 Dz Dxf m + D0 À DN x + DN z + S2mk2:\n\nThe temporally ﬂuctuating state is therefore described by a closed set of equations for the mean activity m, the overlap k, the zerolag variance D0 and the long-time variance DN. The difference D0 À DN represents the amplitude of temporal ﬂuctuations. If temporal ﬂuctuations are absent, D0 = DN, and the system of equations we just derived reduces to the DMF description for stationary solutions given in Equation 40.\nA similar set of equations can be derived for single unit activity. As for static stationary states, the mean activity of unit i is given by\n\nmi = mik:\n\n(78)\n\nThe static variance around this mean activity is identical for all units and given by\n\nZ\n\nZ\n\n  pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pﬃﬃﬃﬃﬃﬃﬃ  !2\n\nDIN = g2 Dz Dxf m + D0 À DN x + DN z = DN À S2mk2\n\n(79)\n\nwhile the temporal component DIT of the variance is identical to the population averaged temporal variance\n\nDIT = D0 À DN:\n\n(80)\n\nTo conclude, similarly to static stationary states, the structured connectivity Pij shapes network activity in the direction deﬁned by its right eigenvector m whenever the overlap k does not vanish. For this reason, the mean-ﬁeld theory predicts in some parameter\n\ne11 Neuron 99, 609–623.e1–e29, August 8, 2018\n\nregions the existence of more than one chaotic solution. A formal analysis of the stability properties of the different solutions has not\n\nbeen performed. We nevertheless observe from numerical simulations that chaotic solutions tend to inherit the stability properties of\n\nthe stationary solution they develop from. Speciﬁcally, when an homogeneous solution generates two heterogeneous bistable ones,\n\nwe notice that the former loses stability in favor of the latter.\n\nWe ﬁnally observe that the critical coupling at which the DMF theory predicts the onset of chaotic ﬂuctuations can be computed by\n\nimposing that, at the critical point, the concavity of the potential function VðDÞ is inverted (Sompolinsky et al., 1988; Harish and\n\nHansel, 2015):\n\nd2 V ðD; dD2\n\nD0\n\nÞ\n\n    \n\nDN\n\n=\n\n0\n\n(81)\n\nand the temporal component of the variance vanishes: D0 = DN. These two conditions are equivalent to the expression: 1 = g2h½fI02i where, as we saw, g2h½fi02i coincides with the squared value of the radius of the compact component of the stability eigenspectrum (Equation 44). In the phase diagram of Figure 1B, we solved this equation for g to derive the position of the instability boundary from\n\nstationary to chaotic regimes.\n\nSpontaneous dynamics: structures overlapping on the unitary direction\nIn this section, we analyze in detail a speciﬁc case, in which the connectivity vectors m and n overlap solely along the unitary direction u = ð1; 1;.1Þ=N. Within the statistical description of vector components, in this situation the joint probability density pðm; nÞ can be replaced by the product two normal distributions (respectively, N ðMm; S2mÞ and N ðMn;S2nÞ). The mean values Mm and Mn represent the projections of m and n on the common direction u, and the overlap between m and n is given by MmMn. The components m and n are otherwise independent, the ﬂuctuations representing the remaining parts of m and n that lie along mutually orthogonal directions. In this situation, the expression for k simpliﬁes to\n\nk = hni½fii = Mnh½fii\n\n(82)\n\nso that a non-zero overlap k can be obtained only if the mean population activity h½fii is non-zero. Choosing independently drawn m and n vectors thus slightly simpliﬁes the mean-ﬁeld network description. The main qualitative features resulting from the interaction between the structured and the random component of the connectivity can however already be observed, and more easily understood, within this simpliﬁed setting.\n\nStationary solutions The DMF description for stationary solutions reduces to a system of two non-linear equations for the population averaged mean m and variance D0:\n\nmD0==Mgm2M Ânfh2i½Ãf i +i\n\n: = Fðm; D0Þ S2m M2n h½fi i2\n\n:\n\n= Gðm; D0Þ:\n\n(83)\n\nThe population averages h½fii and h½f2i i are computed as Gaussian integrals similarly to Equation 39. Equation 83 can be solved numerically for m and D0 by iterating the equations up to convergence, which is equivalent to numerically simulating the two-dimen-\nsional dynamical system given by\n\nm_ ðtÞ = À m + Fðm; D0Þ D_ 0ðtÞ = À D0 + Gðm; D0Þ;\n\n(84)\n\nsince the ﬁxed points of this dynamical system correspond to solutions of Equation 83. Gaussian integrals in the form of h½fii are evaluated numerically through Gauss-Hermite quadrature with a sampling over 200 points. Unstable solutions can be computed by iterating the same equations after having inverted the sign of the time variable in the ﬁrst equation.\nAs the system of equations in Equation 83 is two-dimensional, we can investigate the number and the nature of stationary solutions through a simple graphical approach (Figure S1G). We plot on the m À D0 plane the loci of points where the two individual equations\n\nm = Fðm; D0Þ D0 = Gðm; D0Þ\n\n(85)\n\nare satisﬁed. In analogy with dynamical systems approaches, we refer to the two corresponding curves as the DMF nullclines. The\nsolutions of Equation 83 are then given by the intersections of the two nullclines. To begin with, we focus on the nullcline deﬁned by the ﬁrst equation (also referred to as the m nullcline). With respect to m, Fðm; D0Þ is\nan odd sigmoidal function whose maximal slope depends on the value of D0 and MmMn. When g = 0 and Sm = 0, the input variance D0 vanishes. In this case, the points of the m nullcline trivially reduce to the roots of the equation: m = MmMnfðmÞ, which admits either one ðMmMn < 1Þ, or three solutions ðMmMn > 1Þ. Non-zero values of g and Sm imply ﬁnite and positive values of D0. As D0 increases, the solutions to the equation m = MmMnh½fii vary smoothly, delineating the full nullcline in the m À D0 plane. As in the case without\n\nNeuron 99, 609–623.e1–e29, August 8, 2018 e12\n\ndisorder (g = 0 and Sm = 0), for low structure strengths (MmMn < 1), the m nullcline consists of a unique branch: m = 0 cD0. At high structure strengths ðMmMn > 1Þ, instead, its shape smoothly transforms into a symmetric pitchfork.\nThe D0 nullcline is given by the solutions of D0 = Gðm; D0Þ for D0 as function of m. As Gðm; D0Þ depends quadratically on m, the D0 nullcline has a symmetric V-shape centered in m = 0. The ordinate of its vertex is controlled by the parameter g, as the second term of the second equation in 83 vanishes at m = 0. For m = 0, the slope of Gðm; D0Þ in D0 = 0 is equal to g2. As a consequence, for g < 1, the vertex of the D0 nullcline is ﬁxed in (0,0), while for g > 1, the vertex is located at D0 > 0 and an isolated point remains at ð0; 0Þ.\nThe stationary solutions of the DMF equations are determined by the intersections between the two nullclines. For all values of the parameters, the nullclines intersect in m = 0, D0 = 0, corresponding to the trivial, homogeneous stationary solution. The existence of other solutions are determined by the qualitative features of the individual nullclines, that depend on whether MmMn and g are smaller or greater than one (Figure S1G). The following qualitative situations can be distinguished: (i) for MmMn < 1 and g < 1, only the trivial solutions exist; (ii) for MmMn > 1, two additional, symmetric solutions exist for non-zero values of m and D0, corresponding to symmetric, heterogeneous stationary states; (iii) for g > 1, an additional solution exist for m = 0 and, corresponding to a heterogeneous solution in which individual units have non-zero stationary activity, but the population-average vanishes. For MmMn > 1, this solution can coexist with the symmetric heterogeneous ones, but in the limit of large g these solutions disappear (Figure S1G).\nThe next step is to assess the stability of the various solutions. As explained earlier on, the stability of the trivial state m = 0, D0 = 0 can be readily assessed using random matrix theory arguments (Figures S1A and S1B). This state is stable only for MmMn < 1 and g < 1. At MmMn = 1, it loses stability due to the outlying eigenvalue of the stability matrix, leading to the bifurcation already observed at the level of nullclines. At g = 1, the instability is due to the radius of the bulk of the spectrum. This leads to a chaotic state, not predicted from the nullclines for the stationary solutions.\nThe stability of heterogeneous stationary states is assessed by determining separately the radius of the bulk of the spectrum and the position of the outlier (Figures S1D–S1F). The radius is determined from Equation 44. The outlier is instead computed as the lead-\ning eigenvalue of the stability matrix given in Equation 71. Note that in the present framework, where the overlap is deﬁned along the unitary direction, it is possible to show that the latter is equivalent to computing the leading stability eigenvalue of the effective dynamical system introduced in Equation 84, linearized around the corresponding ﬁxed point. The bifurcation obtained when the\noutlier crosses unity is equivalent to the bifurcation predicted from the nullclines when the symmetric solutions disappear in favor of the heterogeneous solution of mean zero (Figure S1G). For MmMn > 1, we however ﬁnd that as g is increased, the radius of the bulk of the spectrum always leads to a chaotic instability before the outlier becomes unstable. Correspondingly, the m = 0 and D0 > 0 stationary state that exist for large g is never stable.\n\nChaotic solutions\n\nFor large g, the instabilities of the stationary points generated by the bulk of the spectrum are expected to give rise to chaotic dy-\n\nnamics. We therefore turn to the DMF theory for chaotic states, which are described by an additional variable that quantiﬁes temporal\n\nﬂuctuations. For the case studied here of connectivity vectors m and n overlapping only along the unitary direction, Equation 77\n\nbecome\n\nZ\n\n  pﬃﬃﬃﬃﬃ  \n\nm = Fðm; D0; DNÞ = MmMn Dzf m + D0 z\n\nD0 = Gðm; D0; DNÞ =\n\n(Z D2N + 2g2\n\n  pﬃﬃﬃﬃﬃ   Z DzF2 m + D0z À\n\nDz\n\nZ\n\n  DxF m\n\n+\n\npﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ D0 À DNx\n\n+\n\npﬃﬃﬃﬃﬃﬃﬃ  !2' DNz\n\n+\n\nM2n S2m h½fi i2 ðD0\n\nÀ\n\ni1 DNÞ 2\n\nZ\n\nZ\n\n  pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pﬃﬃﬃﬃﬃﬃﬃ  !2\n\nDN = Hðm; D0; DNÞ = g2 Dz Dxf m + D0 À DN x + DN z + M2nS2mh½fii2:\n\n(86)\n\nAs the system to be solved is now three-dimensional, graphical approaches have only limited use. Similarly to the stationary state, a practical and stable way to ﬁnd numerically the solutions is to iterate the dynamical system given by\n\nm_ = À m + Fðm; D0; DNÞ\n\nD_ 0 = À D0 + Gðm; D0; DNÞ\n\n(87)\n\nD_ N = À DN + Hðm; D0; DNÞ:\n\nwhere the double Gaussian integrals from Equation 86 can be evaluated numerically as two nested Gauss-Hermite quadratures. Note that stationary states simply correspond to solutions for which D0 = DN.\nAs for stationary solutions, different types of chaotic solutions appear depending on the values of the structure strength MmMn and the disorder strength g. If g > 1 and MmMn < 1, a single chaotic state exists corresponding to m = 0 and DN = 0, meaning that the temporally averaged activity of all units vanishes, so that ﬂuctuations are only temporal (Figure 1B red). As MmMn crosses unity,\n\ne13 Neuron 99, 609–623.e1–e29, August 8, 2018\n\ntwo symmetric states appear with non-zero values of m and DN. These states correspond to bistable heterogeneous chaotic states (Figure 1B orange) that are analogous to bistable heterogeneous stationary states.\n\nThe critical disorder strength gB at which heterogeneous chaotic states emerge (gray boundary in the phase diagram of Figure 1) is computed by evaluating the linear stability of the dynamics in 87 around the central solution ð0; D0; 0Þ. A long but straightforward\n\nalgebra reveals that the stability matrix, evaluated in, is simply given by\n\n0 BBBB@\n\nMm\n\nMn 0\n\n f0\n\n \n\n0 g2À f2  +  Ff0   À hFi f0  Á\nD0\n\n1\n\n0\n\n0\n\nCCCCA;\n\n(88)\n\n0\n\n0\n\ng2 f0  2\n\nsuch that gB corresponds to the value of the random strength g for which the largest of its three eigenvalues crosses unity.\n\nSpontaneous dynamics: structures overlapping on an arbitrary direction In the previous section, we focused on the simpliﬁed scenario where the connectivity vectors m and n overlapped only in the unitary direction. Here, we brieﬂy turn to the opposite case where the overlap along the unitary direction u vanishes (i.e., Mm = 0; Mn = 0), but the overlap r along a direction orthogonal to u is non-zero. As we will show, although the equations describing the network activity present some formal differences, they lead to qualitatively similar regimes. The same qualitative results apply as well to the general case, where an overlap exists on both the unitary and an orthogonal direction.\nThe network dynamics can be studied by solving the DMF Equations 40 and 77 by setting m = 0. Stationary solutions are now determined by:\n\nDk =0 =rkgS2 mÂSfn2i hð½0f;0\n\niDð00Þ; ÃD 0+ÞiS:2m\n\n=F k2 :\n\nðk; D0Þ = Gðk;\n\nD0\n\nÞ:\n\n(89)\n\nNote that, in this more general case, the relevant ﬁrst-order statistics of network activity is given by the overlap k, which now can\n\ntake non-zero values even when the population-averaged activity h½fii vanishes.\n\nAs in the previous case, the stationary solutions can be analyzed in terms of nullclines (Figure S2A). The main difference lies in the k\n\nnullcline given by k = rkSmSnh½f0i ð0;D0Þi. As both sides of the ﬁrst equation are linear and homogeneous in k, two classes of solutions exist: a trivial solution (k = 0 for any D0), and a non-trivial one (D0 = D~0 for any k), with D~0 determined by:\n\n Âf0i\n\nÀ 0;\n\nD~ 0ÁÃ \n\n=\n\n1=ðrSm\n\nSn\n\nÞ:\n\n(90)\n\nBecause 0 < f0ðxÞ < 1, Equation 90 admits non-trivial solutions only for sufﬁciently large overlap values: r > 1=SmSn. In consequence, the k nullcline takes qualitatively different shapes depending on the value of r: (i) for r < 1=SmSn, it consists only of a vertical branch k = 0 (ii) for r > 1=SmSn an additional horizontal branch D0 = D~0 appears (Figure S2A).\nThe D0 branch is qualitatively similar to the previously studied case of m and n overlapping along the unitary direction, with a qualitative change when the disorder parameter g crosses unity.\nThe stationary solutions are given by the intersections between the two nullclines. Although the shape of the k nullcline is distinct from the shape of the m nullcline studied in the previous case, qualitatively similar regimes are found. The trivial stationary state k = 0, D0 = 0 exists for all parameter values. When the structure strength rSmSn exceeds unity, two symmetric heterogeneous states appear with non-zero k values of opposite signs (but vanishing mean m). Finally for large g an additional state appears with k = 0, D0 > 0.\nSimilarly to Figure 1, the solutions of Equation 89, which correspond to stationary activity states, are shown in blue in Figures S2B–S2D.\nIn Figure S2B we address their stability properties: again we ﬁnd that when non-centered stationary solutions exist, the central ﬁxed point becomes unstable. The instability is led by the outlier eigenvalue of the stability eigenspectrum. Similarly to Figure 1, furthermore, the DMF theory predicts an instability to chaotic phases for high g values. As for stationary states, both heterogeneous and homogeneous chaotic solutions are admitted (Figures S2C and S2D); heterogeneous chaotic states exist in a parameter region where the values of g and r are comparable.\n\nResponse to external inputs\nIn this section, we examine the effect of non-vanishing external inputs on the network dynamics. We consider the situation in which every unit receives a potentially different input Ii, so that the pattern of inputs at the network level is characterized by the N-dimensional vector I = fIig. The network dynamics in general depend on the geometrical arrangement of the vector I with respect to the connectivity vectors m and n. Within the statistical description used in DMF theory, the input pattern is therefore characterized by the ﬁrst- and second-order statistics MI and SI of its elements, as well as by the value of the correlations SmI and SnI with the vectors m and n. In geometric terms, MI quantiﬁes the component of I along the unit direction u, while SmI and SnI quantify the overlaps with m\n\nNeuron 99, 609–623.e1–e29, August 8, 2018 e14\n\nand n along directions orthogonal to u. For the sake of simplicity, here we consider two connectivity vectors m and n that overlap solely on the unitary direction (r = 0). The two vectors thus read (see Equation 34):\n\nm = Mm + Smx1 n = Mn + Snx2:\n\n(91)\n\nThe input pattern can overlap with the connectivity vectors on the common (u) and on the orthogonal directions (x1 and x2). It can moreover include further orthogonal components of strength St. The most general expression for the input vector can thus be written as:\n\nI\n\n=\n\nMI\n\n+\n\nSmI Sm\n\nx1\n\n+\n\nSnI Sn\n\nx2\n\n+\n\nSt\n\nh\n\n(92)\n\nwhere h is a standard normal vector. We ﬁrst focus on the equilibrium response to constant inputs, and then turn to transient\ndynamics.\nThe mean-ﬁeld equations in presence of external inputs can be derived in a straightforward fashion by following the same steps as in the input-free case. We start by considering the statistics of the effective coupling term, which is given by xiðtÞ = hiðtÞ + IiðtÞ, with hiðtÞ deﬁned as in Equation 20. We can then exploit the statistics of hiðtÞ which have been computed in the previous paragraphs to obtain the equation for the mean activity:\n\nmi = ½xi = mik + Ii:\n\n(93)\n\nEquation 93 indicates that the direction of the average network activity is determined by a combination of the structured recurrent connectivity and the external input pattern. The ﬁnal direction of the activation vector in the N-dimensional population space is controlled by the value of the overlap k, which depends on the relative orientations of m, n and I. Its value is given by the self-consistent equation:\n\nk = (hni½fZii  \n\nqﬃﬃﬃﬃﬃ  )\n\n= ni Dzf mik + Ii + DI0 z\n\n(94)\n\n=\n\nMn\n\nh½fi\n\ni\n\n+\n\nSnI\n\n Âf0i\n\nÃ  ;\n\nas both vectors m and I share non-trivial overlap directions with n.\n\nThe second-order statistics of the noise are given by:\n\nÂ xi\n\nðtÞxj\n\nðt\n\n+\n\nÃ tÞ\n\n=\n\ndij\n\ng2\n\nh½fi\n\nðtÞfi\n\nðt\n\n+\n\ntÞi\n\n+\n\nmi\n\nmj\n\nk2\n\n+\n\nðmi\n\nIj\n\n+\n\nmj\n\nIi\n\nÞk\n\n+\n\nIi\n\nIj\n\n:\n\n(95)\n\nAveraging across the population we obtain:\n\nh½xi\n\nðtÞxi\n\nðt\n\n+\n\ntÞi\n\nÀ\n\nh½xi\n\nðtÞi2\n\n=\n\ng2\n\n Â f2i\n\nÃ \n\n+\n\nS2m\n\nk2\n\n+\n\n2SmI\n\nk\n\n+\n\nS2I\n\n:\n\n(96)\n\nThe ﬁrst term of the r.h.s. represents the quenched variability inherited from the random connectivity matrix, while S2m = S2mk2 + 2SmIk + S2I represents the variance induced by the structure, which is inherited from both vectors m and I (Equation 93). From Equation 92, the variance of the input reads:\n\nS2I\n\n=\n\nS2mI S2m\n\n+\n\nS2nI S2n\n\n+\n\nS2t :\n\n(97)\n\nThe ﬁnal DMF equations to be solved are given by the following system:\n\nmDk€===MMDnmÀhk½fÈ+igMi2+hI½fSinðIt ÞÂffð0itÃ+  tÞi + S2mk2 + 2SmIk + S2I É\n\n(98)\n\nwhich, similarly to the cases we examined in detail so far, admits both stationary and chaotic solutions. As for spontaneous dynamics,\n\nthe instabilities to chaos are computed by evaluating the radius of the eigenspectrum of the stability matrix Sij (Equation 44). The sta-\n\nbility matrix can admit an outlier eigenvalue as well, whose value can be predicted with a mean-ﬁeld stability analysis. Extending the\n\narguments already presented in the previous paragraphs allows to show that the effective stability matrix M is given by:\n\nM\n\n=\n\n0 @\n\n22bgg22  Â0Âffifif0i Ã0i Ã  \n\nbgg22ÈÈ  ÂÂffi0 2i0 2ÃÃ  0++  ÂÂffi fi f0i00iÃ0 Ã  ÉÉ\n\nMm\n\n1\n\nbÀ22SS2m2mkk00++22SSmmIÁI + a A;\n\n(99)\n\ne15 Neuron 99, 609–623.e1–e29, August 8, 2018\n\nwith:\n\na = MmMn Âf0i Ã  + MmSnI Âf0i0Ã \n\nb\n\n=\n\n1 2\n\nÈMn Âf}i\n\nÃ \n\n+\n\nSnI\n\n Âf0i00\n\nÃ É :\n\n(100)\n\nAs in the input-free case, when the stability eigenspectrum contains one outlier eigenvalue, its position is well predicted by the largest eigenvalue of M.\nIn the following, we refer to Figure 2 and analyze in detail the contribution of every input direction to the ﬁnal network dynamics. In Figure 2D (left), we consider a unit-rank structure whose vectors m and n are orthogonal: Mm = Mn = 0. The input direction is orthogonal to the connectivity vectors: SmI = SnI = 0, so that the input strength is quantiﬁed by the amplitude of the component along h ðStÞ. In this conﬁguration, because of Equation 94, the amount of structured activity quantiﬁed by k systematically vanishes. In Figure 2D (center), we consider again orthogonal connectivity vectors, but we take an input pattern which overlaps with n along x2. We keep St = 1 ﬁxed and we vary the component of the input along n by increasing SnI. As can be seen from the equation for k (Equation 98), the overlap SnI between the input and the left vector n has the effect of increasing the value of k, which would otherwise vanish since the structure has null strength ðMn = 0Þ. In response to the input, a structured state emerges. From the same equation, furthermore, one can notice that the SnI term has the effect of breaking the sign reversal symmetry ðx/ À xÞ that characterizes the mean-ﬁeld equations in the case of spontaneous dynamics. In Figure 2D (right), we include strong non-vanishing structure strengths ðMmMn = 3:5Þ. In absence of external activity, the network dynamics thus admit two bistable solutions (Figure 1). We consider an input pattern that correlates with n but is orthogonal to the structure overlap direction (MI = 0, SnI > 0). In this conﬁguration, the external input has the effect of disrupting the symmetry between the two stable solutions. For sufﬁciently strong input values, one of the two stable solutions disappears by annihilating with the unstable one. In Figure S4C, we show that the value of the critical input strength for which one of the two stable solution disappears can be controlled by an additional external input that overlaps with n on a different, orthogonal direction. Speciﬁcally, in Figure S4C, we tune the additional input along the direction of the structure overlap u. This input component can be thought as a modulatory signal which controls the way the network dynamics process the input stimulus along x2. In models of computational tasks that employ nonlinear input responses (Figure 4), a modulatory input along the structure overlap can regulate the threshold value of the input strength that the network has learnt to detect. Similarly, in Figures 5 and 6, modulatory inputs are used to completely block the response to the non-relevant input stimulus, so that the readout can produce context-dependent outputs.\n\nAsymmetric solutions A major effect of external inputs is that they break the sign reversal symmetry ðx/ À xÞ present in the network dynamics without inputs. As a consequence, in the parameter regions where the network dynamics admit bistable structured states, the two stable solutions are characterized by different statistics and stability properties.\nTo illustrate this effect, we focus on the simple case where the external input pattern I overlaps with the connectivity vectors m and n solely on the unitary direction (MIs0, SmI = SnI = 0). The solutions of the system of equations corresponding to stationary states can be visualized with the help of the graphical approach, which unveils the symmetry breaking of network dynamics induced by external inputs (Figure S4D).\nSimilarly to the input-free case, the D0 nullcline consists of a symmetric V-shaped curve. In contrast to before, however, the vertex of the nullcline is no longer ﬁxed in ð0; 0Þ, but takes positive ordinate values also at low g values. The value of Gð0;D0Þ, indeed, does not vanish, because of the ﬁnite contribution from the input pattern S2I .\nThe nullcline curves of m are instead strongly asymmetric. For low MmMn values, one single m nullcline exists. In contrast to the input-free case, this nullcline is no longer centered in zero. As a consequence, it intersects the D0 nullclines in one non-zero point, corresponding to a unique heterogeneous stationary solution. As MmMn increases, a second, separated branch can appear. In contrast to the input-free case, the structure strength at which the second branch appears is not always equal to unity, but depends on the mean value of the input. If MmMn is strong enough, the negative branch of the nullcline can intersect the D0 nullcline in two different ﬁxed points, while a third solution is built on the positive m nullcline. As g increases, the two intersections on the negative branch become closer and closer and they eventually collapse together. At a critical value gB, the network activity discontinuously jumps from negative to positive mean solutions.\nAs they are no longer symmetrical, the stability of the positive and the negative ﬁxed points has to be assessed separately, and gives rise to different instability boundaries. Computing the position of the outlier reveals that, when more than one solution is admitted by the mean-ﬁeld system of equations, the centered one is always unstable.\nAs the stability boundaries of different stationary solutions do not necessarily coincide, in presence of external input patterns the phase diagram of the dynamics are in general more complex (Figures S4A–S4C). Speciﬁcally, hybrid dynamical regimes, where one static solution co-exists with a chaotic attractor, can be observed.\n\nNeuron 99, 609–623.e1–e29, August 8, 2018 e16\n\nTransient dynamics We now turn to transient dynamics evoked by a temporal step in the external input (Figure 2B). We speciﬁcally examine the projection of the activation vector and its average onto the two salient directions spanned by vectors m and I.\nThe transient dynamics of relaxation to a stationary solution can be assessed by linearizing the mean-ﬁeld dynamics. We compute the time course of the average activation vector mi, and we ﬁnally project it onto the two orthogonal directions which are indicated in the small insets of Figure 2B.\nSimilarly to Equation 45, the time evolution of mi is governed by:\n\nm_ iðtÞ = À miðtÞ + mikðtÞ + IiðtÞ\n\n(101)\n\nso that, at every point in time:\n\nmiðtÞ = mi~kðtÞ + eIi ðtÞ;\n\n(102)\n\nwhere ~kðtÞ and eIi ðtÞ coincide with the low-pass ﬁltered versions of kðtÞ and IðtÞ. When the network activity is freely decaying back to an equilibrium stationary state, eIi ðtÞ coincides with a simple exponential relax-\n\nation to the pattern Ii. The decay timescale is set by the time evolution of activity (Equation 6), which is taken here to be equal to unity:\n\neIi ðtÞ = Ii + ÀIiic À IiÁeÀt:\n\n(103)\n\nThe timescale of ~kðtÞ is inherited from the dynamics of kðtÞ. We thus refer to our mean-ﬁeld stability analysis, and we compute the\n\nrelaxation time of the population statistics kðtÞ as the largest eigenvalue of the stability matrix M. The eigenvalue predicts a time con-\n\nstant tr, which is in general larger than unity. As a consequence, the relaxation of kðtÞ obeys, for small displacements:\n\nkðtÞ = k0 + Àkic À k0ÁeÀttr ;\n\n(104)\n\nwhere the asymptotic value of k0 is determined from the equilibrium mean-ﬁeld equations (Equations 98). Finally, the time course of ~kðtÞ is derived as the low-pass ﬁlter version of Equation 104 with unit decay timescale.\n\nRank-two connectivity structures In the following paragraphs, we provide the detailed analysis for network models with rank-two connectivity structures. The structured component of the connectivity can be written as:\n\nPij\n\n=\n\nmði 1Þnðj 1Þ N\n\n+\n\nmði 2Þnðj 2Þ; N\n\n(105)\n\nwhere the vector pairs mð1Þ and mð2Þ, nð1Þ and nð2Þ are assumed to be linearly independent. As in the case of unit-rank structures, we determine the network statistics by exploiting the link between linear stability analysis and\nmean-ﬁeld description. The study of the properties of eigenvalues and eigenvectors for the low-dimensional matrix Pij helps to predict the complex behavior of activity above the instability and to restrict our attention to the cases of interest.\nThe mean activity of the network in response to a ﬁxed input pattern Ii is given by:\n\nmi = k1mði 1Þ + k2mði 2Þ + Ii:\n\n(106)\n\nThe ﬁnal direction of the population activity is thus determined by the overlap values k1 = hnði 1Þ½fii and k2 = hnði 2Þ½fii. The expression of the mean-ﬁeld equations for the ﬁrst- and second-order statistics are determined by the geometrical arrange-\nment of the connectivity and the input vectors. Similarly to the unit-rank case, the simplest mean-ﬁeld solutions correspond to sta-\ntionary states, which inherit the structure of the most unstable eigenvectors of the connectivity matrix Jij. The stability of the heterogeneous stationary states can be assessed as before by evaluating separately the value of the radius (Equation 44) and the\n\nposition of the outliers of the linear stability matrix Sij. Similarly to the unit-rank case, it is possible to compute the position of the outlier eigenvalues by studying the linearized dynamics\n\nof the network statistics close to the ﬁxed point, that is given by:\n\n0 m1 1\n\n0 m1 1\n\n0 m1 1\n\nd dt\n\nBB@\n\nD10 k11\n\nCCA\n\n=\n\nÀ\n\nBB@\n\nD10 k11\n\nCCA\n\n+\n\nMBB@\n\nD10 k11\n\nCCA:\n\nk12\n\nk12\n\nk12\n\n(107)\n\nNote that, in klk, the subscript k = 1; 2 refers to the left vector nðkÞ with which the overlap is computed, while the superscript l = 0; 1 indicates the order of the perturbation away from the ﬁxed point.\nIn order to compute the elements of the linear stability matrix M, we follow and extend the reasoning discussed in details for the unit-rank case. We start by considering the time evolution of the linearized activity m1i , which similarly to Equation 45 reads:\n\nm_ 1i ðtÞ = À m1i + mði 1Þk11 + mði 2Þk12:\n\n(108)\n\ne17 Neuron 99, 609–623.e1–e29, August 8, 2018\n\nAt every point in time, we can write: mti = mði 1Þ~kt1 + mði 2Þ~kt2, where ~ktk is the low-pass ﬁltered version of ktk: ð1 + d=dtÞ~ktk = ktk. In the case of orthogonal (zero mean), random connectivity vectors, we get:\n\nm_ 1ðtÞ = À m1;\n\n(109)\n\nso that the elements in the ﬁrst row of M vanish. In analogy with Equation 64, the linearized dynamics of D0 gives instead:\n\nD_ 10 =\n\nÀ\n\nD10\n\n+\n\n2g2\n\n Â fi\n\nf0i\n\nÃ m1\n\n+\n\ng2È Âfi02Ã \n\n+\n\n Â fi\n\nf0i0\n\nÃ ÉD10\n\n+ 2S2mk01k11 + 2S2mk02k12:\n\n(110)\n\nSimilarly to the unit-rank case (Equation 47), in order to determine the linear response of k1 we need to compute:\n\nk11\n\n=\n\nD nði 1Þ\n\nÂxi1\n\nf0\n\nÀxi0ÁÃE\n\n=\n\nD nði 1Þ\n\nmi\n\nÂf0i\n\nÃE\n\n+\n\n D10 2\n\nÀ\n\n m1i m0i  \n\nÀ\n\n m1i   m0i   Dnði 1ÞÂf}i ÃE\n\n(111)\n\nA similar expression can be derived for k12. In general, the integrals in the r.h.s. can be expressed in terms of the perturbations ~k11, ~k12 and D10, leading to expressions of the form:\n\nk11 = a11~k11 + a12~k12 + b1D10 k12 = a21~k11 + a22~k12 + b2D10:\n\n(112)\n\nApplying the operator ð1 + d=dtÞ to the Equation 111 allows to reshape the results in the ﬁnal matrix form:\n\nM\n\n=\n\n0 BB@\n\n222bbg12gg2 22Â  0fÂÂffifiiff0i Ã0i0i ÃÃ  \n\nbbg12 gg2 È22 ÈÈ Â  fÂÂff0i20i0iÃ22 ÃÃ0  +++ Â  fÂÂffi fii ff0i0 Ã0i0i00 ÃÃÉ  ÉÉ\n\n0\n2S2mk01 2b1S2mk01 + a11 2b2S2mk01 + a21\n\n0\n\n1\n\n2b1\n\n2S2mk02 S2mk02 +\n\na12\n\nCCA;\n\n2b2S2mk02 + a22\n\n(113)\n\nwhere the values of the constants a and b depend on the geometric arrangement of the structure and the input vectors. In the following, we consider several speciﬁc cases of interest. Note that the non-linear network dynamics is determined by the\nrelative orientation of the structure and input vectors, but also by the characteristics of the statistical distribution of their elements. In contrast to the cases we analyzed so far, the precise shape of the distribution of the entries in the connectivity vectors can play an important role when the rank of Pij is larger than unity. In the following, we focus on the case of broadly, normally distributed patterns.\n\nRank-two structures with null overlap The simplest case we consider consists of rank-two matrices whose four connectivity vectors mð1Þ, mð2Þ, nð1Þ, and nð2Þ are mutually\northogonal. From the point of view of responses to inputs, networks with this structure behave as superpositions of two independent\nunit-rank structures. Similarly to the unit-rank case, if the connectivity vectors are orthogonal, the network is silent in absence of external inputs: k1 =\nk2 = 0. A single homogeneous state – stationary or chaotic – is the unique stable attractor of the dynamics. Consistently, the eigenspectrum of Jij does not contain any outlier, since every eigenvalue of Pij vanishes.\nIn order to compute the eigenspectrum of Pij, we can rotate the matrix onto a basis deﬁned by an orthonormal set of vectors, and compute its eigenvalues in the transformed basis. For simplicity, we consider an orthonormal set whose ﬁrst four vectors are built\nfrom the connectivity vectors:\n\nu1 = a1mð1Þ u2 = a2mð2Þ\nu3 = a3nð1Þ u4 = a4nð2Þ;\n\n(114)\n\nwhere the coefﬁcient ak ðk = 1; .; 4Þ denote the normalization factors. In this basis, the ﬁrst four rows and columns of the rotated\n\nmatrix P0ij read:\n\n0\n\n1\n\n1\n\nP0ij\n\n=\n\n1 N\n\nBBBBBBBB@\n\n0 0 0\n\n0 0 0\n\na1a3 0 0\n\n0\n1 a2a4\n0\n\nCCCCCCCCA;\n\n(115)\n\n00 0 0\n\nall the remaining entries being ﬁxed to 0. From the present matrix form, it easy to verify that all the eigenvalues of P0ij, and thus all the eigenvalues of Pij, vanish. Note that rewriting Pij in an orthonormal basis simpliﬁes the search for its eigenvalues also in more complex cases where the connectivity vectors share several overlap directions. In those cases, a proper basis needs to be built starting from\nthe connectivity vectors through a Gram-Schmidt orthonormalization process.\n\nNeuron 99, 609–623.e1–e29, August 8, 2018 e18\n\nAs a side note we observe that, even though P0ij (and thus Pij) admits only vanishing eigenvalues, its rank is still equal to two. Indeed, the rank can be computed as N minus the dimensionality of the kernel associated to P0ij, deﬁned by any vector x obeying P0 x = 0. As P0ij contains N À 2 empty rows, the last equations impose two independent contraints on the components of x. As a consequence, the\n\ndimensionality of the kernel equals N À 2, and the rank is equal to two.\n\nWe turn to responses that are obtained in presence of external inputs. We examine the network dynamics in response to a normal-\n\nized input I which partially correlates with one of the left-connectivity vectors, here nð1Þ:\n\nsﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\n\nI\n\n=\n\nnð1Þ\n\nSnI S2n\n\n+\n\nx\n\nS2I\n\nÀ\n\nS2nI S4n\n\n:\n\n(116)\n\nSimilarly to the unit-rank case, we ﬁnd that I elicits a network response in the plane I À mð1Þ. The overlap values are given by:\n\nk1 = SnI Âf0i Ã  k2 = 0;\n\n(117)\n\nand they can be used to close the mean-ﬁeld equations together with the equation for the ﬁrst (m = 0) and second-order statistics. In\n\nthe case of stationary states we have:\n\nD0 = g2 Âf2i Ã  + S2mÀk21 + k22Á + S2I :\n\n(118)\n\nSimilar arguments allow to derive the two equations needed for the chaotic states. In order to assess the stability of the stationary states, we evaluate the position of the outliers in the stability eigenspectrum by computing the eigenvalues of M (Equation 113). In the case of orthogonal structures and correlated input patterns I, a little algebra reveals that all the a values vanish, while we have:\n\nb1 = 12SnI Âf0i0Ã  b2 = 0:\n\n(119)\n\nWe conclude that the ﬁrst and the last row of M always vanish. Furthermore, the second and the third rows are proportional one to the other. As a consequence, the stability analysis predicts at most one outlier eigenvalue, which is indeed observed in the spectrum (not shown). The outlier is negative, as the effect of introducing inputs in the direction of the left vector nð1Þ is to further stabilize the dynamics. As it will be shown, more than one outlier can be observed in the case where the low-dimensional structure involves overlap directions.\n\nRank-two structures with internal pairwise overlap\n\nAs a second case, we consider structured matrices where the two connectivity pairs mð1Þ and nð1Þ, mð2Þ and nð2Þ share two different\n\noverlap directions, deﬁned by vectors y1 and y2. We set: qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\nmð1Þ = qSﬃﬃﬃﬃ2ﬃﬃﬃÀﬃﬃﬃﬃﬃrﬃﬃ21ﬃ x1 + r1y1 mð2Þ =qﬃﬃSﬃﬃﬃ2ﬃﬃﬃÀﬃﬃﬃﬃﬃrﬃﬃ22 x2 + r2y2 nð1Þ = qSﬃﬃﬃﬃ2ﬃﬃﬃÀﬃﬃﬃﬃﬃrﬃﬃ21ﬃ x3 + r1y1 nð2Þ = S2 À r22 x4 + r2y2:\n\n(120)\n\nwhere S2 is the variance of the connectivity vectors and r21 and r22 quantify the overlaps along the directions y1 and y2. By rotating Pij onto the orthonormal basis that can be built from mð1Þ and mð2Þ by orthogonalizing the left vectors nð1Þ and nð2Þ, one\ncan easily check that the two non-zero eigenvalues of Pij are given by l1 = r21 and l2 = r22. They correspond, respectively, to the two right-eigenvectors mð1Þ and mð2Þ. In absence of external inputs, an instability is thus likely to occur in the direction of the mðkÞ vector\n\nwhich corresponds to the strongest overlap.\n\nWe speciﬁcally focus on the degenerate condition where the two overlaps are equally strong, r1 = r2 = r, and any combination of mð1Þ and mð2Þ is a right-eigenvector. The mean-ﬁeld equations for the ﬁrst-order statistics read:\n\nk1 k2\n\n= =\n\nr2 r2\n\nk1 k2\n\n  ÂÂff0i0i\n\nÃ  Ã \n:\n\n(121)\n\nSimilarly to Equation 89, the two equations admit a silent ðk1 = k2 = 0Þ and a non-trivial state, determined by two identical conditions\n\nwhich read:\n\n1\n\n=\n\nr2\n\n Âf0i\n\nð0;\n\nD0\n\nÃ  Þ:\n\n(122)\n\nThe equation above determines the value of D0. Note that the non-trivial state exists only for r > 1.\n\ne19 Neuron 99, 609–623.e1–e29, August 8, 2018\n\nA second condition is imposed by the equation for the second-order momentum which reads, for stationary solutions: D0 = g2 Âf2i Ã  + S2Àk21 + k22Á:\n\n(123)\n\nAs the value of D0 is ﬁxed, the mean-ﬁeld set of equations ﬁxes only the sum k21 + k22, but not each singqle ﬃcﬃﬃoﬃﬃﬃmﬃﬃﬃﬃpﬃﬃﬃonent. The meanﬁeld thus returns a one-dimensional continuum of solutions, the shape of which resembles a ring of radius k21 + k22 in the mð1Þ À mð2Þ\n\nplane (see Figures S5D and S5E). Similarly to the unit-rank case, the value of the radius can be computed explicitly by solving numer-\n\nically the two mean-ﬁeld equations (three in the case of chaotic regimes), and depends on the relative magnitude of r2 compared to g\n\n(Figure S5F). Highly disordered connectivities have the usual effect of suppressing non-trivial structured solutions in favor of homo-\n\ngeneous and unstructured states. For sufﬁciently high g values, furthermore, structured solution can display chaotic dynamics (Fig-\n\nures S5E and S5F, red).\n\nA linear stability analysis reveals that the one-dimensional solution consists of a continuous set of marginally stable states. Similarly\n\nto the orthogonal vectors case, the position of the outliers in the eigenspectra of Sij can be evaluated by computing the reduced stability matrix M, which reads:\n\nM\n\n=\n\n0 BB@\n\n222bbg12gg2 22Â  0fÂÂffifiiff0i Ã0i0i ÃÃ  \n\nbbg12 gg2 È22 ÈÈ Â  fÂÂffi0 2ii00Ã22 ÃÃ0  +++ Â  fÂÂffi fii ff0i0 Ã0i0i00 ÃÃÉ  ÉÉ\n\n0\n2S2m k01 2b1S2mk01 + a11\n2b2 S2m k01\n\n0\n\n1\n\n2S2m k02 2b1 S2m k02\n\nCCA;\n\n2b2S2mk02 + a22\n\n(124)\n\nwith:\n\na11 = r2 Âf0i Ã  b1 = 12r2k01 Âf0i00Ã \n\n(125)\n\nand\n\na22 = r2 Âf0i Ã \n\nb2\n\n=\n\n12r2k02\n\n Âf0i00\n\nÃ  :\n\n(126)\n\nAs shown in Figure S5G, diagonalizing the stability matrix M returns the values of two distinct outlier eigenvalues. The third non-\n\nzero eigenvalue of M lies instead systematically inside the compact component of the spectrum, and corresponds to an average\n\nmeasure of the timescales inherited by the random modes. One of the two outliers is tuned exactly to the stability boundary for every\n\nvalue of the parameters which generate a ring solution. This marginally stable eigenvalue is responsible for the slow dynamical time-\n\nscales which are observed in numerical simulations of the network activity (Figures S5D and S5E).\n\nThe DMF predictions formally hold in the limit of inﬁnite-size networks; in simulations of ﬁnite-size networks, the dynamics instead\n\nalways converge on a small number of equilibrium spontaneous states located on the ring (see Figures S5D and S5E). The equilibrium\n\nreached in a given situation is determined by the corresponding realization of the random part of the connectivity, and the initial con-\n\nditions. Different realizations of the random connectivity lead to different equilibrium states, which all however lie on the predicted ring\n\n(see Figures S5D and S5E). For a given realization of the random connectivity, transient dynamics moreover show a clear signature of\n\nthe ring structure. Indeed the points on the ring are close to stable and form a slow manifold. The convergence to the equilibrium\n\nactivity is therefore very slow, and the temporal dynamics explore the ring structure.\n\nWe next examine how the structured, ring-shaped solution is perturbed by the injection of external input patterns.\n\nWe consider an input pattern I of variance S2I . When I does not share any overlap direction with the left vectors nð1Þ and nð2Þ, the mean-ﬁeld equations are affected solely by an extra term SI which needs to be included in the equation for the second-order statistics\n\n(Equation 123). As the equations for the ﬁrst-order statistics do not change, the one-dimensional degeneracy of the solution persists.\n\nThe extra term S2I however decreases the value of the radius of the ring. When the input contains a component which overlaps with one or both left vectors nð1Þ and nð2Þ, the degeneracy in the two equa-\n\ntions for k1 and k2 is broken. As a consequence, the one-dimensional solution collapses onto a unique stable point. Consider for\n\nexample an input pattern of the form:\n\n pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pﬃﬃﬃ   I = SI 1 À a x3 + a x4 :\n\n(127)\n\nThe equations for the ﬁrst order become:\n\nk1\n\n=\n\n  r2\n\nk1\n\n+\n\nSI\n\npﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1Àa\n\nqﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ S2 À r2\n\n  Âf0i\n\nÃ \n\nk2\n\n=\n\n  r2\n\nk2\n\n+\n\nSI\n\npﬃﬃﬃ a\n\nqﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ S2 À r2\n\n  Âf0i\n\nÃ \n\n(128)\n\nNeuron 99, 609–623.e1–e29, August 8, 2018 e20\n\nor, alternatively:\n\nk1 = SIpﬃ1ﬃﬃﬃÀﬃﬃ1ﬃﬃﬃaÀﬃﬃqr2ﬃSﬃﬃ ﬃ2ﬃÂﬃﬃfÀﬃﬃﬃ0iﬃÃﬃrﬃ ﬃ2ﬃﬃ Âf0iÃ  k2 = SIpﬃa1ﬃﬃqÀﬃSﬃrﬃﬃ2ﬃ2ﬃﬃÀ ﬃﬃÂﬃﬃfﬃrﬃﬃ0i2ﬃﬃÃ  Âf0iÃ :\n\n(129)\n\nThe values of k1 and k2 are thus uniquely speciﬁed, and can be computed by iterating the two equations together with the expres-\n\nsion for the second-order statistics:\n\nD0 = g2 Âf2i Ã  + S2Àk21 + k22Á + S2I :\n\n(130)\n\nIn a similar way, the presence of correlated external inputs affect the values of the entries of the reduced stability matrix M:\n\nb1\n\n=\n\n1 2\n\n  r2\n\nk01\n\n+\n\nSI\n\npﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1Àa\n\nqﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ S2 À r2\n\n  Âf0i00\n\nÃ \n\nb2\n\n=\n\n1 2\n\n  r2\n\nk02\n\n+\n\nSI\n\npﬃﬃﬃ a\n\nqﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ S2 À r2\n\n  Âf0i00\n\nÃ  :\n\n(131)\n\nIn Figures S5H and S5I, we focus on the case of an external input pattern aligned with x3 (and thus nð1Þ). We ﬁx a = 0, that implies k2 = 0.\nSolving the mean-ﬁeld equations reveals that, according to the strength of the input SI, one or three ﬁxed points exist. When the input is weak with respect to the structure overlap r2, two ﬁxed points appear in the proximity of the ring, along the direction deﬁned by the axis k2 = 0 (Figure S5H, top). In particular, when I positively correlates with nð1Þ, only the ﬁxed point with positive value of k1 gets stabilized. The remaining two solutions are characterized by one outlier eigenvalue which lays above the instability boundary, and are\nthus unstable. On the other hand, when the input is sufﬁciently strong, solely the stable ﬁxed point survives (Figure S5H, bottom). Activity is then robustly projected in the direction deﬁned by the right vector mð1Þ.\n\nRank-two structures for oscillations We ﬁnally consider the following conﬁguration:\n\nmð1Þ = ax1 + ry1\nmð2Þ = ax2 + ry2 nð1Þ = ax3 + ry2 + gry1 nð2Þ = ax4 À ry1;\n\n(132)\n\nwhere the right- and the left-connectivity vectors share two cross-overlap directions y1 and y2. Note that the vectors in one of the two pairs, mð1Þ À nð2Þ, are negatively correlated. A second overlap is introduced internally to the mð1Þ À nð1Þ pair, and scales with the\n\nparameter g. The directions xj, with k = 1; .; 4, represent uncorrelated terms. Note that different values of a affect quantitatively\n\nthe network statistics, but they do not change the phase diagram in Figure S8A.\n\nBy rotating Pij on a proper orthonormal basis, one can check that its eigenvalues are given by:\n\nsﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ !\n\nl±\n\n= gr2 2\n\n1±\n\n1À 4 g2\n\n;\n\n(133)\n\nand they are complex conjugate for g < 2. In this case, the internal overlap g has the effect of returning a non-vanishing real part. The\n\ntwo complex conjugate eigenvectors are given by:\n\ne±\n\n  =\n\nÀ\n\ngmð1Þ 2\n\n  + mð2Þ ±\n\nisﬃ    ﬃ1ﬃﬃﬃﬃÀﬃﬃﬃﬃﬃgﬃ4ﬃ2ﬃﬃﬃ    ﬃﬃmð1Þ:\n\n(134)\n\nThe eigenspectrum of Jij = gcij + Pij inherits the pair of non-zero eigenvalues of Pij. When g < 1 and g < 2, the trivial ﬁxed point thus undergoes a Hopf bifurcation when the real part of l crosses unity (Figure S8A, blue). When g > 2, instead, the two eigenvalues are\nreal. One bifurcation to bistable stationary activity occurs when the largest eigenvalue l + crosses unity (Figure S8A, gray). On the boundary corresponding to the Hopf bifurcation, the frequency of instability uH is determined by the imaginary part of Equa-\ntion 133. At the instability, the oscillatory activity of unit i can be represented as a point on the complex plane. Since close to the bifur-\ncation we can write:\n\nmi = ei+ eiuHt + c:c: ;\n\n(135)\n\ne21 Neuron 99, 609–623.e1–e29, August 8, 2018\n\nits coordinates are given by the real and the imaginary part of the ith component of the complex eigenvector e + . The phase of oscil-\n\nlation can then be computed as the angle deﬁned by this point with respect to the real axis. Note that the disorder in the elements of\n\nthe eigenvector, which is inherited by the random distribution of the entries of the connectivity vectors mð1Þ and mð2Þ, tends to favor a\n\nbroad distribution of phases across the population.\n\nIn the limit case where the real and the imaginary parts of the complex amplitude of the oscillators are randomly and independently\n\ndistributed, the population response resembles a circular cloud in the complex plane. In this case, the phase distribution across the\n\npopulation is ﬂat. Note that a completely ﬂat phase distribution can be obtained for arbitrary frequency values by adopting a rank-two\n\nstructure where an internal overlap of magnitude gr2 exists between vectors mð2Þ and nð2Þ as well.\n\nIn the present case, for every ﬁnite value of g, the real and the imaginary part of ei+ are anti-correlated through mð1Þ (Equation 134). Correlations tend to align the network response on two main and opposite phases, as shown in the phase histograms of Figures S8C\n\nand S8D. The distribution of phases becomes sharper and sharper in the g/2 limit, as the distribution in the complex plane collapses\n\non the real axis.\n\nThe phase distribution across the population is reﬂected in the shape of the closed orbit deﬁned by activity on the mð1Þ À mð2Þ plane,\n\nwhose components are given by k1 and k2. The phase of the oscillations in k1 (resp. k2) can be computed by projecting the eigen-\n\nvector e + on the right-connectivity vectors nð1Þ and nð2Þ:\n\nD\n\nE\n\nk1\n\n=\n\njk1\n\nj\n\neiðF1\n\n+\n\nuH tÞ\n\n+\n\nc:c:\n\n=\n\nDnði 1Þ½fi\n\n E\n\nk2 = jk2 j eiðF2 + uHtÞ + c:c: = nði 2Þ½fi\n\n(136)\n\nBy using Equations 134 and 135 we get, in the linear regime:\n\nk1\n\n=\n\n\"D nði 1Þ\n\nmði 2Þ\n\nE\n\nÀ\n\ng 2\n\nD nði 1Þ\n\nmði 1Þ\n\nE\n\n+\n\nD i nði 1Þ\n\nmði 1Þ\n\nEsﬃ    ﬃ1ﬃﬃﬃﬃÀﬃﬃﬃﬃﬃgﬃ4ﬃ2ﬃﬃﬃ    ﬃﬃ\n\n# eiuH\n\nt\n\n+\n\nc:c:\n\n=\n\n\"  r2 1\n\nÀ\n\ng2 2\n\n \n\n+\n\nigr2sﬃ    ﬃ1ﬃﬃﬃﬃÀﬃﬃﬃﬃﬃgﬃ4ﬃ2ﬃﬃﬃ    ﬃﬃ\n\n# eiuH t\n\n+\n\nc:c:\n\n(137)\n\nwhile:\n\nk2\n\n=\n\n\"D nði 2Þ\n\nmði 2Þ\n\nE\n\nÀ\n\ng 2\n\nD nði 2Þ\n\nmði 1Þ\n\nE\n\n+\n\nD i nði 2Þ\n\nmði 1Þ\n\nEsﬃ    ﬃ1ﬃﬃﬃﬃÀﬃﬃﬃﬃﬃgﬃ4ﬃ2ﬃﬃﬃ    ﬃﬃ\n\n# eiuH\n\nt\n\n+\n\nc:c:\n\n=\n\n\" r2 g 2\n\nÀ\n\nir2sﬃ    ﬃ1ﬃﬃﬃﬃÀﬃﬃﬃﬃﬃgﬃ4ﬃ2ﬃﬃﬃ    ﬃﬃ\n\n# eiuH t\n\n+\n\nc:c:\n\n(138)\n\nWhen g is close to 2, the complex amplitudes of k1 and k2 vanish. However, their real parts have different signs. We thus get:\n\nF2 = 0, F1 = p. As a consequence, at large g values, the oscillatory activity in k1 and k2 tends to be strongly in anti-phase.\n\nStationary solutions can be instead easily analyzed with the standard mean-ﬁeld approach. The equations for the ﬁrst order sta-\n\ntistics read:\n\nk1 k2\n\n= =\n\nÀÀgrr22kk11+ Ârf20ikÃ 2Á:  Âf0i Ã \n\n(139)\n\nThe two equations can be combined together to give the following condition on h½f0ii, which in turn determines the value of D0:\n\nr4 Âf0i Ã 2 À gr2 Âf0i Ã  + 1 = 0:\n\n(140)\n\nThe mean-ﬁeld equations thus admit two solutions, given by:\n\nsﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ !\n\n Âf0i Ã  ±\n\ng = 2r2\n\n1+ ±\n\n4 1 À g2\n\n(141)\n\nwhich, similarly to ﬁeld solutions are\n\naEcqcueaptitoanbl1e3o3n, ltyakife  hr½efa0iliv  a<lu1e.sAfosritgc>a2n.bBeeecaasuislyecohfetchkeecdo,ntshteraciontnsdoitniotnheh½fsi0igimÀ o<id1acloaicntcividaetisonwiftuhnicmtipoons, itnhge\n\nmeanl + > 1.\n\nWe conclude that two stationary solutions exist above the instability boundary of the trivial ﬁxed point (Figure S8A, gray). A second\n\npair of solutions appears for h½f0ii + < 1, which coincide with lÀ > 1 (Figure S8A, dashed), where the second outlier of Jij becomes\n\nunstable. This second pair of solutions is however always dynamically unstable, as it can be checked by evaluating the outliers of\n\nNeuron 99, 609–623.e1–e29, August 8, 2018 e22\n\ntheir stability matrix through Equation 113. The coefﬁcients of the reduced matrix M read: a11 = gr2 Âf0i Ã  a12 = r2 Âf0i Ã \n\nb1\n\n=\n\n1r2 2\n\nÀk20\n\n+\n\ngk10Á Âf0i0Ã \n\n(142)\n\nand a21 = À r2 Âf0i Ã \n\na22 = 0\n\nb2\n\n=\n\nÀ\n\n1r2k10 2\n\n Âf0i0\n\nÃ  :\n\n(143)\n\nOn the phase diagram boundary corresponding to g = 2, the stable and the unstable pair of stationary solutions annihilate and disappear. At slightly smaller values of g ðg(2Þ, the network develops highly non-linear and slow oscillations which can be thought of as smooth jumps between the two annihilation points (Figure S8D).\n\nIMPLEMENTATION OF COMPUTATIONAL TASKS\n\nGo-Nogo discrimination\nHere we describe and analyze the unit-rank implementation of the Go-Nogo discrimination task (Figure 3). The network receives inputs speciﬁed by N-dimensional vectors Ik. In every trial, the input vector coincides with one among the two\nvectors IA and IB, representing respectively the Go and the Nogo stimuli. The components of the two input patterns are generated\nindependently from a Gaussian distribution of mean zero and variance SI. As the components of the inputs are uncorrelated, the two vectors are mutually orthogonal in the limit of large N.\nThe network activity is readout linearly through a vector w generated from a Gaussian distribution of mean zero and variance S2w. The readout value is given by:\n\nz\n\n=\n\n1 N\n\nXN\ni=1\n\nwi fðxi Þ:\n\n(144)\n\nWe ﬁx the connectivity vectors m and n such that: (i) the readout is selective, i.e., zs0 if the input is IA and z = 0 for the input IB; (ii) the readout is speciﬁc to the vector w, i.e., it is zero for any readout vector uncorrelated with w. The simplest network architecture which satisﬁes these requirements is given by:\n\nm=w n = IA;\n\n(145)\n\ni.e., the right-connectivity vector m corresponds to the readout vector, and the left-connectivity vector corresponds to the preferred stimulus IA.\nThe response of the network can be analyzed by referring to the stationary and chaotic solutions of Equation 98. In the case\nanalyzed here, the connectivity vectors have no overlap direction, so we set Mm = Mn = MI = SmI = 0, which implies m = 0. The ﬁrst-order network statistics are determined by the overlap SnI between the left-connectivity vector and the input vector. As the left-connectivity is given by IA, SnI is the overlap between the current input pattern I and the preferred pattern IA, and it takes values SnI = S2I during the Go stimulus presentation and SnI = 0 otherwise. From Equation 94 we have:\n\nk\n\n= =\n\n hnIAi i ½½ffiii :\n\n(146)\n\nAs a consequence, when the Go stimulus is presented ðI = IAÞ:\n\nk\n\n=\n\nS2I\n\n Âf0i\n\nÃ  ;\n\n(147)\n\nwhile the ﬁrst-order statistics k vanishes in response to any orthogonal pattern IB. When activity is read out by the speciﬁc decoding vector w, the readout value is:\n\ne23 Neuron 99, 609–623.e1–e29, August 8, 2018\n\nz = h(wi½fZii  \n\nqﬃﬃﬃﬃﬃ  )\n\n= wi Dzf mik + Ii + DI0 z\n\n(Z  \n\nqﬃﬃﬃﬃﬃ  )\n\n= wi Dzf wik + Ii + DI0 z\n\n=\n\nkS2w Âf0i\n\nÃ  ;\n\n(148)\n\nwhile we trivially obtain z = 0 for any decoding set orthogonal to both connectivity vectors m and n.\n\nIn Figure 3C, we display the transient dynamics predicted by the mean-ﬁeld theory within the m À I plane. In order to compute the\n\npredicted trajectory, we use Equations 103 and 104, where the slowest time-scale of k is computed by diagonalizing the reduced\n\nstability matrix in Equation 99.\n\nIn Figure 3G, we test the generalization properties of a network which responds to two Go patterns IA1 and IA2 . We examine the response to a normalized mixture input deﬁned as:\n\nI\n\n=\n\npﬃaﬃﬃIA1\n\n+\n\npﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1 À aIA2\n\n;\n\n(149)\n\nso that the variance of the total input is ﬁxed and equal to S2I . We set n = IA1 + IA2 , so that the equation for the ﬁrst-order statis-\n\ntics reads:\n\nk\n\n= =\n\n  IpA1i½ﬃafﬃﬃ i+ p+ﬃ1 ﬃﬃﬃIÀﬃA2ﬃﬃiﬃ½ﬃfaﬃﬃi  S2I\n\n Âf0i\n\nÃ  :\n\n(150)\n\nDetection of a continuous noisy stimulus\nIn Figure 4, we construct a network model which performs a Go-Nogo detection task on a one-dimensional continuous stimulus. The stimulus consists of an input of time-varying amplitude cðtÞI. As in Figure 3, the input direction I is a centered Gaussian vector of\nvariance S2I . The strength value cðtÞ includes a stationary component c together with additive white noise of standard deviation s. Less importantly, we include in the input an orthogonal component of quenched noise of unitary variance. The network output is deﬁned at the level of an orthogonal readout as in Equation 144, and the task consists in responding to the stimulus when the strength of the input c is larger than a given threshold.\nWe obtain highly non-linear readout responses by considering non-vanishing overlaps between the connectivity vectors m and n. The simplest setup consists of taking:\n\nm = w + rmy n = I + rny;\n\n(151)\n\nwhere y is a standard Gaussian vector which deﬁnes a direction common to m and n, but orthogonal both to w and I.\n\nFor this conﬁguration, as in Equation 94, the mean-ﬁeld equation for the ﬁrst-order statistics includes two terms, generated respec-\n\ntively by the input and the rank-one structure:\n\nk\n\n=\n\nÀ rm\n\nrn\n\nk\n\n+\n\nc\n\nS2I\n\nÁ Âf0i\n\nÃ  :\n\n(152)\n\nBefore the stimulus presentation (c = 0, s = 0), the structure overlap rmrn is strong enough to generate two bistable solutions (Figure 1). We set the negative k solution to represent the Nogo condition, and we initialize the network in this state. To have a zero output in this condition, we add an offset to the readout.\nWhen an input along the preferred direction is presented ðc > 0Þ, two asymmetric solutions exist only when the strength of the input c is not too large (Figure 2D, right). When the correlation c is large, instead, only the positive branch of the solution is retrieved (Figure 2D, right). As a consequence, the average value of k (and thus the readout signal) jumps to positive values, which deﬁne the Go\noutput condition. More generally, in order to compute the network performance (Figure 4G), the network is said to respond to the stimulus if the\nreadout z at the end of the stimulus presentation takes values larger than one half of the readout value expected for the upper state.\nThe threshold value for c at which the bistability disappears is mostly determined by the strength of the structure overlap, but depends also the input and readout parameters SI and Sw. For practical purposes, in order to obtain the model implementation illustrated in Figure 4, we ﬁrst ﬁx the values of SI = 1:2, Sw = 1:2 and rn = 2. We then tune the value of rm in order to obtain a threshold value for c close to 0.5. This leads to rm = 2.\nIn Figure 4F we vary rm and we show that the value of the threshold decreases to zero as the structure strength rmrn decreases from its original value (rmrn = 4). Rank-one structures characterized by different strengths thus correspond to different thresholds, but also induce different dynamical time-scales in the network. As a rough estimate of this time-scale, we compute the inverse of the outlier eigenvalue from the stability matrix of the ﬁxed point corresponding to the Go resting state ðc = 0Þ. The value of the outlier can be computed from the linearized mean-ﬁeld equations (Equation 71). We show that arbitrarily large time-scales are only obtained\n\nNeuron 99, 609–623.e1–e29, August 8, 2018 e24\n\nby decreasing the value of the structure strength to the critical point where the two bistable branches of the solution emerge from the trivial ﬁxed point. In this conﬁguration, the threshold detected by the network is arbitrarily small.\n\nContextual modulation of threshold value\n\nHere we brieﬂy illustrate how the threshold of detection can be controlled by an additional modulatory input (Figure 5B). Modulatory\n\ninputs are used in Figures 5 and 6 to implement more complex tasks which require context-dependent responses to stimuli. Any input\n\ndirection which overlaps with the left-connectivity vector n and is orthogonal to the stimulus axis I can serve as modulatory input. For\n\nsimplicity, we consider modulatory inputs which are aligned with the overlap direction y (see Equation 151). The total external input to\n\nthe network contains the modulatory component gy together with the stimulus term cðtÞI, where g is a scalar which controls the\n\nstrength of the modulation. The mean-ﬁeld equation for the ﬁrst-order statistics reads:\n\nk\n\n=\n\nÀ rm rn k\n\n+\n\nrn\n\ng\n\n+\n\nc\n\nS2I\n\nÁ Âf0i\n\nÃ  :\n\n(153)\n\nEquation 153 indicates that the modulatory component of the input acts as a constant offset to the stimulus strength. Its net effect is to shift the response curve of the network along the x axis (Figure 5B) by an amount directly regulated by the parameter g. Varying g thus results in network models which detect variable threshold values.\n\nRank-two structures for context-dependent computations\nHere we provide details on the rank-two implementation of the context-dependent tasks. The same model has been used for both\ntasks in Figures 5 and 6. The stimuli consist of combinations of two different features A and B that correspond to inputs along two directions IA and IB,\ngenerated as Gaussian random vectors of variance S2I . Contextual cues are represented as additional inputs along directions IctxA and IctxB of unit variance. The total input pattern to the network on a given trial is therefore given by:\n\nIðtÞ = cAðtÞIA + cBðtÞIB + gAIctxA + gBIctxB:\n\n(154)\n\nThe values cA and cB express the strength of the stimulus along the two feature directions. They are given by the sum of stationary average values (cA, cB), and temporary ﬂuctuating components generated from independent realizations of white noise with standard deviation s. In the simple discrimination version of the task (Figure 5), inputs are noise-free ðs = 0Þ and consist of a single feature in each trial (cA = 1 and cB = 0 or vice versa). In the evidence integration version of the task (Figure 6), inputs are noisy ðs > 0Þ and include non-zero average components along both feature directions. Finally, the parameters gA and gB control the two modulatory inputs which are taken in the directions deﬁned by IctxA and IctxB.\nIn order to implement context-dependent computations, we deﬁne a unique readout signal zðtÞ by using a common readout set w of unit variance (Equation 144), to which we add an offset so that the baseline Nogo output is set to zero. The network is said to respond to the stimulus if the value of the total readout at the end of the stimulus presentation takes values larger than one half of the largest predicted value for the upper state.\nThe rank-two connectivity matrix we consider is given by:\n\nmð1Þ = yA + rmIctxA + bmw nð1Þ = IA + rnIctxA + bnw mð2Þ = yB + rmIctxB + bmw nð2Þ = IB + rnIctxB + bnw;\n\n(155)\n\nwhere vectors yA and yB represent the orthogonal components of the right-connectivity vectors and are generated as Gaussian vectors of ﬁxed variance (for simplicity, we set Sy = SI).\nFor our choice of the parameters, the network solves the two different tasks by relying on the strongly non-linear responses generated by the interplay between the recurrent connectivity and the feed-forward inputs (details given below).\nFor weak input values, the network dynamics is characterized by two stable attractors (Figure 6F). As in Figure 4, we initialize the network in the state characterized by negative k1 and k2 values before the stimulus presentation. This dynamical attractor corresponds to the Nogo state. For strong input strengths, the network can jump to the Go state, deﬁned as the stable attractor characterized by positive k1 and k2 values.\nThe rank-two connectivity matrix has been designed as an extension of the unit-rank recurrent connectivity employed in Figure 4. We started by setting:\n\nmð1Þ = yA + rmIctxA nð1Þ = IA + rnIctxA mð2Þ = yB + rmIctxB nð2Þ = IB + rnIctxB:\n\n(156)\n\nNote that, because the only overlap directions (IctxA and IctxB) are internal to the mð1Þ À nð1Þ and mð1Þ À nð1Þ pairs, Equation 156 describes a rank-two structure which generates a continuous ring attractor as in Figures S5D–S5I (gray circles in Figure 6F).\n\ne25 Neuron 99, 609–623.e1–e29, August 8, 2018\n\nThe readout zðtÞ should detect the presence of both stimuli directions. As a consequence, it should be sensitive to both overlap\n\nvalues k1 and k2. For this reason, we introduce a common term in the four connectivity vectors that is aligned to the common readout (Equation 155).\n\nIntroducing a common overlap direction has the effect of destabilizing the continuous attractor dynamics along the direction k1 = k2\n\n(dashed line in Figure 6F), where two stable and symmetric ﬁxed points are generated. The equations for the ﬁrst-order spontaneous\n\ndynamics read indeed:\n\nk1 k2\n\n= =\n\n  nnðð12ÞÞ\n\n½fi ½fi\n\n    \n\n= =\n\nrm rm\n\nrn rn\n\nk1 k2\n\n  ÂÂff0i0i\n\nÃ  Ã \n\n+ +\n\nbm bn bm bn\n\nðk1 ðk1\n\n+ +\n\nk2 k2\n\nÞÞ  ÂÂff0i0i\n\nÃ  Ã \n\n(157)\n\nfrom which the value of k1 = k2 = k can be derived by dividing and multiplying together the two equations. The ﬁnal readout signal\n\ncontains a contribution from both ﬁrst-order statistics:\n\nzðtÞ\n\n=\n\nhwi\n\n½fi\n\ni\n\n=\n\nbm\n\nðk1\n\n+\n\nk2\n\nÞ Âf0i\n\nÃ  :\n\n(158)\n\nThe input-driven dynamics of the network are determined by the interplay between the structure strength and the contextual and\n\nstimulus inputs. Crucially, the modulatory inputs along IctxA and IctxB are used to gate a context-dependent response. Similarly to Figure 5B, a strong and negative gating variable along IctxA can completely suppress the response to stimulus IA, so that the readout signal is left free to respond to IB.\n\nThe overall effects of the inputs on the dynamics can be quantiﬁed by solving the mean-ﬁeld equations. For the ﬁrst-order statistics,\n\nwe obtain:\n\nk1 k2\n\n= =\n\n  ÂÂff0i0i\n\nÃ È Ã Èrm\nrm\n\nrnk1 rnk2\n\n+ +\n\nbmbnðk1 bmbnðk1\n\n+ +\n\nk2 Þ k2 Þ\n\n+ +\n\ncA S2I cB S2I\n\n+ +\n\nÉ rn gA É rngB\n\n(159)\n\nwhile the second-order gives, in the case of stationary regimes: D0 = g2 Âf2i Ã  + S2wÀk21 + k22Á + b2mÀk21 + k22Á + S2I Àc2A + c2BÁ + ðrmk1 + gAÞ2 + ðrmk2 + gBÞ2:\n\n(160)\n\nFigures S5L–S5M displays the values of the ﬁrst-order statistics and the readout response in the two contexts. Note that, when the response to IA (resp. IB) is blocked at the level of the readout, the relative ﬁrst-order statistics k1 (resp. k2) does not vanish, but actively contributes to the ﬁnal network response.\n\nThe average activation variable of single neurons contains entangled contributions from the main directions of the dynamics, which\n\nare inherited both from the external inputs and the recurrent architecture:\n\nmi\n\n=\n\n½xi \n\n=\n\nÀ yA;i\n\n+\n\nrm IctxA;i\n\n+\n\nÁ bmwi k1\n\n+\n\nÀ yB;i\n\n+\n\nrm IctxB;i\n\n+\n\nÁ bmwi k2\n\n+\n\ncA IAi\n\n+\n\ncB IBi\n\n+\n\ng1 IctxA;i\n\n+\n\ng2 IctxB;i :\n\n(161)\n\nIn Figures 5E and 6D, we project the averaged activation mi in the directions that are more salient to the task. The projection along w, which reﬂects the output decision, is proportional to the readout value (Equation 158). The input signals affect instead the average\nactivity through the values of k1 and k2, but can be also read out directly along the input directions. Note that the projection on the input direction IA (resp. IB) is proportional to the signal cA (resp. cB) regardless of the conﬁguration of the modulatory inputs selecting one input channel or the other.\nIn practical terms, in order to obtain the network architecture that has been used in Figures 5 and 6, we ﬁxed the parameters step by step. We ﬁrst considered input patterns only along IA (cB = 0), and we ﬁxed two arbitrary values of bm and bn. In particular, we considered intermediate values of b. Large values of b tend to return large activity variance, which requires evaluating with very high precision the Gaussian integrals present in the mean-ﬁeld equations. Small values of b bring instead the network activity closer to a\ncontinuous-attractor structure, and turn into larger ﬁnite-size effects. In a second step, we ﬁx rm and rn such that the network detects normalized input components along IA only when they are larger than a threshold value, that is taken around 0.5. We then looked for a pair of gating variables strengths ½gA; gB which completely suppresses the response to IA by extending the range of bistable activity. The opposite pattern can be used to block the response in IB and allow a response in IA.\nOnce the response in IA has been blocked, it can be veriﬁed that the network solely responds to inputs which contain a response along IB that is larger than a threshold close to 0.5. Note that, as in Figures S5L–S5M, different values of cA only minimally affect the exact position of the threshold.\nTo conclude, we remark that this procedure leaves the freedom of ﬁxing the network parameters in many different conﬁgurations.\nThe complex rank-two architecture leads to larger ﬁnite-size effects than the respective unit-rank setup which acts as a single de-\ntector of correlations. In particular, the error at the level of the readout is larger but it decays with the system size, as expected for\ndeviations induced by ﬁnite-size effects (Figure S5N). Finally, note that when the noise in the input stimuli becomes extremely large,\nthe network loses its ability to respond in a totally context-dependent fashion, as strong ﬂuctuations in the non-relevant stimulus\nbecome likely to elicit a response.\n\nNeuron 99, 609–623.e1–e29, August 8, 2018 e26\n\nMETHOD DETAILS FOR MAIN FIGURES\nFigure 1 In this ﬁgure, Sm = Sn = 1:0. Note that the precise position of the instability to chaos depends on the value of Sm. The connectivity vectors m and n were generated from bivariate Gaussian distributions (means Mm and Mn, variances Sm and Sn, correlation r). Here we display the case where m and n overlap only along the unitary direction (Mm > 0; Mn > 0, r = 0, see STAR Methods). As shown in Figure S2, qualitatively similar regimes are obtained when the overlap is deﬁned on an arbitrary direction. C-D: Network simulations were performed starting from initial conditions centered around m and À m. Activity is integrated up to T = 800. In simulations, N = 5000, and statistics are averaged over 15 different connectivity realizations. The error bars, when visible, correspond to the standard deviation of the mean (as in every other ﬁgure, if not differently speciﬁed).\nFigure 2 In this ﬁgure, g = 0:8. Other parameters are set as in Figure 1. B: The asymptotic input parameters are indicated by gray dots in D (middle). The simulation results (dark gray traces) correspond to 20 trajectories for different network realizations (different trajectories strongly overlap). We simulated Ntr = 20 different networks, each consisting of N = 3500 units. In every network realization, the random part of the connectivity cij is varied, while the low-rank part minj is kept ﬁxed. I (resp. m) scale: 0.7 (resp. 0.25) units. D: The external input is increased along nt, the component of n that is perpendicular to the overlap direction.\nFigure 3 The input and the readout vectors are Gaussian patterns of standard deviation S = 2. C (right): Colored traces: 20 trajectories from different network realizations (different trajectories strongly overlap). We simulated Ntr = 20 different realizations of the network, each consisting of N = 2500 units. In every network realization, the random part of the connectivity cij is generated independently, while the low-rank part minj is kept ﬁxed. IA, IB and m scale: 1.5 units. D: Here, and in every plot if not differently stated, r indicates the Pearson correlation coefﬁcient. F: The PC axis are determined by analyzing separately the trials corresponding to the Go (top) and the Nogo (bottom) stimuli. Connectivity is measured as the average reciprocal synaptic strength; it includes both the random and the unit-rank components and it is averaged across network realizations. Note that the value of the correlation coefﬁcient r increases with the number of realizations Ntr and the structure strength.\nFigure 4 The input and the readout vectors are Gaussian patterns of standard deviation S = 1:2. The overlap between the connectivity vectors m and n leading to non-linear responses is quantiﬁed by rm = rn = 2:0. B: The input is generated as white noise of mean c = 0:6 and standard deviation s = 0:4 (the noise trace in the ﬁgure is only for illustration purposes). The red dashed line indicates the threshold in the implemented network. C: The gray bar indicates the time point at which the network output is measured. Here and in the following ﬁgures, the readout includes an offset, so that the baseline value is set to zero. D: We simulated many input noise traces for Ntr = 4 different realizations of the network, each consisting of N = 2500 units. In every network realization, the random part of the connectivity cij is varied, while the low-rank part minj is kept ﬁxed. Trajectories are smoothed with a Gaussian ﬁlter of standard deviation equal to one normalized time unit. I (resp. m) scale: 0.5 (resp. 3.5) units. F: The structure strength corresponds to the overlap rmrn. The effective timescale is measured as the inverse of the value of the outlier eigenvalue of the stability matrix for c = 0. G: The psychometric curve was measured across Ntr = 100 different realizations. The network produces an output to the stimulus if at the end of the stimulus presentation (vertical gray line in B) the value of the readout z is larger than one half of the largest readout value predicted by the theory. H: Details as in Figure 3F.\nFigure 5 The stimuli vectors are Gaussian patterns of standard deviation S = 1:2. We furthermore set: g = 0:8, bm = 0:6, bn = 1, rm = 3, rn = 1:6. The amplitudes of the two context directions are ﬁxed to ½0:08; À0:14 (resp. ½ À 0:14; 0:08) during the context A (resp. context B) trials. B: We consider in this case a unit-rank network as in Figure 2D, and we show in the two panels the network response for two different values of the input strength along the overlap axis (we set, respectively, MI = À 0:3 and 0.6). Details on the effect of contextual modulation on the full rank-two model are further illustrated in Figures S5L–S5N. E: We simulated Ntr = 4 different realizations of the network, each consisting of N = 3000 units. In every network realization, the random part of the connectivity cij is varied, while the low-rank part minj is kept ﬁxed. IA and IB (resp. w) scale: 1.0 (resp. 2.0) units. F: The network performance was measured across Ntr = 50 different network realizations of size N = 7500. The network produces an output to the stimulus if at the end of the stimulus presentation (vertical gray line in D) the value of the readout z is larger than one half of the largest readout value predicted by the theory. G: Details as in Figure 3F.\nFigure 6 The stimuli vectors are Gaussian patterns of standard deviation S = 1:2. We furthermore set: g = 0:8, bm = 0:6, bn = 1, rm = 3, rn = 1:38. The amplitudes of the two context directions are ﬁxed to ½0:08; À0:18 (resp. ½ À 0:18; 0:08) during the context A (resp. context B) trials. B: Here cA = 0:6 and cB = 0:1, while the standard deviation of the noise in the input is s = 0:3 (the noise trace in\ne27 Neuron 99, 609–623.e1–e29, August 8, 2018\n\nthe ﬁgure is only for illustration purposes). D: We simulated many noisy input traces for Ntr = 5 different realizations of the network,\neach consisting of N = 4000 units. In every network realization, the random part of the connectivity cij is varied, while the low-rank part minj is kept ﬁxed. For the sake of clarity, only correct trials have been included. IA and IB (resp. w) scale: 1 (resp. 1.5) units. E: Network\nperformance was measured across Ntr = 50 different network realizations of size N = 7500.\n\nQUANTIFICATION AND STATISTICAL ANALYSIS\n\nIn this section, we brieﬂy describe the analysis techniques that have been applied to the datasets generated from direct simulations of activity in ﬁnite-size networks (Figures 2, 3, 4, 5, and 6).\n\nDimensionality reduction\nIn order to extract from the high-dimensional population activity the low-dimensional subspace which contains most of the relevant dynamics, we performed dimensionality reduction via a standard Principal Component (PC) analysis.\nTo begin with, we constructed the activation matrix X. In X, every column corresponds to the time trace of the activation variable xiðtÞ for unit i, averaged across trials. We indicate as trials different network simulations, where different noisy inputs, or different quenched noise in the random connectivity matrix have been generated (details are speciﬁed in the ﬁgure captions). The activation matrix X is normalized through Z-scoring: to every column, we subtract its average over time, and we divide by its standard deviation. Note that Z-scoring distorts the shape of the population trajectory in the phase space. For this reason, in order to facilitate the comparison with the trajectory predicted by the mean-ﬁeld theory, in Figure S3 we more simply consider the mean-subtracted matrix X. Applying the PCA analysis to one of the two data formats impacts the results from a quantitative point of view, but does not change their general validity.\nThe principal components (PC) are computed as the normalized eigenvectors felgl = 1;.;N of the correlation matrix C = XT X. The PC are sorted in decreasing order according to the corresponding real eigenvalue ll. The activation matrix X can be projected on the orthonormal basis generated by the PC vectors by computing: X0 = XE, where E is the N3N matrix containing the PC eigenvectors ordered as columns. The variance explained by the l-th PC mode el can be computed as the l-th entry on the diagonal of the rotated correlation matrix C0 = X0T X0 .\nWhile in our network models the low-rank part of the connectivity determines a purely low-dimensional dynamics (Figure S3A), the random part of the connectivity generates a continuum of components whose amplitude is determined by strength of the random connectivity g with respect to the connectivity and input vectors. In Figure 2, where g = 0:8, the low-dimensional nature of the dynamics is revealed by considering averages across several ðNtr = 20Þ realizations of the random connectivity. In Figure S3B, we illustrate the result of performing PCA on the activity generated by a single network. In this case, even if more PC components contribute to the total variance, the two ﬁrst axis bear a strong resemblance with the directions predicted with the theory. In Figure S3C we show that, in the same spirit, a PCA analysis can be used to extract the relevant geometry of the network model also when activity is strongly chaotic.\nIn order to more easily connect with the theoretical predictions, we systematically applied dimensionality reduction on datasets constructed from the activation variable xi. We veriﬁed that our results still hold, from a qualitative point of view, when the analysis is performed on the non-linearly transformed variables fðxiÞ. In the network models we considered, the activation variables fðxiÞ indeed form a non-linear but dominantly low-dimensional manifold in the phase space. The axes predicted by the mean-ﬁeld theory determine the dominant linear geometry of this manifold, and can be still captured (although less precisely) by looking at the ﬁrst PC components.\n\nLinear regression\nIn order to estimate how single units in the network are tuned to different task variables (such as input stimuli or decision variables), we used a multi-variate linear regression analysis.\nTo this end, we considered the full population response xikðtÞ, where k = 1; .; Ntr indicates the trial number. Following (Mante et al., 2013), our aim was to describe the network activation variables as linear combinations of the M relevant task variables. In Figure 3, the two variables we considered were the strength of the Go and of the Nogo inputs, that we indicate here with cGo and cNogo:\n\nxik ðtÞ = bGi;tocGoðkÞ + bNi;togocNogoðkÞ:\n\n(162)\n\nIn a Go, or in a Nogo trial, only one of the two strength coefﬁcients is non-zero. In Figure 4, the two relevant task variables are assumed to be the input strength along I, quantiﬁed by c, and the network output, quantiﬁed as the value of the readout z at the end of the stimulus presentation:\n\nxik ðtÞ = biin;tputcðkÞ + bci;thoicezðkÞ:\n\n(163)\n\nNeuron 99, 609–623.e1–e29, August 8, 2018 e28\n\nIn Figures 5 and 6, the relevant variables are four: the strength of stimuli A and B, the trial context and the network output. We thus have:\n\nxik ðtÞ = bAi;tcAðkÞ + bBi;tcBðkÞ + + bci;ttxyðkÞ + bci;thoicezðkÞ:\n\n(164)\n\nwhere the context variable is represented by a unique symbolic variable y, which takes value y = 1 in context A and y = À 1 in\n\ncontext B.\n\nMore generally, we indicate with bni;t the regression coefﬁcient of unit i with respect to the task feature n at time t. The vector bi;t = fbni;tgn = 1;::;M indicates the collection of the M variables regressors for a given unit at the time point t. We compute the regression coefﬁcients by deﬁning a matrix F of size M 3 Ntr, where every row contains the value of the M relevant task variables across trials.\n\nThe regression coefﬁcient vectors are then estimated by least-square inversion:\n\nbi;t\n\n=\n\nÀ FF\n\nT\n\nÁÀ1\n\nFxi;t\n\n(165)\n\nwhere the vector xi;t is constructed by collecting across trials the value the activation variable of unit i at time t. In order to get rid of the time dependence of our result, we simply consider the coefﬁcients bi;t at the time point where the two-\ndimensional array bi;t for every i has maximal norm (Mante et al., 2013). The resulting set of M-dimensional vectors bi contains the regression coefﬁcients of unit i with respect to the M relevant task variables. The N-dimensional regression axis for a given task variable n is ﬁnally constructed by collecting the n-th components of bi across different population units: fbni gi = 1;::;N.\n\nDATA AND SOFTWARE AVAILABILITY\n\nSoftware was written in the Python (http://python.org) programming languages. Implementations of algorithms used to compute quantities presented in this study are available at: https://github.com/fmastrogiuseppe/lowrank/.\n\ne29 Neuron 99, 609–623.e1–e29, August 8, 2018\n\n"}