{"indexedPages":31,"totalPages":31,"version":"242","text":"arXiv:2210.03310v3 [cs.LG] 2 Mar 2023\n\nPublished as a conference paper at ICLR 2023\nSCALING FORWARD GRADIENT WITH LOCAL LOSSES\nMengye Ren1∗, Simon Kornblith2, Renjie Liao3, Geoffrey Hinton2,4 1NYU, 2Google, 3UBC, 4Vector Institute\nABSTRACT\nForward gradient learning computes a noisy directional gradient and is a biologically plausible alternative to backprop for learning deep neural networks. However, the standard forward gradient algorithm, when applied naively, suffers from high variance when the number of parameters to be learned is large. In this paper, we propose a series of architectural and algorithmic modiﬁcations that together make forward gradient learning practical for standard deep learning benchmark tasks. We show that it is possible to substantially reduce the variance of the forward gradient estimator by applying perturbations to activations rather than weights. We further improve the scalability of forward gradient by introducing a large number of local greedy loss functions, each of which involves only a small number of learnable parameters, and a new MLPMixer-inspired architecture, LocalMixer, that is more suitable for local learning. Our approach matches backprop on MNIST and CIFAR-10 and signiﬁcantly outperforms previously proposed backprop-free algorithms on ImageNet. Code is released at https://github.com/google-research/ google-research/tree/master/local_forward_gradient.\n1 INTRODUCTION\nMost deep neural networks today are trained using the backpropagation algorithm (a.k.a. backprop) (Werbos, 1974; LeCun, 1985; Rumelhart et al., 1986), which efﬁciently computes the gradients of the weight parameters by propagating the error signal backwards from the loss function to each layer. Although artiﬁcial neural networks were originally inspired by biological neurons, backprop has always been considered as “biologically implausible” as the brain does not form symmetric backward connections or perform synchronized computations. From an engineering perspective, backprop is incompatible with a massive level of model parallelism, and restricts potential hardware designs. These concerns call for a drastically different learning algorithm for deep networks.\nIn the past, there have been attempts to address the above weight transport problem by introducing random backward weights (Lillicrap et al., 2016; Nøkland, 2016), but they have been found to scale poorly on larger datasets such as ImageNet (Bartunov et al., 2018). Addressing the issue of global synchronization, several papers showed that greedy local loss functions can be almost as good as end-to-end learning (Belilovsky et al., 2019; Löwe et al., 2019; Xiong et al., 2020). However, they still rely on backprop for learning a number of internal layers within each local module.\nApproaches based on weight perturbation, on the other hand, directly send the loss signal back to the weight connections and hence do not require any backward weights. In the forward pass, the network adds a slight perturbation to the synaptic connections and the weight update is then multiplied by the negative change in the loss. Weight perturbation was previously proposed as a biologically plausible alternative to backprop (Xie & Seung, 1999; Seung, 2003; Fiete & Seung, 2006). Instead of directly perturbing the weights, it is also possible to use forward-mode automatic differentiation (AD) to compute a directional gradient of the ﬁnal loss along the perturbation direction (Pearlmutter, 1994). Algorithms based on forward-mode AD have recently received renewed interest in the context of deep learning (Baydin et al., 2022; Silver et al., 2022). However, existing approaches suffer from the curse of dimensionality, and the variance of the estimated gradients is too high to effectively train large networks.\nIn this paper, we revisit activity perturbation (Le Cun et al., 1988; Widrow & Lehr, 1990; Fiete & Seung, 2006) as an alternative to weight perturbation. As previous works focused on speciﬁc settings, we explore the general applicability to large networks trained on challenging vision tasks.\n∗Work done as a visiting faculty researcher at Google. Correspondence to: mengye@cs.nyu.edu.\n1\n\nPublished as a conference paper at ICLR 2023\nWe prove that activity perturbation yields lower-variance gradient estimates than weight perturbation, and provide a continuous-time rate-based interpretation of our algorithm. We directly address the scalability issue of forward gradient learning by designing an architecture with many local greedy loss functions, isolating the network into local modules and hence reducing the number of learnable parameters per loss. Unlike prior work that only adds local losses along the depth dimension, we found that having patch-wise and channel group-wise losses is also critical. Lastly, inspired by the design of MLPMixer (Tolstikhin et al., 2021), we designed a network called LocalMixer, featuring a linear token mixing layer and grouped channels for better compatibility with local learning.\nWe evaluate our local greedy forward gradient algorithm on supervised and self-supervised image classiﬁcation problems. On MNIST and CIFAR-10, our learning algorithm performs comparably with backprop, and on ImageNet, it performs signiﬁcantly better than other biologically plausible alternatives using asymmetric forward and backward weights. Although we have not fully matched backprop on larger-scale problems, we believe that local loss design could be a critical ingredient for biologically plausible learning algorithms and the next generation of model-parallel computation.\n2 RELATED WORK\nEver since the perceptron era, the design of learning algorithms for neural networks, especially algorithms that could be realized by biological brains, has been a central interest. Review papers by Whittington & Bogacz (2019) and Lillicrap et al. (2020) have systematically summarized the progress of biologically plausible deep learning. Here, we discuss related work in the following subtopics.\nForward gradient and reinforcement learning. Our work leverages forward-mode automatic differentiation (AD), which was ﬁrst proposed by Wengert (1964). Later it was used to learn recurrent neural networks (Williams & Zipser, 1989) and to compute Hessian vector products (Pearlmutter, 1994). Computing the true gradient using forward-mode AD requires the full Jacobian, which is often large and expensive to compute. Recently, Baydin et al. (2022) and Silver et al. (2022) proposed to update the weights based on the directional gradient along a random or learned perturbation direction. They found that this approach is sufﬁcient for small-scale problems. This general family of algorithms is also related to reinforcement learning (RL) and evolution strategies (ES), since in each case the network receives a global reward. RL and ES have a long history of application in neural networks (Whitley, 1993; Stanley & Miikkulainen, 2002; Salimans et al., 2017), and they are effective for certain continuous control and decision-making tasks. Clark et al. (2021) found global credit assignment can also work well in vector neural networks where weights are only present between vectorized groups of neurons.\nGreedy local learning. There have been numerous attempts to use local greedy learning objectives for training deep neural networks. Greedy layerwise pretraining (Bengio et al., 2006; Hinton et al., 2006; Vincent et al., 2010) trains individual layers or modules one at a time to greedily optimize an objective. Local losses are typically applied to different layers or residual stages, using common supervised and self-supervised loss formulations (Belilovsky et al., 2019; Nøkland & Eidnes, 2019; Löwe et al., 2019; Belilovsky et al., 2020). Xiong et al. (2020); Gomez et al. (2020) proposed to use overlapped losses to reduce the impact of greedy learning. Patel et al. (2022) proposed to split a network into neuron groups. Laskin et al. (2020) applied greedy local learning on model parallelism training, and Wang et al. (2021) proposed to add a local reconstruction loss for preserving information. However, most local learning approaches proposed in the last decade rely on backprop to compute the weight updates within a local module. One exception is the work of Nøkland & Eidnes (2019), which avoided backprop by using layerwise objectives coupled with a similarity loss or a feedback alignment mechanism. Gated linear networks and their variants (Veness et al., 2017; 2021; Sezener et al., 2021) ask every neuron to make a prediction, and have shown interesting results on avoiding catastrophic forgetting. From a theoretical perspective, Baldi & Sadowski (2016) provided insights and proofs on why local learning can be worse than global learning.\nAsymmetric feedback weights. Backprop relies on weight symmetry: the backward weights are the same as the forward weights. Past research has looked at whether this constraint is necessary. Lillicrap et al. (2016) proposed feedback alignment (FA) that uses random and ﬁxed backward weights and found it can support error driven learning in neural networks. Direct FA (Nøkland, 2016) uses a single backward layer to wire the loss function back to each layer. There have also been methods that aim to explicitly update backward weights. Recirculation (Hinton & McClelland, 1987) and target propagation (TP) (Bengio, 2014; Lee et al., 2015; Bartunov et al., 2018) use local reconstruction\n2\n\nPublished as a conference paper at ICLR 2023\n\nobjective to learn separate forward and backward weights as approximate inverses of each other. Ladder networks (Rasmus et al., 2015) found local reconstruction objectives and asymmetric weights can help achieve strong semi-supervised learning performance. However, Bartunov et al. (2018) reported both FA and TP algorithms do not scale to larger problems such as ImageNet, where their error rates are over 90%. Liao et al. (2016); Xiao et al. (2019) proposed sign symmetry (SS) where each backward connection weight share the same sign as the forward counterpart. Akrout et al. (2019) proposed weight mirroring and the modiﬁed Kolen-Pollack algorithm (Kolen & Pollack, 1994) to align forward and backward weights. Woo et al. (2021) proposed to update using activities from several layers below to avoid bidirectional connections. Compared to these works, we circumvent the issue of weight symmetry, and more generally network symmetry, by using only reward (and the change rate thereof), instead of backward weights.\nBiologically plausible perturbation learning. Forward gradient is related to perturbation learning in the biology context. Traditionally, neural plasticity learning rules focus on deriving weight updates as a function of the input and output activity of a neuron (Hebb, 1949; Widrow & Hoff, 1960; Oja, 1982; Bienenstock et al., 1982; Abbott & Nelson, 2000). Weight perturbation learning (Jabri & Flower, 1992), on the other hand, is much more general as it permits any form of global reward (Schultz et al., 1997). It was developed in both rated-based and spiking-based formuations (Xie & Seung, 1999; Seung, 2003). Activity (or node) perturbation was proposed in shallow networks (Le Cun et al., 1988; Widrow & Lehr, 1990) and later in a spike-based continuous time network (Fiete & Seung, 2006), where it was interpreted as the perturbation of the conductance of neurons. Werfel et al. (2003) showed that backprop has a faster convergence rate than perturbation learning, and activity perturbation wins over weight perturbation by another factor. In our work, we show activity perturbation has lower gradient estimation variance compared to weight perturbation.\n\n3 FORWARD GRADIENT LEARNING\n\nIn this section, we review and establish the technical background for our learning algorithm. We ﬁrst review the technique of forward-mode automatic differentiation (AD). Second, we formulate two different types of perturbation in the weight space or activity space.\n\n3.1 FORWARD-MODE AUTOMATIC DIFFERENTIATION (AD)\n\nLet f : Rm → Rn. The Jacobian of f , Jf , is a matrix of size n × m. Forward-mode AD computes the matrix-vector product Jf v, where v ∈ Rm. It is deﬁned as the directional gradient along v evaluated at x:\n\nf (x + δv) − f (x)\n\nJf v := lim\nδ→0\n\nδ\n\n.\n\n(1)\n\nFor comparison, backprop, also known as reverse-mode AD, computes the vector-Jacobian product vJf , where v ∈ Rn, which corresponds to the last term in the chain rule. In contrast to reverse-mode\nAD, forward-mode AD only requires one forward pass, which is augmented with the derivative\n\ninformation. To compute the Jacobian vector product of a node in a computation graph, ﬁrst the\n\ninput node will be augmented with v, which is the vector to be multiplied. Then for other nodes, we\n\nsend in a tuple of (x, x ) as inputs and compute a tuple (y, y ) as outputs, where x and y are the\n\nintermediate derivatives at node x and node y, i.e. y\n\n=\n\ndy dx\n\nx\n\n,\n\nand\n\ndy dx\n\nis the Jacobian between y and\n\nx. In the JAX library (Bradbury et al., 2018), forward-mode AD is implemented as jax.jvp.\n\n3.2 WEIGHT-PERTURBED FORWARD GRADIENT\n\nWeight perturbation to generate weight updates was originally explored in (Barto et al., 1983; Xie & Seung, 1999; Seung, 2003). Baydin et al. (2022) uses the technique of forward-mode AD to implement weight perturbation, which is better than ﬁnite differences in terms of numerical stability. Let wij be the weight connection between unit i and j, and f be the loss function. We can estimate the gradient by sampling a random matrix with iid elements vij drawn from a zero-mean unit-variance Gaussian distribution. The estimator is\n\ngw(wij ) = i j ∇wi j vi j vij .\n\n(2)\n\nIntuitively, this estimator samples a random perturbation direction vij and tests how it aligns with the true gradient ∇wi j by using forward-mode to perform the dot product, and then multiplies the scalar alignment with the perturbation direction again. Baydin et al. (2022) referred this form of\ngradient estimation using forward-mode AD as “forward gradient”. To distinguish with another form\n\n3\n\nPublished as a conference paper at ICLR 2023\n\ngw (·) ga(·)\n\nUnbiased?\nYes Yes\n\nAvg. Variance (shared)\n\npq+2 N\n\nV\n\n+ (pq + 1)S\n\nq+2 N\n\nV\n\n+ (q + 1)S\n\nAvg. Variance (independent)\n\npq+2 N\n\nV\n\n+\n\npq+1 N\n\nS\n\nq+2 N\n\nV\n\n+\n\nq+1 N\n\nS\n\nTable 1: Comparing weight (gw) and activity (ga) perturbation. V =dimension-wise avg. gradient variance, S=dimension-wise avg. squared gradient norm; p=fan-in; q=fan-out; N =batch size.\n\nof perturbation we detail later, we refer this to as “weight-perturbed forward gradient”, or simply as “weight perturbation”.\n\n3.3 ACTIVITY-PERTURBED FORWARD GRADIENT\n\nAn alternative to perturbing the weights is to instead perturb the activities, which can reduce the number of perturbation dimensions per example. Activity perturbation was originally explored in Le Cun et al. (1988); Widrow & Lehr (1990) under restrictive assumptions. Here, we introduce a general way to estimate gradients using activity perturbation. It is potentially biologically plausible, since it could correspond to perturbation of the conductance in each neuron (Fiete & Seung, 2006). Here, we focus on a discrete-time rate-based formulation for simplicity. Let xi denote the activity of the i-th presynaptic neuron and zj denote that of the j-th post-synaptic neuron before the non-linear activation function, and uj be the perturbation of zj. The activity-perturbed forward gradient estimator is\n\nga(wij ) = xi j ∇zj uj uj ,\n\n(3)\n\nwhere the inner product between ∇z and u is again computed by using forward-mode AD.\n\n3.4 THEORETICAL PROPERTIES\n\nIn this section we aim to analyze the expectation and variance properties of forward gradient estimators. We focus our analysis on the gradient of one weight matrix {wij}, but the conclusion holds for a network with many weight matrices too.\n\nTable 1 summarizes the theoretical results1. With a batch size of N , independent perturbation can achieve 1/N reduction of variance, whereas shared perturbation has a constant variance term dominated by the squared gradient norm. However, when performing independent weight perturbation, matrix multiplications cannot be batched because each example’s activation vector is multiplied with a different weight matrix. By contrast, independent activity perturbation admits batched matrix multiplications. Moreover, activity perturbation enjoys a factor of fan-in (p) times smaller variance compared to weight perturbation since the number of perturbed elements is the number of output units instead of the size of the whole weight matrix. The only drawback of activity perturbation is the memory required for storage of intermediate activations, in exchange for a factor of N p reduction in variance. However, for both activity and weight perturbation, the variance still grows with larger networks. In Section 4 we will further reduce the variance by introducing local loss functions.\n\n3.5 CONTINUOUS-TIME RATE-BASED MODELS\n\nForward-mode AD can be viewed as computing the ﬁrst-order time derivative in a continuous-time\n\nphysical system. Suppose the tuples passed between nodes of the computation graph are (x, x˙ ),\n\nwhere x˙ is the change in x over time. The computation is then the same as forward-mode AD. For\n\neach node,\n\ny˙\n\n=\n\ndy dx\n\nx˙ ,\n\nwhere\n\ndy dx\n\nis\n\nthe\n\nJacobian between\n\nthe output and\n\nthe\n\ninput.\n\nNote that\n\nin\n\na\n\nphysical system we don’t have to explicitly perform the differentiation operation by running two\n\nforward passes. Instead the ﬁrst-order derivative information is readily available in the analog signal,\n\nand we only need to plug the output signal into a differentiator circuit.\n\nThe activity-perturbed learning rule for a continuous time system is thus w˙ ij ∝ xiy˙jr˙, where xi is the pre-synaptic activity, and y˙j is the rate of change in the post-synaptic activity, which is the perturbation direction for a small period of time, and r˙ is the rate of change of reward (or the negative loss). The reward controls whether learning is Hebbian or anti-Hebbian. Both Hinton et al. (2007) and Bengio et al. (2017) propose to use a product of pre-synaptic activity and the rate of change of postsynaptic activity. However, they did not consider using the rate of change of reward as a modulator and instead relied on another set of feedback weights to communicate the error signal through inputs. In contrast, we show that by broadcasting the rate of change of reward, we can actually bypass the weight transport problem.\n\n1All proofs can be found in Appendix 8 and 9. Numerical simulation results can be found in Appendix 10.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nLocal Losses\n\nChannel Mixing\n\nA\n\nToken Mixing\n\nLocal Losses\n\nChannel Mixing\n\n+\n\nA ……\n\nToken Mixing\n\nLocal Losses\n\nChannel Mixing\n\n+\n\nA\n\nFigure 1: A LocalMixer network consists of several mixer blocks. A=Activation function (ReLU).\n\nChannels\n\nToken Mixing\n\nPatches\n\nLN+FC+LN+A\n\nT\nLN+FC+LN+A\n\nChannel Mixing\nLN+FC+LN+A LN+FC+LN+A\nT\n\nReshape\n\nLN+FC+LN LN+FC+LN LN+FC+LN\n\nProject\nLN+FC LN+FC LN+FC\n\nLocal Losses\n\nChannels\n\n… … … … … …\n…\nPatches\n\nLN+FC+LN+A\nChannel Groups\n\nLN+FC+LN LN+FC+LN LN+FC+LN\n\nLN+FC LN+FC LN+FC\n\nN x HW x C N x C x HW\n\nN x HW x C\n\nN x HW x G x C/G\n\n+A\n\nN x HW x G\n\nFigure 2: A LocalMixer residual block with local losses. Token mixing consists of a linear layer and\n\nchannels are grouped in the channel mixing layers. Layer norm is applied before and after every linear\n\nlayer. LN=Layer Norm; FC=Fully Connected layer; A=Activation function (ReLU); T=Transpose.\n\n3.6 ACTIVATION SPARSITY AND NORMALIZATION FUNCTIONS\nIn networks with ReLU activations, we can leverage ReLU sparsity to achieve further variance reduction, because the inactivated units will have zero gradient and therefore we should not perturb these units, and set the perturbation to be zero.\nNormalization layers are often added in deep neural networks after the linear layer. To compute the correct gradient in activity perturbation, we also need to account for normalization in the weight update rule. Since there is no backward weight connections, one option is to simply apply backprop on normalization layers. However, we also found that it is also ﬁne to ignore the gradient of normalization layer when using layer normalization.\n4 SCALING WITH LOCAL LOSSES\nAs we have explained in the previous section, perturbation learning can suffer from a curse of dimensionality: the variance grows with the number of perturbation dimensions, and in deep networks there are often millions of parameters changing at the same time. One way to limit the number of learnable dimensions is to divide the network into submodules, each with a separate loss function. In this section, we will explore several ways to increase the number of local losses to tame the variance.\n1) Blockwise loss. First, we will divide the network into modules in depth. Each module consists of several layers. At the end of each module, we compute a loss function, and that loss is used to update the parameters in that module. This approach is equivalent of adding a “stop gradient” operator in between modules. Such local greedy losses were previously explored in Belilovsky et al. (2019) and Löwe et al. (2019).\n2) Patchwise loss. Sensory input signals such as images have spatial dimensions. We will apply a separate loss patchwise along these spatial dimensions. In the Vision Transformer architecture (Vaswani et al., 2017; Dosovitskiy et al., 2021), each spatial token represents a patch in the image. In modern deep networks, parameters in each spatial location are often shared to improve data efﬁciency and reduce memory bandwidth utilization. Although naive weight sharing is not biologically plausible, we still consider shared weights in this work. It may be possible to mimic the effect of weight sharing by adding knowledge distillation (Hinton et al., 2015) losses in between patches.\n3) Groupwise loss. Lastly, we turn to the channel dimension. To create multiple losses, we split the channels into a number of groups, and each group is attached to a loss function (Patel et al., 2022). To prevent groups from communicating between each other, channels are only connected to other channels within the same group. A grouped linear layer is computed as zg,j = i wg,i,jxg,i, for individual group g. Whereas previous work used channel groups to improve computational efﬁciency (Krizhevsky et al., 2012; Ioannou et al., 2017; Xie et al., 2017), in our work, adding groups contributes to the total number of losses and thus reduces variance.\nFeature aggregators. Naively applying losses separately to the spatial and channel dimensions leads to suboptimal performances, since each dimension contains only local information. For losses of\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nA. Conventional\n\nN x HW x C\n\nN x C\n\nB. Replicated\n\nAvg Patches\n\nGroups Patches\n\nN x HW x G x C/G N x HW x G x C Stack + StopGradient\n\nN x HW x HW x G x C N x HW x G x C\nAvg Avg Avg Avg AvgPool + StopGradient\n\nFigure 3: Feature aggregator designs. A) In the conventional design, average pooling is performed to aggregate features from different spatial locations. B) We propose the replicated design, features are ﬁrst concatenated across groups and then averaged across spatial locations. We create copies of the same feature with different stop gradient masks so that we obtain more local losses instead of a global one. The stop gradient mask makes sure that perturbation in one spatial group corresponds to its loss function. The numerical value of the loss function is the same as the conventional design.\n\nstandard tasks such as classiﬁcation, the model needs a global view of the inputs to make a decision. Standard architectures obtain this global view by performing global average pooling layer before the ﬁnal classiﬁcation layer. We therefore explore strategies for aggregating information from other groups and spatial patches before the local loss function.\n\nWe would prefer to perform aggregation without reducing the total number of dimensions. We thus propose a replicated design for feature aggregation, shown in Figure 3. First, channel groups are copied and communicated to one another, but every group except the active group itself is masked with stop gradient so that other groups do not affect the forward gradient computation:\n\nxp,g = [StopGrad(xp,1...xp,g−1), xp,g, StopGrad(xp,g+1, ..., xp,G)],\n\n(4)\n\nwhere p and g index the patches and groups respectively. Similarly, each spatial location is also\n\ncopied, communicated, and masked, and then averaged locally:\n\nxp,g\n\n=\n\n1 P\n\nxp,g +\n\np =p StopGrad(xp ,g) .\n\n(5)\n\nThe output of feature aggregation is the same as that of the conventional global average pooling layer. The difference is that here the loss is replicated and different patch groups are activated in each loss.\n\nLearning objectives. We consider the supervised classiﬁcation loss and the contrastive InfoNCE loss (van den Oord et al., 2018; Chen et al., 2020), which are the two most commonly used losses in image representation learning. For supervised classiﬁcation, we attach a shared linear layer (shared across p, g) on top of the aggregated features for a cross entropy loss: Lsp,g = − k tk log softmax(Wlxp,g)k. The loss is of the same value across each group and patch location.\nFor contrastive learning, the linear layer becomes a linear feature projector. Suppose x(n1) and x(n2) are the two different views of the n-th example, the InfoNCE loss for contrastive learning is:\n\nLcp,g = − log\nn\n\n(W x(n1,)p,g) StopGrad(W x(n2)) . m(W x(n1,)p,g) StopGrad(W x(m2))\n(6)\n\nOriginal 15\n\nDifferent Perturbation\n\nStopGrad\n\n10\n\nTraining Loss\n\nNote that we add a stop gradient operator on the second view. It is usually unnecessary to add this stop gradient in the InfoNCE loss; however, we found that perturbationbased methods require a stop gradient and otherwise the loss will not go down. This is likely because we share the perturbations on both views, and having the same perturbation will increase the dot product between the two views but is not desired from a representation learning perspective. Figure 4 shows a comparison of the loss curves. Non-shared perturbations also work but are worse than stop gradient.\n\n5\n0 20000 40000 60000 80000\nStep\nFigure 4: Importance of StopGradient in the InfoNCE loss, using M/8 on CIFAR-10 with 256 channels 1 group.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nType\n\nBlocks Patches Channels Groups Params\n\nLocalMixer S/1/1\n\n1\n\n1×1\n\n256\n\nLocalMixer M/1/16 1\n\n1×1\n\n512\n\nLocalMixer M/8/16 4\n\n8×8\n\n512\n\nLocalMixer L/8/64\n\n4\n\n8×8\n\n2048\n\nLocalMixer L/32/64 4 32×32 2048\n\n1\n\n272K\n\n16\n\n429K\n\n16\n\n919K\n\n64 13.1M\n\n64 17.3M\n\nTable 2: LocalMixer Architecture Details\n\nDataset\nMNIST MNIST CIFAR-10 CIFAR-10 ImageNet\n\n5 IMPLEMENTATION\n\nNetwork architecture. We propose the LocalMixer architecture that is more suitable for local learning. It takes inspiration from MLPMixer (Tolstikhin et al., 2021), which consists of fully connected networks and residual blocks. We leverage the fully connected networks so that each spatial patch performs computations without interfering with other patches, which is more compatible with our local learning objective. An image is divided into non-overlapping patches (i.e. tokens), and each block consists of token and channel mixing layers. Figure 1 shows the high level architecture, and Figure 2 shows the detailed diagram for one residual block. We add a linear projector/classiﬁcation layer to attach a loss function at the end of each block. The last layer always uses backprop to update weights. For token mixing layers, we use one linear fully connected layer instead of an MLP, since we would like to make each block as shallow as possible. Before the last channel mixing layer, features are reshaped into a number of groups, and the last layer is fully connected within each feature group. Table 2 shows architectural details for the different sizes of models we investigate.\nNormalization. There are many ways of performing normalization within a neural network across different tensor dimensions (Krizhevsky et al., 2012; Ioffe & Szegedy, 2015; Ba et al., 2016; Ren et al., 2017; Wu & He, 2018). We opted for a local variant of layer normalization that normalizes within each local spatial patch of features (Ren et al., 2017). For grouped linear layers, each group is normalized separately (Wu & He, 2018). Empirically, we found such local normalization performs better on contrastive learning experiments and about the same as layer normalization on supervised experiments. Local normalization is also more biologically plausible as it does not perform global communication. Conventionally, normalization layers are placed after linear layers. In MLPMixer (Tolstikhin et al., 2021), layer normalization is placed at the beginning of each residual block. We found it is the best to place normalization before and after each linear layer, as shown in Figure 2. Empirically this design choice does not make much difference for backprop, but it allows forward gradient learning to learn much faster and achieve lower training errors.\n\nEfﬁcient implementation of replicated losses. Due to the design of feature aggregation and replicated losses, a naïve implementation of groups can be very inefﬁcient in terms of both memory consumption and compute. However, each spatial group actually computes the same aggregated feature and loss function. This means that it is possible to share most of the computation across loss functions when performing both backprop and forward gradient. We implemented our custom JAX JVP/VJP functions (Bradbury et al., 2018) and observed signiﬁcant memory savings and compute speed-ups for replicated losses, which would otherwise not be feasible to run on modern hardware. The results are reported in Figure 5. A code snippet is included in Appendix 12.\n\nGPU Memory (G) 1 2 4 8 16 32\nCompute (sec/epoch) 1 2 4 8 16 32\n\nNaïve 40 30 20 10\n0\n\nFused\n\nNaïve 200 150 100\n50 0\n\nFused\n\nNumber of Groups\n\nNumber of Groups\n\nFigure 5: Memory and compute usage of naïve and fused implementation of replicated losses.\n\n6 EXPERIMENTS\n\nWe compare our proposed algorithm to a set of alternatives: Backprop, Feedback Alignment and other global variants of Forward Gradient. Backprop is a biologically implausible oracle, since it computes true gradients whereas we compute noisy gradients. Feedback alignment computes approximate gradients by using a set of random backward weights. We explain each method below.\n1) Backprop (BP). We include the standard backprop algorithm as well as its local variants. Local Backprop (L-BP) adds local losses as proposed, but still permits gradient to ﬂow in an end-to-end fashion. Local Greedy Backprop (LG-BP) in addition adds stop gradient operators in between blocks. This is to provide a comparison to our methods by computing true local gradients. LG-BP is similar in spirit to recent local learning algorithms (Belilovsky et al., 2019; Löwe et al., 2019).\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nDataset Network Metric\nBP L-BP LG-BP\nFA L-FA LG-FA DFA\nFG-W FG-A LG-FG-W LG-FG-A\n\nMNIST\n\nMNIST\n\nCIFAR-10\n\nImageNet\n\nS/1/1\n\nM/1/16\n\nM/8/16\n\nL/32/64\n\nTest / Train Err. (%) Test / Train Err. (%) Test / Train Err. (%) Test / Train Err. (%)\n\n2.66 / 0.00 2.38 / 0.00 2.43 / 0.00\n\n2.41 / 0.00 2.16 / 0.00 2.81 / 0.00\n\n33.62 / 0.00 30.75 / 0.00 33.84 / 0.05\n\n36.82 / 14.69 42.38 / 22.80 54.37 / 39.66\n\n2.82 / 0.00 3.21 / 0.00 3.11 / 0.00 3.31 / 0.00\n\nBP-free algorithms\n2.90 / 0.00 2.90 / 0.00 2.50 / 0.00 3.17 / 0.00\n\n39.94 / 28.44 39.74 / 28.98 39.73 / 32.32 38.80 / 33.69\n\n94.55 / 94.13 87.20 / 85.69 85.45 / 82.83 91.17 / 90.28\n\n9.25 / 8.93 3.24 / 1.53 9.25 / 8.93 3.24 / 1.53\n\n8.56 / 8.64 3.76 / 1.75 5.66 / 4.59 2.55 / 0.00\n\n55.95 / 54.28 59.72 / 41.29 52.70 / 51.71 30.68 / 19.39\n\n97.71 / 97.58 98.83 / 98.80 97.39 / 97.29 58.37 / 44.86\n\nTable 3: Supervised learning for image classiﬁcation\n\nDataset Network Metric\n\nCIFAR-10\n\nCIFAR-10\n\nImageNet\n\nM/8/16\n\nL/8/64\n\nL/32/64\n\nTest / Train Err. (%) Test / Train Err. (%) Test / Train Err. (%)\n\nBP L-BP LG-BP\n\n24.11 / 21.08 24.69 / 21.80 29.63 / 25.60\n\n17.53 / 13.35 19.13 / 13.60 23.62 / 16.80\n\n55.66 / 49.79 59.11 / 52.50 68.36 / 62.53\n\nBP-free algorithms\n\nFA L-FA LG-FA DFA\n\n45.87 / 44.06 37.73 / 36.13 36.72 / 34.06 46.09 / 42.76\n\n67.93 / 65.32 31.05 / 26.97 30.49 / 25.56 39.26 / 37.17\n\n82.86 / 80.21 83.18 / 79.80 82.57 / 79.53 93.51 / 92.51\n\nFG-W FG-A LG-FG-W LG-FG-A\n\n53.37 / 51.56 54.59 / 52.96 52.66 / 50.23 32.88 / 29.73\n\n50.45 / 45.64 56.63 / 56.09 52.27 / 48.67 26.81 / 23.90\n\n91.94 / 89.69 97.83 / 97.79 91.36 / 88.81 73.24 / 66.89\n\nTable 4: Self-supervised contrastive learning with linear readout\n\n2) Feedback Alignment (FA). The standard FA algorithm (Lillicrap et al., 2016) adds a set of random and ﬁxed backward weights. We assume that the gradients to normalization layers and activation functions are known since they do not have weight connections. Local Feedback Alignment (L-FA) adds local losses as proposed, but still permits error signals to ﬂow back. Local Greedy Feedback Alignment (LG-FA) adds a stop gradient to prevent error signals from ﬂowing back, similar to the backprop-free algorithm in Nøkland & Eidnes (2019).\n3) Forward Gradient (FG). This family of methods comprises our proposed algorithm and related approaches. Weight-perturbed forward gradient (FG-W) was proposed by Baydin et al. (2022). In this paper, we propose the activity perturbation variant (FG-A). We further add local objective functions, producing LG-FG-W and LG-FG-A, which stand for Local Greedy Forward Gradient Weight/Activity-Perturbed. For local perturbation to work, we have to add a stop gradient in between blocks so each perturbation has a single corresponding loss. We expect LG-FG-A to achieve the best performance among other variants because it can leverage the variance reduction beneﬁt from both activity perturbation and local losses.\nDatasets. We use standard image classiﬁcation datasets to benchmark the learning algorithms. MNIST (LeCun, 1998) contains 70,000 28×28 handwritten digit images of class 0-9. CIFAR10 (Krizhevsky et al., 2009) contains 60,000 32×32 natural images of 10 semantic classes. ImageNet (Deng et al., 2009) contains 1.3 million natural images of 1000 classes, which we resized to 224×224. For CIFAR-10 and ImageNet, we applied both supervised learning and contrastive learning. For MNIST, we applied supervised learning only. We designed different conﬁgurations of the LocalMixer architecture for each dataset, listed in Table 2.\nData augmentation. For MNIST and CIFAR-10 supervised experiments, we do not apply data augmentation. Data augmentation on ImageNet follows the open source implementation by Grill\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nAccuracy (%) Accuracy (%) Accuracy (%)\n\nTest\n\nTrain\n\n85\n\n75\n\n65\n\n55\n\n45\n\n35 Global Block Block + Patch + Block + Patch Group Patch + Group\n\nTest\n\nTrain\n\n70\n\n65\n\n60\n\n55\n\n50\n\n45\n\n40\n\n35 Global Block Block + Patch + Block + Patch Group Patch + Group\n\nTest\n\nTrain\n\n60\n\n40\n\n20\n\n0 Global Block Patch + Block + Block + Group Patch Patch + Group\n\n(a) CIFAR-10 Supervised M/8 (b) CIFAR-10 Contrastive M/8 (c) ImageNet Supervised L/32 Figure 6: Effect of adding local losses at different locations on the performance of forward gradient\n\nError Rate (%) Error Rate (%) Error Rate (%) Error Rate (%)\n\n65\n\n65 Group 1\n\n6G5roup 1\n\nGroup 2\n\nGroup 2\n\n55\n\n55\n\nGroup 4\n\n5G5roup 4\n\n45 Group 8\n\nGroup 8\n\n45\n\nGroup 16\n\n4G5roup 16\n\n35 Group 32\n\nGroup 32\n\n35\n\n35\n\n25\n\n25\n\n15\n\n25\n\n0\n\n25000\n\n50000\n\n75000 0\n\n25000\n\n50000\n\n75000\n\n20000 40000 60000 80000\n\nSteps\n\nSteps\n\nSteps\n\n(a) Supervised Test (b) Supervised Train (c) Contrastive Test\n\n6G5 roup 1 Group 2 5G5 roup 4 Group 8 4G5 roup 16 Group 32 35\n\nGroup 1 Group 2 Group 4 Group 8 Group 16 Group 32\n\n25 20000 40000 60000 80000\nSteps\n(d) Contrastive Train\n\nFigure 7: Error rate of M/8/* during CIFAR-10 training using different number of groups.\n\net al. (2020). Because forward gradient suffers from variance, we apply weaker augmentations for contrastive learning experiments, increasing the area lower bound for random crops from 0.08 to 0.3-0.5. We ﬁnd that this change has relatively little effect on the performance of backprop.\nMain results. Our main results are shown in Table 3 and Table 4. In supervised experiments, there is almost no cost of introducing local greedy losses, and our local forward gradient method can match the test error of backprop on MNIST and CIFAR. Note that LG-FG-A fails to overﬁt the training set to 0% error when trained without data augmentation. This suggests that variance could still be an issue. For CIFAR-10 contrastive learning, our method obtains an error rate approaching that obtained by backprop (26.81% vs. 17.53%), and most of the gap is due to greedy learning vs. gradient estimation (6.09% vs. 3.19%). On ImageNet, we achieve reasonable performance compared to backprop (58.37% vs. 36.82% for supervised and 73.24% vs. 55.66% for contrastive). However, we ﬁnd that the error due to greediness grows as the problem gets more complex and requires more layers to cooperate. We signiﬁcantly outperform the FA family on ImageNet (by 25% for supervised and 10% for contrastive). Interestingly, local greedy FA also performs better than global feedback alignment, which suggests that the beneﬁt of local learning transfers to other types of gradient approximation. TP-based methods were evaluated in Bartunov et al. (2018) and were found to be worse than FA on ImageNet. In sum, although there is still some noticeable gap between our method and backprop, we have made a large stride forward compared to backprop-free algorithms. More results are included in the Appendix 14.\nEffect of local losses. In Figure 6 we ablate the beneﬁt of placing local losses at different locations: blockwise, patchwise and groupwise. A combination of all three is the strongest. Global perturbation learning fails to learn as the accuracy is similar to initializing with random weights.\nEffect of groups. In Figure 7 we investigate the effect of different number of groups by showing the training curves. Adding more groups bring signiﬁcant improvement to local perturbation learning in terms of lowering both training and test errors, but the effect vanishes around 8 channels / group.\n7 CONCLUSION\nIt is often believed that perturbation-based learning cannot scale to large and deep networks. We show that this is to some extent true because the gradient estimation variance grows with the number of hidden dimensions for activity perturbation, and is even worse for shared weight perturbation. But more optimistically, we show that a huge number of local greedy losses can help forward gradient learning scale much better. We explored blockwise, patchwise, and groupwise local losses, and a combination of all three, with a total of a quarter of a million losses in one of the larger networks, performs the best. Local activity-perturbed forward gradient performs better than previous backpropfree algorithms on larger networks. The idea of local losses opens up opportunities for different loss designs and sheds light on the search for biologically plausible learning algorithms in the brain and alternative computing devices.\n\n9\n\nPublished as a conference paper at ICLR 2023\nACKNOWLEDGMENT\nWe thank Timothy Lillicrap for his helpful feedback on our earlier draft.\nREFERENCES\nLarry F Abbott and Sacha B Nelson. Synaptic plasticity: taming the beast. Nature neuroscience, 3 (11):1178–1183, 2000.\nMohamed Akrout, Collin Wilson, Peter C. Humphreys, Timothy P. Lillicrap, and Douglas B. Tweed. Deep learning without weight transport. In Advances in Neural Information Processing Systems 32, NeurIPS, 2019.\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450, 2016.\nPierre Baldi and Peter J. Sadowski. A theory of local learning, the learning channel, and the optimality of backpropagation. Neural Networks, 83:51–74, 2016. doi: 10.1016/j.neunet.2016.07.006.\nAndrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can solve difﬁcult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, (5):834–846, 1983.\nSergey Bartunov, Adam Santoro, Blake A. Richards, Luke Marris, Geoffrey E. Hinton, and Timothy P. Lillicrap. Assessing the scalability of biologically-motivated deep learning algorithms and architectures. In Advances in Neural Information Processing Systems 31, NeurIPS, 2018.\nAtilim Günes Baydin, Barak A. Pearlmutter, Don Syme, Frank Wood, and Philip H. S. Torr. Gradients without backpropagation. CoRR, abs/2202.08587, 2022.\nEugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Greedy layerwise learning can scale to imagenet. In Proceedings of the 36th International Conference on Machine Learning, ICML, 2019.\nEugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Decoupled greedy learning of cnns. In Proceedings of the 37th International Conference on Machine Learning, ICML, 2020.\nYoshua Bengio. How auto-encoders could provide credit assignment in deep networks via target propagation. CoRR, abs/1407.7906, 2014.\nYoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In Advances in Neural Information Processing Systems 19, NIPS, 2006.\nYoshua Bengio, Thomas Mesnard, Asja Fischer, Saizheng Zhang, and Yuhuai Wu. Stdp as presynaptic activity times rate of change of postsynaptic activity approximates back-propagation. Neural Computation, 10, 2017.\nElie L Bienenstock, Leon N Cooper, and Paul W Munro. Theory for the development of neuron selectivity: orientation speciﬁcity and binocular interaction in visual cortex. Journal of Neuroscience, 2 (1):32–48, 1982.\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML, 2020.\nDavid G. Clark, L. F. Abbott, and SueYeon Chung. Credit assignment through broadcasting a global error vector. In Advances in Neural Information Processing Systems 34, NeurIPS, 2021.\n10\n\nPublished as a conference paper at ICLR 2023\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2009.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR, 2021.\nIla R Fiete and H Sebastian Seung. Gradient learning in spiking neural networks by dynamic perturbation of conductances. Physical review letters, 97(4):048104, 2006.\nAidan N. Gomez, Oscar Key, Stephen Gou, Nick Frosst, Jeff Dean, and Yarin Gal. Interlocking backpropagation: Improving depthwise model-parallelism. CoRR, abs/2010.04116, 2020.\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Ávila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent - A new approach to self-supervised learning. In Advances in Neural Information Processing Systems 33, NeurIPS, 2020.\nDonald Olding Hebb. The organization of behavior: a neuropsychological theory. J. Wiley; Chapman & Hall, 1949.\nGeoffrey Hinton et al. How to do backpropagation in a brain. In Invited talk at the NIPS 2007 deep learning workshop, 2007.\nGeoffrey E. Hinton and James L. McClelland. Learning representations by recirculation. In Neural Information Processing Systems, 1987.\nGeoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief nets. Neural Comput., 18(7):1527–1554, 2006. doi: 10.1162/neco.2006.18.7.1527.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015.\nYani Ioannou, Duncan P. Robertson, Roberto Cipolla, and Antonio Criminisi. Deep roots: Improving CNN efﬁciency with hierarchical ﬁlter groups. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2017.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, ICML, 2015.\nMarwan Jabri and Barry Flower. Weight perturbation: An optimal architecture and learning technique for analog vlsi feedforward and recurrent multilayer networks. IEEE Transactions on Neural Networks, 3(1):154–157, 1992.\nJohn F Kolen and Jordan B Pollack. Backpropagation without weight transport. In Proceedings of IEEE International Conference on Neural Networks, ICNN, 1994.\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, NIPS, 2012.\nMichael Laskin, Luke Metz, Seth Nabarrao, Mark Sarouﬁm, Badreddine Noune, Carlo Luschi, Jascha Sohl-Dickstein, and Pieter Abbeel. Parallel training of deep networks with local updates. CoRR, abs/2012.03837, 2020.\nYann Le Cun, Conrad Galland, and Geoffrey E Hinton. Gemini: Gradient estimation through matrix inversion after noise injection. In Advances in Neural Information Processing Systems, NIPS, volume 1. Morgan-Kaufmann, 1988.\n11\n\nPublished as a conference paper at ICLR 2023\nYann LeCun. A learning scheme for asymmetric threshold networks. Proceedings of COGNITIVA, 85(537):599–604, 1985.\nYann LeCun. The mnist database of handwritten digits, 1998. URL http://yann.lecun.com/ exdb/mnist/.\nDong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation. In Joint european conference on machine learning and knowledge discovery in databases, pp. 498–515. Springer, 2015.\nQianli Liao, Joel Z. Leibo, and Tomaso A. Poggio. How important is weight symmetry in backpropagation? In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence, AAAI, 2016.\nTimothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed, and Colin J. Akerman. Random synaptic feedback weights support error backpropagation for deep learning. Nature Communications, 7(1): 13276, Nov 2016. ISSN 2041-1723. doi: 10.1038/ncomms13276.\nTimothy P. Lillicrap, Adam Santoro, Luke Marris, Colin J. Akerman, and Geoffrey Hinton. Backpropagation and the brain. Nature Reviews Neuroscience, 21(6):335–346, Jun 2020. ISSN 1471-0048. doi: 10.1038/s41583-020-0277-3.\nSindy Löwe, Peter O’Connor, and Bastiaan S. Veeling. Putting an end to end-to-end: Gradientisolated learning of representations. In Advances in Neural Information Processing Systems 32, NeurIPS, 2019.\nArild Nøkland. Direct feedback alignment provides learning in deep neural networks. In Advances in Neural Information Processing Systems 29, NeurIPS, 2016.\nArild Nøkland and Lars Hiller Eidnes. Training neural networks with local error signals. In Proceedings of the 36th International Conference on Machine Learning, ICML, 2019.\nErkki Oja. Simpliﬁed neuron model as a principal component analyzer. Journal of mathematical biology, 15(3):267–273, 1982.\nAdeetya Patel, Michael Eickenberg, and Eugene Belilovsky. Local learning with neuron groups. In From Cells to Societies: Collective Learning Across Scales - ICLR 2022 Workshop, 2022.\nBarak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147–160, 1994.\nAntti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-supervised learning with ladder networks. In Advances in Neural Information Processing Systems 28, NIPS, 2015.\nMengye Ren, Renjie Liao, Raquel Urtasun, Fabian H. Sinz, and Richard S. Zemel. Normalizing the normalizers: Comparing and extending network normalization schemes. In Proceedings of the 5th International Conference on Learning Representations, ICLR, 2017.\nD. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by error propagation. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations, pp. 318–362, Cambridge, MA, USA, 1986. MIT Press. ISBN 026268053X.\nTim Salimans, Jonathan Ho, Xi Chen, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. CoRR, abs/1703.03864, 2017.\nWolfram Schultz, Peter Dayan, and P Read Montague. A neural substrate of prediction and reward. Science, 275(5306):1593–1599, 1997.\nH Sebastian Seung. Learning in spiking neural networks by reinforcement of stochastic synaptic transmission. Neuron, 40(6):1063–1073, 2003.\nEren Sezener, Agnieszka Grabska-Barwin´ska, Dimitar Kostadinov, Maxime Beau, Sanjukta Krishnagopal, David Budden, Marcus Hutter, Joel Veness, Matthew Botvinick, Claudia Clopath, et al. A rapid and efﬁcient learning rule for biological neural circuits. BioRxiv, 2021.\n12\n\nPublished as a conference paper at ICLR 2023\nDavid Silver, Anirudh Goyal, Ivo Danihelka, Matteo Hessel, and Hado van Hasselt. Learning by directional gradient descent. In Proceedings of the 10th International Conference on Learning Representations, ICLR, 2022.\nKenneth O Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topologies. Evol Comput, 10(2):99–127, 2002.\nIlya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision. In Advances in Neural Information Processing Systems 34, NeurIPS, 2021.\nAäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30, NIPS, 2017.\nJoel Veness, Tor Lattimore, Avishkar Bhoopchand, Agnieszka Grabska-Barwinska, Christopher Mattern, and Peter Toth. Online learning with gated linear networks. arXiv preprint arXiv:1712.01897, 2017.\nJoel Veness, Tor Lattimore, David Budden, Avishkar Bhoopchand, Christopher Mattern, Agnieszka Grabska-Barwinska, Eren Sezener, Jianan Wang, Peter Toth, Simon Schmitt, et al. Gated linear networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, AAAI, 2021.\nPascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Mach. Learn. Res., 11:3371–3408, 2010. doi: 10.5555/1756006.1953039.\nYulin Wang, Zanlin Ni, Shiji Song, Le Yang, and Gao Huang. Revisiting locally supervised learning: an alternative to end-to-end training. In 9th International Conference on Learning Representations, ICLR, 2021.\nYeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger B. Grosse. Flipout: Efﬁcient pseudoindependent weight perturbations on mini-batches. In 6th International Conference on Learning Representations, ICLR, 2018.\nR. E. Wengert. A simple automatic derivative evaluation program. Commun. ACM, 7(8):463–464, 1964.\nPaul Werbos. Beyond regression:\" new tools for prediction and analysis in the behavioral sciences. Ph. D. dissertation, Harvard University, 1974.\nJustin Werfel, Xiaohui Xie, and H Seung. Learning curves for stochastic gradient descent in linear feedforward networks. Advances in Neural Information Processing Systems 16, NIPS, 2003.\nL. Darrell Whitley. Genetic reinforcement learning for neurocontrol problems. Mach. Learn., 13: 259–284, 1993.\nJames C.R. Whittington and Rafal Bogacz. Theories of error back-propagation in the brain. Trends in Cognitive Sciences, 23(3):235–250, Mar 2019. ISSN 1364-6613. doi: 10.1016/j.tics.2018.12.005.\nBernard Widrow and Marcian E Hoff. Adaptive switching circuits. Technical report, Stanford Univ Ca Stanford Electronics Labs, 1960.\nBernard Widrow and Michael A. Lehr. 30 years of adaptive neural networks: perceptron, madaline, and backpropagation. Proc. IEEE, 78(9):1415–1442, 1990.\nRonald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270–280, 1989.\n13\n\nPublished as a conference paper at ICLR 2023\nSunghyeon Woo, Jeongwoo Park, Jiwoo Hong, and Dongsuk Jeon. Activation sharing with asymmetric paths solves weight transport problem without bidirectional connection. In Advances in Neural Information Processing Systems 34, NeurIPS, 2021.\nYuxin Wu and Kaiming He. Group normalization. In 15th European Conference on Computer Vision, ECCV, 2018.\nWill Xiao, Honglin Chen, Qianli Liao, and Tomaso A. Poggio. Biologically-plausible learning algorithms can scale to large datasets. In Proceedings of the 7th International Conference on Learning Representations, ICLR, 2019.\nSaining Xie, Ross B. Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2017.\nXiaohui Xie and H Sebastian Seung. Spike-based learning rules and stabilization of persistent neural activity. Advances in Neural Information Processing Systems 12, NIPS, 1999.\nYuwen Xiong, Mengye Ren, and Raquel Urtasun. Loco: Local contrastive representation learning. In Advances in Neural Information Processing Systems 33, NeurIPS, 2020.\n14\n\nPublished as a conference paper at ICLR 2023\n\n8 PROOFS OF UNBIASEDNESS\nIn this section, we show the unbiasedness of gw(wij) and ga(wij). The ﬁrst proof was given by Baydin et al. (2022).\nProposition 1. gw(wij) is an unbiased gradient estimator if {vij} are independent zero-mean uni-variance random variables (Baydin et al., 2022).\n\nProof. We can rewrite the weight perturbation estimator as\n\n\n\n\n\ngw(wij ) =  ∇wi j vi j  vij = ∇wij vi2j +\n\n∇wi j vij vi j .\n\n(7)\n\nij\n\ni j =ij\n\nNote that since each dimension of v is an independent zero-mean uni-variance random variable,\n\nE[vij] = 0, E[vi2j] = Var[vij] + E[vij]2 = 1 + 0 = 1, and E[vijvi j ] = 0 if ij = i j .\n\n\n\n\n\nE[gw(wij )] = E ∇wij vi2j + E \n\n∇wi j vij vi j \n\n(8)\n\ni j =ij\n\n= ∇wij E vi2j +\n\n∇wi j E [vij vi j ]\n\n(9)\n\ni j =ij\n\n= ∇wij · 1 +\n\n∇wi j · 0\n\n(10)\n\ni j =ij\n\n= ∇wij .\n\n(11)\n\nProposition 2. ga(wij) is an unbiased gradient estimator if {uj} are independent zero-mean univariance random variables.\n\nProof. The true gradient to the weights ∇wij is the product between xj and ∇zk. Therefore, we can rewrite the weight perturbation estimator as\n\n\n\n\n\n\n\n\n\nga(wij ) = xi  ∇zj uj  uj = xj ∇zj u2j + xi  ∇zj uj  uj\n\n(12)\n\nj\n\nj =j\n\n\n\n\n\n= xi∇zj u2j +  xi∇zj uj  uj\n\n(13)\n\nj =j\n\n\n\n\n\n= ∇wij u2j +  ∇wij uj  uj .\n\n(14)\n\nj =j\n\nSince each dimension of u is an independent zero-mean uni-variance random variable, E[uj] = 0, E[u2j ] = Var[uj] + E[uj]2 = 1 + 0 = 1, and E[ujuj ] = 0 if j = j .\n\n\n\n\n\n\n\nE[ga(wij )] = E ∇wij u2j +  ∇wij uj  uj \n\n(15)\n\nj =j\n\n= ∇wij E u2j + ∇wij E [uj uj ]\n\n(16)\n\nj =j\n\n= ∇wij · 1 + ∇wij · 0\n\n(17)\n\nj =j\n\n= ∇wij .\n\n(18)\n\n15\n\nPublished as a conference paper at ICLR 2023\n\n9 PROOFS OF VARIANCES\n\nWe followed Wen et al. (2018) and show that the variance of the gradient estimators can be decomposed.\n\nLemma 1. The variance of the gradient estimator can be decomposed into three parts:\n\nVar (g(wij)|x)\n\n=\n\nZ1\n\n+ Z2\n\n+ Z3,\n\nwhere\n\nZ1\n\n=\n\n1 N\n\nV1\n\nVarx\n\n(∇wij |x),\n\nZ2\n\n=\n\n1 N\n\nEx\n\n[Varv\n\n( g(wij)| x)],\n\nZ3\n\n=\n\n1 N2\n\nEB\n\nx(n)∈B x(m)∈B\\{x(n)} Covv( g(wij )| x(n), g(wij )| x(m)) .\n\nProof. By the law of total variance,\n\nVar (g(wij)) = Var\nB\n\nE\nv\n\n[g(wij\n\n)|B\n\n]\n\n+E\nB\n\nVvar(g(wij )|B )\n\n.\n\n(19)\n\nThe ﬁrst term comes from the gradient variance from data sampling, and it vanishes as batch size\n\ngrows:\n\nVar\nB\n\nE\nv\n\n[g(wij\n\n)|B]\n\n(20)\n\n\n\n\n\n1\n\n=\n\nVar\nB\n\nE\nv\n\n\n\nN\n\ng(wij )|x(n)\n\n(21)\n\nx(n) ∈B\n\n\n\n\n\n1\n\n=N2\n\nVar\nB\n\nE\nv\n\n\n\ng(wij )|x(n)\n\n(22)\n\nx(n) ∈B\n\n\n\n\n\n1\n\n=N2\n\nVar\nB\n\n\n\nE\nv\n\ng(wij )|x(n)\n\n\n\n(23)\n\nx(n) ∈B\n\n\n\n\n\n1\n\n=N2\n\nVar\nB\n\n\n\n∇wij |x(n) \n\nx(n) ∈B\n\n(24)\n\n1\n\n1\n\n=N2\n\nVar\nx\n\n(∇wij\n\n|x)\n\n=\n\nN\n\nVar\nx\n\n(∇wij\n\n|x)\n\n=\n\nZ1.\n\n(25)\n\nn\n\nThe second term comes from the gradient estimation variance:\n\nE\nB\n\nVar\nv\n\n(g(wij\n\n)|B)\n\n(26)\n\n\n\n\n\n1\n\n=\n\nE\nB\n\nVar\nv\n\n\n\nN\n\ng(wij ) x(n)\n\n(27)\n\nx(n) ∈B\n\n\n\n\n\n\n\n1\n\n=\n\nE\nB\n\nN2\n\nVar\nv\n\n\n\ng(wij ) x(n)\n\n(28)\n\nx(n) ∈B\n\n\n\n1\n\n=\n\nE\nB\n\n\n\nN\n\n2\n\nVar\nv\n\nx(n) ∈B\n\ng(wij )| x(n)\n\n\n\n+\n\nCov(\nv\n\ng(wij )|\n\nx(n),\n\ng(wij )|\n\nx(m))\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(29)\n\n\n\n\n\n1\n\n1\n\n= N\n\nE\nx\n\nVar\nv\n\n(\n\ng(wij\n\n)|\n\nx)\n\n+\n\nN2\n\nE\nB\n\n\n\nCov(\nv\n\ng(wij\n\n)|\n\nx(n),\n\ng(wij\n\n)|\n\nx(m))\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(30)\n\n=Z2 + Z3.\n\n(31)\n\nRemark. Z2 is the variance of the gradient estimator in the deterministic case, and Z3 measures the correlation between different gradient estimation within the batch. The Z3 is zero if the perturbations are independent, and non-zero if the perturbations are shared within the mini-batch.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nProposition 3. Let p × q be the size of the weight matrix, the element-wise average variance of the\n\nweight perturbed gradient estimator\n\nwith\n\na batch size N\n\nis\n\npq+2 N\n\nV\n\n+ (pq\n\n+ 1)S\n\nif the perturbations\n\nare shared across the batch,\n\nand\n\npq+2 N\n\nV\n\n+\n\npq+1 N\n\nS\n\nif they are independent, where V\n\nis the element-wise\n\naverage variance of the true gradient, and S is the element-wise average squared gradient.\n\nProof. We ﬁrst derive Z2.\n\n1\n\nZ2\n\n= N\n\nE\nx\n\nVar\nv\n\n(\n\ngw\n\n(wij\n\n)|\n\nx)\n\n(32)\n\n \n\n \n\n1\n\n= N\n\nE Var \nxv\n\n∇wi j vi j  vij \n\n(33)\n\nij\n\n\n\n\n\n1 =\nN\n\nE\nx\n\nVar\nv\n\n∇wij\n\nvi2j\n\n+\n\n∇wi j vij vi j \n\n(34)\n\ni j =ij\n\n\n\n1\n\n= N\n\nE Var\nxv\n\n∇wij vi2j\n\n\n\n\n\n\n\n\n\n+ Var \nv\n\n∇wi\n\nj\n\nvij vi\n\nj\n\n+\n\n2\n\nCov\nv\n\n∇wij\n\nvi2j\n\n,\n\n∇wij vij vi j \n\ni j =ij\n\ni j =ij\n\n(35)\n\n\n\n\n\n\n\n1\n\n= N\n\nE Var\nx\n\n∇wij vi2j\n\n+ Var \nv\n\n∇wi j vij vi j  +\n\n(36)\n\ni j =ij\n\n\n\n\n\n\n\n\n\n2E\nv\n\n∇wij ∇wi\n\nj\n\nvi3j vi\n\nj\n\n−\n\n2E\nv\n\n∇wij vi2j\n\nE\nv\n\n∇wi j vij vi j \n\ni j =ij\n\ni j =ij\n\n(37)\n\n\n\n\n\n\n\n1 =\nN\n\nE\nx\n\n∇wi2j\n\nVar\nv\n\nvi2j\n\n+ Var \nv\n\n∇wi j vij vi j  +\n\n(38)\n\ni j =ij\n\n\n\n\n\n2\n\n∇wij ∇wi j\n\nE\nv\n\nvi3j vi j\n\n−\n\n2∇wij\n\nE\nv\n\nvi2j\n\n\n\n∇wi j\n\nE\nv\n\n[vij\n\nvi\n\nj\n\n]\n\ni j =ij\n\ni j =ij\n\n(39)\n\n\n\n\n\n\n\n1 =\nN\n\nE\nx\n\n∇wi2j\n\nVar\nv\n\nvi2j\n\n+ Var \nv\n\n∇wi j vij vi j  +\n\n(40)\n\ni j =ij\n\n\n\n\n\n2\n\n∇wij∇wi j · 0 − 2∇wij · 1 \n\n∇wi j · 0\n\n(41)\n\ni j =ij\n\ni j =ij\n\n\n\n\n\n\n\n1 =\nN\n\nE\nx\n\n∇wi2j\n\nVar\nv\n\nvi2j\n\n+ Var \nv\n\n∇wi j vij vi j \n\n(42)\n\ni j =ij\n\n\n\n\n\n1 =\nN\n\nE\nx\n\n∇wi2j\n\n·\n\n(E[vi4j ]\n\n−\n\nEv [vi2j ]2)\n\n+\n\nVar\nv\n\n(∇wi\n\nj\n\nvij vi\n\nj\n\n)\n\n(43)\n\ni j =ij\n\n\n\n\n\n1 =\nN\n\nE\nx\n\n∇wi2j\n\n(3\n\nVar[vij\nv\n\n]2\n\n−\n\nEv [vi2j ]2)\n\n+\n\n∇wi2 j\n\nVar (vij vi j )\nv\n\n(44)\n\ni j =ij\n\n\n\n\n\n1\n\n= N\n\nE\nx\n\n2∇wi2j\n\n1\n\n+\n\nN\n\nE\nx\n\n∇wi2 j (Vvar[vij ] + Ev [vi j ]2)(Vvar[vi j ] + Ev [vi j ]2) − Ev [vij ]2 Ev [vi j ]2\n\ni j =ij\n\n(45)\n\n17\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n2 =\nN\n\n∇w2ij\n\n+\n\n2 N\n\nVxar(∇wij |x)\n\n+\n\n1 N\n\nE\nx\n\n∇wi2 j Vvar(vij ) Vvar(vi j )\n\n(46)\n\ni j =ij\n\n2 =\nN\n\n∇w2ij\n\n+\n\n2 N\n\nVar(∇wij |x)\nx\n\n+\n\n1 N\n\nE\nx\n\n∇wi2 j\n\n(47)\n\ni j =ij\n\n\n\n\n\n1 =\nN\n\n∇w2ij\n\n+\n\nVar(∇wij |x)\nx\n\n+\n\n∇w2i j\n\n+ Var(∇wi j |x)\nx\n\n.\n\n(48)\n\nij\n\nZ3 is nonzero if the perturbations are shared within a batch. Assuming that the perturbations are\n\nshared,\n\n\n\n\n\n1\n\nZ3\n\n=N2\n\nE\nB\n\n\n\nCov(\nv\n\ngw(wij )|\n\nx(n),\n\ngw (wij\n\n)|\n\nx(m))\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(49)\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nv\n\nx(n)∈B x(m)∈B\\{x(n)}\n\ngw(wij )| x(n) gw(wij )| x(m)\n\n−E\nv\n\ngw(wij )| x(n)\n\n E gw(wij )| x(m) \nv\n(50)\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nv\n\nx(n)∈B x(m)∈B\\{x(n)}\n\ngw(wij )| x(n) gw(wij )| x(m)\n\n − ∇wij |x(n)∇wij |x(m)\n(51)\n\n\n\n\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE \nv\n\n∇wi j |x(n)vi j  vij \n\n∇wi j |x(m)vi j  vij  −\n\nx(n)∈B x(m)∈B\\{x(n)}\n\nij\n\nij\n\n(52)\n\n∇wij |x(n)∇wij |x(m)\n\n(53)\n\n\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nv\n\n∇wij\n\n|x(n)vi2j\n\n+\n\n∇wi j |x(n)vi j vij \n\nx(n)∈B x(m)∈B\\{x(n)}\n\ni j =ij\n\n\n\n\n\n(54)\n\n∇wij |x(m)vi2j +\n\n∇wi j |x(m)vi j vij  −\n\ni j =ij\n\n\n\n\n\n1\n\nN2\n\nE\nB\n\n\n\n∇wij |x(n)∇wij |x(m)\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nv\n\n∇wij |x(n)∇wij |x(m)vi4j +\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(55) (56) (57)\n\n∇wij |x(n)vi2j\n\n∇wi j |x(m)vi j vij + ∇wij |x(m)vi2j\n\n∇wi j |x(n)vi j vij + (58)\n\ni j =ij\n\ni j =ij\n\n\n\n∇wi j |x(n)vi j vij\n\n∇wi j |x(m)vi j vij  −\n\ni j =ij\n\ni j =ij\n\n\n\n\n\n\n\n1\n\nN2\n\nE\nx(n)\n\nE\nx(m)\n\n\n\nn\n\n∇wij |x(n)∇wij |x(m)\nm=n\n\n(59) (60)\n\n18\n\nPublished as a conference paper at ICLR 2023\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nv\n\n∇wij |x(n)∇wij |x(m)vi4j +\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(61)\n\n\n\n\n\n \n\n\n\n\n\n\n\n∇wi j |x(n)vi j  \n\n∇wi\n\nj\n\n|x(m)vi\n\nj\n\n vi2j \n\n−\n\n1 N2\n\n\n\n∇w2ij (62)\n\ni j =ij\n\ni j =ij\n\nn m=n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nv\n\n∇wij |x(n)∇wij |x(m)vi4j +\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(63) \n\n∇wi j |x(n)∇wi j |x(m)vi2 j vi2j +\n\n∇wi j |x(n)∇wi j |x(m)vi j vi j vi2j  −\n\ni j =ij\n\ni j =ij i j =ij,i j\n\n(64)\n\n\n\n\n\n1 N2 \n\n∇w2ij \n\n(65)\n\nn m=n\n\n\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nv\n\n∇wij\n\n|x(n)∇wij\n\n|x(m)\n\nvi4j\n\n+\n\n∇wi j |x(n)∇wi j |x(m)vi2 j vi2j  −\n\nx(n)∈B x(m)∈B\\{x(n)}\n\ni j =ij\n\n(66)\n\n\n\n\n\n1 N2 \n\n∇w2ij \n\n(67)\n\nn m=n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\n∇wij\n\n|x(n)∇wij\n\n|x(m)\n\nE\nv\n\nvi4j\n\n+\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n∇wi\n\nj\n\n|x(n)∇wi\n\nj\n\n|x(m) E\nv\n\nvi2 j\n\ni j =ij\n\n\n\n\n\n\n\nE\nv\n\nvi2j\n\n1  − N2 \n\n∇w2ij \n\nn m=n\n\n(68) (69)\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\n3∇wij |x(n)∇wij |x(m) +\n\n∇wi j |x(n)∇wi j |x(m) −\n\nx(n)∈B x(m)∈B\\{x(n)}\n\ni j =ij\n\n(70)\n\n\n\n\n\n1 N2 \n\n∇w2ij \n\n(71)\n\nn m=n\n\n\n\n\n\n\n\n\n\n\n\n1 =N2 \n\n3 E [∇wij|x]2 +\nx\n\nE [∇wi\nx\n\nj\n\n|x]2\n\n−\n\n1 N2\n\n\n\n∇w2ij \n\n(72)\n\nn m=n\n\ni j =ij\n\nn m=n\n\n\n\n\n\n\n\n\n\n\n\n1 =N2 \n\n3∇w2ij +\n\n∇w2i\n\nj\n\n\n\n−\n\n1 N2\n\n\n\n∇w2ij \n\n(73)\n\nn m=n\n\ni j =ij\n\nn m=n\n\n\n\n\n\n\n\n\n\n1 =N2\n\n2∇w2ij +\n\n∇w2i j\n\n\n\n=\n\nN (N − 1) N2\n\n∇w2ij\n\n+\n\n∇w2i j  .\n\n(74)\n\nn m=n\n\ni j =ij\n\nij\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nLastly, we average the variance across all weight dimensions:\n\n1\n\nmVar(gw(wij)) = pq Var(gw(wij))\n\n(75)\n\nij\n\n1\n\n= pq\n\n{Z1 + Z2 + Z3}\n\n(76)\n\nij\n\n1\n\n1\n\n= pq\n\nN\n\nVar (∇wij|x) +\nx\n\n(77)\n\nij\n\n\n\n\n\n1 N\n\n∇w2ij\n\n+\n\nVxar(∇wij |x)\n\n+\n\n∇w2i j + Vxar(∇wi j |x)  +\n\n(78)\n\nij\n\n\n\n\n\nN (N − N2\n\n1)\n\n∇w2ij\n\n+\n\nij\n\n∇w2i j\n\n \n\n\n\n(79)\n\n1\n\n1\n\n= pq\n\nN\n\nVar (∇wij|x) +\nx\n\n(80)\n\nij\n\n\n\n\n\n\n\n1 N Vxar(∇wij|x) +\nij\n\nVxar(∇wi j |x) + ∇w2ij +\nij\n\n∇w2i j\n\n \n\n\n\n(81)\n\n2\n\npq\n\n= mVar (∇w) + mVar (∇w) + (pq + 1) mSqNorm(∇w)\n\n(82)\n\nN\n\nN\n\npq + 2\n\n=\n\nV + (pq + 1)S.\n\n(83)\n\nN\n\nIf the perturbations are independent, we show that Z3 is 0.\n\n\n\n\n\n1\n\nZ3\n\n=N2\n\nE\nB\n\n\n\nCov( gw(wij )| x(n), gw(wij )| x(m))\nv\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(84)\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nv\n\ngw(wij )| x(n) gw(wij )| x(m)\n\n−E\nv\n\ngw(wij )| x(n)\n\nE\nv\n\ngw(wij )| x(m) \n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(85)\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nv\n\nx(n)∈B x(m)∈B\\{x(n)}\n\ngw(wij )| x(n) gw(wij )| x(m)\n\n − ∇wij |x(n)∇wij |x(m)\n(86)\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE \nv\n\nx(n)∈B x(m)∈B\\{x(n)}\n\nj\n\n\n∇wij |x(n)vi(nj)  vi(jn) \nj\n\n ∇wij |x(m)vi(mj ) vi(jm)\n(87)\n\n−∇wij |x(n)∇wij |x(m)\n\n(88)\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nv\n\nx(n)∈B x(m)∈B\\{x(n)}\n\nj\n\n\n∇wij |x(n)∇wij |x(m)vi(nj) vi(mj) vi(jn)vi(jm) −\nj\n(89)\n\n\n\n\n\n1\n\nN2\n\nE\nB\n\n\n\n∇wij |x(n)∇wij |x(m)\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(90)\n\n20\n\nPublished as a conference paper at ICLR 2023\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nv\n\n∇wij |x(n)∇wij |x(m)vi(jn)2vi(jm)2+\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(91)\n\n∇wij |x(n)vi(jn)2vi(jm)\n\n∇wij |x(m)vi(mj ) + ∇wij |x(m)vi(jm)2vi(jn)\n\n∇wij |x(n)vi(nj) +\n\ni j =j\n\ni j =j\n\n(92)\n\n∇wij |x(m)∇wij |x(n)vi(mj )vi(nj) vi(jm)vi(jn)+\ni j =ij\n\n\n(93)\n\n∇wi j |x(n)vi j ∇wij |x(m)vi(nj) vi(mj) vi(jn)vi(jm) −\ni j =ij i j ∈/{ij,j j }\n\n(94)\n\n\n\n\n\n\n\n1\n\nN2\n\nE\nx(n)\n\nE\nx(m)\n\n\n\nn\n\n∇wij |x(n)∇wij |x(m)\nm=n\n\n(95)\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nv\n\n∇wij |x(n)∇wij |x(m)vi(jn)2vi(jm)2+\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(96)\n\n\n\n∇wi j |x(m)∇wi j |x(n)vi(mj )vi(nj) vi(jm)vi(jn) −\ni j =ij\n\n(97)\n\n\n\n\n\n\n\n1\n\nN2\n\nE\nx(n)\n\nE\nx(m)\n\n\n\nn\n\n∇wij |x(n)∇wij |x(m)\nm=n\n\n(98)\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nv\n\n∇wij |x(n)∇wij |x(m)vi(jn)2vi(jm)2\n\n−\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(99)\n\n\n\n\n\n1 N2 \n\n∇w2ij \n\nn m=n\n\n(100)\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\n∇wij\n\n|x(n)∇wij\n\n|x(m)\n\nE\nv\n\nvi(jn)2\n\nE\nv\n\nvi(jm)2  −\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(101)\n\n\n\n\n\n1 N2 \n\n∇w2ij \n\nn m=n\n\n(102)\n\n\n\n\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\n∇wij |x(n)∇wij |x(m)\n\n−\n\n1 N2\n\n\n\n∇w2ij \n\nx(n)∈B x(m)∈B\\{x(n)}\n\nn m=n\n\n(103)\n\n\n\n\n\n\n\n\n\n1 =N2 \n\nE\nx\n\n[∇wij |x]2 \n\n−\n\n1 N2\n\n\n\n∇w2ij \n\nn m=n\n\nn m=n\n\n(104)\n\n=0.\n\n(105)\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nThen the average variance becomes: 1\nmVar(gw(wij)) = pq Var(gw(wij))\nij\n\n1\n\n= pq\n\n{Z1 + Z2 + Z3}\n\nij\n\n1 =\npq\nij\n\n1\n\nN\n\nVar (∇wij|x) +\nx\n\n\n\n1 N\n\n∇w2ij\n\n+\n\nVxar(∇wij|x) +\n\nij\n\n ∇w2i j + Vxar(∇wi j |x) \n\npq + 2\n\npq + 1\n\n=\n\nmVar (∇w) +\n\nmSqNorm(∇w)\n\nN\n\nN\n\npq + 2 pq + 1\n\n=\n\nV+\n\nS.\n\nN\n\nN\n\n(106) (107) (108)\n(109) (110) (111)\n\nProposition 4. Let p × q be the size of the weight matrix, the element-wise average variance of the\n\nactivity perturbed gradient estimator\n\nwith a batch size N\n\nis\n\nq+2 N\n\nV\n\n+ (q + 1)S\n\nif\n\nthe perturbations\n\nare shared across the batch, and\n\nq+2 N\n\nV\n\n+\n\nq+1 N\n\nS\n\nif\n\nthey\n\nare\n\nindependent,\n\nwhere\n\nV\n\nis the element-wise\n\naverage variance of the true gradient, and S is the element-wise average squared gradient.\n\nProof.\n\n1\n\nZ2\n\n= N\n\nE\nx\n\nVar ( ga(wij)| x)\nu\n\n \n\n \n\n1\n\n= N\n\nE Var \nxu\n\n∇wij uj  uj \n\nj\n\n\n\n\n\n1 =\nN\n\nE\nx\n\nVar\nu\n\n∇wij\n\nu2j\n\n+\n\n∇wj uj uj \n\nj =j\n\n\n\n\n\n\n\n1\n\n= N\n\nE Var\nxu\n\n∇wij u2j\n\n+ Var \nu\n\n∇wij uj uj  +\n\nj =j\n\n\n\n\n\n2\n\nCov\nu\n\n∇wij\n\nu2j\n\n,\n\n∇wij uj uj \n\nj =j\n\n\n\n\n\n\n\n1\n\n= N\n\nE Var\nxu\n\n∇wij u2j\n\n+ Var \nu\n\n∇wij uj uj  +\n\ni j =ij\n\n\n\n\n\n\n\n\n\n2E\nu\n\n∇wij ∇wij\n\nu3j uj\n\n−\n\n2E\nu\n\n∇wij u2j\n\nE\nu\n\n∇wij uj uj \n\nj =j\n\nj =j\n\n\n\n\n\n\n\n1 =\nN\n\nE\nx\n\n∇wi2j\n\nVar\nu\n\nu2j\n\n+ Var \nu\n\n∇wij uj uj  +\n\nj =j\n\n\n\n\n\n2\n\n∇wij ∇wij\n\nE\nu\n\nu3j uj\n\n− 2∇wij E\nu\n\nu2j\n\n\n\n∇wij E [ujuj ]\nu\n\nj =j\n\nj =j\n\n(112) (113) (114) (115) (116) (117) (118) (119) (120)\n\n22\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n\n\n1 =\nN\n\nE\nx\n\n∇wi2j\n\nVar\nu\n\nu2j\n\n+ Var \nu\n\n∇wij uj uj  +\n\nj =j\n\n\n\n\n\n(121)\n\n2 ∇wij∇wij · 0 − 2∇wij · 1  ∇wi j · 0\n\nj =j\n\nj =j\n\n\n\n\n\n\n\n1 =\nN\n\nE\nx\n\n∇wi2j\n\nVar\nu\n\nu2j\n\n+ Var \nu\n\n∇wij uj uj \n\nj =j\n\n\n\n\n\n1 =\nN\n\nE\nx\n\n∇wi2j\n\n·\n\n(Eu [u4j ]\n\n−\n\nEu [u2j ]2)\n\n+\n\nVar\nu\n\n(∇wij\n\nuj uj\n\n)\n\nj =j\n\n\n\n\n\n1 =\nN\n\nE\nx\n\n∇wi2j\n\n(3\n\nVuar(uj\n\n)2\n\n−\n\nEu [u2j ]2)\n\n+\n\n∇wj2\n\nVar\nu\n\n(uj\n\nuj\n\n)\n\nj =j\n\n1\n\n= N\n\nE\nx\n\n2∇wi2j\n\n+\n\n\n\n\n\n1\n\nN\n\nE\nx\n\n∇wi2j\n\n(Var[uj ]\nu\n\n+\n\nE[uj\nu\n\n]2)(Var[uj\nu\n\n]\n\n+\n\nE[uj\nu\n\n]2)\n\n−\n\nE[uj ]2\nu\n\nE[uj\nu\n\n]2\n\nj =j\n\n\n\n\n\n2 =\nN\n\n∇w2ij\n\n+\n\n2 N\n\nVxar(∇wij |x)\n\n+\n\n1 N\n\nE\nx\n\n∇wi2j Vuar(uj ) Vuar(uj )\n\nj =j\n\n2 =\nN\n\n∇w2ij\n\n+\n\n2 N\n\nVar(∇wij |x)\nx\n\n+\n\n1 N\n\nE\nx\n\n∇wj2\n\nj =j\n\n\n\n1 =\nN\n\n∇w2ij\n\n+\n\nVar(∇wij |x)\nx\n\n+\n\nj\n\n\n\n∇w2ij\n\n+ Var(∇wij |x)\nx\n\n.\n\n(122) (123) (124) (125) (126) (127) (128) (129) (130)\n\nZ3 is nonzero if the perturbations are shared within a batch. Assuming that the perturbations are\n\nshared,\n\n\n\n\n\n1\n\nZ3\n\n=N2\n\nE\nB\n\n\n\nCov(\nu\n\nga(wij\n\n)|\n\nx(n),\n\nga(wij )|\n\nx(m))\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(131)\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE ga(wij )| x(n) ga(wij )| x(m) − E ga(wij )| x(n) E ga(wij )| x(m) \n\nu\n\nu\n\nu\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(132)\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nu\n\nx(n)∈B x(m)∈B\\{x(n)}\n\nga(wij )| x(n) ga(wij )| x(m)\n\n − ∇wij |x(n)∇wij |x(m)\n(133)\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE \nu\n\nx(n)∈B x(m)∈B\\{x(n)}\n\nj\n\n\n∇wij |x(n)uj  uj \nj\n\n ∇wij |x(m)uj  uj  −\n(134)\n\n∇wij |x(n)∇wij |x(m)\n\n(135)\n\n23\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nu\n\nx(n)∈B x(m)∈B\\{x(n)}\n\nj\n\n\n∇wij |x(n)∇wij |x(m)uj uj u2j  −\nj\n\n\n\n\n\n1\n\nN2\n\nE\nB\n\n\n\n∇wij |x(n)∇wij |x(m)\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nu\n\n∇wij |x(n)∇wij |x(m)vi4j +\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(136) (137) (138)\n\n∇wij |x(n)u3j\n\n∇wij |x(m)uj + ∇wij |x(m)u3j\n\n∇wij |x(n)uj +\n\nj =j\n\nj =j\n\n(139)\n\n∇wij |x(m)∇wij |x(n)u2j u2j +\nj =j\n\n\n(140)\n\n∇wij |x(n)uj ∇wij |x(m)uj uj u2j  −\nj =j j ∈/{j,j }\n\n\n\n\n\n\n\n1\n\nN2\n\nE\nx(n)\n\nE\nx(m)\n\n\n\nn\n\n∇wij |x(n)∇wij |x(m)\nm=n\n\n(141) (142)\n\n\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nu\n\n∇wij |x(n)∇wij |x(m)vi4j\n\n+\n\n∇wij |x(m)∇wij |x(n)u2j u2j  −\n\nx(n)∈B x(m)∈B\\{x(n)}\n\nj =j\n\n(143)\n\n\n\n\n\n\n\n1\n\nN2\n\nE\nx(n)\n\nE\nx(m)\n\n\n\nn\n\n∇wij |x(n)∇wij |x(m)\nm=n\n\n(144)\n\n\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nu\n\n∇wij |x(n)∇wij |x(m)vi4j\n\n+\n\n∇wij |x(m)∇wij |x(n)u2j u2j  −\n\nx(n)∈B x(m)∈B\\{x(n)}\n\nj =j\n\n(145)\n\n\n\n\n\n1 N2 \n\n∇w2ij \n\nn m=n\n\n(146)\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\n∇wij\n\n|x(n)\n\n∇wij\n\n|x(m)\n\nE\nv\n\nvi4j\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n\n\n\n\n1 N2 \n\n∇w2ij \n\nn m=n\n\n+\n\n∇wij\n\n|x(n)∇wij\n\n|x(m) E\nu\n\nu2j\n\nj =j\n\n(147)\n\n(148)\n\n\n\nE\nu\n\nu2j\n\n−\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\n3∇wij |x(n)∇wij |x(m) +\n\n∇wij |x(n)∇wij |x(m) −\n\nx(n)∈B x(m)∈B\\{x(n)}\n\nj =j\n\n\n\n\n\n1 N2 \n\n∇w2ij \n\nn m=n\n\n(149) (150)\n\n24\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n\n\n\n\n\n\n1 =N2 \n\n3\n\nE\nx\n\n[∇wij\n\n|x]2\n\n+\n\nE\nx\n\n[∇wij\n\n|x]2\n\n−\n\n1 N2\n\n\n\n∇w2ij \n\nn m=n\n\nj =j\n\nn m=n\n\n\n\n\n\n\n\n\n\n\n\n1 =N2 \n\n2∇w2ij +\n\n∇w2ij\n\n\n\n−\n\n1 N2\n\n\n\n∇w2ij \n\nn m=n\n\nj =j\n\nn m=n\n\n\n\n\n\n\n\n1 =N2 \n\n∇w2ij +\n\n∇w2ij \n\nn m=n\n\nj =j\n\n\n\n\n\n=\n\nN (N − N2\n\n1)\n\n∇w2ij\n\n+\n\n∇w2ij  .\n\nj =j\n\n(151) (152) (153) (154)\n\nThen we compute the average variance across all weight dimensions (for shared perturbation):\n\n1 mVar(ga(wij)) = pq Var(ga(wij))\nij\n\n(155)\n\n1\n\n= pq\n\n{Z1 + Z2 + Z3}\n\nij\n\n(156)\n\n1 =\npq\nij\n\n1\n\nN\n\nVar\nx\n\n(∇wij\n\n|x\n\n)\n\n+\n\n(157)\n\n\n\n1 N\n\n∇w2ij\n\n+\n\nVxar(∇wij |x)\n\n+\n\nj\n\n ∇w2ij + Vxar(∇wij |x)  +\n\n(158)\n\n\n\n\n\nN (N − N2\n\n1)\n\n∇w2ij\n\n+\n\n∇w2ij \n\nj =j\n\n(159)\n\n1 =\npq\nij\n\n1\n\nN\n\nVar (∇wij|x) +\nx\n\n\n\n\n\n\n\n1 N Vxar(∇wij|x) +\nj\n\nVxar(∇wij |x) + ∇w2ij +\nj\n\n∇w2ij\n\n \n\n\n\n2\n\nq\n\n= mVar (∇w) + mVar (∇w) + (q + 1) mSqNorm(∇w)\n\nN\n\nN\n\nq+2 = V + (q + 1)S.\nN\n\n(160)\n(161) (162) (163)\n\nIf the perturbations are independent, we show that Z3 is 0.\n\n\n\n\n\n1\n\nZ3\n\n=N2\n\nE\nB\n\n\n\nCov( ga(wij )| x(n), ga(wij )| x(m))\nu\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(164)\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE ga(wij )| x(n) ga(wij )| x(m) − E ga(wij )| x(n) E ga(wij )| x(m) \n\nu\n\nu\n\nu\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(165)\n\n25\n\nPublished as a conference paper at ICLR 2023\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nu\n\nx(n)∈B x(m)∈B\\{x(n)}\n\nga(wij )| x(n) ga(wij )| x(m)\n\n − ∇wij |x(n)∇wij |x(m)\n\n(166)\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE \nu\n\nx(n)∈B x(m)∈B\\{x(n)}\n\nj\n\n\n∇wij |x(n)u(jn) u(jn) \nj\n\n ∇wij |x(m)u(jm) u(jm) −\n(167)\n\n∇wij |x(n)∇wij |x(m)\n\n(168)\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nu\n\nx(n)∈B x(m)∈B\\{x(n)}\n\nj\n\n\n∇wij |x(n)∇wij |x(m)u(jn)u(jm)u(jn)u(jm) − (169)\nj\n\n\n\n\n\n1\n\nN2\n\nE\nB\n\n\n\n∇wij |x(n)∇wij |x(m)\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(170)\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nu\n\n∇wij |x(n)∇wij |x(m)u(jn)2u(jm)2+\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(171)\n\n∇wij |x(n)u(jn)2u(jm)\n\n∇wij |x(m)u(jm) + ∇wij |x(m)u(jm)2u(jn)\n\n∇wij |x(n)u(jn)+\n\nj =j\n\nj =j\n\n(172)\n\n∇wij |x(m)∇wij |x(n)u(jm)u(jn)u(jm)u(jn)+\nj =j\n\n(173)\n\n\n\n∇wij |x(n)uj ∇wij |x(m)u(jn)u(jm)u(jn)u(jm) −\nj =j j ∈/{j,j }\n\n(174)\n\n\n\n\n\n\n\n1\n\nN2\n\nE\nx(n)\n\nE\nx(m)\n\n\n\nn\n\n∇wij |x(n)∇wij |x(m)\nm=n\n\n(175)\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nu\n\n∇wij |x(n)∇wij |x(m)u(jn)2u(jm)2+\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(176)\n\n\n\n∇wij |x(m)∇wij |x(n)u(jm)u(jn)u(jm)u(jn) −\nj =j\n\n(177)\n\n\n\n\n\n\n\n1\n\nN2\n\nE\nx(n)\n\nE\nx(m)\n\n\n\nn\n\n∇wij |x(n)∇wij |x(m)\nm=n\n\n(178)\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\nE\nu\n\n∇wij |x(n)∇wij |x(m)u(jn)2u(jm)2\n\n−\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(179)\n\n\n\n\n\n1 N2 \n\n∇w2ij \n\nn m=n\n\n(180)\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n\n\n∇wij |x(n)∇wij |x(m) E\nu\n\nu(jn)2\n\nE\nu\n\nu(jm)2  −\n\nx(n)∈B x(m)∈B\\{x(n)}\n\n(181)\n\n\n\n\n\n1 N2 \n\n∇w2ij \n\nn m=n\n\n(182)\n\n26\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n\n\n\n\n1\n\n=N2\n\nE\nB\n\n∇wij |x(n)∇wij |x(m)\n\n−\n\n1 N2\n\n\n\n∇w2ij \n\nx(n)∈B x(m)∈B\\{x(n)}\n\nn m=n\n\n\n\n\n\n\n\n\n\n1 =N2 \n\nE\nx\n\n[∇wij |x]2 \n\n−\n\n1 N2\n\n\n\n∇w2ij \n\nn m=n\n\nn m=n\n\n=0.\n\nThen the average variance becomes: 1\nmVar(ga(wij)) = pq Var(ga(wij))\nij\n\n1\n\n= pq\n\n{Z1 + Z2 + Z3}\n\nij\n\n1 =\npq\nij\n\n1\n\nN\n\nVar (∇wij|x) +\nx\n\n\n\n1 N\n\n∇w2ij\n\n+\n\nVxar(∇wij |x)\n\n+\n\nj\n\n\n\n∇w2ij\n\n+ Vxar(∇wij |x)\n\n \n\n\n\nq+2\n\nq+1\n\n=\n\nmVar (∇w) +\n\nmSqNorm(∇w)\n\nN\n\nN\n\nq+2 q+1\n\n= V+\n\nS.\n\nN\n\nN\n\n(183)\n(184) (185) (186) (187) (188) (189) (190) (191)\n\nwt perturb ind wt perturb act perturb backprop wt perturb (theory) ind wt perturb (theory) act perturb (theory) backprop (theory)\n\nGradient Variance\n\n102 101 100 10 1 10 2\n100 101 102 103 N (Batch Size) (p=4, q=4)\n\n102\n\n101\n\n100\n\n100\n\n101\n\n102\n\np (Fan In) (N=4, q=4)\n\n105 104 103 102 101 100\n\n100\n\n101\n\n102\n\nq (Fan Out) (N=4, p=4)\n\nFigure 8: Numerical veriﬁcation of the theoretical variance properties\n\n10 NUMERICAL SIMULATION OF VARIANCES\n\nIn Figure 8, we ran numerical simulation experiments to verify our analytical variance properties.\n\nWe used a multi-layer network with 4 input units, 4 hidden units, 1 output unit, a tanh activation\n\nfunction, and the mean squared error loss. We varied the batch size (N ) between 1 and 4096. We\n\ntested the gradient estimator of the ﬁrst layer weights using 5000 random samples. We also calculated\n\nthe theoretical variance by applying the gradient norm and gradient variance constants found by\n\nbackprop, from 5000 mini-batch true gradients. We then ﬁxed the batch size to be 4 and vary the\n\nnumber of input units (p, fan in) and the number of hidden units (q, fan out) between 1 and 256. The\n\ntheoretical variance for backprop was only computed for the batch size experiment since it is an\n\ninverse\n\nrelationship\n\n(\n\n1 N\n\n),\n\nbut\n\nfor\n\nfan\n\nin\n\nand\n\nfan\n\nout,\n\nwe\n\ndo\n\nnot\n\naim\n\nto\n\nanalyze\n\nthe\n\ntheoretical\n\nvariances\n\nhere. “wt perturb” stands for weight perturbation with shared noise; “ind wt perturb” stands for\n\nweight perturbation with independent noise; and “act perturb” stands for activity perturbation with\n\nindependent noise. Note that indepedent weight perturbation is much more costly to compute in\n\n27\n\nPublished as a conference paper at ICLR 2023\nneural networks. As shown in the ﬁgure, the empirical variances match very well with our theoretical predictions.\n11 TRAINING DETAILS\nHere we provide more training details.\nMNIST. We use a batch size of 128, and the SGD optimizer with learning rate 0.01 and momentum 0.9 for a total of 1000 epochs with no data augmentation and a linear learning rate decay schedule.\nCIFAR-10. For the supervised experiments, we use a batch size of 128 and the SGD optimizer with learning rate 0.01 and momentum 0.9 for a total of 200 epochs with no data augmentation and a linear learning rate decay schedule. For the contrastive M/8 experiments, we use a batch size of 512 and the SGD optimizer with learning rate 1.0 and momentum 0.9 for a total of 1000 epochs with BYOL data augmentation using area crop lower bound to be 0.5 and a cosine decay schedule with a warm-up period of 10 epochs. For the contrastive L/8 experiments, we use a batch size of 2048 and the SGD optimizer with learning rate 4.0 and momentum 0.9 for a total of 1000 epochs with BYOL data augmentation (Grill et al., 2020) using area crop lower bound to be 0.3 and a cosine decay schedule with a warm-up period of 10 epochs.\nImageNet. For the supervised experiments, we use a batch size of 256 and the SGD optimizer with learning rate 0.05 and momentum 0.9 for a total of 120 epochs with BYOL data augmentation (Grill et al., 2020) using area crop lower bound to be 0.3 and a cosine learning rate decay schedule with a warm-up period of 10 epochs. For the contrastive experiments, we use a batch size of 2048 and the LARS optimizer with learning rate 0.1 and momentum 0.9 for a total of 800 epochs with BYOL data augmentation (Grill et al., 2020) using area crop lower bound to be 0.08 and a cosine learning rate decay schedule with a warm-up period of 10 epochs.\n12 FUSED JVP/VJP DETAILS\nIn Algorithm 1, we provide a JAX code snippet implementing fused operators for the supervised cross entropy loss. “Fused” here means that we package several operations into one function. In the supervised cross entropy loss, we combine average pooling, channel concatenation, a linear classiﬁer layer, and cross entropy all together. Key steps and expected tensor shapes are annotated in the comments. The fused InfoNCE loss implementation will be included in our full code release.\n13 LOCALMIXER ARCHITECTURE\nIn Algorithm 2, we provide code in JAX style that implements our proposed LocalMixer architecture.\n14 ADDITIONAL RESULTS\nIn this section we provide additional experimental results.\nNormalization scheme. Table 5 compares different normalization schemes. Layer normalization (LN) is often better than batch normalization (BN) on our mixer architecture. Local LN is better on contrastive learning experiments and achieves lower error rates using forward gradient learning. Although in our main paper, backprop were used in normalization layers, backprop is not necessary for Local LN, c.f . “NG” (No Gradient) columns in Table 5.\nPlace of normalization. We investigate the places where we add normalization layers. Traditionally, normalization is added after linear layers. In MLPMixer, LN is added at the beginning of each block. With our forward gradient learning, it is now a question of which location is the optimal design. Adding it after the linear layer has the advantage of shaping the activations to be more well behaved, which can make perturbation learning more effective. Adding it before the linear layer can also help reduce the variance since the inputs always get multiplied with the gradient of the output activity. The results are reported in Table 6. Adding normalization both before and after the linear layer helps forward gradient to achieve lower training errors. While this could result in some overﬁtting on supervised learning, it is good for contrastive learning which needs more model capacity. This is reasonable as forward gradient introduce a lot of variances, and more normalization layers help achieve better training performance.\n28\n\nPublished as a conference paper at ICLR 2023\nAlgorithm 1 Naïve and fused local cross entropy, with custom JVP and VJP operators.\n# N: batch size; P: num patches; G: num grps; C: num channels; D: channels / grp; K: num cls # x: encoder features [N,P,G,C/G] # w: classifier weights [C,K]; b: classifier bias [K] # labels: class labels [N,K] import jax import jax.numpy as jnp from jax.scipy.special import logsumexp\ndef naive_avg_group_linear_xent(x, w, b, labels): N, P, G, _ = x.shape # Average pooling, with stop gradients. [N,P,G,C/G] -> [N,1,G,C/G] avg_pool_p = jnp.mean(x, axis=1, keepdims=True) x_div_p = x / float(P) # [N,P,G,C/G] x = x_div_p + jax.lax.stop_gradient(avg_pool_p - x_div_p) # Concatenate everything, with stop gradients. [N,P,G,C] -> [N,P,G,G,C/G] x = jnp.tile(jnp.reshape(x, [N, P, 1, G, -1]), [1, 1, G, 1, 1]) mask = jnp.eye(G)[None, None, :, :, None] x = mask * x + jax.lax.stop_gradient((1.0 - mask) * x) # [N,P,G,G,C/G] -> [N,P,G,C] x = jnp.reshape(x, [N, P, G, -1]) logits = jnp.einsum(’npgc,cd->npgd’, x, w) + b logits = logits - logsumexp(logits, axis=-1, keepdims=True) loss = -jnp.sum(logits * labels[:, None, None, :], axis=-1) return loss\ndef fused_avg_group_linear_xent(x, w, b, labels): # This is for forward pass. The numerical value of each local loss should be the same. # So we compute one and replicate it many times. N, P, G, _ = x.shape # [N,P,G,C/G] -> [N,G,C/G] x_avg = jnp.mean(x, axis=1) # [N,G,C/G] -> [N,C] x_grp = jnp.reshape(x_avg, [x_avg.shape[0], -1]) # [N,C] -> [N,K] logits = jnp.einsum(’nc,ck->nk’, x_grp, w) + b logits = logits - logsumexp(logits, axis=-1, keepdims=True) loss = -jnp.sum(logits * labels, axis=-1) # Key step: after computing the loss, replicate it for PxG times. [N] -> [N,P,G] return jnp.tile(jnp.reshape(loss, [N, 1, 1]), [1, P, G])\ndef fused_avg_group_linear_xent_jvp(primals, tangents): # This JVP operator performs both regular forward pass and the forward autodiff. x, w, b, labels = primals dx, dw, db, dlabels = tangents N, P, G, D = x.shape dx_avg = dx / float(P) # Reshape the classifier weights, since only one group passes gradient at a time. w_ = jnp.reshape(w, [G, D, -1]) b = jnp.reshape(b, [-1]) # Regular forward pass # [N,P,G,C/G] -> [N,G,C/G] x_avg = jnp.mean(x, axis=1) # [N,G,C/G] -> [N,C] x_grp = jnp.reshape(x_avg, [x_avg.shape[0], -1]) # [N,C] -> [N,K] logits = jnp.einsum(’nd,dk->nk’, x_grp, w) + b logits = logits - logsumexp(logits, axis=-1, keepdims=True) loss = -jnp.sum(logits * labels, axis=-1) # We can compute the gradient through cross entropy first. dlogits_bwd = jax.nn.softmax(logits, axis=-1) - labels # [N,K] # Key step: dloss = dx * w * dloss/dlogit + (x * dw + db) * dloss/dlogit # Do the einsum together to avoid replicating outputs. dloss = jnp.einsum(’npgd,gdk,nk->npg’, dx_avg, w_, dlogits_bwd) + jnp.einsum(’nk,nk->n’, (jnp.einsum(’nc,ck->nk’, x_grp, dw) + db), dlogits_bwd)[:, None, None] # [N,P,G] # Return loss and loss gradients [N,P,G]. return jnp.tile(jnp.reshape(loss, [N, 1, 1]), [1, P, G]), dloss\ndef fused_avg_group_linear_xent_vjp(res, g): # This is a fused backprop (VJP) operator. x, w, logits, labels = res N, P, G, D = x.shape x_avg = jnp.mean(x, axis=1) x_grp = jnp.reshape(x_avg, [x_avg.shape[0], -1]) # Key step: only the first patch/group gradients since everything is the same. g_ = g[:, 0:1, 0] dlogits = g_ * (jax.nn.softmax(logits, axis=-1) - labels) # [N,K] # Remember to multiply gradients by PG times due to weight sharing. db = jnp.reshape(jnp.sum(dlogits, axis=[0]), [-1]) * float(P * G) dw = jnp.reshape(jnp.einsum(’nc,nk->ck’, x_grp, dlogits), [G * D, -1]) * float(P * G) # Key step: use grouped weights to perform backprop. dx = jnp.einsum(’nd,gcd->ngc’, dlogits, jnp.reshape(w, [G, C, -1])) / float(P) # Broadcast gradients across patches. dx = jnp.tile(dx[:, None, :, :], [1, P, 1, 1]) return dx, dw, db, None\n29\n\nPublished as a conference paper at ICLR 2023\nAlgorithm 2 A LocalMixer architecture implemented with JAX style code.\nimport jax import jax.numpy as jnp\ndef linear(x, w, b): \"\"\"Linear layer.\"\"\" return jnp.einsum(’npc,cd->npd’, x, w) + b\ndef group_linear(x, w, b): \"\"\"Linear layer with groups.\"\"\" return jnp.einsum(’npgc,gcd->npgd’, x, w) + b\ndef normalize(x, axis=-1, eps=1e-5): \"\"\"Normalization layer.\"\"\" mean = jnp.mean(x, axis=axis, keepdims=True) mean_of_squares = jnp.mean(jnp.square(x), axis=axis, keepdims=True) var = mean_of_squares - jnp.square(mean) inv = jax.lax.rsqrt(var + eps) y = (x - mean) * inv return y\ndef block0(x, params): \"\"\"Initial block with only channel mixing.\"\"\" N, P, _ = x.shape G = num_groups x = normalize(x) x = linear(x, params[0][0], params[0][1]) x = normalize(x) x = jax.nn.relu(x) x = jnp.reshape(x, [N, P, G, -1]) x = normalize(x) x = group_linear(x, params[1][0], params[1][1]) x = normalize(x) x = jax.nn.relu(x) return x\ndef mlp_block(x, params): \"\"\"Regular MLP block with token & channel mixing.\"\"\" N, P, G, _ = x.shape inputs = x # Token mixing. x = jnp.reshape(x, [N, P, -1]) x = normalize(x) x = jnp.swapaxes(x, 1, 2) x = linear(x, params[0][0], params[0][1]) x = jnp.swapaxes(x, 1, 2) x = normalize(x) x = jax.nn.relu(x)\n# Channel mixing. x = normalize(x) x = linear(x, params[1][0], params[1][1]) x = normalize_layer(x) x = jax.nn.relu(x) x = jnp.reshape(x, [N, P, G, -1]) x = normalize(x) x = group_linear(x, params[2][0], params[2][1]) x = normalize(x) x = x + inputs x = jax.nn.relu(x) return x\ndef local_mixer(x, params): \"\"\"LocalMixer.\"\"\" x = preprocess(x, image_mean, image_std, num_patches) pred_local = [] # Local predictions. # Build network blocks. for blk in range(num_blocks): if blk == 0: x = block0(x, params[f’block_{blk}’]) else: x = mlp_block(x, params[f’block_{blk}’])\n# Projector connects to local losses. x_proj = normalize(x) pred_local.append(linear(x_proj, params[f’proj_{blk}’][0], params[f’proj_{blk}’][1]))\n# Disconnect gradients. x = jax.lax.stop_gradient(x) x = jnp.reshape(x, [x.shape[0], x.shape[1], -1]) x = jnp.mean(x, axis=1) # [N,C] x = normalize(x) pred = linear(x, params[’classifier’][0], params[’classifier’][1]) return pred, pred_local\n30\n\nPublished as a conference paper at ICLR 2023\n\nBN LN Local LN\n\nBP\n30.38 / 0.00 30.55 / 0.00 32.89 / 0.00\n\nSupervised M/8/16\n\nLG-BP\n\nLG-FG-A\n\n33.41 / 5.41 32.84 / 23.09 33.17 / 7.09 29.03 / 17.63 33.84 / 0.05 30.68 / 19.39\n\nLG-FG-A (NG)\n33.48 / 20.80 29.33 / 19.26 30.44 / 17.12\n\nBP\n27.56 / 24.39 23.52 / 20.71 23.24 / 21.03\n\nContrastive M/8/16\n\nLG-BP\n\nLG-FG-A\n\n30.27 / 28.03 35.47 / 32.71 27.41 / 24.73 34.21 / 31.38 28.42 / 25.20 32.89 / 31.01\n\nLG-FG-A (NG)\n37.41 / 31.52 36.24 / 34.12 32.25 / 30.17\n\nTable 5: Comparing different normalization schemes. NG=No normalization gradient. CIFAR-10 test / train error (%)\n\nLN\nBegin Block Before Linear After Linear Before + After Linear\n\nSupervised M/8/16\n\nBP\n\nLG-BP\n\nLG-FG-A\n\n31.43 / 0.00 32.50 / 0.00 30.38 / 0.00 33.62 / 0.00\n\n34.73 / 1.45 34.15 / 0.05 29.83 / 0.44 33.84 / 0.05\n\n34.89 / 31.11 33.88 / 27.94 29.35 / 23.19 30.68 / 19.39\n\nContrastive M/8/16\n\nBP\n\nLG-BP\n\nLG-FG-A\n\n23.27 / 20.69 22.62 / 20.38 26.50 / 23.98 23.24 / 21.03\n\n25.82 / 22.96 NaN / NaN 28.97 / 26.67 28.42 / 25.20\n\n89.93 / 90.71 35.01 / 32.91 34.10 / 33.18 32.89 / 31.01\n\nTable 6: Place of LayerNorm on CIFAR-10, test / train error. (%)\n\nError Rate (%) Error Rate (%)\n\n40\n\nBP Train\n\n45\n\nBP Train\n\nLG-BP Train\n\nLG-BP Train\n\n30\n\nLG-FG-A Train\n\nLG-FG-A Train\n\nBP Test\n\n35\n\nBP Test\n\n20\n\nLG-BP Test\n\nLG-BP Test\n\nLG-FG-A Test 25\n\nLG-FG-A Test\n\n10\n\n0 1\n\n2\n\n4\n\n8 16 32 64\n\nNumber of Groups\n\n(a) CIFAR-10 Supervised M/8/*\n\n15 1\n\n2\n\n4\n\n8 16 32 64\n\nNumber of Groups\n\n(b) CIFAR-10 Contrastive M/8/*\n\nFigure 9: Effect of groups. For BP algorithms, groups has a minor effect on the ﬁnal performance, but for local forward gradient, it signiﬁcantly reduces the variance and achieves lower error rate on both training and test sets.\n\nEffect of groups. We provide additional results summarizing the training and test performance of adding more groups in Figure 9. Backprop and local greedy backprop always achieve zero training error with increasing number of groups on CIFAR-10 supervised, but adding groups has a signiﬁcant beneﬁt lowering training errors for forward gradient. This suggests that the main opponent here is still the gradient estimation variance, and lowering training errors can generally make test errors lower too; on the other hand adding groups have negligible effect on backprop. For contrastive learning, here the task requires higher model capacity, and adding groups effectively reduce the model capacity by introducing sparsity in the weight matrix. As a result, we observe a slight drop of less than 5% performance on both backprop and local greedy backprop. By contrast, forward gradient gains over 10% of performance by adding 16 groups.\n\n31\n\n"}