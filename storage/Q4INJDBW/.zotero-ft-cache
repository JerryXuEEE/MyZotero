arXiv:2210.03310v3 [cs.LG] 2 Mar 2023

Published as a conference paper at ICLR 2023
SCALING FORWARD GRADIENT WITH LOCAL LOSSES
Mengye Ren1∗, Simon Kornblith2, Renjie Liao3, Geoffrey Hinton2,4 1NYU, 2Google, 3UBC, 4Vector Institute
ABSTRACT
Forward gradient learning computes a noisy directional gradient and is a biologically plausible alternative to backprop for learning deep neural networks. However, the standard forward gradient algorithm, when applied naively, suffers from high variance when the number of parameters to be learned is large. In this paper, we propose a series of architectural and algorithmic modiﬁcations that together make forward gradient learning practical for standard deep learning benchmark tasks. We show that it is possible to substantially reduce the variance of the forward gradient estimator by applying perturbations to activations rather than weights. We further improve the scalability of forward gradient by introducing a large number of local greedy loss functions, each of which involves only a small number of learnable parameters, and a new MLPMixer-inspired architecture, LocalMixer, that is more suitable for local learning. Our approach matches backprop on MNIST and CIFAR-10 and signiﬁcantly outperforms previously proposed backprop-free algorithms on ImageNet. Code is released at https://github.com/google-research/ google-research/tree/master/local_forward_gradient.
1 INTRODUCTION
Most deep neural networks today are trained using the backpropagation algorithm (a.k.a. backprop) (Werbos, 1974; LeCun, 1985; Rumelhart et al., 1986), which efﬁciently computes the gradients of the weight parameters by propagating the error signal backwards from the loss function to each layer. Although artiﬁcial neural networks were originally inspired by biological neurons, backprop has always been considered as “biologically implausible” as the brain does not form symmetric backward connections or perform synchronized computations. From an engineering perspective, backprop is incompatible with a massive level of model parallelism, and restricts potential hardware designs. These concerns call for a drastically different learning algorithm for deep networks.
In the past, there have been attempts to address the above weight transport problem by introducing random backward weights (Lillicrap et al., 2016; Nøkland, 2016), but they have been found to scale poorly on larger datasets such as ImageNet (Bartunov et al., 2018). Addressing the issue of global synchronization, several papers showed that greedy local loss functions can be almost as good as end-to-end learning (Belilovsky et al., 2019; Löwe et al., 2019; Xiong et al., 2020). However, they still rely on backprop for learning a number of internal layers within each local module.
Approaches based on weight perturbation, on the other hand, directly send the loss signal back to the weight connections and hence do not require any backward weights. In the forward pass, the network adds a slight perturbation to the synaptic connections and the weight update is then multiplied by the negative change in the loss. Weight perturbation was previously proposed as a biologically plausible alternative to backprop (Xie & Seung, 1999; Seung, 2003; Fiete & Seung, 2006). Instead of directly perturbing the weights, it is also possible to use forward-mode automatic differentiation (AD) to compute a directional gradient of the ﬁnal loss along the perturbation direction (Pearlmutter, 1994). Algorithms based on forward-mode AD have recently received renewed interest in the context of deep learning (Baydin et al., 2022; Silver et al., 2022). However, existing approaches suffer from the curse of dimensionality, and the variance of the estimated gradients is too high to effectively train large networks.
In this paper, we revisit activity perturbation (Le Cun et al., 1988; Widrow & Lehr, 1990; Fiete & Seung, 2006) as an alternative to weight perturbation. As previous works focused on speciﬁc settings, we explore the general applicability to large networks trained on challenging vision tasks.
∗Work done as a visiting faculty researcher at Google. Correspondence to: mengye@cs.nyu.edu.
1

Published as a conference paper at ICLR 2023
We prove that activity perturbation yields lower-variance gradient estimates than weight perturbation, and provide a continuous-time rate-based interpretation of our algorithm. We directly address the scalability issue of forward gradient learning by designing an architecture with many local greedy loss functions, isolating the network into local modules and hence reducing the number of learnable parameters per loss. Unlike prior work that only adds local losses along the depth dimension, we found that having patch-wise and channel group-wise losses is also critical. Lastly, inspired by the design of MLPMixer (Tolstikhin et al., 2021), we designed a network called LocalMixer, featuring a linear token mixing layer and grouped channels for better compatibility with local learning.
We evaluate our local greedy forward gradient algorithm on supervised and self-supervised image classiﬁcation problems. On MNIST and CIFAR-10, our learning algorithm performs comparably with backprop, and on ImageNet, it performs signiﬁcantly better than other biologically plausible alternatives using asymmetric forward and backward weights. Although we have not fully matched backprop on larger-scale problems, we believe that local loss design could be a critical ingredient for biologically plausible learning algorithms and the next generation of model-parallel computation.
2 RELATED WORK
Ever since the perceptron era, the design of learning algorithms for neural networks, especially algorithms that could be realized by biological brains, has been a central interest. Review papers by Whittington & Bogacz (2019) and Lillicrap et al. (2020) have systematically summarized the progress of biologically plausible deep learning. Here, we discuss related work in the following subtopics.
Forward gradient and reinforcement learning. Our work leverages forward-mode automatic differentiation (AD), which was ﬁrst proposed by Wengert (1964). Later it was used to learn recurrent neural networks (Williams & Zipser, 1989) and to compute Hessian vector products (Pearlmutter, 1994). Computing the true gradient using forward-mode AD requires the full Jacobian, which is often large and expensive to compute. Recently, Baydin et al. (2022) and Silver et al. (2022) proposed to update the weights based on the directional gradient along a random or learned perturbation direction. They found that this approach is sufﬁcient for small-scale problems. This general family of algorithms is also related to reinforcement learning (RL) and evolution strategies (ES), since in each case the network receives a global reward. RL and ES have a long history of application in neural networks (Whitley, 1993; Stanley & Miikkulainen, 2002; Salimans et al., 2017), and they are effective for certain continuous control and decision-making tasks. Clark et al. (2021) found global credit assignment can also work well in vector neural networks where weights are only present between vectorized groups of neurons.
Greedy local learning. There have been numerous attempts to use local greedy learning objectives for training deep neural networks. Greedy layerwise pretraining (Bengio et al., 2006; Hinton et al., 2006; Vincent et al., 2010) trains individual layers or modules one at a time to greedily optimize an objective. Local losses are typically applied to different layers or residual stages, using common supervised and self-supervised loss formulations (Belilovsky et al., 2019; Nøkland & Eidnes, 2019; Löwe et al., 2019; Belilovsky et al., 2020). Xiong et al. (2020); Gomez et al. (2020) proposed to use overlapped losses to reduce the impact of greedy learning. Patel et al. (2022) proposed to split a network into neuron groups. Laskin et al. (2020) applied greedy local learning on model parallelism training, and Wang et al. (2021) proposed to add a local reconstruction loss for preserving information. However, most local learning approaches proposed in the last decade rely on backprop to compute the weight updates within a local module. One exception is the work of Nøkland & Eidnes (2019), which avoided backprop by using layerwise objectives coupled with a similarity loss or a feedback alignment mechanism. Gated linear networks and their variants (Veness et al., 2017; 2021; Sezener et al., 2021) ask every neuron to make a prediction, and have shown interesting results on avoiding catastrophic forgetting. From a theoretical perspective, Baldi & Sadowski (2016) provided insights and proofs on why local learning can be worse than global learning.
Asymmetric feedback weights. Backprop relies on weight symmetry: the backward weights are the same as the forward weights. Past research has looked at whether this constraint is necessary. Lillicrap et al. (2016) proposed feedback alignment (FA) that uses random and ﬁxed backward weights and found it can support error driven learning in neural networks. Direct FA (Nøkland, 2016) uses a single backward layer to wire the loss function back to each layer. There have also been methods that aim to explicitly update backward weights. Recirculation (Hinton & McClelland, 1987) and target propagation (TP) (Bengio, 2014; Lee et al., 2015; Bartunov et al., 2018) use local reconstruction
2

Published as a conference paper at ICLR 2023

objective to learn separate forward and backward weights as approximate inverses of each other. Ladder networks (Rasmus et al., 2015) found local reconstruction objectives and asymmetric weights can help achieve strong semi-supervised learning performance. However, Bartunov et al. (2018) reported both FA and TP algorithms do not scale to larger problems such as ImageNet, where their error rates are over 90%. Liao et al. (2016); Xiao et al. (2019) proposed sign symmetry (SS) where each backward connection weight share the same sign as the forward counterpart. Akrout et al. (2019) proposed weight mirroring and the modiﬁed Kolen-Pollack algorithm (Kolen & Pollack, 1994) to align forward and backward weights. Woo et al. (2021) proposed to update using activities from several layers below to avoid bidirectional connections. Compared to these works, we circumvent the issue of weight symmetry, and more generally network symmetry, by using only reward (and the change rate thereof), instead of backward weights.
Biologically plausible perturbation learning. Forward gradient is related to perturbation learning in the biology context. Traditionally, neural plasticity learning rules focus on deriving weight updates as a function of the input and output activity of a neuron (Hebb, 1949; Widrow & Hoff, 1960; Oja, 1982; Bienenstock et al., 1982; Abbott & Nelson, 2000). Weight perturbation learning (Jabri & Flower, 1992), on the other hand, is much more general as it permits any form of global reward (Schultz et al., 1997). It was developed in both rated-based and spiking-based formuations (Xie & Seung, 1999; Seung, 2003). Activity (or node) perturbation was proposed in shallow networks (Le Cun et al., 1988; Widrow & Lehr, 1990) and later in a spike-based continuous time network (Fiete & Seung, 2006), where it was interpreted as the perturbation of the conductance of neurons. Werfel et al. (2003) showed that backprop has a faster convergence rate than perturbation learning, and activity perturbation wins over weight perturbation by another factor. In our work, we show activity perturbation has lower gradient estimation variance compared to weight perturbation.

3 FORWARD GRADIENT LEARNING

In this section, we review and establish the technical background for our learning algorithm. We ﬁrst review the technique of forward-mode automatic differentiation (AD). Second, we formulate two different types of perturbation in the weight space or activity space.

3.1 FORWARD-MODE AUTOMATIC DIFFERENTIATION (AD)

Let f : Rm → Rn. The Jacobian of f , Jf , is a matrix of size n × m. Forward-mode AD computes the matrix-vector product Jf v, where v ∈ Rm. It is deﬁned as the directional gradient along v evaluated at x:

f (x + δv) − f (x)

Jf v := lim
δ→0

δ

.

(1)

For comparison, backprop, also known as reverse-mode AD, computes the vector-Jacobian product vJf , where v ∈ Rn, which corresponds to the last term in the chain rule. In contrast to reverse-mode
AD, forward-mode AD only requires one forward pass, which is augmented with the derivative

information. To compute the Jacobian vector product of a node in a computation graph, ﬁrst the

input node will be augmented with v, which is the vector to be multiplied. Then for other nodes, we

send in a tuple of (x, x ) as inputs and compute a tuple (y, y ) as outputs, where x and y are the

intermediate derivatives at node x and node y, i.e. y

=

dy dx

x

,

and

dy dx

is the Jacobian between y and

x. In the JAX library (Bradbury et al., 2018), forward-mode AD is implemented as jax.jvp.

3.2 WEIGHT-PERTURBED FORWARD GRADIENT

Weight perturbation to generate weight updates was originally explored in (Barto et al., 1983; Xie & Seung, 1999; Seung, 2003). Baydin et al. (2022) uses the technique of forward-mode AD to implement weight perturbation, which is better than ﬁnite differences in terms of numerical stability. Let wij be the weight connection between unit i and j, and f be the loss function. We can estimate the gradient by sampling a random matrix with iid elements vij drawn from a zero-mean unit-variance Gaussian distribution. The estimator is

gw(wij ) = i j ∇wi j vi j vij .

(2)

Intuitively, this estimator samples a random perturbation direction vij and tests how it aligns with the true gradient ∇wi j by using forward-mode to perform the dot product, and then multiplies the scalar alignment with the perturbation direction again. Baydin et al. (2022) referred this form of
gradient estimation using forward-mode AD as “forward gradient”. To distinguish with another form

3

Published as a conference paper at ICLR 2023

gw (·) ga(·)

Unbiased?
Yes Yes

Avg. Variance (shared)

pq+2 N

V

+ (pq + 1)S

q+2 N

V

+ (q + 1)S

Avg. Variance (independent)

pq+2 N

V

+

pq+1 N

S

q+2 N

V

+

q+1 N

S

Table 1: Comparing weight (gw) and activity (ga) perturbation. V =dimension-wise avg. gradient variance, S=dimension-wise avg. squared gradient norm; p=fan-in; q=fan-out; N =batch size.

of perturbation we detail later, we refer this to as “weight-perturbed forward gradient”, or simply as “weight perturbation”.

3.3 ACTIVITY-PERTURBED FORWARD GRADIENT

An alternative to perturbing the weights is to instead perturb the activities, which can reduce the number of perturbation dimensions per example. Activity perturbation was originally explored in Le Cun et al. (1988); Widrow & Lehr (1990) under restrictive assumptions. Here, we introduce a general way to estimate gradients using activity perturbation. It is potentially biologically plausible, since it could correspond to perturbation of the conductance in each neuron (Fiete & Seung, 2006). Here, we focus on a discrete-time rate-based formulation for simplicity. Let xi denote the activity of the i-th presynaptic neuron and zj denote that of the j-th post-synaptic neuron before the non-linear activation function, and uj be the perturbation of zj. The activity-perturbed forward gradient estimator is

ga(wij ) = xi j ∇zj uj uj ,

(3)

where the inner product between ∇z and u is again computed by using forward-mode AD.

3.4 THEORETICAL PROPERTIES

In this section we aim to analyze the expectation and variance properties of forward gradient estimators. We focus our analysis on the gradient of one weight matrix {wij}, but the conclusion holds for a network with many weight matrices too.

Table 1 summarizes the theoretical results1. With a batch size of N , independent perturbation can achieve 1/N reduction of variance, whereas shared perturbation has a constant variance term dominated by the squared gradient norm. However, when performing independent weight perturbation, matrix multiplications cannot be batched because each example’s activation vector is multiplied with a different weight matrix. By contrast, independent activity perturbation admits batched matrix multiplications. Moreover, activity perturbation enjoys a factor of fan-in (p) times smaller variance compared to weight perturbation since the number of perturbed elements is the number of output units instead of the size of the whole weight matrix. The only drawback of activity perturbation is the memory required for storage of intermediate activations, in exchange for a factor of N p reduction in variance. However, for both activity and weight perturbation, the variance still grows with larger networks. In Section 4 we will further reduce the variance by introducing local loss functions.

3.5 CONTINUOUS-TIME RATE-BASED MODELS

Forward-mode AD can be viewed as computing the ﬁrst-order time derivative in a continuous-time

physical system. Suppose the tuples passed between nodes of the computation graph are (x, x˙ ),

where x˙ is the change in x over time. The computation is then the same as forward-mode AD. For

each node,

y˙

=

dy dx

x˙ ,

where

dy dx

is

the

Jacobian between

the output and

the

input.

Note that

in

a

physical system we don’t have to explicitly perform the differentiation operation by running two

forward passes. Instead the ﬁrst-order derivative information is readily available in the analog signal,

and we only need to plug the output signal into a differentiator circuit.

The activity-perturbed learning rule for a continuous time system is thus w˙ ij ∝ xiy˙jr˙, where xi is the pre-synaptic activity, and y˙j is the rate of change in the post-synaptic activity, which is the perturbation direction for a small period of time, and r˙ is the rate of change of reward (or the negative loss). The reward controls whether learning is Hebbian or anti-Hebbian. Both Hinton et al. (2007) and Bengio et al. (2017) propose to use a product of pre-synaptic activity and the rate of change of postsynaptic activity. However, they did not consider using the rate of change of reward as a modulator and instead relied on another set of feedback weights to communicate the error signal through inputs. In contrast, we show that by broadcasting the rate of change of reward, we can actually bypass the weight transport problem.

1All proofs can be found in Appendix 8 and 9. Numerical simulation results can be found in Appendix 10.

4

Published as a conference paper at ICLR 2023

Local Losses

Channel Mixing

A

Token Mixing

Local Losses

Channel Mixing

+

A ……

Token Mixing

Local Losses

Channel Mixing

+

A

Figure 1: A LocalMixer network consists of several mixer blocks. A=Activation function (ReLU).

Channels

Token Mixing

Patches

LN+FC+LN+A

T
LN+FC+LN+A

Channel Mixing
LN+FC+LN+A LN+FC+LN+A
T

Reshape

LN+FC+LN LN+FC+LN LN+FC+LN

Project
LN+FC LN+FC LN+FC

Local Losses

Channels

… … … … … …
…
Patches

LN+FC+LN+A
Channel Groups

LN+FC+LN LN+FC+LN LN+FC+LN

LN+FC LN+FC LN+FC

N x HW x C N x C x HW

N x HW x C

N x HW x G x C/G

+A

N x HW x G

Figure 2: A LocalMixer residual block with local losses. Token mixing consists of a linear layer and

channels are grouped in the channel mixing layers. Layer norm is applied before and after every linear

layer. LN=Layer Norm; FC=Fully Connected layer; A=Activation function (ReLU); T=Transpose.

3.6 ACTIVATION SPARSITY AND NORMALIZATION FUNCTIONS
In networks with ReLU activations, we can leverage ReLU sparsity to achieve further variance reduction, because the inactivated units will have zero gradient and therefore we should not perturb these units, and set the perturbation to be zero.
Normalization layers are often added in deep neural networks after the linear layer. To compute the correct gradient in activity perturbation, we also need to account for normalization in the weight update rule. Since there is no backward weight connections, one option is to simply apply backprop on normalization layers. However, we also found that it is also ﬁne to ignore the gradient of normalization layer when using layer normalization.
4 SCALING WITH LOCAL LOSSES
As we have explained in the previous section, perturbation learning can suffer from a curse of dimensionality: the variance grows with the number of perturbation dimensions, and in deep networks there are often millions of parameters changing at the same time. One way to limit the number of learnable dimensions is to divide the network into submodules, each with a separate loss function. In this section, we will explore several ways to increase the number of local losses to tame the variance.
1) Blockwise loss. First, we will divide the network into modules in depth. Each module consists of several layers. At the end of each module, we compute a loss function, and that loss is used to update the parameters in that module. This approach is equivalent of adding a “stop gradient” operator in between modules. Such local greedy losses were previously explored in Belilovsky et al. (2019) and Löwe et al. (2019).
2) Patchwise loss. Sensory input signals such as images have spatial dimensions. We will apply a separate loss patchwise along these spatial dimensions. In the Vision Transformer architecture (Vaswani et al., 2017; Dosovitskiy et al., 2021), each spatial token represents a patch in the image. In modern deep networks, parameters in each spatial location are often shared to improve data efﬁciency and reduce memory bandwidth utilization. Although naive weight sharing is not biologically plausible, we still consider shared weights in this work. It may be possible to mimic the effect of weight sharing by adding knowledge distillation (Hinton et al., 2015) losses in between patches.
3) Groupwise loss. Lastly, we turn to the channel dimension. To create multiple losses, we split the channels into a number of groups, and each group is attached to a loss function (Patel et al., 2022). To prevent groups from communicating between each other, channels are only connected to other channels within the same group. A grouped linear layer is computed as zg,j = i wg,i,jxg,i, for individual group g. Whereas previous work used channel groups to improve computational efﬁciency (Krizhevsky et al., 2012; Ioannou et al., 2017; Xie et al., 2017), in our work, adding groups contributes to the total number of losses and thus reduces variance.
Feature aggregators. Naively applying losses separately to the spatial and channel dimensions leads to suboptimal performances, since each dimension contains only local information. For losses of

5

Published as a conference paper at ICLR 2023

A. Conventional

N x HW x C

N x C

B. Replicated

Avg Patches

Groups Patches

N x HW x G x C/G N x HW x G x C Stack + StopGradient

N x HW x HW x G x C N x HW x G x C
Avg Avg Avg Avg AvgPool + StopGradient

Figure 3: Feature aggregator designs. A) In the conventional design, average pooling is performed to aggregate features from different spatial locations. B) We propose the replicated design, features are ﬁrst concatenated across groups and then averaged across spatial locations. We create copies of the same feature with different stop gradient masks so that we obtain more local losses instead of a global one. The stop gradient mask makes sure that perturbation in one spatial group corresponds to its loss function. The numerical value of the loss function is the same as the conventional design.

standard tasks such as classiﬁcation, the model needs a global view of the inputs to make a decision. Standard architectures obtain this global view by performing global average pooling layer before the ﬁnal classiﬁcation layer. We therefore explore strategies for aggregating information from other groups and spatial patches before the local loss function.

We would prefer to perform aggregation without reducing the total number of dimensions. We thus propose a replicated design for feature aggregation, shown in Figure 3. First, channel groups are copied and communicated to one another, but every group except the active group itself is masked with stop gradient so that other groups do not affect the forward gradient computation:

xp,g = [StopGrad(xp,1...xp,g−1), xp,g, StopGrad(xp,g+1, ..., xp,G)],

(4)

where p and g index the patches and groups respectively. Similarly, each spatial location is also

copied, communicated, and masked, and then averaged locally:

xp,g

=

1 P

xp,g +

p =p StopGrad(xp ,g) .

(5)

The output of feature aggregation is the same as that of the conventional global average pooling layer. The difference is that here the loss is replicated and different patch groups are activated in each loss.

Learning objectives. We consider the supervised classiﬁcation loss and the contrastive InfoNCE loss (van den Oord et al., 2018; Chen et al., 2020), which are the two most commonly used losses in image representation learning. For supervised classiﬁcation, we attach a shared linear layer (shared across p, g) on top of the aggregated features for a cross entropy loss: Lsp,g = − k tk log softmax(Wlxp,g)k. The loss is of the same value across each group and patch location.
For contrastive learning, the linear layer becomes a linear feature projector. Suppose x(n1) and x(n2) are the two different views of the n-th example, the InfoNCE loss for contrastive learning is:

Lcp,g = − log
n

(W x(n1,)p,g) StopGrad(W x(n2)) . m(W x(n1,)p,g) StopGrad(W x(m2))
(6)

Original 15

Different Perturbation

StopGrad

10

Training Loss

Note that we add a stop gradient operator on the second view. It is usually unnecessary to add this stop gradient in the InfoNCE loss; however, we found that perturbationbased methods require a stop gradient and otherwise the loss will not go down. This is likely because we share the perturbations on both views, and having the same perturbation will increase the dot product between the two views but is not desired from a representation learning perspective. Figure 4 shows a comparison of the loss curves. Non-shared perturbations also work but are worse than stop gradient.

5
0 20000 40000 60000 80000
Step
Figure 4: Importance of StopGradient in the InfoNCE loss, using M/8 on CIFAR-10 with 256 channels 1 group.

6

Published as a conference paper at ICLR 2023

Type

Blocks Patches Channels Groups Params

LocalMixer S/1/1

1

1×1

256

LocalMixer M/1/16 1

1×1

512

LocalMixer M/8/16 4

8×8

512

LocalMixer L/8/64

4

8×8

2048

LocalMixer L/32/64 4 32×32 2048

1

272K

16

429K

16

919K

64 13.1M

64 17.3M

Table 2: LocalMixer Architecture Details

Dataset
MNIST MNIST CIFAR-10 CIFAR-10 ImageNet

5 IMPLEMENTATION

Network architecture. We propose the LocalMixer architecture that is more suitable for local learning. It takes inspiration from MLPMixer (Tolstikhin et al., 2021), which consists of fully connected networks and residual blocks. We leverage the fully connected networks so that each spatial patch performs computations without interfering with other patches, which is more compatible with our local learning objective. An image is divided into non-overlapping patches (i.e. tokens), and each block consists of token and channel mixing layers. Figure 1 shows the high level architecture, and Figure 2 shows the detailed diagram for one residual block. We add a linear projector/classiﬁcation layer to attach a loss function at the end of each block. The last layer always uses backprop to update weights. For token mixing layers, we use one linear fully connected layer instead of an MLP, since we would like to make each block as shallow as possible. Before the last channel mixing layer, features are reshaped into a number of groups, and the last layer is fully connected within each feature group. Table 2 shows architectural details for the different sizes of models we investigate.
Normalization. There are many ways of performing normalization within a neural network across different tensor dimensions (Krizhevsky et al., 2012; Ioffe & Szegedy, 2015; Ba et al., 2016; Ren et al., 2017; Wu & He, 2018). We opted for a local variant of layer normalization that normalizes within each local spatial patch of features (Ren et al., 2017). For grouped linear layers, each group is normalized separately (Wu & He, 2018). Empirically, we found such local normalization performs better on contrastive learning experiments and about the same as layer normalization on supervised experiments. Local normalization is also more biologically plausible as it does not perform global communication. Conventionally, normalization layers are placed after linear layers. In MLPMixer (Tolstikhin et al., 2021), layer normalization is placed at the beginning of each residual block. We found it is the best to place normalization before and after each linear layer, as shown in Figure 2. Empirically this design choice does not make much difference for backprop, but it allows forward gradient learning to learn much faster and achieve lower training errors.

Efﬁcient implementation of replicated losses. Due to the design of feature aggregation and replicated losses, a naïve implementation of groups can be very inefﬁcient in terms of both memory consumption and compute. However, each spatial group actually computes the same aggregated feature and loss function. This means that it is possible to share most of the computation across loss functions when performing both backprop and forward gradient. We implemented our custom JAX JVP/VJP functions (Bradbury et al., 2018) and observed signiﬁcant memory savings and compute speed-ups for replicated losses, which would otherwise not be feasible to run on modern hardware. The results are reported in Figure 5. A code snippet is included in Appendix 12.

GPU Memory (G) 1 2 4 8 16 32
Compute (sec/epoch) 1 2 4 8 16 32

Naïve 40 30 20 10
0

Fused

Naïve 200 150 100
50 0

Fused

Number of Groups

Number of Groups

Figure 5: Memory and compute usage of naïve and fused implementation of replicated losses.

6 EXPERIMENTS

We compare our proposed algorithm to a set of alternatives: Backprop, Feedback Alignment and other global variants of Forward Gradient. Backprop is a biologically implausible oracle, since it computes true gradients whereas we compute noisy gradients. Feedback alignment computes approximate gradients by using a set of random backward weights. We explain each method below.
1) Backprop (BP). We include the standard backprop algorithm as well as its local variants. Local Backprop (L-BP) adds local losses as proposed, but still permits gradient to ﬂow in an end-to-end fashion. Local Greedy Backprop (LG-BP) in addition adds stop gradient operators in between blocks. This is to provide a comparison to our methods by computing true local gradients. LG-BP is similar in spirit to recent local learning algorithms (Belilovsky et al., 2019; Löwe et al., 2019).

7

Published as a conference paper at ICLR 2023

Dataset Network Metric
BP L-BP LG-BP
FA L-FA LG-FA DFA
FG-W FG-A LG-FG-W LG-FG-A

MNIST

MNIST

CIFAR-10

ImageNet

S/1/1

M/1/16

M/8/16

L/32/64

Test / Train Err. (%) Test / Train Err. (%) Test / Train Err. (%) Test / Train Err. (%)

2.66 / 0.00 2.38 / 0.00 2.43 / 0.00

2.41 / 0.00 2.16 / 0.00 2.81 / 0.00

33.62 / 0.00 30.75 / 0.00 33.84 / 0.05

36.82 / 14.69 42.38 / 22.80 54.37 / 39.66

2.82 / 0.00 3.21 / 0.00 3.11 / 0.00 3.31 / 0.00

BP-free algorithms
2.90 / 0.00 2.90 / 0.00 2.50 / 0.00 3.17 / 0.00

39.94 / 28.44 39.74 / 28.98 39.73 / 32.32 38.80 / 33.69

94.55 / 94.13 87.20 / 85.69 85.45 / 82.83 91.17 / 90.28

9.25 / 8.93 3.24 / 1.53 9.25 / 8.93 3.24 / 1.53

8.56 / 8.64 3.76 / 1.75 5.66 / 4.59 2.55 / 0.00

55.95 / 54.28 59.72 / 41.29 52.70 / 51.71 30.68 / 19.39

97.71 / 97.58 98.83 / 98.80 97.39 / 97.29 58.37 / 44.86

Table 3: Supervised learning for image classiﬁcation

Dataset Network Metric

CIFAR-10

CIFAR-10

ImageNet

M/8/16

L/8/64

L/32/64

Test / Train Err. (%) Test / Train Err. (%) Test / Train Err. (%)

BP L-BP LG-BP

24.11 / 21.08 24.69 / 21.80 29.63 / 25.60

17.53 / 13.35 19.13 / 13.60 23.62 / 16.80

55.66 / 49.79 59.11 / 52.50 68.36 / 62.53

BP-free algorithms

FA L-FA LG-FA DFA

45.87 / 44.06 37.73 / 36.13 36.72 / 34.06 46.09 / 42.76

67.93 / 65.32 31.05 / 26.97 30.49 / 25.56 39.26 / 37.17

82.86 / 80.21 83.18 / 79.80 82.57 / 79.53 93.51 / 92.51

FG-W FG-A LG-FG-W LG-FG-A

53.37 / 51.56 54.59 / 52.96 52.66 / 50.23 32.88 / 29.73

50.45 / 45.64 56.63 / 56.09 52.27 / 48.67 26.81 / 23.90

91.94 / 89.69 97.83 / 97.79 91.36 / 88.81 73.24 / 66.89

Table 4: Self-supervised contrastive learning with linear readout

2) Feedback Alignment (FA). The standard FA algorithm (Lillicrap et al., 2016) adds a set of random and ﬁxed backward weights. We assume that the gradients to normalization layers and activation functions are known since they do not have weight connections. Local Feedback Alignment (L-FA) adds local losses as proposed, but still permits error signals to ﬂow back. Local Greedy Feedback Alignment (LG-FA) adds a stop gradient to prevent error signals from ﬂowing back, similar to the backprop-free algorithm in Nøkland & Eidnes (2019).
3) Forward Gradient (FG). This family of methods comprises our proposed algorithm and related approaches. Weight-perturbed forward gradient (FG-W) was proposed by Baydin et al. (2022). In this paper, we propose the activity perturbation variant (FG-A). We further add local objective functions, producing LG-FG-W and LG-FG-A, which stand for Local Greedy Forward Gradient Weight/Activity-Perturbed. For local perturbation to work, we have to add a stop gradient in between blocks so each perturbation has a single corresponding loss. We expect LG-FG-A to achieve the best performance among other variants because it can leverage the variance reduction beneﬁt from both activity perturbation and local losses.
Datasets. We use standard image classiﬁcation datasets to benchmark the learning algorithms. MNIST (LeCun, 1998) contains 70,000 28×28 handwritten digit images of class 0-9. CIFAR10 (Krizhevsky et al., 2009) contains 60,000 32×32 natural images of 10 semantic classes. ImageNet (Deng et al., 2009) contains 1.3 million natural images of 1000 classes, which we resized to 224×224. For CIFAR-10 and ImageNet, we applied both supervised learning and contrastive learning. For MNIST, we applied supervised learning only. We designed different conﬁgurations of the LocalMixer architecture for each dataset, listed in Table 2.
Data augmentation. For MNIST and CIFAR-10 supervised experiments, we do not apply data augmentation. Data augmentation on ImageNet follows the open source implementation by Grill

8

Published as a conference paper at ICLR 2023

Accuracy (%) Accuracy (%) Accuracy (%)

Test

Train

85

75

65

55

45

35 Global Block Block + Patch + Block + Patch Group Patch + Group

Test

Train

70

65

60

55

50

45

40

35 Global Block Block + Patch + Block + Patch Group Patch + Group

Test

Train

60

40

20

0 Global Block Patch + Block + Block + Group Patch Patch + Group

(a) CIFAR-10 Supervised M/8 (b) CIFAR-10 Contrastive M/8 (c) ImageNet Supervised L/32 Figure 6: Effect of adding local losses at different locations on the performance of forward gradient

Error Rate (%) Error Rate (%) Error Rate (%) Error Rate (%)

65

65 Group 1

6G5roup 1

Group 2

Group 2

55

55

Group 4

5G5roup 4

45 Group 8

Group 8

45

Group 16

4G5roup 16

35 Group 32

Group 32

35

35

25

25

15

25

0

25000

50000

75000 0

25000

50000

75000

20000 40000 60000 80000

Steps

Steps

Steps

(a) Supervised Test (b) Supervised Train (c) Contrastive Test

6G5 roup 1 Group 2 5G5 roup 4 Group 8 4G5 roup 16 Group 32 35

Group 1 Group 2 Group 4 Group 8 Group 16 Group 32

25 20000 40000 60000 80000
Steps
(d) Contrastive Train

Figure 7: Error rate of M/8/* during CIFAR-10 training using different number of groups.

et al. (2020). Because forward gradient suffers from variance, we apply weaker augmentations for contrastive learning experiments, increasing the area lower bound for random crops from 0.08 to 0.3-0.5. We ﬁnd that this change has relatively little effect on the performance of backprop.
Main results. Our main results are shown in Table 3 and Table 4. In supervised experiments, there is almost no cost of introducing local greedy losses, and our local forward gradient method can match the test error of backprop on MNIST and CIFAR. Note that LG-FG-A fails to overﬁt the training set to 0% error when trained without data augmentation. This suggests that variance could still be an issue. For CIFAR-10 contrastive learning, our method obtains an error rate approaching that obtained by backprop (26.81% vs. 17.53%), and most of the gap is due to greedy learning vs. gradient estimation (6.09% vs. 3.19%). On ImageNet, we achieve reasonable performance compared to backprop (58.37% vs. 36.82% for supervised and 73.24% vs. 55.66% for contrastive). However, we ﬁnd that the error due to greediness grows as the problem gets more complex and requires more layers to cooperate. We signiﬁcantly outperform the FA family on ImageNet (by 25% for supervised and 10% for contrastive). Interestingly, local greedy FA also performs better than global feedback alignment, which suggests that the beneﬁt of local learning transfers to other types of gradient approximation. TP-based methods were evaluated in Bartunov et al. (2018) and were found to be worse than FA on ImageNet. In sum, although there is still some noticeable gap between our method and backprop, we have made a large stride forward compared to backprop-free algorithms. More results are included in the Appendix 14.
Effect of local losses. In Figure 6 we ablate the beneﬁt of placing local losses at different locations: blockwise, patchwise and groupwise. A combination of all three is the strongest. Global perturbation learning fails to learn as the accuracy is similar to initializing with random weights.
Effect of groups. In Figure 7 we investigate the effect of different number of groups by showing the training curves. Adding more groups bring signiﬁcant improvement to local perturbation learning in terms of lowering both training and test errors, but the effect vanishes around 8 channels / group.
7 CONCLUSION
It is often believed that perturbation-based learning cannot scale to large and deep networks. We show that this is to some extent true because the gradient estimation variance grows with the number of hidden dimensions for activity perturbation, and is even worse for shared weight perturbation. But more optimistically, we show that a huge number of local greedy losses can help forward gradient learning scale much better. We explored blockwise, patchwise, and groupwise local losses, and a combination of all three, with a total of a quarter of a million losses in one of the larger networks, performs the best. Local activity-perturbed forward gradient performs better than previous backpropfree algorithms on larger networks. The idea of local losses opens up opportunities for different loss designs and sheds light on the search for biologically plausible learning algorithms in the brain and alternative computing devices.

9

Published as a conference paper at ICLR 2023
ACKNOWLEDGMENT
We thank Timothy Lillicrap for his helpful feedback on our earlier draft.
REFERENCES
Larry F Abbott and Sacha B Nelson. Synaptic plasticity: taming the beast. Nature neuroscience, 3 (11):1178–1183, 2000.
Mohamed Akrout, Collin Wilson, Peter C. Humphreys, Timothy P. Lillicrap, and Douglas B. Tweed. Deep learning without weight transport. In Advances in Neural Information Processing Systems 32, NeurIPS, 2019.
Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450, 2016.
Pierre Baldi and Peter J. Sadowski. A theory of local learning, the learning channel, and the optimality of backpropagation. Neural Networks, 83:51–74, 2016. doi: 10.1016/j.neunet.2016.07.006.
Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can solve difﬁcult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, (5):834–846, 1983.
Sergey Bartunov, Adam Santoro, Blake A. Richards, Luke Marris, Geoffrey E. Hinton, and Timothy P. Lillicrap. Assessing the scalability of biologically-motivated deep learning algorithms and architectures. In Advances in Neural Information Processing Systems 31, NeurIPS, 2018.
Atilim Günes Baydin, Barak A. Pearlmutter, Don Syme, Frank Wood, and Philip H. S. Torr. Gradients without backpropagation. CoRR, abs/2202.08587, 2022.
Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Greedy layerwise learning can scale to imagenet. In Proceedings of the 36th International Conference on Machine Learning, ICML, 2019.
Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Decoupled greedy learning of cnns. In Proceedings of the 37th International Conference on Machine Learning, ICML, 2020.
Yoshua Bengio. How auto-encoders could provide credit assignment in deep networks via target propagation. CoRR, abs/1407.7906, 2014.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In Advances in Neural Information Processing Systems 19, NIPS, 2006.
Yoshua Bengio, Thomas Mesnard, Asja Fischer, Saizheng Zhang, and Yuhuai Wu. Stdp as presynaptic activity times rate of change of postsynaptic activity approximates back-propagation. Neural Computation, 10, 2017.
Elie L Bienenstock, Leon N Cooper, and Paul W Munro. Theory for the development of neuron selectivity: orientation speciﬁcity and binocular interaction in visual cortex. Journal of Neuroscience, 2 (1):32–48, 1982.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML, 2020.
David G. Clark, L. F. Abbott, and SueYeon Chung. Credit assignment through broadcasting a global error vector. In Advances in Neural Information Processing Systems 34, NeurIPS, 2021.
10

Published as a conference paper at ICLR 2023
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2009.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR, 2021.
Ila R Fiete and H Sebastian Seung. Gradient learning in spiking neural networks by dynamic perturbation of conductances. Physical review letters, 97(4):048104, 2006.
Aidan N. Gomez, Oscar Key, Stephen Gou, Nick Frosst, Jeff Dean, and Yarin Gal. Interlocking backpropagation: Improving depthwise model-parallelism. CoRR, abs/2010.04116, 2020.
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Ávila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent - A new approach to self-supervised learning. In Advances in Neural Information Processing Systems 33, NeurIPS, 2020.
Donald Olding Hebb. The organization of behavior: a neuropsychological theory. J. Wiley; Chapman & Hall, 1949.
Geoffrey Hinton et al. How to do backpropagation in a brain. In Invited talk at the NIPS 2007 deep learning workshop, 2007.
Geoffrey E. Hinton and James L. McClelland. Learning representations by recirculation. In Neural Information Processing Systems, 1987.
Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief nets. Neural Comput., 18(7):1527–1554, 2006. doi: 10.1162/neco.2006.18.7.1527.
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015.
Yani Ioannou, Duncan P. Robertson, Roberto Cipolla, and Antonio Criminisi. Deep roots: Improving CNN efﬁciency with hierarchical ﬁlter groups. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, ICML, 2015.
Marwan Jabri and Barry Flower. Weight perturbation: An optimal architecture and learning technique for analog vlsi feedforward and recurrent multilayer networks. IEEE Transactions on Neural Networks, 3(1):154–157, 1992.
John F Kolen and Jordan B Pollack. Backpropagation without weight transport. In Proceedings of IEEE International Conference on Neural Networks, ICNN, 1994.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, NIPS, 2012.
Michael Laskin, Luke Metz, Seth Nabarrao, Mark Sarouﬁm, Badreddine Noune, Carlo Luschi, Jascha Sohl-Dickstein, and Pieter Abbeel. Parallel training of deep networks with local updates. CoRR, abs/2012.03837, 2020.
Yann Le Cun, Conrad Galland, and Geoffrey E Hinton. Gemini: Gradient estimation through matrix inversion after noise injection. In Advances in Neural Information Processing Systems, NIPS, volume 1. Morgan-Kaufmann, 1988.
11

Published as a conference paper at ICLR 2023
Yann LeCun. A learning scheme for asymmetric threshold networks. Proceedings of COGNITIVA, 85(537):599–604, 1985.
Yann LeCun. The mnist database of handwritten digits, 1998. URL http://yann.lecun.com/ exdb/mnist/.
Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation. In Joint european conference on machine learning and knowledge discovery in databases, pp. 498–515. Springer, 2015.
Qianli Liao, Joel Z. Leibo, and Tomaso A. Poggio. How important is weight symmetry in backpropagation? In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence, AAAI, 2016.
Timothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed, and Colin J. Akerman. Random synaptic feedback weights support error backpropagation for deep learning. Nature Communications, 7(1): 13276, Nov 2016. ISSN 2041-1723. doi: 10.1038/ncomms13276.
Timothy P. Lillicrap, Adam Santoro, Luke Marris, Colin J. Akerman, and Geoffrey Hinton. Backpropagation and the brain. Nature Reviews Neuroscience, 21(6):335–346, Jun 2020. ISSN 1471-0048. doi: 10.1038/s41583-020-0277-3.
Sindy Löwe, Peter O’Connor, and Bastiaan S. Veeling. Putting an end to end-to-end: Gradientisolated learning of representations. In Advances in Neural Information Processing Systems 32, NeurIPS, 2019.
Arild Nøkland. Direct feedback alignment provides learning in deep neural networks. In Advances in Neural Information Processing Systems 29, NeurIPS, 2016.
Arild Nøkland and Lars Hiller Eidnes. Training neural networks with local error signals. In Proceedings of the 36th International Conference on Machine Learning, ICML, 2019.
Erkki Oja. Simpliﬁed neuron model as a principal component analyzer. Journal of mathematical biology, 15(3):267–273, 1982.
Adeetya Patel, Michael Eickenberg, and Eugene Belilovsky. Local learning with neuron groups. In From Cells to Societies: Collective Learning Across Scales - ICLR 2022 Workshop, 2022.
Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147–160, 1994.
Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-supervised learning with ladder networks. In Advances in Neural Information Processing Systems 28, NIPS, 2015.
Mengye Ren, Renjie Liao, Raquel Urtasun, Fabian H. Sinz, and Richard S. Zemel. Normalizing the normalizers: Comparing and extending network normalization schemes. In Proceedings of the 5th International Conference on Learning Representations, ICLR, 2017.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by error propagation. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations, pp. 318–362, Cambridge, MA, USA, 1986. MIT Press. ISBN 026268053X.
Tim Salimans, Jonathan Ho, Xi Chen, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. CoRR, abs/1703.03864, 2017.
Wolfram Schultz, Peter Dayan, and P Read Montague. A neural substrate of prediction and reward. Science, 275(5306):1593–1599, 1997.
H Sebastian Seung. Learning in spiking neural networks by reinforcement of stochastic synaptic transmission. Neuron, 40(6):1063–1073, 2003.
Eren Sezener, Agnieszka Grabska-Barwin´ska, Dimitar Kostadinov, Maxime Beau, Sanjukta Krishnagopal, David Budden, Marcus Hutter, Joel Veness, Matthew Botvinick, Claudia Clopath, et al. A rapid and efﬁcient learning rule for biological neural circuits. BioRxiv, 2021.
12

Published as a conference paper at ICLR 2023
David Silver, Anirudh Goyal, Ivo Danihelka, Matteo Hessel, and Hado van Hasselt. Learning by directional gradient descent. In Proceedings of the 10th International Conference on Learning Representations, ICLR, 2022.
Kenneth O Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topologies. Evol Comput, 10(2):99–127, 2002.
Ilya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision. In Advances in Neural Information Processing Systems 34, NeurIPS, 2021.
Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30, NIPS, 2017.
Joel Veness, Tor Lattimore, Avishkar Bhoopchand, Agnieszka Grabska-Barwinska, Christopher Mattern, and Peter Toth. Online learning with gated linear networks. arXiv preprint arXiv:1712.01897, 2017.
Joel Veness, Tor Lattimore, David Budden, Avishkar Bhoopchand, Christopher Mattern, Agnieszka Grabska-Barwinska, Eren Sezener, Jianan Wang, Peter Toth, Simon Schmitt, et al. Gated linear networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, AAAI, 2021.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Mach. Learn. Res., 11:3371–3408, 2010. doi: 10.5555/1756006.1953039.
Yulin Wang, Zanlin Ni, Shiji Song, Le Yang, and Gao Huang. Revisiting locally supervised learning: an alternative to end-to-end training. In 9th International Conference on Learning Representations, ICLR, 2021.
Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger B. Grosse. Flipout: Efﬁcient pseudoindependent weight perturbations on mini-batches. In 6th International Conference on Learning Representations, ICLR, 2018.
R. E. Wengert. A simple automatic derivative evaluation program. Commun. ACM, 7(8):463–464, 1964.
Paul Werbos. Beyond regression:" new tools for prediction and analysis in the behavioral sciences. Ph. D. dissertation, Harvard University, 1974.
Justin Werfel, Xiaohui Xie, and H Seung. Learning curves for stochastic gradient descent in linear feedforward networks. Advances in Neural Information Processing Systems 16, NIPS, 2003.
L. Darrell Whitley. Genetic reinforcement learning for neurocontrol problems. Mach. Learn., 13: 259–284, 1993.
James C.R. Whittington and Rafal Bogacz. Theories of error back-propagation in the brain. Trends in Cognitive Sciences, 23(3):235–250, Mar 2019. ISSN 1364-6613. doi: 10.1016/j.tics.2018.12.005.
Bernard Widrow and Marcian E Hoff. Adaptive switching circuits. Technical report, Stanford Univ Ca Stanford Electronics Labs, 1960.
Bernard Widrow and Michael A. Lehr. 30 years of adaptive neural networks: perceptron, madaline, and backpropagation. Proc. IEEE, 78(9):1415–1442, 1990.
Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270–280, 1989.
13

Published as a conference paper at ICLR 2023
Sunghyeon Woo, Jeongwoo Park, Jiwoo Hong, and Dongsuk Jeon. Activation sharing with asymmetric paths solves weight transport problem without bidirectional connection. In Advances in Neural Information Processing Systems 34, NeurIPS, 2021.
Yuxin Wu and Kaiming He. Group normalization. In 15th European Conference on Computer Vision, ECCV, 2018.
Will Xiao, Honglin Chen, Qianli Liao, and Tomaso A. Poggio. Biologically-plausible learning algorithms can scale to large datasets. In Proceedings of the 7th International Conference on Learning Representations, ICLR, 2019.
Saining Xie, Ross B. Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2017.
Xiaohui Xie and H Sebastian Seung. Spike-based learning rules and stabilization of persistent neural activity. Advances in Neural Information Processing Systems 12, NIPS, 1999.
Yuwen Xiong, Mengye Ren, and Raquel Urtasun. Loco: Local contrastive representation learning. In Advances in Neural Information Processing Systems 33, NeurIPS, 2020.
14

Published as a conference paper at ICLR 2023

8 PROOFS OF UNBIASEDNESS
In this section, we show the unbiasedness of gw(wij) and ga(wij). The ﬁrst proof was given by Baydin et al. (2022).
Proposition 1. gw(wij) is an unbiased gradient estimator if {vij} are independent zero-mean uni-variance random variables (Baydin et al., 2022).

Proof. We can rewrite the weight perturbation estimator as





gw(wij ) =  ∇wi j vi j  vij = ∇wij vi2j +

∇wi j vij vi j .

(7)

ij

i j =ij

Note that since each dimension of v is an independent zero-mean uni-variance random variable,

E[vij] = 0, E[vi2j] = Var[vij] + E[vij]2 = 1 + 0 = 1, and E[vijvi j ] = 0 if ij = i j .





E[gw(wij )] = E ∇wij vi2j + E 

∇wi j vij vi j 

(8)

i j =ij

= ∇wij E vi2j +

∇wi j E [vij vi j ]

(9)

i j =ij

= ∇wij · 1 +

∇wi j · 0

(10)

i j =ij

= ∇wij .

(11)

Proposition 2. ga(wij) is an unbiased gradient estimator if {uj} are independent zero-mean univariance random variables.

Proof. The true gradient to the weights ∇wij is the product between xj and ∇zk. Therefore, we can rewrite the weight perturbation estimator as









ga(wij ) = xi  ∇zj uj  uj = xj ∇zj u2j + xi  ∇zj uj  uj

(12)

j

j =j





= xi∇zj u2j +  xi∇zj uj  uj

(13)

j =j





= ∇wij u2j +  ∇wij uj  uj .

(14)

j =j

Since each dimension of u is an independent zero-mean uni-variance random variable, E[uj] = 0, E[u2j ] = Var[uj] + E[uj]2 = 1 + 0 = 1, and E[ujuj ] = 0 if j = j .







E[ga(wij )] = E ∇wij u2j +  ∇wij uj  uj 

(15)

j =j

= ∇wij E u2j + ∇wij E [uj uj ]

(16)

j =j

= ∇wij · 1 + ∇wij · 0

(17)

j =j

= ∇wij .

(18)

15

Published as a conference paper at ICLR 2023

9 PROOFS OF VARIANCES

We followed Wen et al. (2018) and show that the variance of the gradient estimators can be decomposed.

Lemma 1. The variance of the gradient estimator can be decomposed into three parts:

Var (g(wij)|x)

=

Z1

+ Z2

+ Z3,

where

Z1

=

1 N

V1

Varx

(∇wij |x),

Z2

=

1 N

Ex

[Varv

( g(wij)| x)],

Z3

=

1 N2

EB

x(n)∈B x(m)∈B\{x(n)} Covv( g(wij )| x(n), g(wij )| x(m)) .

Proof. By the law of total variance,

Var (g(wij)) = Var
B

E
v

[g(wij

)|B

]

+E
B

Vvar(g(wij )|B )

.

(19)

The ﬁrst term comes from the gradient variance from data sampling, and it vanishes as batch size

grows:

Var
B

E
v

[g(wij

)|B]

(20)





1

=

Var
B

E
v



N

g(wij )|x(n)

(21)

x(n) ∈B





1

=N2

Var
B

E
v



g(wij )|x(n)

(22)

x(n) ∈B





1

=N2

Var
B



E
v

g(wij )|x(n)



(23)

x(n) ∈B





1

=N2

Var
B



∇wij |x(n) 

x(n) ∈B

(24)

1

1

=N2

Var
x

(∇wij

|x)

=

N

Var
x

(∇wij

|x)

=

Z1.

(25)

n

The second term comes from the gradient estimation variance:

E
B

Var
v

(g(wij

)|B)

(26)





1

=

E
B

Var
v



N

g(wij ) x(n)

(27)

x(n) ∈B







1

=

E
B

N2

Var
v



g(wij ) x(n)

(28)

x(n) ∈B



1

=

E
B



N

2

Var
v

x(n) ∈B

g(wij )| x(n)



+

Cov(
v

g(wij )|

x(n),

g(wij )|

x(m))

x(n)∈B x(m)∈B\{x(n)}

(29)





1

1

= N

E
x

Var
v

(

g(wij

)|

x)

+

N2

E
B



Cov(
v

g(wij

)|

x(n),

g(wij

)|

x(m))

x(n)∈B x(m)∈B\{x(n)}

(30)

=Z2 + Z3.

(31)

Remark. Z2 is the variance of the gradient estimator in the deterministic case, and Z3 measures the correlation between different gradient estimation within the batch. The Z3 is zero if the perturbations are independent, and non-zero if the perturbations are shared within the mini-batch.

16

Published as a conference paper at ICLR 2023

Proposition 3. Let p × q be the size of the weight matrix, the element-wise average variance of the

weight perturbed gradient estimator

with

a batch size N

is

pq+2 N

V

+ (pq

+ 1)S

if the perturbations

are shared across the batch,

and

pq+2 N

V

+

pq+1 N

S

if they are independent, where V

is the element-wise

average variance of the true gradient, and S is the element-wise average squared gradient.

Proof. We ﬁrst derive Z2.

1

Z2

= N

E
x

Var
v

(

gw

(wij

)|

x)

(32)

 

 

1

= N

E Var 
xv

∇wi j vi j  vij 

(33)

ij





1 =
N

E
x

Var
v

∇wij

vi2j

+

∇wi j vij vi j 

(34)

i j =ij



1

= N

E Var
xv

∇wij vi2j









+ Var 
v

∇wi

j

vij vi

j

+

2

Cov
v

∇wij

vi2j

,

∇wij vij vi j 

i j =ij

i j =ij

(35)







1

= N

E Var
x

∇wij vi2j

+ Var 
v

∇wi j vij vi j  +

(36)

i j =ij









2E
v

∇wij ∇wi

j

vi3j vi

j

−

2E
v

∇wij vi2j

E
v

∇wi j vij vi j 

i j =ij

i j =ij

(37)







1 =
N

E
x

∇wi2j

Var
v

vi2j

+ Var 
v

∇wi j vij vi j  +

(38)

i j =ij





2

∇wij ∇wi j

E
v

vi3j vi j

−

2∇wij

E
v

vi2j



∇wi j

E
v

[vij

vi

j

]

i j =ij

i j =ij

(39)







1 =
N

E
x

∇wi2j

Var
v

vi2j

+ Var 
v

∇wi j vij vi j  +

(40)

i j =ij





2

∇wij∇wi j · 0 − 2∇wij · 1 

∇wi j · 0

(41)

i j =ij

i j =ij







1 =
N

E
x

∇wi2j

Var
v

vi2j

+ Var 
v

∇wi j vij vi j 

(42)

i j =ij





1 =
N

E
x

∇wi2j

·

(E[vi4j ]

−

Ev [vi2j ]2)

+

Var
v

(∇wi

j

vij vi

j

)

(43)

i j =ij





1 =
N

E
x

∇wi2j

(3

Var[vij
v

]2

−

Ev [vi2j ]2)

+

∇wi2 j

Var (vij vi j )
v

(44)

i j =ij





1

= N

E
x

2∇wi2j

1

+

N

E
x

∇wi2 j (Vvar[vij ] + Ev [vi j ]2)(Vvar[vi j ] + Ev [vi j ]2) − Ev [vij ]2 Ev [vi j ]2

i j =ij

(45)

17

Published as a conference paper at ICLR 2023





2 =
N

∇w2ij

+

2 N

Vxar(∇wij |x)

+

1 N

E
x

∇wi2 j Vvar(vij ) Vvar(vi j )

(46)

i j =ij

2 =
N

∇w2ij

+

2 N

Var(∇wij |x)
x

+

1 N

E
x

∇wi2 j

(47)

i j =ij





1 =
N

∇w2ij

+

Var(∇wij |x)
x

+

∇w2i j

+ Var(∇wi j |x)
x

.

(48)

ij

Z3 is nonzero if the perturbations are shared within a batch. Assuming that the perturbations are

shared,





1

Z3

=N2

E
B



Cov(
v

gw(wij )|

x(n),

gw (wij

)|

x(m))

x(n)∈B x(m)∈B\{x(n)}

(49)



1

=N2

E
B



E
v

x(n)∈B x(m)∈B\{x(n)}

gw(wij )| x(n) gw(wij )| x(m)

−E
v

gw(wij )| x(n)

 E gw(wij )| x(m) 
v
(50)



1

=N2

E
B



E
v

x(n)∈B x(m)∈B\{x(n)}

gw(wij )| x(n) gw(wij )| x(m)

 − ∇wij |x(n)∇wij |x(m)
(51)









1

=N2

E
B



E 
v

∇wi j |x(n)vi j  vij 

∇wi j |x(m)vi j  vij  −

x(n)∈B x(m)∈B\{x(n)}

ij

ij

(52)

∇wij |x(n)∇wij |x(m)

(53)







1

=N2

E
B



E
v

∇wij

|x(n)vi2j

+

∇wi j |x(n)vi j vij 

x(n)∈B x(m)∈B\{x(n)}

i j =ij





(54)

∇wij |x(m)vi2j +

∇wi j |x(m)vi j vij  −

i j =ij





1

N2

E
B



∇wij |x(n)∇wij |x(m)

x(n)∈B x(m)∈B\{x(n)}



1

=N2

E
B



E
v

∇wij |x(n)∇wij |x(m)vi4j +

x(n)∈B x(m)∈B\{x(n)}

(55) (56) (57)

∇wij |x(n)vi2j

∇wi j |x(m)vi j vij + ∇wij |x(m)vi2j

∇wi j |x(n)vi j vij + (58)

i j =ij

i j =ij



∇wi j |x(n)vi j vij

∇wi j |x(m)vi j vij  −

i j =ij

i j =ij







1

N2

E
x(n)

E
x(m)



n

∇wij |x(n)∇wij |x(m)
m=n

(59) (60)

18

Published as a conference paper at ICLR 2023



1

=N2

E
B



E
v

∇wij |x(n)∇wij |x(m)vi4j +

x(n)∈B x(m)∈B\{x(n)}

(61)





 







∇wi j |x(n)vi j  

∇wi

j

|x(m)vi

j

 vi2j 

−

1 N2



∇w2ij (62)

i j =ij

i j =ij

n m=n



1

=N2

E
B



E
v

∇wij |x(n)∇wij |x(m)vi4j +

x(n)∈B x(m)∈B\{x(n)}

(63) 

∇wi j |x(n)∇wi j |x(m)vi2 j vi2j +

∇wi j |x(n)∇wi j |x(m)vi j vi j vi2j  −

i j =ij

i j =ij i j =ij,i j

(64)





1 N2 

∇w2ij 

(65)

n m=n







1

=N2

E
B



E
v

∇wij

|x(n)∇wij

|x(m)

vi4j

+

∇wi j |x(n)∇wi j |x(m)vi2 j vi2j  −

x(n)∈B x(m)∈B\{x(n)}

i j =ij

(66)





1 N2 

∇w2ij 

(67)

n m=n



1

=N2

E
B



∇wij

|x(n)∇wij

|x(m)

E
v

vi4j

+

x(n)∈B x(m)∈B\{x(n)}

∇wi

j

|x(n)∇wi

j

|x(m) E
v

vi2 j

i j =ij







E
v

vi2j

1  − N2 

∇w2ij 

n m=n

(68) (69)





1

=N2

E
B



3∇wij |x(n)∇wij |x(m) +

∇wi j |x(n)∇wi j |x(m) −

x(n)∈B x(m)∈B\{x(n)}

i j =ij

(70)





1 N2 

∇w2ij 

(71)

n m=n











1 =N2 

3 E [∇wij|x]2 +
x

E [∇wi
x

j

|x]2

−

1 N2



∇w2ij 

(72)

n m=n

i j =ij

n m=n











1 =N2 

3∇w2ij +

∇w2i

j



−

1 N2



∇w2ij 

(73)

n m=n

i j =ij

n m=n









1 =N2

2∇w2ij +

∇w2i j



=

N (N − 1) N2

∇w2ij

+

∇w2i j  .

(74)

n m=n

i j =ij

ij

19

Published as a conference paper at ICLR 2023

Lastly, we average the variance across all weight dimensions:

1

mVar(gw(wij)) = pq Var(gw(wij))

(75)

ij

1

= pq

{Z1 + Z2 + Z3}

(76)

ij

1

1

= pq

N

Var (∇wij|x) +
x

(77)

ij





1 N

∇w2ij

+

Vxar(∇wij |x)

+

∇w2i j + Vxar(∇wi j |x)  +

(78)

ij





N (N − N2

1)

∇w2ij

+

ij

∇w2i j

 



(79)

1

1

= pq

N

Var (∇wij|x) +
x

(80)

ij







1 N Vxar(∇wij|x) +
ij

Vxar(∇wi j |x) + ∇w2ij +
ij

∇w2i j

 



(81)

2

pq

= mVar (∇w) + mVar (∇w) + (pq + 1) mSqNorm(∇w)

(82)

N

N

pq + 2

=

V + (pq + 1)S.

(83)

N

If the perturbations are independent, we show that Z3 is 0.





1

Z3

=N2

E
B



Cov( gw(wij )| x(n), gw(wij )| x(m))
v

x(n)∈B x(m)∈B\{x(n)}

(84)





1

=N2

E
B



E
v

gw(wij )| x(n) gw(wij )| x(m)

−E
v

gw(wij )| x(n)

E
v

gw(wij )| x(m) 

x(n)∈B x(m)∈B\{x(n)}

(85)



1

=N2

E
B



E
v

x(n)∈B x(m)∈B\{x(n)}

gw(wij )| x(n) gw(wij )| x(m)

 − ∇wij |x(n)∇wij |x(m)
(86)





1

=N2

E
B



E 
v

x(n)∈B x(m)∈B\{x(n)}

j


∇wij |x(n)vi(nj)  vi(jn) 
j

 ∇wij |x(m)vi(mj ) vi(jm)
(87)

−∇wij |x(n)∇wij |x(m)

(88)





1

=N2

E
B



E
v

x(n)∈B x(m)∈B\{x(n)}

j


∇wij |x(n)∇wij |x(m)vi(nj) vi(mj) vi(jn)vi(jm) −
j
(89)





1

N2

E
B



∇wij |x(n)∇wij |x(m)

x(n)∈B x(m)∈B\{x(n)}

(90)

20

Published as a conference paper at ICLR 2023



1

=N2

E
B



E
v

∇wij |x(n)∇wij |x(m)vi(jn)2vi(jm)2+

x(n)∈B x(m)∈B\{x(n)}

(91)

∇wij |x(n)vi(jn)2vi(jm)

∇wij |x(m)vi(mj ) + ∇wij |x(m)vi(jm)2vi(jn)

∇wij |x(n)vi(nj) +

i j =j

i j =j

(92)

∇wij |x(m)∇wij |x(n)vi(mj )vi(nj) vi(jm)vi(jn)+
i j =ij


(93)

∇wi j |x(n)vi j ∇wij |x(m)vi(nj) vi(mj) vi(jn)vi(jm) −
i j =ij i j ∈/{ij,j j }

(94)







1

N2

E
x(n)

E
x(m)



n

∇wij |x(n)∇wij |x(m)
m=n

(95)



1

=N2

E
B



E
v

∇wij |x(n)∇wij |x(m)vi(jn)2vi(jm)2+

x(n)∈B x(m)∈B\{x(n)}

(96)



∇wi j |x(m)∇wi j |x(n)vi(mj )vi(nj) vi(jm)vi(jn) −
i j =ij

(97)







1

N2

E
x(n)

E
x(m)



n

∇wij |x(n)∇wij |x(m)
m=n

(98)





1

=N2

E
B



E
v

∇wij |x(n)∇wij |x(m)vi(jn)2vi(jm)2

−

x(n)∈B x(m)∈B\{x(n)}

(99)





1 N2 

∇w2ij 

n m=n

(100)





1

=N2

E
B



∇wij

|x(n)∇wij

|x(m)

E
v

vi(jn)2

E
v

vi(jm)2  −

x(n)∈B x(m)∈B\{x(n)}

(101)





1 N2 

∇w2ij 

n m=n

(102)









1

=N2

E
B



∇wij |x(n)∇wij |x(m)

−

1 N2



∇w2ij 

x(n)∈B x(m)∈B\{x(n)}

n m=n

(103)









1 =N2 

E
x

[∇wij |x]2 

−

1 N2



∇w2ij 

n m=n

n m=n

(104)

=0.

(105)

21

Published as a conference paper at ICLR 2023

Then the average variance becomes: 1
mVar(gw(wij)) = pq Var(gw(wij))
ij

1

= pq

{Z1 + Z2 + Z3}

ij

1 =
pq
ij

1

N

Var (∇wij|x) +
x



1 N

∇w2ij

+

Vxar(∇wij|x) +

ij

 ∇w2i j + Vxar(∇wi j |x) 

pq + 2

pq + 1

=

mVar (∇w) +

mSqNorm(∇w)

N

N

pq + 2 pq + 1

=

V+

S.

N

N

(106) (107) (108)
(109) (110) (111)

Proposition 4. Let p × q be the size of the weight matrix, the element-wise average variance of the

activity perturbed gradient estimator

with a batch size N

is

q+2 N

V

+ (q + 1)S

if

the perturbations

are shared across the batch, and

q+2 N

V

+

q+1 N

S

if

they

are

independent,

where

V

is the element-wise

average variance of the true gradient, and S is the element-wise average squared gradient.

Proof.

1

Z2

= N

E
x

Var ( ga(wij)| x)
u

 

 

1

= N

E Var 
xu

∇wij uj  uj 

j





1 =
N

E
x

Var
u

∇wij

u2j

+

∇wj uj uj 

j =j







1

= N

E Var
xu

∇wij u2j

+ Var 
u

∇wij uj uj  +

j =j





2

Cov
u

∇wij

u2j

,

∇wij uj uj 

j =j







1

= N

E Var
xu

∇wij u2j

+ Var 
u

∇wij uj uj  +

i j =ij









2E
u

∇wij ∇wij

u3j uj

−

2E
u

∇wij u2j

E
u

∇wij uj uj 

j =j

j =j







1 =
N

E
x

∇wi2j

Var
u

u2j

+ Var 
u

∇wij uj uj  +

j =j





2

∇wij ∇wij

E
u

u3j uj

− 2∇wij E
u

u2j



∇wij E [ujuj ]
u

j =j

j =j

(112) (113) (114) (115) (116) (117) (118) (119) (120)

22

Published as a conference paper at ICLR 2023







1 =
N

E
x

∇wi2j

Var
u

u2j

+ Var 
u

∇wij uj uj  +

j =j





(121)

2 ∇wij∇wij · 0 − 2∇wij · 1  ∇wi j · 0

j =j

j =j







1 =
N

E
x

∇wi2j

Var
u

u2j

+ Var 
u

∇wij uj uj 

j =j





1 =
N

E
x

∇wi2j

·

(Eu [u4j ]

−

Eu [u2j ]2)

+

Var
u

(∇wij

uj uj

)

j =j





1 =
N

E
x

∇wi2j

(3

Vuar(uj

)2

−

Eu [u2j ]2)

+

∇wj2

Var
u

(uj

uj

)

j =j

1

= N

E
x

2∇wi2j

+





1

N

E
x

∇wi2j

(Var[uj ]
u

+

E[uj
u

]2)(Var[uj
u

]

+

E[uj
u

]2)

−

E[uj ]2
u

E[uj
u

]2

j =j





2 =
N

∇w2ij

+

2 N

Vxar(∇wij |x)

+

1 N

E
x

∇wi2j Vuar(uj ) Vuar(uj )

j =j

2 =
N

∇w2ij

+

2 N

Var(∇wij |x)
x

+

1 N

E
x

∇wj2

j =j



1 =
N

∇w2ij

+

Var(∇wij |x)
x

+

j



∇w2ij

+ Var(∇wij |x)
x

.

(122) (123) (124) (125) (126) (127) (128) (129) (130)

Z3 is nonzero if the perturbations are shared within a batch. Assuming that the perturbations are

shared,





1

Z3

=N2

E
B



Cov(
u

ga(wij

)|

x(n),

ga(wij )|

x(m))

x(n)∈B x(m)∈B\{x(n)}

(131)





1

=N2

E
B



E ga(wij )| x(n) ga(wij )| x(m) − E ga(wij )| x(n) E ga(wij )| x(m) 

u

u

u

x(n)∈B x(m)∈B\{x(n)}

(132)



1

=N2

E
B



E
u

x(n)∈B x(m)∈B\{x(n)}

ga(wij )| x(n) ga(wij )| x(m)

 − ∇wij |x(n)∇wij |x(m)
(133)





1

=N2

E
B



E 
u

x(n)∈B x(m)∈B\{x(n)}

j


∇wij |x(n)uj  uj 
j

 ∇wij |x(m)uj  uj  −
(134)

∇wij |x(n)∇wij |x(m)

(135)

23

Published as a conference paper at ICLR 2023





1

=N2

E
B



E
u

x(n)∈B x(m)∈B\{x(n)}

j


∇wij |x(n)∇wij |x(m)uj uj u2j  −
j





1

N2

E
B



∇wij |x(n)∇wij |x(m)

x(n)∈B x(m)∈B\{x(n)}



1

=N2

E
B



E
u

∇wij |x(n)∇wij |x(m)vi4j +

x(n)∈B x(m)∈B\{x(n)}

(136) (137) (138)

∇wij |x(n)u3j

∇wij |x(m)uj + ∇wij |x(m)u3j

∇wij |x(n)uj +

j =j

j =j

(139)

∇wij |x(m)∇wij |x(n)u2j u2j +
j =j


(140)

∇wij |x(n)uj ∇wij |x(m)uj uj u2j  −
j =j j ∈/{j,j }







1

N2

E
x(n)

E
x(m)



n

∇wij |x(n)∇wij |x(m)
m=n

(141) (142)







1

=N2

E
B



E
u

∇wij |x(n)∇wij |x(m)vi4j

+

∇wij |x(m)∇wij |x(n)u2j u2j  −

x(n)∈B x(m)∈B\{x(n)}

j =j

(143)







1

N2

E
x(n)

E
x(m)



n

∇wij |x(n)∇wij |x(m)
m=n

(144)







1

=N2

E
B



E
u

∇wij |x(n)∇wij |x(m)vi4j

+

∇wij |x(m)∇wij |x(n)u2j u2j  −

x(n)∈B x(m)∈B\{x(n)}

j =j

(145)





1 N2 

∇w2ij 

n m=n

(146)



1

=N2

E
B



∇wij

|x(n)

∇wij

|x(m)

E
v

vi4j

x(n)∈B x(m)∈B\{x(n)}





1 N2 

∇w2ij 

n m=n

+

∇wij

|x(n)∇wij

|x(m) E
u

u2j

j =j

(147)

(148)



E
u

u2j

−





1

=N2

E
B



3∇wij |x(n)∇wij |x(m) +

∇wij |x(n)∇wij |x(m) −

x(n)∈B x(m)∈B\{x(n)}

j =j





1 N2 

∇w2ij 

n m=n

(149) (150)

24

Published as a conference paper at ICLR 2023











1 =N2 

3

E
x

[∇wij

|x]2

+

E
x

[∇wij

|x]2

−

1 N2



∇w2ij 

n m=n

j =j

n m=n











1 =N2 

2∇w2ij +

∇w2ij



−

1 N2



∇w2ij 

n m=n

j =j

n m=n







1 =N2 

∇w2ij +

∇w2ij 

n m=n

j =j





=

N (N − N2

1)

∇w2ij

+

∇w2ij  .

j =j

(151) (152) (153) (154)

Then we compute the average variance across all weight dimensions (for shared perturbation):

1 mVar(ga(wij)) = pq Var(ga(wij))
ij

(155)

1

= pq

{Z1 + Z2 + Z3}

ij

(156)

1 =
pq
ij

1

N

Var
x

(∇wij

|x

)

+

(157)



1 N

∇w2ij

+

Vxar(∇wij |x)

+

j

 ∇w2ij + Vxar(∇wij |x)  +

(158)





N (N − N2

1)

∇w2ij

+

∇w2ij 

j =j

(159)

1 =
pq
ij

1

N

Var (∇wij|x) +
x







1 N Vxar(∇wij|x) +
j

Vxar(∇wij |x) + ∇w2ij +
j

∇w2ij

 



2

q

= mVar (∇w) + mVar (∇w) + (q + 1) mSqNorm(∇w)

N

N

q+2 = V + (q + 1)S.
N

(160)
(161) (162) (163)

If the perturbations are independent, we show that Z3 is 0.





1

Z3

=N2

E
B



Cov( ga(wij )| x(n), ga(wij )| x(m))
u

x(n)∈B x(m)∈B\{x(n)}

(164)





1

=N2

E
B



E ga(wij )| x(n) ga(wij )| x(m) − E ga(wij )| x(n) E ga(wij )| x(m) 

u

u

u

x(n)∈B x(m)∈B\{x(n)}

(165)

25

Published as a conference paper at ICLR 2023



1

=N2

E
B



E
u

x(n)∈B x(m)∈B\{x(n)}

ga(wij )| x(n) ga(wij )| x(m)

 − ∇wij |x(n)∇wij |x(m)

(166)





1

=N2

E
B



E 
u

x(n)∈B x(m)∈B\{x(n)}

j


∇wij |x(n)u(jn) u(jn) 
j

 ∇wij |x(m)u(jm) u(jm) −
(167)

∇wij |x(n)∇wij |x(m)

(168)





1

=N2

E
B



E
u

x(n)∈B x(m)∈B\{x(n)}

j


∇wij |x(n)∇wij |x(m)u(jn)u(jm)u(jn)u(jm) − (169)
j





1

N2

E
B



∇wij |x(n)∇wij |x(m)

x(n)∈B x(m)∈B\{x(n)}

(170)



1

=N2

E
B



E
u

∇wij |x(n)∇wij |x(m)u(jn)2u(jm)2+

x(n)∈B x(m)∈B\{x(n)}

(171)

∇wij |x(n)u(jn)2u(jm)

∇wij |x(m)u(jm) + ∇wij |x(m)u(jm)2u(jn)

∇wij |x(n)u(jn)+

j =j

j =j

(172)

∇wij |x(m)∇wij |x(n)u(jm)u(jn)u(jm)u(jn)+
j =j

(173)



∇wij |x(n)uj ∇wij |x(m)u(jn)u(jm)u(jn)u(jm) −
j =j j ∈/{j,j }

(174)







1

N2

E
x(n)

E
x(m)



n

∇wij |x(n)∇wij |x(m)
m=n

(175)



1

=N2

E
B



E
u

∇wij |x(n)∇wij |x(m)u(jn)2u(jm)2+

x(n)∈B x(m)∈B\{x(n)}

(176)



∇wij |x(m)∇wij |x(n)u(jm)u(jn)u(jm)u(jn) −
j =j

(177)







1

N2

E
x(n)

E
x(m)



n

∇wij |x(n)∇wij |x(m)
m=n

(178)





1

=N2

E
B



E
u

∇wij |x(n)∇wij |x(m)u(jn)2u(jm)2

−

x(n)∈B x(m)∈B\{x(n)}

(179)





1 N2 

∇w2ij 

n m=n

(180)





1

=N2

E
B



∇wij |x(n)∇wij |x(m) E
u

u(jn)2

E
u

u(jm)2  −

x(n)∈B x(m)∈B\{x(n)}

(181)





1 N2 

∇w2ij 

n m=n

(182)

26

Published as a conference paper at ICLR 2023









1

=N2

E
B

∇wij |x(n)∇wij |x(m)

−

1 N2



∇w2ij 

x(n)∈B x(m)∈B\{x(n)}

n m=n









1 =N2 

E
x

[∇wij |x]2 

−

1 N2



∇w2ij 

n m=n

n m=n

=0.

Then the average variance becomes: 1
mVar(ga(wij)) = pq Var(ga(wij))
ij

1

= pq

{Z1 + Z2 + Z3}

ij

1 =
pq
ij

1

N

Var (∇wij|x) +
x



1 N

∇w2ij

+

Vxar(∇wij |x)

+

j



∇w2ij

+ Vxar(∇wij |x)

 



q+2

q+1

=

mVar (∇w) +

mSqNorm(∇w)

N

N

q+2 q+1

= V+

S.

N

N

(183)
(184) (185) (186) (187) (188) (189) (190) (191)

wt perturb ind wt perturb act perturb backprop wt perturb (theory) ind wt perturb (theory) act perturb (theory) backprop (theory)

Gradient Variance

102 101 100 10 1 10 2
100 101 102 103 N (Batch Size) (p=4, q=4)

102

101

100

100

101

102

p (Fan In) (N=4, q=4)

105 104 103 102 101 100

100

101

102

q (Fan Out) (N=4, p=4)

Figure 8: Numerical veriﬁcation of the theoretical variance properties

10 NUMERICAL SIMULATION OF VARIANCES

In Figure 8, we ran numerical simulation experiments to verify our analytical variance properties.

We used a multi-layer network with 4 input units, 4 hidden units, 1 output unit, a tanh activation

function, and the mean squared error loss. We varied the batch size (N ) between 1 and 4096. We

tested the gradient estimator of the ﬁrst layer weights using 5000 random samples. We also calculated

the theoretical variance by applying the gradient norm and gradient variance constants found by

backprop, from 5000 mini-batch true gradients. We then ﬁxed the batch size to be 4 and vary the

number of input units (p, fan in) and the number of hidden units (q, fan out) between 1 and 256. The

theoretical variance for backprop was only computed for the batch size experiment since it is an

inverse

relationship

(

1 N

),

but

for

fan

in

and

fan

out,

we

do

not

aim

to

analyze

the

theoretical

variances

here. “wt perturb” stands for weight perturbation with shared noise; “ind wt perturb” stands for

weight perturbation with independent noise; and “act perturb” stands for activity perturbation with

independent noise. Note that indepedent weight perturbation is much more costly to compute in

27

Published as a conference paper at ICLR 2023
neural networks. As shown in the ﬁgure, the empirical variances match very well with our theoretical predictions.
11 TRAINING DETAILS
Here we provide more training details.
MNIST. We use a batch size of 128, and the SGD optimizer with learning rate 0.01 and momentum 0.9 for a total of 1000 epochs with no data augmentation and a linear learning rate decay schedule.
CIFAR-10. For the supervised experiments, we use a batch size of 128 and the SGD optimizer with learning rate 0.01 and momentum 0.9 for a total of 200 epochs with no data augmentation and a linear learning rate decay schedule. For the contrastive M/8 experiments, we use a batch size of 512 and the SGD optimizer with learning rate 1.0 and momentum 0.9 for a total of 1000 epochs with BYOL data augmentation using area crop lower bound to be 0.5 and a cosine decay schedule with a warm-up period of 10 epochs. For the contrastive L/8 experiments, we use a batch size of 2048 and the SGD optimizer with learning rate 4.0 and momentum 0.9 for a total of 1000 epochs with BYOL data augmentation (Grill et al., 2020) using area crop lower bound to be 0.3 and a cosine decay schedule with a warm-up period of 10 epochs.
ImageNet. For the supervised experiments, we use a batch size of 256 and the SGD optimizer with learning rate 0.05 and momentum 0.9 for a total of 120 epochs with BYOL data augmentation (Grill et al., 2020) using area crop lower bound to be 0.3 and a cosine learning rate decay schedule with a warm-up period of 10 epochs. For the contrastive experiments, we use a batch size of 2048 and the LARS optimizer with learning rate 0.1 and momentum 0.9 for a total of 800 epochs with BYOL data augmentation (Grill et al., 2020) using area crop lower bound to be 0.08 and a cosine learning rate decay schedule with a warm-up period of 10 epochs.
12 FUSED JVP/VJP DETAILS
In Algorithm 1, we provide a JAX code snippet implementing fused operators for the supervised cross entropy loss. “Fused” here means that we package several operations into one function. In the supervised cross entropy loss, we combine average pooling, channel concatenation, a linear classiﬁer layer, and cross entropy all together. Key steps and expected tensor shapes are annotated in the comments. The fused InfoNCE loss implementation will be included in our full code release.
13 LOCALMIXER ARCHITECTURE
In Algorithm 2, we provide code in JAX style that implements our proposed LocalMixer architecture.
14 ADDITIONAL RESULTS
In this section we provide additional experimental results.
Normalization scheme. Table 5 compares different normalization schemes. Layer normalization (LN) is often better than batch normalization (BN) on our mixer architecture. Local LN is better on contrastive learning experiments and achieves lower error rates using forward gradient learning. Although in our main paper, backprop were used in normalization layers, backprop is not necessary for Local LN, c.f . “NG” (No Gradient) columns in Table 5.
Place of normalization. We investigate the places where we add normalization layers. Traditionally, normalization is added after linear layers. In MLPMixer, LN is added at the beginning of each block. With our forward gradient learning, it is now a question of which location is the optimal design. Adding it after the linear layer has the advantage of shaping the activations to be more well behaved, which can make perturbation learning more effective. Adding it before the linear layer can also help reduce the variance since the inputs always get multiplied with the gradient of the output activity. The results are reported in Table 6. Adding normalization both before and after the linear layer helps forward gradient to achieve lower training errors. While this could result in some overﬁtting on supervised learning, it is good for contrastive learning which needs more model capacity. This is reasonable as forward gradient introduce a lot of variances, and more normalization layers help achieve better training performance.
28

Published as a conference paper at ICLR 2023
Algorithm 1 Naïve and fused local cross entropy, with custom JVP and VJP operators.
# N: batch size; P: num patches; G: num grps; C: num channels; D: channels / grp; K: num cls # x: encoder features [N,P,G,C/G] # w: classifier weights [C,K]; b: classifier bias [K] # labels: class labels [N,K] import jax import jax.numpy as jnp from jax.scipy.special import logsumexp
def naive_avg_group_linear_xent(x, w, b, labels): N, P, G, _ = x.shape # Average pooling, with stop gradients. [N,P,G,C/G] -> [N,1,G,C/G] avg_pool_p = jnp.mean(x, axis=1, keepdims=True) x_div_p = x / float(P) # [N,P,G,C/G] x = x_div_p + jax.lax.stop_gradient(avg_pool_p - x_div_p) # Concatenate everything, with stop gradients. [N,P,G,C] -> [N,P,G,G,C/G] x = jnp.tile(jnp.reshape(x, [N, P, 1, G, -1]), [1, 1, G, 1, 1]) mask = jnp.eye(G)[None, None, :, :, None] x = mask * x + jax.lax.stop_gradient((1.0 - mask) * x) # [N,P,G,G,C/G] -> [N,P,G,C] x = jnp.reshape(x, [N, P, G, -1]) logits = jnp.einsum(’npgc,cd->npgd’, x, w) + b logits = logits - logsumexp(logits, axis=-1, keepdims=True) loss = -jnp.sum(logits * labels[:, None, None, :], axis=-1) return loss
def fused_avg_group_linear_xent(x, w, b, labels): # This is for forward pass. The numerical value of each local loss should be the same. # So we compute one and replicate it many times. N, P, G, _ = x.shape # [N,P,G,C/G] -> [N,G,C/G] x_avg = jnp.mean(x, axis=1) # [N,G,C/G] -> [N,C] x_grp = jnp.reshape(x_avg, [x_avg.shape[0], -1]) # [N,C] -> [N,K] logits = jnp.einsum(’nc,ck->nk’, x_grp, w) + b logits = logits - logsumexp(logits, axis=-1, keepdims=True) loss = -jnp.sum(logits * labels, axis=-1) # Key step: after computing the loss, replicate it for PxG times. [N] -> [N,P,G] return jnp.tile(jnp.reshape(loss, [N, 1, 1]), [1, P, G])
def fused_avg_group_linear_xent_jvp(primals, tangents): # This JVP operator performs both regular forward pass and the forward autodiff. x, w, b, labels = primals dx, dw, db, dlabels = tangents N, P, G, D = x.shape dx_avg = dx / float(P) # Reshape the classifier weights, since only one group passes gradient at a time. w_ = jnp.reshape(w, [G, D, -1]) b = jnp.reshape(b, [-1]) # Regular forward pass # [N,P,G,C/G] -> [N,G,C/G] x_avg = jnp.mean(x, axis=1) # [N,G,C/G] -> [N,C] x_grp = jnp.reshape(x_avg, [x_avg.shape[0], -1]) # [N,C] -> [N,K] logits = jnp.einsum(’nd,dk->nk’, x_grp, w) + b logits = logits - logsumexp(logits, axis=-1, keepdims=True) loss = -jnp.sum(logits * labels, axis=-1) # We can compute the gradient through cross entropy first. dlogits_bwd = jax.nn.softmax(logits, axis=-1) - labels # [N,K] # Key step: dloss = dx * w * dloss/dlogit + (x * dw + db) * dloss/dlogit # Do the einsum together to avoid replicating outputs. dloss = jnp.einsum(’npgd,gdk,nk->npg’, dx_avg, w_, dlogits_bwd) + jnp.einsum(’nk,nk->n’, (jnp.einsum(’nc,ck->nk’, x_grp, dw) + db), dlogits_bwd)[:, None, None] # [N,P,G] # Return loss and loss gradients [N,P,G]. return jnp.tile(jnp.reshape(loss, [N, 1, 1]), [1, P, G]), dloss
def fused_avg_group_linear_xent_vjp(res, g): # This is a fused backprop (VJP) operator. x, w, logits, labels = res N, P, G, D = x.shape x_avg = jnp.mean(x, axis=1) x_grp = jnp.reshape(x_avg, [x_avg.shape[0], -1]) # Key step: only the first patch/group gradients since everything is the same. g_ = g[:, 0:1, 0] dlogits = g_ * (jax.nn.softmax(logits, axis=-1) - labels) # [N,K] # Remember to multiply gradients by PG times due to weight sharing. db = jnp.reshape(jnp.sum(dlogits, axis=[0]), [-1]) * float(P * G) dw = jnp.reshape(jnp.einsum(’nc,nk->ck’, x_grp, dlogits), [G * D, -1]) * float(P * G) # Key step: use grouped weights to perform backprop. dx = jnp.einsum(’nd,gcd->ngc’, dlogits, jnp.reshape(w, [G, C, -1])) / float(P) # Broadcast gradients across patches. dx = jnp.tile(dx[:, None, :, :], [1, P, 1, 1]) return dx, dw, db, None
29

Published as a conference paper at ICLR 2023
Algorithm 2 A LocalMixer architecture implemented with JAX style code.
import jax import jax.numpy as jnp
def linear(x, w, b): """Linear layer.""" return jnp.einsum(’npc,cd->npd’, x, w) + b
def group_linear(x, w, b): """Linear layer with groups.""" return jnp.einsum(’npgc,gcd->npgd’, x, w) + b
def normalize(x, axis=-1, eps=1e-5): """Normalization layer.""" mean = jnp.mean(x, axis=axis, keepdims=True) mean_of_squares = jnp.mean(jnp.square(x), axis=axis, keepdims=True) var = mean_of_squares - jnp.square(mean) inv = jax.lax.rsqrt(var + eps) y = (x - mean) * inv return y
def block0(x, params): """Initial block with only channel mixing.""" N, P, _ = x.shape G = num_groups x = normalize(x) x = linear(x, params[0][0], params[0][1]) x = normalize(x) x = jax.nn.relu(x) x = jnp.reshape(x, [N, P, G, -1]) x = normalize(x) x = group_linear(x, params[1][0], params[1][1]) x = normalize(x) x = jax.nn.relu(x) return x
def mlp_block(x, params): """Regular MLP block with token & channel mixing.""" N, P, G, _ = x.shape inputs = x # Token mixing. x = jnp.reshape(x, [N, P, -1]) x = normalize(x) x = jnp.swapaxes(x, 1, 2) x = linear(x, params[0][0], params[0][1]) x = jnp.swapaxes(x, 1, 2) x = normalize(x) x = jax.nn.relu(x)
# Channel mixing. x = normalize(x) x = linear(x, params[1][0], params[1][1]) x = normalize_layer(x) x = jax.nn.relu(x) x = jnp.reshape(x, [N, P, G, -1]) x = normalize(x) x = group_linear(x, params[2][0], params[2][1]) x = normalize(x) x = x + inputs x = jax.nn.relu(x) return x
def local_mixer(x, params): """LocalMixer.""" x = preprocess(x, image_mean, image_std, num_patches) pred_local = [] # Local predictions. # Build network blocks. for blk in range(num_blocks): if blk == 0: x = block0(x, params[f’block_{blk}’]) else: x = mlp_block(x, params[f’block_{blk}’])
# Projector connects to local losses. x_proj = normalize(x) pred_local.append(linear(x_proj, params[f’proj_{blk}’][0], params[f’proj_{blk}’][1]))
# Disconnect gradients. x = jax.lax.stop_gradient(x) x = jnp.reshape(x, [x.shape[0], x.shape[1], -1]) x = jnp.mean(x, axis=1) # [N,C] x = normalize(x) pred = linear(x, params[’classifier’][0], params[’classifier’][1]) return pred, pred_local
30

Published as a conference paper at ICLR 2023

BN LN Local LN

BP
30.38 / 0.00 30.55 / 0.00 32.89 / 0.00

Supervised M/8/16

LG-BP

LG-FG-A

33.41 / 5.41 32.84 / 23.09 33.17 / 7.09 29.03 / 17.63 33.84 / 0.05 30.68 / 19.39

LG-FG-A (NG)
33.48 / 20.80 29.33 / 19.26 30.44 / 17.12

BP
27.56 / 24.39 23.52 / 20.71 23.24 / 21.03

Contrastive M/8/16

LG-BP

LG-FG-A

30.27 / 28.03 35.47 / 32.71 27.41 / 24.73 34.21 / 31.38 28.42 / 25.20 32.89 / 31.01

LG-FG-A (NG)
37.41 / 31.52 36.24 / 34.12 32.25 / 30.17

Table 5: Comparing different normalization schemes. NG=No normalization gradient. CIFAR-10 test / train error (%)

LN
Begin Block Before Linear After Linear Before + After Linear

Supervised M/8/16

BP

LG-BP

LG-FG-A

31.43 / 0.00 32.50 / 0.00 30.38 / 0.00 33.62 / 0.00

34.73 / 1.45 34.15 / 0.05 29.83 / 0.44 33.84 / 0.05

34.89 / 31.11 33.88 / 27.94 29.35 / 23.19 30.68 / 19.39

Contrastive M/8/16

BP

LG-BP

LG-FG-A

23.27 / 20.69 22.62 / 20.38 26.50 / 23.98 23.24 / 21.03

25.82 / 22.96 NaN / NaN 28.97 / 26.67 28.42 / 25.20

89.93 / 90.71 35.01 / 32.91 34.10 / 33.18 32.89 / 31.01

Table 6: Place of LayerNorm on CIFAR-10, test / train error. (%)

Error Rate (%) Error Rate (%)

40

BP Train

45

BP Train

LG-BP Train

LG-BP Train

30

LG-FG-A Train

LG-FG-A Train

BP Test

35

BP Test

20

LG-BP Test

LG-BP Test

LG-FG-A Test 25

LG-FG-A Test

10

0 1

2

4

8 16 32 64

Number of Groups

(a) CIFAR-10 Supervised M/8/*

15 1

2

4

8 16 32 64

Number of Groups

(b) CIFAR-10 Contrastive M/8/*

Figure 9: Effect of groups. For BP algorithms, groups has a minor effect on the ﬁnal performance, but for local forward gradient, it signiﬁcantly reduces the variance and achieves lower error rate on both training and test sets.

Effect of groups. We provide additional results summarizing the training and test performance of adding more groups in Figure 9. Backprop and local greedy backprop always achieve zero training error with increasing number of groups on CIFAR-10 supervised, but adding groups has a signiﬁcant beneﬁt lowering training errors for forward gradient. This suggests that the main opponent here is still the gradient estimation variance, and lowering training errors can generally make test errors lower too; on the other hand adding groups have negligible effect on backprop. For contrastive learning, here the task requires higher model capacity, and adding groups effectively reduce the model capacity by introducing sparsity in the weight matrix. As a result, we observe a slight drop of less than 5% performance on both backprop and local greedy backprop. By contrast, forward gradient gains over 10% of performance by adding 16 groups.

31

