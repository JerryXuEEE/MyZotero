Article

Linking Connectivity, Dynamics, and Computations in Low-Rank Recurrent Neural Networks

Highlights
d We study network models characterized by minimal connectivity structures

Authors
Francesca Mastrogiuseppe, Srdjan Ostojic

d For such models, low-dimensional dynamics can be directly inferred from connectivity
d Computations emerge from distributed and mixed representations
d Implementing speciﬁc tasks yields predictions linking connectivity and computations

Correspondence
srdjan.ostojic@ens.fr
In Brief
Neural recordings show that cortical computations rely on low-dimensional dynamics over distributed representations. How are these generated by the underlying connectivity? Mastrogiuseppe et al. use a theoretical approach to infer lowdimensional dynamics and computations from connectivity and produce predictions linking connectivity and functional properties of neurons.

Mastrogiuseppe & Ostojic, 2018, Neuron 99, 609–623 August 8, 2018 ª 2018 Elsevier Inc. https://doi.org/10.1016/j.neuron.2018.07.003

Neuron
Article

Linking Connectivity, Dynamics, and Computations in Low-Rank Recurrent Neural Networks
Francesca Mastrogiuseppe1,2 and Srdjan Ostojic1,3,* 1Laboratoire de Neurosciences Cognitives, INSERM U960, E´ cole Normale Supe´ rieure - PSL Research University, 75005 Paris, France 2Laboratoire de Physique Statistique, CNRS UMR 8550, E´ cole Normale Supe´ rieure - PSL Research University, 75005 Paris, France 3Lead Contact *Correspondence: srdjan.ostojic@ens.fr https://doi.org/10.1016/j.neuron.2018.07.003

SUMMARY
Large-scale neural recordings have established that the transformation of sensory stimuli into motor outputs relies on low-dimensional dynamics at the population level, while individual neurons exhibit complex selectivity. Understanding how low-dimensional computations on mixed, distributed representations emerge from the structure of the recurrent connectivity and inputs to cortical networks is a major challenge. Here, we study a class of recurrent network models in which the connectivity is a sum of a random part and a minimal, low-dimensional structure. We show that, in such networks, the dynamics are low dimensional and can be directly inferred from connectivity using a geometrical approach. We exploit this understanding to determine minimal connectivity required to implement speciﬁc computations and ﬁnd that the dynamical range and computational capacity quickly increase with the dimensionality of the connectivity structure. This framework produces testable experimental predictions for the relationship between connectivity, low-dimensional dynamics, and computational features of recorded neurons.
INTRODUCTION
Understanding the relationship between synaptic connectivity, neural activity, and behavior is a central endeavor of neuroscience. Networks of neurons encode incoming stimuli in terms of electrical activity and transform this information into decisions and motor actions through synaptic interactions, thus implementing computations that underlie behavior. Reaching a simple, mechanistic grasp of the relation between connectivity, activity, and behavior is, however, highly challenging. Cortical networks, which are believed to constitute the fundamental computational units in the mammalian brain, consist of thousands of neurons that are highly inter-connected

through recurrent synapses. Even if one were able to experimentally record the activity of every neuron and the strength of each synapse in a behaving animal, understanding the causal relationships between these quantities would remain a daunting challenge because an appropriate conceptual framework is currently lacking (Gao and Ganguli, 2015). Simpliﬁed, computational models of neural networks provide a test bed for developing such a framework. In computational models and trained artiﬁcial neural networks, the strengths of all synapses and the activity of all neurons are known, yet an understanding of the relation between connectivity, dynamics, and input-output computations has been achieved only in very speciﬁc cases (e.g., Hopﬁeld (1982); Ben-Yishai et al. (1995); Wang (2002)).
One of the most popular and best-studied classes of network models is based on fully random recurrent connectivity (Sompolinsky et al., 1988; Brunel, 2000; van Vreeswijk and Sompolinsky, 1996). Such networks display internally generated irregular activity that closely resembles spontaneous cortical patterns recorded in vivo (Shadlen and Newsome, 1998). However, randomly connected recurrent networks display only very stereotyped responses to external inputs (Rajan et al., 2010), can implement only a limited range of input-output computations, and their spontaneous dynamics are typically high dimensional (Williamson et al., 2016). To implement more elaborate computations and low-dimensional dynamics, classical network models rely instead on highly structured connectivity, in which every neuron belongs to a distinct cluster and is selective to only one feature of the task (e.g., Wang (2002); Amit and Brunel (1997); Litwin-Kumar and Doiron (2012)). Actual cortical connectivity appears to be neither fully random nor fully structured (Harris and Mrsic-Flogel, 2013), and the activity of individual neurons displays a similar mixture of stereotypy and disorder (Rigotti et al., 2013; Mante et al., 2013; Churchland and Shenoy, 2007). To take these observations into account and implement generalpurpose computations, a large variety of functional approaches have been developed for training recurrent networks and designing appropriate connectivity matrices (Hopﬁeld, 1982; Jaeger and Haas, 2004; Maass et al., 2007; Sussillo and Abbott, 2009; Eliasmith and Anderson, 2004; Boerlin et al., 2013; Pascanu et al., 2013; Martens and Sutskever, 2011). A uniﬁed conceptual picture of how connectivity determines dynamics and

Neuron 99, 609–623, August 8, 2018 ª 2018 Elsevier Inc. 609

computations is, however, currently missing (Barak, 2017; Sussillo, 2014).
Remarkably, albeit developed independently and motivated by different goals, several of the functional approaches for designing connectivity appear to have reached similar solutions (Hopﬁeld, 1982; Jaeger and Haas, 2004; Sussillo and Abbott, 2009; Eliasmith and Anderson, 2004; Boerlin et al., 2013), in which the implemented computations do not determine every single entry in the connectivity matrix but instead rely on a speciﬁc type of minimal, low-dimensional structure, so that in mathematical terms the obtained connectivity matrices are low rank. In classical Hopﬁeld networks (Hopﬁeld, 1982; Amit et al., 1985), a rank-one term is added to the connectivity matrix for every item to be memorized, and each of these terms ﬁxes a single dimension, i.e., row/column combination, of the connectivity matrix. In echo state (Jaeger and Haas, 2004; Maass et al., 2007) and FORCE learning (Sussillo and Abbott, 2009), and similarly within the Neural Engineering Framework (Eliasmith and Anderson, 2004), computations are implemented through feedback loops from readout units to the bulk of the network. Each feedback loop is mathematically equivalent to adding a rank-one component and ﬁxing a single row/column combination of the otherwise random connectivity matrix. In the predictive spiking theory (Boerlin et al., 2013), the requirement that information is represented efﬁciently leads again to a connectivity matrix with similar low-rank form. Taken together, the results of these studies suggest that a minimal, low-rank structure added on top of random recurrent connectivity may provide a general and unifying framework for implementing computations in recurrent networks.
Based on this observation, here we study a class of recurrent networks in which the connectivity is a sum of a structured, low-rank part and a random part. We show that in such networks, both spontaneous and stimulus-evoked activity are low-dimensional and can be predicted from the geometrical relationship between a small number of high-dimensional vectors that represent the connectivity structure and the feedforward inputs. This understanding of the relationship between connectivity and network dynamics allows us to directly design minimal, low-rank connectivity structures that implement speciﬁc computations. We focus on four tasks of increasing complexity, starting with basic binary discrimination and ending with context-dependent evidence integration (Mante et al., 2013). We ﬁnd that the dynamical repertoire of the network increases quickly with the dimensionality of the connectivity structure, so that rank-two connectivity structures are already sufﬁcient to implement complex, context-dependent tasks (Mante et al., 2013; Saez et al., 2015). For each task, we illustrate the relationship between connectivity, low-dimensional dynamics, and the performed computation. In particular, our framework naturally captures the ubiquitous observation that single-neuron responses are highly heterogeneous and mixed (Rigotti et al., 2013; Mante et al., 2013; Churchland and Shenoy, 2007; Machens et al., 2010), while the dimensionality of the dynamics underlying computations is low and increases with task complexity (Gao and Ganguli, 2015). Crucially, for each task, our framework produces experimentally testable predictions that directly relate connectivity, the dominant di-

mensions of the dynamics, and the computational features of individual neurons.

RESULTS

We studied a class of models that we call low-rank recurrent networks. In these networks, the connectivity matrix was given by a sum of an uncontrolled, random matrix and a structured, controlled matrix P. The structured matrix P was low-rank, i.e., it consisted only of a small number of independent rows and columns, and its entries were assumed to be weak (of order 1=N, where N is the number of units in the network). We considered P moreover to be ﬁxed and known, and uncorrelated with the random part gc, which was considered unknown except for its statistics (mean 0, variance g2=N). As in classical models, the networks consisted of N ﬁring rate units with a sigmoid inputoutput transfer function (Sompolinsky et al., 1988; Sussillo and Abbott, 2009):

XN x_iðtÞ = À xiðtÞ + JijfðxjðtÞÞ + Ii;
j=1

(Equation 1)

where xiðtÞ is the total input current to unit i, Jij = gcij + Pij is the connectivity matrix, fðxÞ = tanhðxÞ is the current-to-rate transfer function, and Ii is the external, feedforward input to unit i.
To connect with the previous literature and introduce the methods that underlie our results, we start by describing the spontaneous dynamics ðIi = 0Þ in a network with a unit-rank structure P. We then turn to the response to external inputs, the core of our results that we exploit to demonstrate how low-rank networks can implement four tasks of increasing complexity.

One-Dimensional Spontaneous Activity in Networks with Unit-Rank Structure We started with the simplest possible type of low-dimensional connectivity, a matrix P with unit rank (Figure 1A). Such a matrix is speciﬁed by two N-dimensional vectors m = fmig and n = fnjg, which fully determine all its entries. Every column in this matrix is a multiple of the vector m, and every row is a multiple of the vector n, so that the individual entries are given by

Pij

=

mi nj N

:

(Equation 2)

We will call m and n, respectively, the right- and left-connectivity vectors (as they correspond to the right and left eigenvectors of the matrix P, see STAR Methods), and we consider them arbitrary but ﬁxed and uncorrelated with the random part of the connectivity. As we will show, the spontaneous network dynamics can be directly understood from the geometrical arrangement of the vectors m and n.
In absence of structured connectivity, the dynamics are determined by the strength g of the random connectivity: for g < 1, the activity in absence of inputs decays to zero, while for g > 1 it displays strong, chaotic ﬂuctuations (Sompolinsky et al., 1988). Our ﬁrst aim was to understand how the interplay between the ﬁxed,

610 Neuron 99, 609–623, August 8, 2018

A

Stationary

Chaotic

C

B

Struct. stationary

Struct. chaot.

D

Hom.

0

chaotic

Figure 1. Spontaneous Activity in Random Networks with Unit-Rank Connectivity Structure (A) The recurrent network model, whose connectivity matrix consists of the sum of a random (gray) and of a structured unit-rank (colored) component. (B) Left: dynamical regimes of the network activity as function of the structure connectivity strength mT n=N and the random strength g. Gray areas: bistable activity; red: chaotic activity. Side panels: samples of dynamics from ﬁnite networks simulations (parameters indicated by colored dots in the phase diagram). (C and D) Activity statistics as the random strength g is increased, and the structure strength is ﬁxed to 2.2 (dashed line in B). (C) Activity along the vector m, as quantiﬁed by k (Equation 3). Blue (resp. red) lines: theoretical prediction for stationary (resp. chaotic) dynamics. (D) Activity variance due to random connectivity. Blue and pink lines: static heterogeneity; red: temporal variance that quantiﬁes chaotic activity. Dots: simulations of ﬁnite-size networks. See STAR Methods for details.

low-rank part and the random part of the connectivity shapes the spontaneous activity in the network.
Our analysis of network dynamics relies on an effective, statistical description that can be mathematically derived if the network is large and the low-dimensional part of the connectivity is weak (i.e., if Pij scales inversely with the number of units N in the network as in Equation 2). Under those assumptions, the activity of each unit can be described in terms of the mean and variance of the total input it receives. Dynamical equations for these quantities can be derived by extending the classical dynamical mean-ﬁeld theory (Sompolinsky et al., 1988). This theory effectively leads to a low-dimensional description of network dynamics in terms of equations for a couple of macroscopic quantities. Full details of the analysis are provided in the STAR Methods; here, we focus only on the main results.
The central ingredient of the theory is an equation for the average equilibrium input mi to unit i:

1 XN Â Ã mi = kmi; where k = N j = 1 nj fj :

(Equation 3)

The scalar quantity k represents the overlap between the left-
connectivity vector n and the N-dimensional vector ½f = f½fjg that describes the mean ﬁring activity of the network (½fj is the ﬁring rate of unit j averaged over different realizations of the

random component of the connectivity, and depends implicitly

on k). The overlap k therefore quantiﬁes the degree of structure

along the vector n in the activity of the network. If k > 0, the equi-

librium activity of each neuron is correlated with the correspond-

ing component of the vector n, while k = 0 implies no such struc-

ture is present. The overlap k is the key macroscopic quantity

describing the network dynamics, and our theory provides equa-

tions specifying its dependence on network parameters.

If one represents the network activity as a point in the NÀ

dimensional state space where every dimension corresponds

to the activity of a single unit, Equation 3 shows that the

structured part of the connectivity induces a one-dimensional

organization of the spontaneous activity along the vector m.

This one-dimensional organization, however, emerges only if

the overlap k does not vanish. As the activity of the network is

organized along the vector m, and k quantiﬁes the projection

of the activity onto the vector n, non-vanishing values of k require

a non-vanishing given by mT n=N

ovePrlap between vectors m and = jmjnj=N, directly quantiﬁes

n. This overlap, the strength of

the structure in the connectivity. The connectivity structure

strength mT n=N and the activity structure strength k are there-

fore directly related, but in a highly non-linear manner. If the

connectivity structure is weak, the network only exhibits

homogeneous, unstructured activity corresponding to k = 0 (Fig-

ure 1B, blue). If the connectivity structure is strong, structured

Neuron 99, 609–623, August 8, 2018 611

heterogeneous activity emerges ðk > 0Þ, and the activity of the network at equilibrium is organized in one dimension along the vector m (Figures 1B, green, and 1C), while the random connectivity induces additional heterogeneity along the remaining N À 1 directions. Note that because of the symmetry in the speciﬁc input-output function we use, when a heterogeneous equilibrium state exists, the conﬁguration with the opposite sign is an equilibrium state too, so that the network activity is bistable (for more general asymmetric transfer functions, this bistability is still present, although the symmetry is lost, see Figure S7).
The random part of the connectivity disrupts the organization of the activity induced by the connectivity structure through two different effects. The ﬁrst effect is that as the random strength g is increased, for any given realization of the random part of the connectivity, the total input to unit i will deviate more strongly from the expected mean mi (Figure 1D). As a consequence, the activity along the N À 1 directions that are orthogonal to m increases, resulting in a noisy input to individual neurons that smoothens the gain of the non-linearity. This effectively leads to a reduction of the overall structure in the activity as quantiﬁed by k (Figure 1C). A second, distinct effect is that increasing the random strength eventually leads to chaotic activity as in purely random networks. Depending on the strength of the structured connectivity, two different types of chaotic dynamics can emerge. If the disorder in the connectivity is much stronger than structure, the overlap k is zero (Figure 1C). As a result, the mean activity of all units vanishes and the dynamics consist of unstructured, NÀ dimensional temporal ﬂuctuations (Figure 1D), as in the classical chaotic state of fully random networks (Figure 1B, red). In contrast, if the strengths of the random and structured connectivity are comparable, a structured type of chaotic activity emerges, in which k > 0 so that the mean activity of different units is organized in one dimension along the direction m as shown by Equation 3, but the activity of different units now ﬂuctuates in time (Figure 1B, orange). As for structured static activity, in this situation the system is bistable as states with opposite signs of k always exist.
The phase diagram in Figure 1B summarizes the different types of spontaneous dynamics that can emerge as a function of the strength of structured and random components of the connectivity matrix. Altogether, the structured component of connectivity favors a one-dimensional organization of network activity, while the random component favors high-dimensional, chaotic ﬂuctuations. Particularly interesting activity emerges when the structure and disorder are comparable, in which case the dynamics show one-dimensional structure combined with high-dimensional temporal ﬂuctuations that can give rise to dynamics with very slow timescales (see Figure S6).

Two-Dimensional Activity in Response to an External Input We now turn to the response to an external, feedforward input (Figure 2A). At equilibrium, the total average input to unit i is the sum of a recurrent input kmi and the feedforward input Ii:

mi = kmi + Ii;

where

k

=

1 N

XN Â Ã nj fj :
j=1

(Equation 4)

Transient, temporal dynamics close to this equilibrium are obtained by including temporal dependencies in k and Ii (see STAR Methods; Equation 102).
Figure 2B illustrates the response of the network to a step input. The response of individual units is highly heterogeneous, different units showing increasing, decreasing, or multi-phasic responses. While every unit responds differently, the theory predicts that, at the level of the N-dimensional state space representing the activity of the whole population, the trajectory of the activity lies on average on the two-dimensional plane spanned by the right-connectivity vector m and the vector I = fIig that corresponds to the pattern of external inputs (Figure 2B). Applying to the simulated activity a dimensionality reduction technique (see Cunningham and Yu [2014] for a recent review) such as principal-component analysis conﬁrms that the two dominant dimensions of the activity indeed lie in the m À I plane (Figure 2C), while the random part of connectivity leads to additional activity in the remaining N À 2 directions that grows quickly with the strength of random connectivity g (see Figure S3). This approach therefore directly links the connectivity in the network to the emerging low-dimensional dynamics and shows that the dominant dimensions of activity are determined by a combination of feedforward inputs and connectivity (Wang et al., 2018).
The contribution of the connectivity vector m to the twodimensional trajectory of activity is quantiﬁed by the overlap k between the network activity ½f and the left-connectivity vector n (Equation 4). If k = 0, the activity trajectory is one dimensional and simply propagates the pattern of feedforward inputs. This is in particular the case for fully random networks. If ks0, the network response is instead a non-trivial two-dimensional combination of the input and connectivity structure patterns. In general, the value of k, and therefore the organization of network activity, depends on the geometric arrangement of the input vector I with respect to the connectivity vectors m and n, as well as on the strength of the random component of the connectivity g.
As the neural activity lies predominantly in the m À I plane, a non-vanishing k, together with non-trivial two-dimensional activity, is obtained when the vector n has a non-zero component in the m À I plane. Two qualitatively different input-output regimes can be distinguished. The ﬁrst one is obtained when the connectivity vectors m and n are orthogonal to each other (Figure 2D, left and center). In that case, the overlap between them is zero, and the spontaneous activity in the network bears no sign of the underlying connectivity structure. Adding an external input can, however, reveal this connectivity structure and generate nontrivial two-dimensional activity if the input vector I has a nonzero overlap with the left-connectivity vector n. In such a situation, the vector n picks up the component of the activity along the feedforward input direction I. This leads to a nonzero overlap k, which in turn implies that the network activity will have a component along the right-connectivity vector m. Increasing the external input along the direction of n will therefore progressively increase the response along m (Figure 2D, center), leading to a two-dimensional output.
A second, qualitatively different input-output regime is obtained when the connectivity vectors m and n have a strong enough overlap along a common direction (Figure 2D, right).

612 Neuron 99, 609–623, August 8, 2018

A

B

C

D

Figure 2. External Inputs Generate Two-Dimensional Activity in Random Networks with Unit-Rank Structure (A) The pattern of external inputs can be represented by an N-dimensional vector I = fIig, where Ii is the input to unit i. (B) Transient dynamics in response to a step input along I in a sample network of N = 3500 units. Left: activity traces for ﬁve units. Right: projections of the population trajectory onto the plane deﬁned by the right-connectivity vector m and the input vector I. Light trace: theoretical prediction. Dark traces: simulations. (C) Principal-component analysis (PCA) of the average activity trajectory. Bottom: fraction of SD explained by successive PCs. Top: correlation between PCs and the vectors m and I. The direction of the projections onto the m À I plane of the two top PCs e1 and e2 are represented in (B). See also Figure S3. (D) The activity k along m is determined by the geometrical arrangement of the vector I and the connectivity vectors m and n. Three different cases are illustrated: (left) I, m, and n mutually orthogonal; (center) m and n mutually orthogonal, but I has a non-zero overlap with n; (right) m and n have non-zero overlap, leading to bistable activity in absence of inputs. Increasing the external input along n suppresses one of the two stable states. Continuous lines: theoretical predictions. Dots: simulations. See STAR Methods for details.

As already shown in Figure 1, an overlap larger than unity between m and n induces bistable, structured spontaneous activity along the dimension m. Adding an external input along the vector n increases the activity along m but also eventually suppresses one of the bistable states. Large external inputs along the n direction therefore reliably set the network into a state in which the activity is a two-dimensional combination of the input direction and the connectivity direction m. This can lead to a strongly non-linear input-output transformation if the network was initially set in the state that lies on the opposite branch (Figure 2D, right).
An additional effect of an external input is that it generally tends to suppress chaotic activity present when the random part of connectivity is strong (Figures S3 and S4). This suppression occurs irrespectively of the speciﬁc geometrical conﬁguration between the input I and connectivity vectors m and n and therefore independently of the two input-output regimes described above. Altogether, external inputs suppress both chaotic and bistable dynamics (Figure S4) and therefore always decrease the amount of variability in the dynamics (Churchland et al., 2010; Rajan et al., 2010).
In summary, external, feedforward inputs to a network with unit-rank connectivity structure in general lead to two-dimen-

sional trajectories of activity. The elicited trajectory depends on the geometrical arrangement of the pattern of inputs with respect to the connectivity vectors m and n, which play different roles. The right-connectivity vector m determines the output pattern of network activity, while the left-connectivity vector n instead selects the inputs that give rise to outputs along m. An output structured along m can be obtained when n selects recurrent inputs (non-zero overlap between n and m) or when it selects external inputs (non-zero overlap between n and I).

Higher-Rank Structure Leads to a Rich Dynamical Repertoire This far we focused on unit-rank connectivity structure, but our framework can be directly extended to higher-rank structure. A more general structured component of rank r ( N can be written as a superposition of r independent unit-rank terms

Pij

=

mði 1Þnðj 1Þ N

+

.

+

mði rÞnðj rÞ; N

(Equation 5)

and is in principle characterized by 2r vectors mðkÞ and nðkÞ. In such a network, the average dynamics lie in the ðr + 1Þ-dimensional

Neuron 99, 609–623, August 8, 2018 613

A

B

C

F

D

E

G

Figure 3. Implementing a Simple Go-Nogo Discrimination Task with a Unit-Rank Connectivity Structure (A) A linear readout is added to the network, with randomly chosen weights wi. The stimuli are represented by random input patterns IA and IB. The task consists in producing an output in response to stimulus A, but not B. The simplest unit-rank structure that implements the task is given by m = w and n = IA. (B) Response of a sample network to the Go (blue) and Nogo (green) inputs. Activity traces for ﬁve units.
(C) Projections of the population trajectories onto the planes predicted to contain the dominant part of the dynamics. Gray: predicted trajectory. Colored traces:
simulations.
(D) Linear regression coefﬁcients for the Go and the Nogo stimuli. Every dot corresponds to a network unit.
(E) Readout dynamics for the Go (blue) and the Nogo (green) stimulus.
(F) Average connectivity strength as a function of the product between the coefﬁcients of the ﬁrst PC. Every dot corresponds to a pair of units. (G) Generalization properties of the network. We select two Go stimuli IA1 and IA2 , and we set n = IA1 + IA2 . We build the input pattern as a normalized mixture of the two preferred patterns, and we gradually increase the component along IA1 . Continuous lines: theoretical predictions. Dots: simulations. See STAR Methods for details.

subspace spanned by the r right-connectivity vectors mðkÞ; k = 1; .; r and the input vector I, while the left connectivity vectors nðkÞ select the inputs ampliﬁed along the corresponding dimension mðkÞ. The details of the dynamics will in general depend on the geometrical arrangement of these 2r vectors among themselves and with respect to the input pattern. The number of possible conﬁgurations increases quickly with the structure rank, leading to a wide repertoire of dynamical states that includes continuous attractors (Figure S5) and sustained oscillatory activity (Figure S8). In the remainder of this manuscript, we will explore only the rank-two case.
Implementing a Simple Discrimination Task Having developed an intuitive, geometric understanding of how a given unit-rank connectivity structure determines the lowdimensional dynamics in a network, we now reverse our approach to ask how a given computation can be implemented by choosing appropriately the structured part of the connectivity. We start with the computation underlying one of the most basic and most common behavioral tasks, Go-Nogo stimulus discrimination. In this task, an animal has to produce a speciﬁc motor

output, e.g., press a lever or lick a spout, in response to a stimulus IA (the Go stimulus), and ignore another stimuli IB (Nogo stimuli). This computation can be implemented in a straightforward way in a recurrent network with a unit-rank connectivity structure. While such a simple computation does not in principle require a recurrent network, the implementation we describe here illustrates in a transparent manner the relationship between connectivity, dynamics, and computations in low-rank networks and leads to non-trivial and directly testable experimental predictions. It also provides the basic building block for more complex tasks, which we turn to in the next sections.
We model the sensory stimuli as random patterns of external inputs to the network, so that the two stimuli are represented by two ﬁxed, randomly chosen N-dimensional vectors IA and IB. To model the motor response, we supplement the network with an output unit, which produces a linear readout
1P zðtÞ = N iwi4ðxiðtÞÞ of network activity (Figure 3A). The readout weights wi are chosen randomly and form also a ﬁxed N-dimensional vector w. The task of the network is to produce an output that is selective to the Go stimulus: the readout z at the end of

614 Neuron 99, 609–623, August 8, 2018

stimulus presentation needs to be non-zero for the input pattern IA that corresponds to the Go stimulus, and zero for the other input IB.
The two N-dimensional vectors m and n that generate the appropriate unit-rank connectivity structure to implement the task can be directly determined from our description of network dynamics. As shown in Equation 4 and Figure 2, the response of the network to the input pattern I is in general two-dimensional and lies in the plane spanned by the vectors m and I. The output unit will therefore produce a non-zero readout only if the readout vector w has a non-vanishing overlap with either m or I. As w is assumed to be uncorrelated, and therefore orthogonal, to all input patterns, this implies that the connectivity vector m needs to have a non-zero overlap with the readout vector w for the network to produce a non-trivial output. This output will depend on the amount of activity along m, quantiﬁed by the overlap k. As shown in Figure 2, the overlap k will be non-zero only if n has a non-vanishing overlap with the input pattern. Altogether, implementing the Go-Nogo task therefore requires that the right-connectivity vector m is correlated with the readout vector w, and that the left-connectivity vector n is correlated with the Go stimulus IA.
Choosing m = w and n = IA therefore provides the simplest unit-rank connectivity that implements the desired computation. Figure 3 illustrates the activity in the corresponding network. At the level of individual units, by construction both stimuli elicit large and heterogeneous responses (Figure 3B) that display mixed selectivity (Figure 3D). As predicted by the theory, the response to stimulus B is dominantly one-dimensional and organized along the input direction IB, while the response to stimulus A is two-dimensional and lies in the plane deﬁned by the rightconnectivity vector m and the input direction IA (Figure 3C). The readout from the network corresponds to the projection of the activity onto the m direction and is non-zero only in response to stimulus A (Figure 3E), so that the network indeed implements the desired Go-Nogo task. Our framework therefore allows us to directly link the connectivity, the low-dimensional dynamics, and the computation performed by the network and leads to two experimentally testable predictions. The ﬁrst one is that performing a dimensionality reduction separately on responses to the two stimuli should lead to larger dimensionality of the trajectories in response to the Go stimulus. The second prediction is that for the Go stimulus, the dominant directions of activity depend on the recurrent connectivity in the network, while for the Nogo stimulus they do not. More speciﬁcally, for the activity elicited by the Go stimulus, the dominant principal components are combinations of the input vector IA and right-connectivity vector m. Therefore, if two neurons have large principal-component weights, they are expected to also have large m weights and therefore stronger mutual connections than average (Figure 3F, top). In contrast, for the activity elicited by the Nogo stimulus, the dominant principal components are determined solely by the feedforward input, so that no correlation between dominant PC weights and recurrent connectivity is expected (Figure 3F, bottom). This prediction can in principle be directly tested in experiments analogous to Ko et al. (2011), where calcium imaging in behaving animals is combined with measurements of connectivity in a subset of recorded neurons. Note that in this setup very

weak structured connectivity is sufﬁcient to implement computations, so that the expected correlations may be weak if the random part of the connectivity is strong (see Figure S5).
The unit-rank connectivity structure forms the fundamental scaffold for the desired input-output transform. The random part of the connectivity adds variability around the target output and can induce additional chaotic ﬂuctuations. Summing the activity of individual units through the readout unit, however, averages out this heterogenpeﬃiﬃtﬃy, so that the readout error decreases with network size as 1= N (Figure S5). The present implementation is therefore robust to noise and has desirable computational properties in terms of generalization to novel stimuli. In particular, it can be extended in a straightforward way to the detection of a category of Go stimuli, rather than a single stimulus (Figure 3G).
Detection of a Noisy Stimulus We now turn to a slightly more complex task: integration of a continuous, noisy stimulus. In contrast to the previous discrimination task, where the stimuli were completely different (i.e., orthogonal), here we consider a continuum of stimuli that differ only along the intensity of a single feature, such as the coherence of a random-dot kinetogram (Newsome et al., 1989). In a given stimulus presentation, this feature moreover ﬂuctuates in time. We therefore represent each stimulus as cðtÞI, where I is a ﬁxed, randomly chosen input vector that encodes the relevant stimulus feature, and cðtÞ is the amplitude of that feature. We consider a Go-Nogo version of this task, in which the network has to produce an output only if the average value of c is larger than a threshold (Figure 4A).
As for the basic discrimination task, the central requirements for a unit-rank network to implement this task are that the right-connectivity vector m is correlated with the readout vector w, and the left-connectivity vector n is correlated with the input pattern I. A key novel requirement in the present task is, however, that the response needs to be non-linear to produce the Go output when the strength of the input along I is larger than the threshold. As shown in Figure 2D, such a non-linearity can be obtained when the left- and right-connectivity vectors n and m have a strong enough overlap. We therefore add a shared component to m and n along a direction orthogonal to both w and I. In that setup, if the stimulus intensity c is low, the network will be in a bistable regime, in which the activity along the direction m can take two distinct values for the same input (Figure 2D, right). Assuming that the lower state represents a Nogo output, and that the network is initialized in this state at the beginning of the trial, increasing the stimulus intensity c above a threshold will lead to a sudden jump, and therefore a non-linear detection of the stimulus. Because the input amplitude ﬂuctuates noisily in time, whether such a jump occurs depends on the integrated estimate of the stimulus intensity. The timescale over which this estimate is integrated is determined by the time constant of the effective exponential ﬁlter describing the network dynamics. In our unit-rank network, this time constant is set by the connectivity strength, i.e., the overlap between the left- and right-connectivity vectors m and n, which also determines the value of the threshold. Arbitrarily large timescales can be obtained by adjusting this overlap close to the bifurcation value, in which case the threshold becomes arbitrarily small (Figure 4F). In this section,

Neuron 99, 609–623, August 8, 2018 615

A F
D B
G

C

E

H

Figure 4. Implementing a Noisy Detection Task with a Unit-Rank Connectivity Structure (A) The network is given a noisy input cðtÞ along a ﬁxed, random pattern of inputs I. The task consists in producing an output if the average input c is larger than a threshold q. (B) Dynamics in a sample network. Top: noisy input and threshold. Bottom: activity traces for four units and two different noise realizations in the stimulus, leading to a Go (dark blue) and a Nogo (light blue) output. (C) Readout dynamics for the two stimuli. (D) Projections of the population trajectory onto the plane deﬁned by the right-connectivity vector m and the input vector I. Left: single-trial trajectories corresponding to (B). Right: trial-averaged trajectories, for Go (top) and Nogo (bottom) outputs, and different values of the mean input c. Stars indicate correct responses. (E) Left: linear regression coefﬁcients for the input amplitude and the decision outcome. Every dot corresponds to a network unit. Right: correlation coefﬁcients between the vectors m and I and the input and choice regression axes (see STAR Methods). Projection directions of the two input and choice regression axes onto the m À I plane are shown in (D). (F) Detection threshold (dashed) and timescale of the effective exponential ﬁlter (full line) for increasing values of the structure strength. (G) Psychometric curve. The shaded area indicates the bistable region. (H) Average connectivity strength as a function of the product of the linear regression coefﬁcients for the choice variable. Every dot corresponds to a pair of network units. See STAR Methods for details.

we ﬁx the structure strength so that the threshold is set to 0.5, which corresponds to an integration timescale of the order of the time constant of individual units.
Figure 4 illustrates the activity in an example implementation of this network. In a given trial, as the stimulus is noisy, the activity of the individual units ﬂuctuates strongly (Figure 4B). Our theory predicts that the population trajectory on average lies in the plane deﬁned by the connectivity vector m and the input pattern I (Figure 4D). Activity along the m direction is picked up by the readout, and its value at the end of stimulus

presentation determines the output (Figure 4C). Because of the bistable dynamics in the network, whether the m direction is explored, and an output produced, depends on the speciﬁc noisy realization of the stimulus. Stimuli with an identical average strength can therefore lead to either two-dimensional trajectories of activity and Go responses or one-dimensional trajectories of activity corresponding to Nogo responses (Figure 4D). The probability of generating an output as function of stimulus strength follows a sigmoidal psychometric curve that reﬂects the underlying bistability (Figure 4G). Note that the

616 Neuron 99, 609–623, August 8, 2018

A

C

E

F B

D

G

Figure 5. Implementing a Context-Dependent Go-Nogo Discrimination Task with a Rank-Two Connectivity Structure (A) As in Figure 3, two stimuli A and B are presented to the network. The task consists in producing an output in response to the Go stimulus, which is determined by the contextual cue (A in context A, B in context B), modeled as inputs along random directions IctxA and IctxB. (B) Inputs along the overlap direction between the left- and the right-connectivity vectors modulate the response threshold of the network (see also Figure S5). (C) Dynamics in a sample network in response to the stimulus A. Top: stimulus and contextual input. Bottom: activity for ﬁve units in context A (crimson) and B (pink). (D) Readout dynamics in the two contexts. (E) Projections of the average population trajectories onto the planes spanned by vectors w, IA and IB. (F) Network performance in the two contexts. (G) Average connectivity strength between pairs of units as a function of the product between the regression coefﬁcients for context. Every dot corresponds to a pair of network units. See STAR Methods for details.

bistability is not clearly apparent on the level of individual units. In particular, the activity of individual units is always far from saturation, as their inputs are distributed along a zero-centered Gaussian (Equation 4).
The responses of individual units are strongly heterogeneous and exhibit mixed selectivity to stimulus strength and output choice (Figure 4E). A popular manner to interpret such activity at the population level is a targeted dimensional reduction approach, in which input and choice dimensions are determined through regression analyses (Mante et al., 2013). As expected from our theoretical analysis, the two dimensions obtained through regression are closely related to m and I; in particular, the choice dimension is highly correlated with the right-connectivity vector m (Figure 4E). As a result, the plane in which network activity dominantly lies corresponds to the plane deﬁned by the choice and the input dimensions (Figure 4D). Our framework therefore directly links recurrent connectivity and effective output choice direction through the low-dimensional dynamics.

A resulting experimentally testable prediction is that neurons with strong choice regressors have stronger mutual connections (Figure 4H).
A Context-Dependent Discrimination Task We next consider a context-dependent discrimination task, in which the relevant response to a stimulus depends on an additional, explicit contextual cue. Speciﬁcally, we focus on the task studied in Saez et al. (2015) where in one context (referred to as context A), the stimulus A requires a Go output, and the stimulus B a Nogo, while in the other context (referred to as context B), the associations are reversed (Figure 5A). This task is a direct extension of the basic binary discrimination task introduced in Figure 3; yet it is signiﬁcantly more complex as it represents a hallmark of cognitive ﬂexibility: a non-linearly separable, XOR-like computation that a single-layer feedforward network cannot solve (Rigotti et al., 2010; Fusi et al., 2016). We will show that this task can be implemented in a rank-two recurrent

Neuron 99, 609–623, August 8, 2018 617

network that is a direct extension of the unit-rank network used for the discrimination task in Figure 4.
This context-dependent task can be seen as a combination of two basic, opposite Go-Nogo discriminations, each of which can be independently implemented by a unit-rank structure with the right-connectivity vector m correlated to the readout, and the left-connectivity vector correlated to the Go input (IA for context A, IB for context B). Combining two such unit-rank structures, with left-connectivity vectors nð1Þ and nð2Þ correlated respectively with IA and IB, leads to a rank-two connectivity structure that serves as a scaffold for the present task. The cues for context A and B are represented by additional inputs along random vectors IctxA and IctxB, presented for the full length of the trial (Remington et al., 2018) (Figure 5C). These inputs are the only contextual information incorporated in the network. In particular, the readout vector w is ﬁxed and independent of the context (Mante et al., 2013). Crucially, since the readout w needs to produce an output for both input stimuli, both right-connectivity vectors mð1Þ and mð2Þ need to be correlated with it.
The key requirement for implementing context-dependent discrimination is that each contextual input effectively switches off the irrelevant association. To implement this requirement, we rely on the same non-linearity as for the noisy discrimination task, based on the overlap between the left- and right-connectivity vectors (Figure 2D). We however exploit an additional property, which is that the threshold of the non-linearity (i.e., the position of the transition from a bistable to a mono-stable region in Figure 2D) can be controlled by an additional modulatory input along the overlap direction between m and n (Figures 5B and S4). Such a modulatory input acts as an effective offset for the bistability at the macroscopic, population level (see Equation 153 in STAR Methods). A stimulus of a given strength (e.g., unit strength in Figure 5B) may therefore induce a transition from the lower to the upper state (Figure 5B, top), or no transition (Figure 5B bottom) depending on the strength of the modulatory input that sets the threshold value. While in the noisy discrimination task, the overlap between m and n was chosen in an arbitrary direction, in the present setting, we take the overlaps between each pair of left- and right-connectivity vectors to lie along the direction of the corresponding contextual input (i.e., mð1Þ and nð1Þ overlap along IctxA, mð2Þ and nð2Þ along IctxB), so that contextual inputs directly modulate the threshold of the non-linearity. The ﬁnal rank-two setup is described in detail in the STAR Methods.
Figure 5 illustrates the activity in an example of the resulting network implementation. The contextual cue is present from the very beginning of the trial and effectively sets the network in a context-dependent initial state (Figure 5C) that corresponds to the lower of the two bistable states. The low-dimensional response of the network to the following stimulus is determined by this initial state and the sustained contextual input. If the cue for context A is present, stimulus A leads to the crossing of the non-linearity, a transition from the lower to the upper state, and therefore a two-dimensional response in the plane determined by IA and w (Figure 5E, top left), generating a Go output (Figure 5D). In contrast, if the cue for context B is present, the threshold of the underlying non-linearity is increased in the direction of input IA (Figure 5B, bottom), so that the presentation of stimulus A does not induce a transition between the lower and

upper states but leads only to a one-dimensional trajectory orthogonal to the readout, and therefore a Nogo response (Figure 5E, top right). The situation is totally symmetric in response to stimulus B (Figure 5E, bottom), so that contextual cues fully reverse the stimulus-response associations (Figure 5F). Overall, this context-dependent discrimination relies on strongly nonlinear interactions between the stimulus and contextual inputs, that on the connectivity level are implemented by overlaps between the connectivity vectors along the contextual inputs. A central, experimentally testable prediction of our framework is therefore that, if a network is implementing this computation, units with strong contextual selectivity have on average stronger mutual connections (Figure 5G).
A Context-Dependent Evidence Integration Task We ﬁnally examine a task inspired by Mante et al. (2013) that combines context-dependent output and ﬂuctuating, noisy inputs. The stimuli now consist of superpositions of two different features A and B, and the strengths of both features ﬂuctuate in time during a given trial. In Mante et al. (2013), the stimuli were random dot kinetograms, and the features A and B corresponded to the direction of motion and color of these stimuli. The task consists in classifying the stimuli according to one of those features, the relevant one being indicated by an explicit contextual cue (Figure 6A).
We implemented a Go-Nogo version of the task, in which the output is required to be non-zero when the relevant feature is stronger than a prescribed threshold (arbitrarily set to 0.5). The present task is therefore a direct combination of the detection task introduced in Figure 4 and the context-dependent discrimination task of Figure 5, but the individual stimuli are now two dimensional, as they consist of two independently varied features A and B. In this task, a signiﬁcant additional difﬁculty is that on every trial the irrelevant feature needs to be ignored, even if it is stronger than the relevant feature (e.g., color coherence stronger than motion coherence on a motion-context trial).
This context-dependent evidence integration task can be implemented with exactly the same rank-two conﬁguration as the basic context-dependent discrimination in Figure 5, with contextual gating relying on the same non-linear mechanism as in Figure 5B. The contextual cue is presented throughout the trial (Figure 6B) and determines which of the features of the twodimensional stimulus leads to non-linear dynamics along the direction of connectivity vectors mð1Þ and mð2Þ (Figure 6D). These directions share a common component along the readout vector w, and the readout unit picks up the activity along that dimension. As a consequence, depending on the contextual cue, the same stimulus can lead to opposite outputs (Figure 6C). Altogether, in context A, the output is independent of the values of feature B, and conversely in context B (Figure 6E). The output therefore behaves as if it were based on two orthogonal readout directions, yet the readout direction is unique and ﬁxed, and the output relies instead on a context-dependent selection of the relevant input feature (Mante et al., 2013).
An important additional requirement in the present task with respect to the basic context-dependent integration is that the network needs to perform temporal integration to average out temporal ﬂuctuations in the stimulus. As illustrated in Figures

618 Neuron 99, 609–623, August 8, 2018

A

B

D

E F
C

Figure 6. Implementing a Context-Dependent Evidence Accumulation Task Using Rank-Two Connectivity Structure (A) The stimuli consist of a superposition of two features cA and cB, which ﬂuctuate in time around mean values cA and cB. In every trial, a pair of contextual inputs determines the relevant input feature. The task consists in producing an output if the average strength of the relevant feature is larger than a threshold. (B) Dynamics in a sample network. Top: stimulus and contextual inputs. Bottom: activity of four units in contexts A (crimson) and B (pink). (C) Readout dynamics in the two contexts. (D) Average population trajectories projected onto the planes spanned by vectors w, IA and IB. Blue (resp. green) trajectories have been sorted according to the value of the strength of stimulus A (resp. B), and averaged across stimulus B (resp. A). (E) Network performance. Top row: probability of response as function of input strengths cA and cB (simulated data). Bottom: probability of response averaged over cB. Continuous line: theoretical prediction; dots: simulations. (F) Projection of the population activity onto the plane deﬁned by the orthogonal components of the vectors mA and mB and comparison with the underlying circular attractor (see STAR Methods). Trajectories are sorted by the strength of the relevant stimulus and averaged across the non-relevant one. The direction of the projections of the regression axes for choice and context are indicated in gray. See STAR Methods for details.

6B and 6C, the network dynamics in response to stimuli indeed exhibit a slow timescale and progressively integrate the input. Strikingly, such slow dynamics do not require additional constraints on network connectivity; they are a direct consequence of the rank-two connectivity structure used for contextual gating (in fact the dynamics are already slow in the basic contextual discrimination task, see Figures 5C and 5D). More speciﬁcally, the symmetry between the two contexts implies that two sets of left- and right-connectivity vectors have identical overlaps (i.e. mð1ÞT nð1Þ = mð2ÞT nð2Þ). Without further constraints on the connectivity, such a symmetric conﬁguration leads to an emergence of a continuous line attractor, with the shape of a two-dimensional ring in the plane deﬁned by mð1Þ and mð2Þ (see STAR Methods and Figure S5). In the implementation of the present task, on top of symmetric overlaps, the four connectivity vectors include a common direction along the readout vector. This additional constraint eliminates the ring attractor and stabilizes only two equilibrium states that correspond to Go and Nogo outputs. Yet, the ring attractor is close in parameter space, and this prox-

imity induces a slow manifold in the dynamics, so that the trajectories leading to a Go output slowly evolve along two different sides of the underlying ring depending on the context (Figure 6F). As a result, the two directions in the plane mð1Þ À mð2Þ correspond to choice and context axis as found by regression analysis (Figure 6F). A similar mechanism for context-dependent evidence integration based on a line attractor was previously identiﬁed by reverse-engineering a trained recurrent network (Mante et al., 2013). Whether the underlying dynamical structure was a ring as in our case or two line attractors for the two contexts depended on the details of the network training protocol (V. Mante, unpublished data). Here, we show that such a mechanism based on a ring attractor can be implemented in a minimal network with rank-two connectivity structure, but other solutions can certainly be found. Note that this rank-two network can also serve as an alternative implementation for context-independent evidence integration in which the integration timescale and the threshold value are fully independent in contrast to the unit-rank implementation (Figure 4).

Neuron 99, 609–623, August 8, 2018 619

DISCUSSION
Motivated by the observation that a variety of approaches for implementing computations in recurrent networks rely on a common type of connectivity structure, we studied a class of models in which the connectivity matrix consists of a sum of a ﬁxed, lowrank term and a random part. Our central result is that the lowrank connectivity structure induces low-dimensional dynamics in the network, a hallmark of population activity recorded in behaving animals (Gao and Ganguli, 2015). While low-dimensional activity is usually detected numerically using dimensional-reduction techniques (Cunningham and Yu, 2014), we showed that a mean-ﬁeld theory allows us to directly predict the low-dimensional dynamics based on the connectivity and input structure. This approach led us to a simple, geometrical understanding of the relationship between connectivity and dynamics and enabled us to design minimal-connectivity implementations of speciﬁc computations. In particular, we found that the dynamical repertoire of the network increases quickly with the rank of the connectivity structure, so that rank-two networks can already implement a variety of computations. In this study, we have not explicitly considered structures with rank higher than two, but our theoretical framework is in principle valid for arbitrary rank r ( N, where N is the size of the network.
While other works have examined dynamics in networks with a mixture of structured and random connectivity (e.g., Roudi and Latham [2007]; Ahmadian et al. [2015]), the most classical approach for implementing computations in recurrent networks has been to endow them with a clustered (Wang, 2002; Amit and Brunel, 1997; Litwin-Kumar and Doiron, 2012) or distancedependent connectivity (Ben-Yishai et al., 1995). Such networks inherently display low-dimensional dynamics similar to our framework (Doiron and Litwin-Kumar, 2014; Williamson et al., 2016), as clustered connectivity is in fact a special case of lowrank connectivity. Clustered connectivity, however, is highly ordered: each neuron belongs to a single cluster and therefore is selective to a single task feature (e.g., a given stimulus, or a given output). Neurons in clustered networks are therefore highly specialized and display pure selectivity (Rigotti et al., 2013). Here, instead, we have considered random low-rank structures, which generate activity organized along heterogeneous directions in state space. As a consequence, stimuli and outputs are represented in a random, highly distributed manner and individual neurons are typically responsive to several stimuli, outputs, or combinations of the two. Such mixed selectivity is a ubiquitous property of cortical neurons (Rigotti et al., 2013; Mante et al., 2013; Churchland and Shenoy, 2007) and confers additional computational properties to our networks (Kanerva, 2009). In particular, it allowed us to easily extend to a contextdependent situation (Mante et al., 2013; Saez et al., 2015), a network implementation of a basic discrimination task. This is typically difﬁcult to do in clustered, purely selective networks (Rigotti et al., 2010).
The type of connectivity used in our study is closely related to the classical framework of Hopﬁeld networks (Hopﬁeld, 1982; Amit et al., 1985). The aim of Hopﬁeld networks is to store in memory speciﬁc patterns of activity by creating for each pattern a corresponding ﬁxed point in the network dynamics. This is

achieved by adding a unit-rank term for each item, and one approach for investigating the capacity of such a setup has relied on the mean-ﬁeld theory of a network with a connectivity that consists of a sum of a rank-one term and a random matrix (Tirozzi and Tsodyks, 1991; Shiino and Fukai, 1993; Roudi and Latham, 2007). While this approach is clearly close to the one adopted in the present study, there are important differences. Within Hopﬁeld networks, the unit-rank terms are symmetric, so that the corresponding left- and right-connectivity vectors are identical for each pattern. Moreover, the unit-rank terms that correspond to different patterns are generally uncorrelated. In contrast, here we have considered the more general case where the left- and right-eigenvectors are different and potentially correlated between different rank-one terms. Most importantly, our main focus was on responses to external inputs and input-output computations, rather than memorizing items. In particular, we showed that left- and right-connectivity vectors play different roles with respect to processing inputs, with the left-connectivity vector implementing input- selection, and the right-connectivity vector determining the output of the network.
Our study is also directly related to echo-state networks (ESNs) (Jaeger and Haas, 2004) and FORCE learning (Sussillo and Abbott, 2009). In those frameworks, randomly connected recurrent networks are trained to produce speciﬁed outputs using a feedback loop from a readout unit to the network, which is mathematically equivalent to adding a rank-one term to the random connectivity matrix (Maass et al., 2007). In their most basic implementation, both ESN and FORCE learning train only the readout weights. The training is performed for a ﬁxed, speciﬁed realization of the random connectivity, so that the ﬁnal rank-one structure is correlated with the random part of the connectivity and may be strong with respect to it. In contrast, the results presented here rely on the assumption that the low-rank structure is weak and independent from the random part. Although ESN and FORCE networks do not necessarily fulﬁll this assumption, in ongoing work we found that our approach describes well networks trained using ESN or FORCE to produce a constant output (Rivkind and Barak, 2017). Note that in our framework, the computations rely solely on the structured part of the connectivity, but ongoing work suggests that the random part of the connectivity may play an important role during training.
The speciﬁc network model used here is identical to most studies based on trained recurrent networks (Sussillo and Abbott, 2009; Mante et al., 2013; Sussillo, 2014). It is highly simpliﬁed and lacks many biophysical constraints, the most basic ones being positive ﬁring rates, the segregation between excitation and inhibition and interactions through spikes. Recent works have investigated extensions of the abstract model used here to networks with biophysical constraints (Ostojic, 2014; Kadmon and Sompolinsky, 2015; Harish and Hansel, 2015; Mastrogiuseppe and Ostojic, 2017; Thalmeier et al., 2016). Additional work will be needed to implement the present framework in networks of spiking neurons.
Our results imply novel, directly testable experimental predictions relating connectivity, low-dimensional dynamics and computational properties of individual neurons. Our main result is that the dominant components of low-dimensional dynamics are a combination of feedforward input patterns, and vectors

620 Neuron 99, 609–623, August 8, 2018

specifying the low-rank recurrent connectivity (Figure 2C). A direct implication is that, if the low-dimensional dynamics in the network are generated by low-rank recurrent connectivity, two neurons that have large loadings in the dominant principal components will tend to have mutual connections stronger than average (Figure 3F, top). In contrast, if the low-dimensional dynamics are not generated by recurrent interactions but instead are driven by feedforward inputs alone, no correlation between principal components and connectivity is expected (Figure 3F, bottom). Since the low-dimensional dynamics based on recurrent connectivity form the scaffold for computations in our model, this basic prediction can be extended to various taskdependent properties of individual neurons. For instance, if the recurrent connectivity implements evidence integration, two units with strong choice regressors are predicted to have mutual connections stronger than average (Figure 4H). Analogously, if recurrent connections implement context-dependent associations, two units with strong context regressors are expected to share connections stronger than average (Figure 5G). Such predictions can in principle be directly tested in experiments that combine calcium imaging of neural activity in behaving animals with measurements of connectivity between a subset of recorded neurons (Ko et al., 2011). It should be noted, however, that very weak structured connectivity is sufﬁcient to implement computations, so that the expected correlations between connectivity and various selectivity indices may be weak.
The class of recurrent networks we considered here is based on connectivity matrices that consist of an explicit sum of a lowrank and a random part. While this may seem as a limited class of models, in fact, any arbitrary matrix can be approximated with a low-rank one, e.g., by keeping a small number of dominant singular values and singular vectors (Markovsky, 2012)—this is the basic principle underlying dimensionality reduction. A recurrent network with any arbitrary connectivity matrix can therefore in principle be approximated by a low-rank recurrent network. From this point of view, our theory suggests a simple conjecture: the low-dimensional structure in connectivity determines lowdimensional dynamics and computational properties of recurrent networks. While more work is needed to establish under which precise conditions a low-rank network provides a good computational approximation of a full recurrent network, this conjecture provides a simple and practically useful working hypothesis for reverse-engineering trained neural networks (Sussillo and Barak, 2013), and relating connectivity, dynamics, and computations in neural recordings.

B Population-averaged equations for stationary solutions
B Transient dynamics and stability of stationary solutions B Homogeneous stationary solutions B Heterogeneous stationary solutions B Mean-ﬁeld analysis of transient dynamics and stability
of stationary solutions B Dynamical Mean Field equations for chaotic solutions B Spontaneous dynamics: structures overlapping on the
unitary direction B Stationary solutions B Chaotic solutions B Spontaneous dynamics: structures overlapping on an
arbitrary direction B Response to external inputs B Asymmetric solutions B Transient dynamics B Rank-two connectivity structures B Rank-two structures with null overlap B Rank-two structures with internal pairwise overlap B Rank-two structures for oscillations d IMPLEMENTATION OF COMPUTATIONAL TASKS B Go-Nogo discrimination B Detection of a continuous noisy stimulus B Contextual modulation of threshold value B Rank-two structures for context-dependent compu-
tations d METHOD DETAILS FOR MAIN FIGURES
B Figure 1 B Figure 2 B Figure 3 B Figure 4 B Figure 5 B Figure 6 d QUANTIFICATION AND STATISTICAL ANALYSIS B Dimensionality reduction B Linear regression d DATA AND SOFTWARE AVAILABILITY
SUPPLEMENTAL INFORMATION
Supplemental Information includes eight ﬁgures and can be found with this article online at https://doi.org/10.1016/j.neuron.2018.07.003.
ACKNOWLEDGMENTS

STAR+METHODS
Detailed methods are provided in the online version of this paper and include the following:
d KEY RESOURCES TABLE d CONTACT FOR REAGENT AND RESOURCE SHARING d METHOD DETAILS
B The network model B Overview of Dynamical Mean-Field Theory d DETAILS OF DYNAMICAL MEAN-FIELD THEORY B Single-unit equations for spontaneous dynamics

We are grateful to Alexis Dubreuil, Vincent Hakim, and Kishore Kuchibhotla for discussions and feedback on the manuscript. This work was funded by the Programme Emergences of City of Paris, Agence Nationale de la Recherche grants ANR-16-CE37-0016-01 and ANR-17-ERC2-0005-01, and the program ‘‘Investissements d’Avenir’’ launched by the French Government and implemented by the ANR, with the references ANR-10-LABX-0087 IEC and ANR11-IDEX-0001-02 PSL* Research University. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.
AUTHOR CONTRIBUTIONS
F.M. and S.O. designed the study and wrote the manuscript. F.M. performed model analyses and simulations.

Neuron 99, 609–623, August 8, 2018 621

DECLARATION OF INTERESTS
The authors declare no competing interests.
Received: December 4, 2017 Revised: April 27, 2018 Accepted: July 2, 2018 Published: July 26, 2018
REFERENCES
Ahmadian, Y., Fumarola, F., and Miller, K.D. (2015). Properties of networks with partially structured and partially random connectivity. Phys. Rev. E Stat. Nonlin. Soft Matter Phys. 91, 012820. Aljadeff, J., Stern, M., and Sharpee, T. (2015b). Transition to chaos in random networks with cell-type-speciﬁc connectivity. Phys. Rev. Lett. 114, 088101. Amit, D.J., and Brunel, N. (1997). Model of global spontaneous activity and local structured activity during delay periods in the cerebral cortex. Cereb. Cortex 7, 237–252. Amit, D.J., Gutfreund, H., and Sompolinsky, H. (1985). Storing inﬁnite numbers of patterns in a spin-glass model of neural networks. Phys. Rev. Lett. 55, 1530–1533. Barak, O. (2017). Recurrent neural networks as versatile tools of neuroscience research. Curr. Opin. Neurobiol. 46, 1–6. Ben-Yishai, R., Bar-Or, R.L., and Sompolinsky, H. (1995). Theory of orientation tuning in visual cortex. Proc. Natl. Acad. Sci. USA 92, 3844–3848. Boerlin, M., Machens, C.K., and Dene` ve, S. (2013). Predictive coding of dynamical variables in balanced spiking networks. PLoS Comput. Biol. 9, e1003258. Brunel, N. (2000). Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons. J. Comput. Neurosci. 8, 183–208. Churchland, M.M., and Shenoy, K.V. (2007). Temporal complexity and heterogeneity of single-neuron activity in premotor and motor cortex. J. Neurophysiol. 97, 4235–4257. Churchland, M.M., Yu, B.M., Cunningham, J.P., Sugrue, L.P., Cohen, M.R., Corrado, G.S., Newsome, W.T., Clark, A.M., Hosseini, P., Scott, B.B., et al. (2010). Stimulus onset quenches neural variability: A widespread cortical phenomenon. Nat. Neurosci. 13, 369–378. Cunningham, J.P., and Yu, B.M. (2014). Dimensionality reduction for largescale neural recordings. Nat. Neurosci. 17, 1500–1509. Doiron, B., and Litwin-Kumar, A. (2014). Balanced neural architecture and the idling brain. Front. Comput. Neurosci. 8, 56. Eliasmith, C., and Anderson, C. (2004). Neural Engineering - Computation, Representation, and Dynamics in Neurobiological Systems (MIT Press). Fusi, S., Miller, E.K., and Rigotti, M. (2016). Why neurons mix: High dimensionality for higher cognition. Curr. Opin. Neurobiol. 37, 66–74. Gao, P., and Ganguli, S. (2015). On simplicity and complexity in the brave new world of large-scale neuroscience. Curr. Opin. Neurobiol. 32, 148–155. Girko, V.L. (1985). Circular law. Theory Probab. Appl. 29, 694–706. Harish, O., and Hansel, D. (2015). Asynchronous rate chaos in spiking neuronal circuits. PLoS Comput. Biol. 11, e1004266. Harris, K.D., and Mrsic-Flogel, T.D. (2013). Cortical connectivity and sensory coding. Nature 503, 51–58. Hopﬁeld, J.J. (1982). Neural networks and physical systems with emergent collective computational abilities. Proc. Natl. Acad. Sci. USA 79, 2554–2558. Jaeger, H., and Haas, H. (2004). Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication. Science 304, 78–80. Kadmon, J., and Sompolinsky, H. (2015). Transition to chaos in random neuronal networks. Phys. Rev. X 5, 041030.

Kanerva, P. (2009). Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Cognit. Comput. 1, 139–159.
Ko, H., Hofer, S.B., Pichler, B., Buchanan, K.A., Sjo¨ stro¨ m, P.J., and MrsicFlogel, T.D. (2011). Functional speciﬁcity of local synaptic connections in neocortical networks. Nature 473, 87–91.
Litwin-Kumar, A., and Doiron, B. (2012). Slow dynamics and high variability in balanced cortical networks with clustered connections. Nat. Neurosci. 15, 1498–1505.
Maass, W., Joshi, P., and Sontag, E.D. (2007). Computational aspects of feedback in neural circuits. PLoS Comput. Biol. 3, e165.
Machens, C.K., Romo, R., and Brody, C.D. (2010). Functional, but not anatomical, separation of ‘‘what’’ and ‘‘when’’ in prefrontal cortex. J. Neurosci. 30, 350–360.
Mante, V., Sussillo, D., Shenoy, K.V., and Newsome, W.T. (2013). Contextdependent computation by recurrent dynamics in prefrontal cortex. Nature 503, 78–84.
Markovsky, I. (2012). Low Rank Approximation - Algorithms, Implementations, Applications (Springer).
Martens, J., and Sutskever, I. (2011). Learning recurrent neural networks with hessian-free optimization. In ICML’11 Proceedings of the 28th International Conference on International Conference on Machine Learning. (ICML), pp. 1033–1040.
Mastrogiuseppe, F., and Ostojic, S. (2017). Intrinsically-generated ﬂuctuating activity in excitatory-inhibitory networks. PLoS Comput. Biol. 13, e1005498.
Newsome, W.T., Britten, K.H., and Movshon, J.A. (1989). Neuronal correlates of a perceptual decision. Nature 341, 52–54.
Ostojic, S. (2014). Two types of asynchronous activity in networks of excitatory and inhibitory spiking neurons. Nat. Neurosci. 17, 594–600.
Pascanu, R., Mikolov, T., and Bengio, Y. (2013). On the difﬁculty of training recurrent neural networks. In ICML’13 Proceedings of the 30th International Conference on International Conference on Machine Learning (ICML), pp. III–1310–III–1318.
Rajan, K., and Abbott, L.F. (2006). Eigenvalue spectra of random matrices for neural networks. Phys. Rev. Lett. 97, 188104.
Rajan, K., Abbott, L.F., and Sompolinsky, H. (2010). Stimulus-dependent suppression of chaos in recurrent neural networks. Phys. Rev. E Stat. Nonlin. Soft Matter Phys. 82, 011903.
Remington, E.D., Narain, D., Hosseini, E.A., and Jazayeri, M. (2018). Flexible sensorimotor computations through rapid reconﬁguration of cortical dynamics. Neuron 98, 1005–1019.e5.
Rigotti, M., Ben Dayan Rubin, D., Wang, X.-J., and Fusi, S. (2010). Internal representation of task rules by recurrent dynamics: The importance of the diversity of neural responses. Front. Comput. Neurosci. 4, 24.
Rigotti, M., Barak, O., Warden, M.R., Wang, X.-J., Daw, N.D., Miller, E.K., and Fusi, S. (2013). The importance of mixed selectivity in complex cognitive tasks. Nature 497, 585–590.
Rivkind, A., and Barak, O. (2017). Local dynamics in trained recurrent neural networks. Phys. Rev. Lett. 118, 258101.
Roudi, Y., and Latham, P.E. (2007). A balanced memory network. PLoS Comput. Biol. 3, 1679–1700.
Saez, A., Rigotti, M., Ostojic, S., Fusi, S., and Salzman, C.D. (2015). Abstract context representations in primate amygdala and prefrontal cortex. Neuron 87, 869–881.
Shadlen, M.N., and Newsome, W.T. (1998). The variable discharge of cortical neurons: Implications for connectivity, computation, and information coding. J. Neurosci. 18, 3870–3896.
Shiino, M., and Fukai, T. (1993). Self-consistent signal-to-noise analysis of the statistical behavior of analog neural networks and enhancement of the storage capacity. Phys. Rev. E Stat. Phys. Plasmas Fluids Relat. Interdiscip. Topics 48, 867–897.

622 Neuron 99, 609–623, August 8, 2018

Sompolinsky, H., Crisanti, A., and Sommers, H.J. (1988). Chaos in random neural networks. Phys. Rev. Lett. 61, 259–262.
Sussillo, D. (2014). Neural circuits as computational dynamical systems. Curr. Opin. Neurobiol. 25, 156–163.
Sussillo, D., and Abbott, L.F. (2009). Generating coherent patterns of activity from chaotic neural networks. Neuron 63, 544–557.
Sussillo, D., and Barak, O. (2013). Opening the black box: Low-dimensional dynamics in high-dimensional recurrent neural networks. Neural Comput. 25, 626–649.
Tao, T. (2013). Outliers in the spectrum of iid matrices with bounded rank perturbations. Probab. Theory Relat. Fields 155, 231–263.
Thalmeier, D., Uhlmann, M., Kappen, H.J., and Memmesheimer, R.M. (2016). Learning universal computations with spikes. PLoS Comput. Biol. 12, e1004895.

Tirozzi, B., and Tsodyks, M. (1991). Chaos in highly diluted neural networks. EPL 14, 727.
van Vreeswijk, C., and Sompolinsky, H. (1996). Chaos in neuronal networks with balanced excitatory and inhibitory activity. Science 274, 1724–1726.
Wang, X.-J. (2002). Probabilistic decision making by slow reverberation in cortical circuits. Neuron 36, 955–968.
Wang, J., Narain, D., Hosseini, E.A., and Jazayeri, M. (2018). Flexible timing by temporal scaling of cortical responses. Nat. Neurosci. 21, 102–110.
Williamson, R.C., Cowley, B.R., Litwin-Kumar, A., Doiron, B., Kohn, A., Smith, M.A., and Yu, B.M. (2016). Scaling properties of dimensionality reduction for neural populations and network models. PLoS Comput. Biol. 12, e1005141.

Neuron 99, 609–623, August 8, 2018 623

STAR+METHODS
KEY RESOURCES TABLE
REAGENT or RESOURCE Software and Algorithms Algorithms for solving Dynamical Mean-Field equations

SOURCE this paper

IDENTIFIER https://github.com/fmastrogiuseppe/lowrank/

CONTACT FOR REAGENT AND RESOURCE SHARING

Further requests for resources should be directed to and will be fulﬁlled by the Lead Contact, Srdjan Ostojic (srdjan.ostojic@ens.fr).

METHOD DETAILS

The network model We study large recurrent networks of rate units. Every unit in the network is characterized by a continuous variable xiðtÞ, commonly interpreted as the total input current. More generically, we also refer to xiðtÞ as the activation variable. The output of each unit is a nonlinear function of its inputs modeled as a sigmoidal function fðxÞ. In line with previous works (Sompolinsky et al., 1988; Sussillo and Abbott, 2009; Rivkind and Barak, 2017), we focus on fðxÞ = tanhðxÞ, but we show that qualitatively similar dynamical regimes appear in network models with more realistic, positively deﬁned activation functions (Figure S7). The transformed variable fðxiðtÞÞ is interpreted as the ﬁring rate of unit i, and is also referred to as the activity variable.
The time evolution is speciﬁed by the following dynamics:

XN

x_iðtÞ = À xiðtÞ + JijfðxjðtÞÞ + Ii:

(6)

j=1

We considered a particular class of connectivity matrices, which can be written as a sum of two terms:

Jij = gcij + Pij:

(7)

Similarly to (Sompolinsky et al., 1988), cij is a Gaussian all-to-all random matrix, where every element is drawn from a centered normal distribution with variance 1=N. The parameter g scales the strength of random connections in the network, and we refer to it also as the random strength. The second term Pij is a low-rank matrix. In this study, we consider the low-rank part of the connectivity ﬁxed, while the random part varies between different realizations of the connectivity. Our results rely on two simplifying assumptions. The ﬁrst one is that the low-rank term and the random term are statistically uncorrelated. The second one is that, as stated in Equation 8p, thﬃﬃﬃe structured connectivity is weak in the large N limit, i.e., it scales as 1=N, while the random connectivity components cij scale as 1= N.
We ﬁrst consider the simplest case where Pij is a rank-one matrix, which can generally be written as the external product between two one-dimensional vectors m and n:

Pij

=

mi nj N

:

(8)

According to our ﬁrst assumption, the entries of vectors m and n are independent of the random bulk of the connectivity cij. Note that the only non-zero eigenvalue of P is given by the scalar product mT n=N, and the corresponding right and left eigenvectors are, respectively, vectors m and n. In the following, we will refer to the eigenvalue mT n=N as the strength of the connectivity structure, and
to m and n as the right- and left-connectivity vectors. Here we focus on vectors obtained by generating the components from a joint
Gaussian distribution.
More general connectivity structures of rank r ( N can be written as a sum of unit-rank terms

Pij

=

mði 1Þnðj 1Þ N

+

.

+

mði rÞnðj rÞ; N

(9)

and are therefore speciﬁed by r pairs of vectors mðkÞ and nðkÞ, where different m vectors are linearly independent, and similarly for n vectors.

e1 Neuron 99, 609–623.e1–e29, August 8, 2018

Overview of Dynamical Mean-Field Theory Our results rely on a mathematical analysis of network dynamics based on Dynamical Mean-Field (DMF) theory (Sompolinsky et al., 1988; Rajan et al., 2010; Kadmon and Sompolinsky, 2015). To help navigate the analysis, here we provide ﬁrst a succint overview of the approach. Full details are given further down in the section Details of Dynamical Mean-Field Theory.
DMF theory allows one to derive an effective description of the dynamics by averaging over the disorder originating from the random part of the connectivity. Across different realizations of the random connectivity matrix cij, the sum of inputs to unit i is approximated by a Gaussian stochastic process hiðtÞ

XN

JijfðxjðtÞÞ + IizhiðtÞ;

(10)

j=1

so that each unit obeys a Langevin-like equation:

x_iðtÞ = À xiðtÞ + hiðtÞ:

(11)

The Gaussian processes hi can in principle have different ﬁrst and second-order statistics for each unit, but are otherwise statistically independent across different units. As a consequence, the activations xi of different units are also independent Gaussian sto-
chastic processes, coupled only through their ﬁrst and second-order statistics. The core of DMF theory consists of self-consistent equations for the mean mi and auto-correlation function DIiðtÞ.
At equilibrium (i.e., in absence of transient dynamics) the equation for the mean mi of xi is obtained by directly averaging Equation 6 over the random part of the connectivity. For a unit-rank connectivity, it reads

mi = kmi + Ii;

(12)

where

k

=

1 N

XN
j=1

ÂÃ nj fj :

(13)

In the last equation, we adopted the short-hand notation fiðtÞ : = fðxiðtÞÞ. Here ½fj is the average ﬁring rate of unit j, i.e., fðxjÞ

averaged over the Gaussian variable xj. In a geometrical interpretation, the quantity k represents the overlap between the left-con-

nectivity vector n and the vector of average ﬁring rates. Equivalently, it is given by a population average of nj½fj, which can also be

expressed as

Z

Z

qﬃﬃﬃﬃﬃ 

k = dm dn dI pðm; n; IÞn Dzf mk + I + DI0 z

(14)

wR here Dz =

pRð+mN;
ÀN

n; IÞ eÀz2

is pthﬃﬃﬃeﬃﬃﬃ joint =2= 2pdz.

distribution

of

components

of

vectors

m,

n

and

I.

DI0

is

the

variance

of

xi

(see

below),

and

The auto-correlation function DIiðtÞ quantiﬁes the ﬂuctuations of the activation xi around the expected mean. Computing this auto-

correlation function shows that it is identical for all units in the network, i.e., independent of i (see Equation 27). It can be decomposed

into a static variance, which quantiﬁes the ﬂuctuations of the equilibrium values of xi across different realizations of the random component of the connectivity, and an additional temporal variance which is present when the network is in a temporally ﬂuctuating, chaotic state. In a stationary state, the variance DI0hDIðt = 0Þ can be expressed as

DI0

=

g2 1 N

XN
j=1

ÂÃ f2i :

(15)

where ½f2i  is the average of f2i ðxÞ over the Gaussian variable xi. The right-hand-sides of Equations 13 and 15 show that both the mean mi and variance DI0 depend on population-averaged, macro-
scopic quantities. To fully close the DMF description, the equations for single-unit statistics need to be averaged over the population. For static equilibrium dynamics, this leads to two coupled equations for two macroscopic quantities, the overlap k and the static, population-averaged variance D0:

k = Fðk; D0Þ D0 = Gðk; D0Þ:

(16)

Here F and G are two non-linear functions, the speciﬁc form of which depends on the geometrical arrangement of the connectivity vectors m and n and the input vector I. For temporally ﬂuctuating, chaotic dynamics an additional macroscopic quantity (corresponding to the temporal variance) needs to be taken into account. In that case, the full DMF description is given by a system of three nonlinear equations for three unknowns. The equilibrium states of the network dynamics are therefore obtained by solving these systems of equations using standard non-linear methods.

Neuron 99, 609–623.e1–e29, August 8, 2018 e2

To describe the transient dynamics and assess the stability of the obtained equilibrium states, we determined the spectrum of eigenvalues at the obtained equilibrium ﬁxed points. This spectrum consists of two components: a continuous, random component distributed within a circle in the complex plane, and a single outlier induced by the structured part of the connectivity (Figures S1A and S1D). The radius of the continuous component and the value of the outlier depend on the connectivity parameters. Although the two quantities in general are non-trivially coupled, the value of the radius is mostly controlled by the strength of the disorder, while the value of the outlier increases with the strength mT n=N of the rank-one structure (Figure S1F). The equilibrium is stable as long as the real part of all eigenvalues is less than unity. For large connectivity structure strengths, the outlier crosses unity, generating an instability that leads to the appearance of one-dimensional structured activity. Increasing the disorder strength on the other hand leads to another instability, corresponding to the radius of the continuous component crossing unity. This instability gives rise to chaotic, ﬂuctuating activity.
When a linear readout with weights wi is added to the network, its average output is given by

zðtÞ

=

1 N

XN
i=1

wi ½fi ðtÞ;

(17)

i.e., by the projection of the average network ﬁring rate on the readout vector w. This quantity is analogous to k, except that the vector

n is replaced by the vector w, so that similarly to Equation 14, the average readout can also be expressed as

Z

Z



qﬃﬃﬃﬃﬃ 

z = dm dw dI pðm; w; IÞ w Dyf mz + I + DI0 y

(18)

and therefore directly depends on the joint distribution pðm; w; IÞ which characterizes the geometric arrangement of vectors m, w and I.
The DMF theory can be directly extended to connectivity structures of rank r greater than one. The equilibrium mean input to unit i is then given by

Xr

mi =

kðkÞmði kÞ + Ii:

(19)

k=1

The activity therefore lives in an ðr + 1Þ-dimensional space determined by the r right-connectivity vectors mðkÞ and the input vector I. It is characterized by r overlaps kðkÞ, each of which quantiﬁes the amount of activity along the corresponding direction mðkÞ. Averaging over the population, the DMF theory then leads to a system of r + 1 nonlinear coupled equations for describing stationary dynamics.

DETAILS OF DYNAMICAL MEAN-FIELD THEORY

Here we provide the full details of the mathematical analysis. We start by examining the activity of a network with a rank-one structure in absence of external inputs (Ii = 0 ci in Equation 6).

Single-unit equations for spontaneous dynamics We start by determining the statistics of the effective noise hi to unit i, deﬁned by

hi

ðtÞ

=

g

XN
j=1

cij

fðxj

ðtÞÞ

+

mi N

XN nj fðxj ðtÞÞ:
j=1

(20)

The DMF theory relies on the hypothesis that a disordered component in the coupling structure, here represented by cij, efﬁciently decorrelates single neuron activity when the network is sufﬁciently large. We will show that this hypothesis of decorrelated activity is
self-consistent for the speciﬁc network architecture we study.
As in standard DMF derivations, we characterize self-consistently the distribution of hi by averaging over different realizations of the random matrix cij (Sompolinsky et al., 1988; Rajan et al., 2010). In the following, ½: indicates an average over the realizations of the random matrix cij, while h:i stands for an average over different units of the network. Note that the network activity can be equivalently characterized in terms of input current variables xiðtÞ or their non-linear transforms fðxiðtÞÞ. As these two quantities are not independent, the statistics of the distribution of the latter can be written in terms of the statistics of the former.
The mean of the effective noise received by unit i is given by:

½hi

ðtÞ

=

g

XN Â cij
j=1

fðxj

Ã ðtÞÞ

+

mi N

XN nj ½fðxj ðtÞÞ:
j=1

(21)

e3 Neuron 99, 609–623.e1–e29, August 8, 2018

Under the hypothesis that in large networks, neural activity decorrelates (more speciﬁcally, that activity fðxjðtÞÞ is independent of its outgoing weights), we have:

½hi

ðtÞ

=

g

XN Â cij
j=1

Ã ½fðxj

ðtÞÞ

+

mi N

XN nj½fðxjðtÞÞ = mik
j=1

(22)

as ½cij = 0. Here we introduced

1 XN

 Â Ã

k: = N

nj½fðxjðtÞÞ =
j=1

nj

fj ðtÞ

;

(23)

which quantiﬁes the overlap between the mean population activity vector and the left-connectivity vector n. Similarly, the noise correlation function is given by

Â hi

ðtÞhj

ðt

+

Ã tÞ

=

g2

XN

k=1

XN Â cik
l=1

cjl

Ã ½fðxk

ðtÞÞfðxl

ðt

+

tÞÞ

+

mi mj N2

XN
k=1

XN nknl½fðxkðtÞÞfðxlðt + tÞÞ:
l=1

(24)

Note that every cross-term in the product vanishes since ½cij = 0. Similarly to standard DMF derivations (Sompolinsky et al., 1988), the ﬁrst term on the r.h.s. vanishes for cross-correlations ðisjÞ while it survives in the auto-correlation function ði = jÞ, as ½cikcjl = dijdkl=N. We get:

Â hi

ðtÞhj

ðt

+

Ã tÞ

=

dij

g2h½fi

ðtÞfi

ðt

+

tÞi

+

mi mj N2

XN
k=1

XN nknl½fðxkðtÞÞfðxlðt + tÞÞ:
l=1

(25)

We focus now on the second term in the right-hand side. The corresponding sum contains N terms where k = l. This contribution

vanishes in the large N limit because of the 1=N2 scaling. According to our starting hypothesis, when ksl, activity decorrelates:

½fkðtÞflðt + tÞ = ½fkðtÞ½flðt + tÞ. To the leading order in N, we get:

Â hi ðtÞhj ðt

+

Ã tÞ

=

dij g2 h½fi ðtÞfi ðt

+

tÞi

+

mi mj N2

X nk
k

½fðxk

X ðtÞÞ nl½fðxlðt
lsk

+

tÞÞ

(26)

= dijg2h½fiðtÞfiðt + tÞi + mimjk2

so that:

Â hi

ðtÞhj

ðt

+

Ã tÞ

À

½hi

Â ðtÞ hj

Ã ðtÞ

=

dij

g2

h½fi

ðtÞfi

ðt

+

tÞi:

(27)

We therefore ﬁnd that the statistics of the effective input are uncorrelated across different units, so that our initial hypothesis is selfconsistent.
To conclude, for every unit i, we computed the ﬁrst- and the second-order statistics of the effective input hiðtÞ. The expressions we obtained show that the individual noise statistics depend on the statistics of the full network activity. In particular, the mean of the effective input depends on the average overlap k, but varies from unit to unit through the components of the right-connectivity vector m. On the other hand, the auto-correlation of the effective input is identical for all units, and determined by the population-averaged ﬁring rate auto-correlation h½fiðtÞfiðt + tÞi.
Once the statistics of hiðtÞ have been determined, a self-consistent solution for the activation variable xiðtÞ can be derived by solving the Langevin-like stochastic process from Equation 11. As a ﬁrst step, we look at its stationary solutions, which correspond to the ﬁxed points of the original network dynamics.

Population-averaged equations for stationary solutions
For any solution that does not depend on time, the mean mi and the variance DI0 of the variable xi with respect to different realizations of the random connectivity coincide with the statistics of the effective noise hi. From Equations 22 and 27, the mean mi and variance DI0 of the input to unit i therefore read

mi : DI0 :

==½xÂxi i2=Ã

mi À

k ½xi

2

=

g2

Âf2i

Ã

(28)

while any other cross-variance ½xixj À ½xi½xj vanishes. We conclude that, on average, the structured connectivity Pij shapes the network activity along the direction speciﬁed by its right eigenvector m. Such a heterogeneous stationary state critically relies on
a non-vanishing overlap k between the left eigenvector n and the average population activity vector ½f. Across different realizations
of the random connectivity, the input currents xi ﬂuctuate around these mean values. The typical size of ﬂuctuations is determined by the individual variance DI0, equal for every unit in the network.

Neuron 99, 609–623.e1–e29, August 8, 2018 e4

The r.h.s. of Equation 28 contains two population averaged quantities, the overlap k and the second moment of the activity h½f2i i. To close the equations, these quantities need to be expressed self-consistently. Averaging Equation 28 over the population, we get
expressions for the population-averaged mean m and variance D0 of the input:

m: D0

= h½xÂii =Ãhmi : = xi2 À

ik h½xi

i2

=

Â Ã g2 f2i

+

  m2i

À

 hmii2 k2:

(29)

Note that the total population variance D0 is a sum of two terms: the ﬁrst term, proportional to the strength of the random part of connectivity, coincides with the individual variability DI0 which emerges from different realizations of cij; the second term, proportional to the variance of the right-connectivity vector m, coincides with the variance induced at the population level by the spread of the

mean values mifmi. When the vector m is homogeneous ðmi = mÞ, input currents xi are centered around the same mean value m, and the second variance term vanishes.

We next derive appropriate expression for the r.h.s. terms k and h½f2i i. To start with, we rewrite ½fi by substituting the average over the random connectivity with the equivalent Gaussian integral:

Z  qﬃﬃﬃﬃﬃ 

½fi = Dzf mi + DI0 z

(30)

where

we

used

the

short-hand

notation

R

Dz

=

R +N
ÀN

eÀz2

=2

pﬃﬃﬃﬃﬃﬃ = 2pdz.

To

obtain

k,

½fi 

needs

to

be

multiplied

by

ni

and

averaged

over

the

population. This average can be expressed by representing the ﬁxed vectors m and n through the joint distribution of their elements

over the components:

pðm;

nÞ

=

1 N

XN
j=1

dðm

À

mj Þdðn

À

nj Þ:

(31)

This leads to

( Z  qﬃﬃﬃﬃﬃ )

k = ni Dzf mi + DI0 z

ZZ

Z  qﬃﬃﬃﬃﬃ 

(32)

= dm dn pðm; nÞ n Dzf mk + DI0 z :

Similarly, a suitable expression for the second-order momentum of the ﬁring rate is given by:

Âf2i Ã = Z

Z dm pðmÞ

 qﬃﬃﬃﬃﬃ  Dzf2 mk + DI0 z :

(33)

Equations 32 and 33, combined with Equation 29, provide a closed set of equations for determining k and D0 once the vectors m

and n have been speciﬁed.

To further simplify the problem, we reduce the full distribution pðm; nÞ of elements mi and ni to their ﬁrst- and second-order

momenta. That is equivalent to substituting the probability density pðm; nÞ with a bivariate Gaussian distribution. We therefore write:

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ m = Mm + Spm ﬃﬃﬃﬃ1ﬃﬃﬃﬃÀﬃﬃﬃ r x1 n = Mn + Sn 1 À r x2 +

+SnSpmﬃrpﬃ yﬃrﬃ

y

(34)

where x1, x2 and y are three normal Gaussian processes. Here, Mm (resp. Mn) and Sm (resp. Sn) correspond to the mean and the stan-

dard deviation of m (resp. n), pretation, Mm and Mn are the

while the covariance between m and projections of N–dimensional vectors

nmisangdivennobnytohtmheinuipinÀﬃiﬃtﬃﬃaﬃMﬃrﬃﬃyﬃmﬃﬃvMecn t=orSupm=ﬃSﬃﬃﬃnðﬃﬃ1rﬃﬃﬃ.;ﬃﬃ1W;.ith1inÞ=aNg, eSommpeﬃrﬃtraicnadl

Sinntperﬃr-ﬃ

are the projections onto a direction orthogonal to u and common to m and n, and Sm 1 À r and Sn 1 À r scale the parts of m and n

that are mutually orthogonal.

The expression for k becomes:

Z k=

Z Dy

Dx2

 Mn

+

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Sn 1 À r

x2

+

Sn

pﬃﬃ r

 y

Z

Z Dz

Dx1

 f k Mm

+

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Sm 1 À r

x1

+

Sm

pﬃﬃ r

 y

+

qﬃﬃﬃﬃﬃ DI0

 z

(35)

which

gives

rise

to

three

terms

when

expanding

the

sum

Mn

+

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Sn 1 À rx2

+

Sn pﬃrﬃ y.

The

ﬁrst

term

can

be

rewritten

as:

Z



qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 

Mn Dz f Mmk + DI0 + S2mk2 z

Z

 pﬃﬃﬃﬃﬃ 

(36)

= Mn Dz f m + D0 z

= Mnh½fii;

e5 Neuron 99, 609–623.e1–e29, August 8, 2018

which coincides with the overlap between vectors n and ½f along the unitary direction u = ð1; 1;.1Þ=N. In the last step, we rewrote

our expression for k in terms of the population averaged statistics m and D0 (Equation 29).

The second term vanishes, while the third one gives:

Snpﬃrﬃ Z

Z Dy y

Z Dz

Dx1

 f k Mm

+

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Sm 1 À r

x1

+

Sm

pﬃﬃ r

 y

+

qﬃﬃﬃﬃﬃ DI0

 z

= krSmSnÂf0i Ã

(37)

which coincides with the overlap between n and ½f in a direction orthogonal to u. Here we used the equality:

Z

Z Dz zfðzÞ =

Dz

df ðzÞ dz

(38)

which is obtained by integrating by parts.

Through a similar reasoning we obtain:

Âf2i Ã = Z

 pﬃﬃﬃﬃﬃ  Dz f2 m + D0 z

(39)

as in standard DMF derivations. To conclude, the mean-ﬁeld description of stationary solutions reduces to the system of three implicit equations for m, k and D0:

m = MmkÂ Ã

D0 = g2 f2i k = Mmh½fii

+

+ S2mk2 krSmSn

Âf0i

Ã :

(40)

Both averages h½:i are performed with respect to a Gaussian distribution of mean m and variance D0. Once m, D0 and k have been determined, the single unit mean mi and the individual variance DI0 are obtained from Equation 28.
The dynamical mean-ﬁeld equations given in Equation 40 can be fully solved to determine stationary solutions. Detailed descrip-
tions of these solutions are provided further down for two particular cases: (i) overlap between m and n only along the unitary direction
u (Mms0, Mns0, r = 0); (ii) overlap between m and n only in a direction orthogonal to u (Mm = Mn = 0, rs0).

Transient dynamics and stability of stationary solutions

We now turn to transient dynamics around ﬁxed points, and to the related problem of evaluating whether the stationary solutions

found within DMF are stable with respect to the original network dynamics (Equation 6).

For any given realization of the connectivity matrix, the network we consider is completely deterministic. We can then study the

local, transient dynamics by linearizing the dynamics around any stationary solution. We therefore look at the time evolution of a small

displacement away from the ﬁxed point: xðtÞ = xi0 + xi1ðtÞ. For any generic stationary solution fxi0g the linearized dynamics are given

by the stability matrix Sij which reads:

Sij

=

f0

  xj0 gcij

+

mi

nj

 :

N

(41)

If the real part of every eigenvalue of Sij is smaller than unity, the perturbation decays in time and thus the stationary solution is stable.

Homogeneous stationary solutions We ﬁrst consider homogeneous stationary solutions, for which xi0 = x for all units. A particular homogeneous solution is the trivial solution x = 0, which the network admits for all parameter values when the transfer function is fðxÞ = tanhðxÞ. Other homogeneous
solutions can be obtained when the vector m is homogeneous, i.e., mi = Mm for all i. For homogeneous solutions, the stability matrix reduces to a scaled version of the connectivity matrix:

Sij = f0ðxÞJij:

(42)

We are thus left with the problem of evaluating the eigenspectrum of the global connectivity matrix Jij. The matrix Jij consists of a full-rank component cij, the entries of which are drawn at random, and of a structured component of small dimensionality with ﬁxed entries. We focus on the limit of large networks; in that limit, an analytical prediction for the spectrum of its eigenvalues can be
derived. Because of the 1=N scaling, the matrix norm of Pij is bounded as N increases. We can then apply results from random matrix theory
(Tao, 2013) which predict that, in the large N limit, the eigenspectra of the random and the structured parts do not interact, but sum
together. The eigenspectrum of Jij therefore consists of two separated components, inherited respectively from the random and the structured terms (Figure S1A). Similarly to (Girko, 1985), the random term cij returns a set of N À 1 eigenvalues which lie on the complex plane in a compact circular region of radius g. In addition to this component, the eigenspectrum of Jij contains the non-zero

Neuron 99, 609–623.e1–e29, August 8, 2018 e6

P eigenvalues of Pij: in the case of a rank-one matrix, one single outlier eigenvalue is centered at the position imini=N = hminii. In Figure S1B we measure both the outlier position and the radius of the compact circular component. We show that deviations from the theoretical predictions are in general small and decay to zero as the system size is increased.
Going back to the stability matrix Sij = f0 ðxÞJij, we conclude that a homogeneous stationary solution can lose stability in two different ways, when either mT n=N or g become larger than 1=f0 ðxÞ. We expect different kinds of instabilities to occur in the two cases. When g crosses the instability line, a large number of random directions become unstable at the same time. As in (Sompolinsky et al., 1988), this instability is expected to lead to the onset of irregular temporal activity. When the instability is lead by the outlier, instead, the trivial ﬁxed point becomes unstable in one unique direction given by the corresponding eigenvector. When g = 0, this eigenvector coincides exactly with m. For ﬁnite values of the disorder g, the outlier eigenvector ﬂuctuates depending on the random part of the connectivity, but remains strongly correlated with m (Figure S1C), which therefore determines the average direction of the instability. Above the instability, as the network dynamics is completely symmetric with respect to a change of sign of the input variables, we expect the non-linear boundaries to generate two symmetric stationary solutions.

Heterogeneous stationary solutions

A second type of possible stationary solutions are heterogeneous ﬁxed points, in which different units reach different equilibrium

values. For such ﬁxed points, the linearized stability matrix Sij is obtained by multiplying each column of the connectivity matrix Jij

by a different gain value (see Equation 41), so that the eigenspectrum of Sij is not trivially related to the spectrum of Jij.

Numerical investigations reveal that, as for Jij, the eigenspectrum of Sij consists of two discrete components: one compact set of N À 1 eigenvalues contained in a circle on the complex plane, and a single isolated outlier eigenvalue (Figure S1D).

As previously noticed in (Harish and Hansel, 2015), the radius of the circular compact set r can be computed as in (Rajan and Ab-

bott, 2006; Aljadeff et al., 2015b) by summing the variances of the distributions in every column of Sij. To the leading order in N:

r = gv u u tﬃN1ﬃﬃﬃﬃﬃX jﬃﬃ=Nﬃﬃ1ﬃﬃﬃfﬃﬃﬃ0ﬃ2ﬃﬃﬃﬃﬃxﬃﬃﬃ0jﬃﬃﬃﬃ

(43)

which, in large networks, can be approximated by the mean-ﬁeld average:

r = gqﬃﬃﬃÂﬃﬃfﬃﬃﬃ0i2ﬃﬃÃﬃﬃﬃﬃ:

(44)

Note that, because of the weak scaling in Pij, the structured connectivity term does not appear explicitly in the expression for the
radius. As the structured part of the connectivity determines the heterogeneous ﬁxed point, the value of r however depends implicitly on the structured connectivity term through h½fi02i, which is computed as a Gaussian integral over a distribution with mean m and variance D0 given by Equation 40. In Figures S1D–S1F, we show that Equation 44 approximates well the radius of ﬁnite-size, numer-
ically computed eigenspectra. Whenever the mean-ﬁeld theory predicts instabilities led by r, we expect the network dynamics to
converge to irregular non-stationary solutions. Consistently, at the critical point, where r = 1, the DMF equations predict the onset
of temporally ﬂuctuating solutions (see later on in STAR Methods).
We now turn to the problem of evaluating the position of the outlier eigenvalue. In the case of heterogeneous ﬁxed points, the structured and the random components of the matrix Sij are strongly correlated, as they both scale with the multiplicative factor f0ðxj0Þ, which correlates with the particular realization of the random part of the connectivity cij. As a consequence, cij cannot be considered as a truly random matrix with respect to mif0ðxj0Þnj=N, and in contrast to the case of homogeneous ﬁxed points, results from (Girko, 1985) do not hold.
We determined numerically the position of the outlier in ﬁnite-size eigenspectra (Figures S1D–S1F). We found that its value indeed signiﬁcantly deviates from the only non-zero eigenvalue of the rank-one structure mif0 ðxj0Þnj=N, which can be computed in the meanﬁeld framework (when r = 0, it corresponds to MmMnh½f0ii + MnkS2mh½f0i0i). On the other hand, the value of the outlier coincides exactly with the eigenvalue of mif0 ðxj0Þnj=N whenever the random component cij is shufﬂed (black dots in Figure S1F). This observation conﬁrms that the position of the outlier critically depends on the correlations existing between the rank-one structure mif0 ðxj0Þnj=N and the speciﬁc realization of the random bulk cij.

Mean-ﬁeld analysis of transient dynamics and stability of stationary solutions

As for heterogeneous ﬁxed points we were not able to assess the position of the outlying eigenvalue using random matrix theory, we turned to a mean-ﬁeld analysis to determine transient activity. This analysis allowed us to determine accurately the position of the outlier, and therefore the stability of heterogeneous ﬁxed points. The approach exploited here is based on (Kadmon and Sompolinsky, 2015).

We consider the stability of the single unit activation xi when averaged across different realizations of the random connectivity and its random eigenmodes. Directly averaging across realizations the network dynamics deﬁned in Equation 6 yields the time evolution of the mean activation mi of unit i:

m_ iðtÞ = À miðtÞ + mikðtÞ:

(45)

e7 Neuron 99, 609–623.e1–e29, August 8, 2018

We observe that we can write: miðtÞ = mi~kðtÞ, where ~k is the low-pass ﬁltered version of k: ð1 + d=dtÞ~kðtÞ = kðtÞ. Small perturbations around the ﬁxed point solution read: miðtÞ = m0i + m1i ðtÞ. The equilibrium values m0i correspond to the DMF stationary solution computed from Equation 28 and 40: m0i = mik0. The ﬁrst-order perturbations thus obey:

m_ 1i ðtÞ = À m1i ðtÞ + mik1ðtÞ;

(46)

indicating that the decay timescale of the mean activity is inherited by the decay time constant of k1. An additional equation for the

time evolution of k1 thus needs to be derived.

When activity is perturbed, the ﬁring activity fi of unit i can be evaluated at the ﬁrst order: f0i /f0i + f1i ðtÞ = fðxi0Þ + f0 ðxi0Þxi1ðtÞ. As a consequence, the ﬁrst-order in k reads:

k1

ðtÞ

=

 ni

Âf0

Àxi0

Áxi1

Ã ðtÞ :

(47)

Summing Equation 47 to its time-derivative, we get:

k_ 1ðtÞ =

À

k1

ðtÞ

+

 1

+

d  dt ni

Âf0

Àxi0

Áxi1

Ã ðtÞ :

(48)

In order to simplify the r.h.s., we start by considering the average with respect to the random part of the connectivity for a single unit

i. In order to compute ½f0 ðxi0Þxi1, we explicitly build xi0 and xit : = xiðtÞ as Gaussian variables centered respectively in m0i and mti . We will call DI00 and DI0t the variances of the two variables, and DI;t:0 their two-times correlation deﬁned by DI;t:0 = ½xitxi0 À ½xit½xi0. We can then

write the two variables as

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pﬃﬃﬃﬃﬃﬃﬃﬃﬃ

xi0

=

m0i

+qﬃﬃDﬃﬃﬃI0ﬃ0ﬃﬃﬃÀﬃﬃﬃﬃﬃDﬃﬃﬃIﬃ;ﬃtﬃ:0

x1

+ DI;t:0 pﬃﬃﬃﬃﬃﬃﬃﬃﬃ

y

(49)

xit = mti + DI0t À DI;t:0x2 + DI;t:0y

The ﬁrst-order response of xi is given by the difference between xit and xi0, and reads: qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ

xi1 = m1i + DI0t À DI;t:0x2 À DI00 À DI;t:0x1:

(50)

As in classical DMF derivations (Sompolinsky et al., 1988; Rajan et al., 2010; Kadmon and Sompolinsky, 2015), x1, x2and y are stan-

dard normal variables. By integrating over their distributions we can write:

Âf0 Àxi0Áxi1Ã = Z

Z Dx1

 qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ  Z Dx2 m1i + DI0t À DI;t:0 x2 À DI00 À DI;t:0 x1

 qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pﬃﬃﬃﬃﬃﬃﬃﬃﬃ  Dyf0 m0i + DI00 À DI;t:0 x1 + DI;t:0 y :

(51)

Integrating by parts as in Equation 38 we get:

Âf0 Àxi0Áxi1Ã = m1i Âf0i Ã + ÀDI;t:0 À DI00ÁÂf0i0 Ã

(52)

where the Gaussian integrals ½f0i  and ½f0i0  are evaluated using the ﬁxed point statistics.

Note that, at the ﬁxed point, DI;t:0 = DI00. As a consequence, DI;t:0 À DI00 gives a ﬁrst-order response:

DI;1:0: = DI;t:0 À DI00 = Âxi1xi0Ã À Âxi1ÃÂxi0Ã = Âxi1xi0Ã À m0i m1i

(53)

which can be rewritten as a function of the global second-order statistics D1:0 = h½xi1xi0i À h½xi1ih½xi0i as:

DI;1:0 = D1:0 À Èm1i m0i  À m1i m0i É = D1:0 À S2m~k0~k1:

(54)

Equation 54 can be rewritten in terms of the ﬁrst-order perturbation for the global equal-time variance: D10 = Dt0 À D00. We consider

that, by deﬁnition:

D1:0

=

XN
j=1

xj1

vDt:0 vxjt

D10

=

XN
j=1

xj1

vDt0 vxjt

 0

 :

0

(55)

Neuron 99, 609–623.e1–e29, August 8, 2018 e8

We then observe that, when the derivatives are evaluated at the ﬁxed point, we have:

vDt:0 vxjt



0

=

1 2

vDt0 vxjt



0

;

(56)

and we conclude that:

D1:0 = 12D10

(57)

Equation 52 thus becomes:

Âf0 Àxi0Áxi1Ã

=

mi

~k1Âf0i Ã

+

D10 2

À

S2m

~k0

~k1

Âf0i0

Ã :

(58)

In a second step, we perform the average across different units of the population, by writing m and n as in Equation 34. After some algebra, we get:

 ni

Âf0

Àxi0

Áxi1

Ã ðtÞ

=

~k1

Â ðMm

Mn

+

rSm Sn

ÞÂf0i

Ã

+

rk0

Mm Sm Sn Âf0i0

ÃÃ

+

D10 2

Â Mn

Âf0i0

Ã

+

rk0 Sm Sn Âf0i00

ÃÃ

(59)

: = ~k1a + D10b

where constants a and b were deﬁned as:

a = ðMmMn + rSmSnÞÂf0i Ã + rk0MmSmSnÂf0i0 Ã

b

=

1 2

È Mn

Âf0i0

Ã

+

rk0

Sm

Sn

Âf0i00

ÃÉ :

(60)

The time evolution of k can be ﬁnally rewritten as:

k_ 1ðtÞ =

À

k1

ðtÞ

+

 1

+

d È~k1 dt

a

+

D10

É b;

(61)

so that the time evolution of the perturbed variance must be considered as well. In order to isolate the evolution law of D0, we rewrite the activation variable xiðtÞ by separating the uniform and the heterogeneous
components: xiðtÞ = mðtÞ + dxiðtÞ. The time evolution for the residual dxiðtÞ is given by:

d_xiðtÞ =

À

XN dxiðtÞ + g cijfðxjðtÞÞ + ðmi

À

MmÞkðtÞ

(62)

j=1

so that, squaring:

 ddxi

ðtÞ2

dt

+

2dxi ðtÞ

ddxi ðtÞ dt

+

dxi ðtÞ2

=

g2

XN
j=1

XN
k=1

cij cik fðxj ðtÞÞfðxk ðtÞÞ

+

ðmi

À

Mm Þ2 kðtÞ2

+

gðmi

À

MmÞkðtÞ

XN
k=1

cij fðxk ðtÞÞ:

(63)

Averaging over i and the realizations of the disorder yields:

dD0ðtÞ = dt

À

D0ðtÞ

+

g2

Âf2i

Ã ðtÞ

+

S2m

kðtÞ2

À

*" ddxi

ðtÞ2

#+

dt

:

=

À

D0

ðtÞ

+

Gðm;

D0

;

kÞ

À

*" ddxi

ðtÞ2

dt

#+

(64)

as by deﬁnition we have: h½dxi2ðtÞi = D0ðtÞ.

Expanding the dynamics of D0 to the ﬁrst order, we get:

D_ 10ðtÞ =

À

D10

ðtÞ

+

m1

vG vm



0

+

D10

vG vD0



0

+

k1

vG vk



0

:

(65)

e9 Neuron 99, 609–623.e1–e29, August 8, 2018

Note that we could neglect the contributions originating from the last term of Equation 64 because they do not enter at the leading

order. Indeed we have:

v vm

*" ddxi

ðtÞ2

dt

#+



( =2

0

ddxi ðtÞ dt

v vm

!) ddxi ðtÞ
dt



0

=

0

(66)

since temporal derivatives for every i vanish when evaluated at the ﬁxed point.

A little algebra returns the last three linear coefﬁcients:

vG vm



vG

vD0

vG vk



=

2g2

Â fi

f0i

Ã

0 = g2ÈÂfi02Ã +

0

= 2S2mk0:
0

Â fi

f00 i

ÃÉ

(67)

Collecting all the results together in Equation 61 we obtain:

k_ 1ðtÞ =

À

k1

ðtÞ

+

ak1

ðtÞ

+

& b m1

vG vm



0

+ D10vvDG0



0

+

k1

vG vk



0

' :

(68)

By averaging Equation 45 we furthermore obtain:

m_ 1ðtÞ = À m1ðtÞ + Mmk1:

(69)

We ﬁnally obtained that the perturbation timescale is determined by the population-averaged dynamics:

01 01 01

d dt

m1 @ D10
k1

A=

À

m1 @ D10
k1

m1 A + M@ D10
k1

A

(70)

where the evolution matrix M is deﬁned as:

0

1

M = @ 22bgg22Â0Âffifif0i Ã0i Ã

bgg22ÈÈÂÂffi02i02ÃÃ0++ÂÂffifif}i }iÃÃÉÉ

Mm 2S2mk0 A:
b2S2mk0 + a

(71)

Note that one eigenvalue of matrix M, which corresponds to the low-pass ﬁltering between k and m, is always ﬁxed to zero. Equations 70 and 71 reveal that, during the relaxation to equilibrium, the transient dynamics of the ﬁrst- and second-order statistics of the activity are tightly coupled. Diagonalizing M allows to retrieve the largest decay timescale of the network, which indicates the average, structural stability of stationary states. When an outlier eigenvalue is present in the eigenspectrum of the stability matrix Sij, the largest decay timescale from M predicts its position. The corresponding eigenvector eb contains indeed a structured component along m, which is not washed out by averaging across different realizations of cij. The second non-zero eigenvalue of M, which vanishes at g = 0, measures a second and smaller effective timescale, which derives from averaging across the remaining N À 1 random modes. Varying g, we computed the largest eigenvalue of M for corresponding stationary solutions of mean-ﬁeld equations. In Figure S1F we show that, when the stability eigenspectrum includes an outlier eigenvalue, its position is correctly predicted by the largest eigenvalue of M. The mismatch between the two values is small and can be understood as a ﬁnite-size effect (Figure S1E, gray). To conclude, we found that the stability of arbitrary stationary solutions can be assessed by evaluating, with the help of mean-ﬁeld theory, both the values of the radius (Equation 44) and the outlier (Equation 71) of the stability eigenspectrum. Instabilities led by the two different components are expected to reshape activity into two qualitatively different classes of dynamical regimes, which are discussed in detail, further in STAR Methods, for two speciﬁc classes of structures.

Dynamical Mean Field equations for chaotic solutions When a stationary state loses stability due to the compact component of the stability eigenspectrum, the network activity starts developing irregular temporal ﬂuctuations. Such temporally ﬂuctuating states can be described within the DMF theory by taking into account the full temporal auto-correlation function of the effective noise hi (Sompolinsky et al., 1988). For the sake of simplicity, here we derive directly the mean-ﬁeld equations for population-averaged statistics, and we eventually link them back to single unit quantities.

Neuron 99, 609–623.e1–e29, August 8, 2018 e10

By differentiating twice Equation 11, and by substituting the appropriate expression for the statistics of the noise hi, we derive that the auto-correlation function DðtÞ = h½xiðt + tÞxiðtÞi À h½xiðtÞi2 obeys the second-order differential equation:

D€ ðtÞ = DðtÞ À g2h½fiðtÞfiðt + tÞi À S2mk2:

(72)

In this context, the activation variance D0 coincides with the peak of the full auto-correlation function: D0 = Dðt = 0Þ. We expect the total variance to include a temporal term, coinciding with the amplitude of chaotic ﬂuctuations, and a quenched one, representing the
spread across the population due to the disorder in cij and the structure imposed by the right-connectivity vector m. In order to compute the full rate auto-correlation function h½fiðtÞfiðt + tÞi, we need to explicitly build two correlated Gaussian vari-
ables xðtÞ and xðt + tÞ, such that:

h½Âxxi ði2tðÞtÞiÃ=

h½xiðt + tÞi À h½xiðtÞi2

= =

mÂxi2

ðt

+

Ã tÞ

À

h½xi

ðtÞi2

=

D0

(73)

h½xiðt + tÞxiðtÞi À h½xiðtÞi2 = DðtÞ:

Following previous studies (Sompolinsky et al., 1988; Rajan et al., 2010), we obtain:

Z

Z

 pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pﬃﬃﬃ !2

h½fiðtÞfiðt + tÞi = Dz Dxf m + D0 À D x + D z

(74)

where we used the short-hand notation D : = DðtÞ and we assumed for simplicity D > 0. As we show later, this requirement is satisﬁed by our ﬁnal solution.
In order to visualize the dynamics of the solutions of Equation 72, we study the equivalent problem of a classical particle moving in a one-dimensional potential (Sompolinsky et al., 1988; Rajan et al., 2010):

D€ðtÞ =

À

vV vD

(75)

where the potential V is given by an integration over D:

VðD; D0Þ =

À

D2 2

+ g2h½FiðtÞFiðt + tÞi + S2mk2D

(76)

and

FðxÞ

=

Rx
ÀN

fðx0

Þdx0

.

As

the

potential

V

depends

self-consistently

on

the

initial

condition

D0,

the

shape of

the

auto-correlation

function DðtÞ depends parametrically on the value of D0. Similarly to previous works, we isolate the solutions that decay monoton-

ically from D0 to an asymptotic value Dðt/NÞ : = DN, where DN is determined by dV=dD j D = DN = 0. This translates into a ﬁrst condition to be imposed. A second equation comes from the energy conservation condition: VðD0;D0Þ = VðDN;D0Þ. Combined with the

usual equation for the mean m and the overlap k, the system of equations to be solved becomes:

m = Mmk

k = Mnh½fii + rkÂf0i Ã

D20

À

D2N

=

& g2

Z

2

 pﬃﬃﬃﬃﬃ  Z DzF2 m + D0 z À

Dz

Z

 pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pﬃﬃﬃﬃﬃﬃﬃ !2'

DxF m + D0 À DN x + DN z

+ S2mk2ðD0 À DNÞ

(77)

Z

Z

 pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pﬃﬃﬃﬃﬃﬃﬃ !2

DN = g2 Dz Dxf m + D0 À DN x + DN z + S2mk2:

The temporally ﬂuctuating state is therefore described by a closed set of equations for the mean activity m, the overlap k, the zerolag variance D0 and the long-time variance DN. The difference D0 À DN represents the amplitude of temporal ﬂuctuations. If temporal ﬂuctuations are absent, D0 = DN, and the system of equations we just derived reduces to the DMF description for stationary solutions given in Equation 40.
A similar set of equations can be derived for single unit activity. As for static stationary states, the mean activity of unit i is given by

mi = mik:

(78)

The static variance around this mean activity is identical for all units and given by

Z

Z

 pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pﬃﬃﬃﬃﬃﬃﬃ !2

DIN = g2 Dz Dxf m + D0 À DN x + DN z = DN À S2mk2

(79)

while the temporal component DIT of the variance is identical to the population averaged temporal variance

DIT = D0 À DN:

(80)

To conclude, similarly to static stationary states, the structured connectivity Pij shapes network activity in the direction deﬁned by its right eigenvector m whenever the overlap k does not vanish. For this reason, the mean-ﬁeld theory predicts in some parameter

e11 Neuron 99, 609–623.e1–e29, August 8, 2018

regions the existence of more than one chaotic solution. A formal analysis of the stability properties of the different solutions has not

been performed. We nevertheless observe from numerical simulations that chaotic solutions tend to inherit the stability properties of

the stationary solution they develop from. Speciﬁcally, when an homogeneous solution generates two heterogeneous bistable ones,

we notice that the former loses stability in favor of the latter.

We ﬁnally observe that the critical coupling at which the DMF theory predicts the onset of chaotic ﬂuctuations can be computed by

imposing that, at the critical point, the concavity of the potential function VðDÞ is inverted (Sompolinsky et al., 1988; Harish and

Hansel, 2015):

d2 V ðD; dD2

D0

Þ



DN

=

0

(81)

and the temporal component of the variance vanishes: D0 = DN. These two conditions are equivalent to the expression: 1 = g2h½fI02i where, as we saw, g2h½fi02i coincides with the squared value of the radius of the compact component of the stability eigenspectrum (Equation 44). In the phase diagram of Figure 1B, we solved this equation for g to derive the position of the instability boundary from

stationary to chaotic regimes.

Spontaneous dynamics: structures overlapping on the unitary direction
In this section, we analyze in detail a speciﬁc case, in which the connectivity vectors m and n overlap solely along the unitary direction u = ð1; 1;.1Þ=N. Within the statistical description of vector components, in this situation the joint probability density pðm; nÞ can be replaced by the product two normal distributions (respectively, N ðMm; S2mÞ and N ðMn;S2nÞ). The mean values Mm and Mn represent the projections of m and n on the common direction u, and the overlap between m and n is given by MmMn. The components m and n are otherwise independent, the ﬂuctuations representing the remaining parts of m and n that lie along mutually orthogonal directions. In this situation, the expression for k simpliﬁes to

k = hni½fii = Mnh½fii

(82)

so that a non-zero overlap k can be obtained only if the mean population activity h½fii is non-zero. Choosing independently drawn m and n vectors thus slightly simpliﬁes the mean-ﬁeld network description. The main qualitative features resulting from the interaction between the structured and the random component of the connectivity can however already be observed, and more easily understood, within this simpliﬁed setting.

Stationary solutions The DMF description for stationary solutions reduces to a system of two non-linear equations for the population averaged mean m and variance D0:

mD0==Mgm2MÂnfh2i½Ãfi +i

: = Fðm; D0Þ S2m M2n h½fi i2

:

= Gðm; D0Þ:

(83)

The population averages h½fii and h½f2i i are computed as Gaussian integrals similarly to Equation 39. Equation 83 can be solved numerically for m and D0 by iterating the equations up to convergence, which is equivalent to numerically simulating the two-dimen-
sional dynamical system given by

m_ ðtÞ = À m + Fðm; D0Þ D_ 0ðtÞ = À D0 + Gðm; D0Þ;

(84)

since the ﬁxed points of this dynamical system correspond to solutions of Equation 83. Gaussian integrals in the form of h½fii are evaluated numerically through Gauss-Hermite quadrature with a sampling over 200 points. Unstable solutions can be computed by iterating the same equations after having inverted the sign of the time variable in the ﬁrst equation.
As the system of equations in Equation 83 is two-dimensional, we can investigate the number and the nature of stationary solutions through a simple graphical approach (Figure S1G). We plot on the m À D0 plane the loci of points where the two individual equations

m = Fðm; D0Þ D0 = Gðm; D0Þ

(85)

are satisﬁed. In analogy with dynamical systems approaches, we refer to the two corresponding curves as the DMF nullclines. The
solutions of Equation 83 are then given by the intersections of the two nullclines. To begin with, we focus on the nullcline deﬁned by the ﬁrst equation (also referred to as the m nullcline). With respect to m, Fðm; D0Þ is
an odd sigmoidal function whose maximal slope depends on the value of D0 and MmMn. When g = 0 and Sm = 0, the input variance D0 vanishes. In this case, the points of the m nullcline trivially reduce to the roots of the equation: m = MmMnfðmÞ, which admits either one ðMmMn < 1Þ, or three solutions ðMmMn > 1Þ. Non-zero values of g and Sm imply ﬁnite and positive values of D0. As D0 increases, the solutions to the equation m = MmMnh½fii vary smoothly, delineating the full nullcline in the m À D0 plane. As in the case without

Neuron 99, 609–623.e1–e29, August 8, 2018 e12

disorder (g = 0 and Sm = 0), for low structure strengths (MmMn < 1), the m nullcline consists of a unique branch: m = 0 cD0. At high structure strengths ðMmMn > 1Þ, instead, its shape smoothly transforms into a symmetric pitchfork.
The D0 nullcline is given by the solutions of D0 = Gðm; D0Þ for D0 as function of m. As Gðm; D0Þ depends quadratically on m, the D0 nullcline has a symmetric V-shape centered in m = 0. The ordinate of its vertex is controlled by the parameter g, as the second term of the second equation in 83 vanishes at m = 0. For m = 0, the slope of Gðm; D0Þ in D0 = 0 is equal to g2. As a consequence, for g < 1, the vertex of the D0 nullcline is ﬁxed in (0,0), while for g > 1, the vertex is located at D0 > 0 and an isolated point remains at ð0; 0Þ.
The stationary solutions of the DMF equations are determined by the intersections between the two nullclines. For all values of the parameters, the nullclines intersect in m = 0, D0 = 0, corresponding to the trivial, homogeneous stationary solution. The existence of other solutions are determined by the qualitative features of the individual nullclines, that depend on whether MmMn and g are smaller or greater than one (Figure S1G). The following qualitative situations can be distinguished: (i) for MmMn < 1 and g < 1, only the trivial solutions exist; (ii) for MmMn > 1, two additional, symmetric solutions exist for non-zero values of m and D0, corresponding to symmetric, heterogeneous stationary states; (iii) for g > 1, an additional solution exist for m = 0 and, corresponding to a heterogeneous solution in which individual units have non-zero stationary activity, but the population-average vanishes. For MmMn > 1, this solution can coexist with the symmetric heterogeneous ones, but in the limit of large g these solutions disappear (Figure S1G).
The next step is to assess the stability of the various solutions. As explained earlier on, the stability of the trivial state m = 0, D0 = 0 can be readily assessed using random matrix theory arguments (Figures S1A and S1B). This state is stable only for MmMn < 1 and g < 1. At MmMn = 1, it loses stability due to the outlying eigenvalue of the stability matrix, leading to the bifurcation already observed at the level of nullclines. At g = 1, the instability is due to the radius of the bulk of the spectrum. This leads to a chaotic state, not predicted from the nullclines for the stationary solutions.
The stability of heterogeneous stationary states is assessed by determining separately the radius of the bulk of the spectrum and the position of the outlier (Figures S1D–S1F). The radius is determined from Equation 44. The outlier is instead computed as the lead-
ing eigenvalue of the stability matrix given in Equation 71. Note that in the present framework, where the overlap is deﬁned along the unitary direction, it is possible to show that the latter is equivalent to computing the leading stability eigenvalue of the effective dynamical system introduced in Equation 84, linearized around the corresponding ﬁxed point. The bifurcation obtained when the
outlier crosses unity is equivalent to the bifurcation predicted from the nullclines when the symmetric solutions disappear in favor of the heterogeneous solution of mean zero (Figure S1G). For MmMn > 1, we however ﬁnd that as g is increased, the radius of the bulk of the spectrum always leads to a chaotic instability before the outlier becomes unstable. Correspondingly, the m = 0 and D0 > 0 stationary state that exist for large g is never stable.

Chaotic solutions

For large g, the instabilities of the stationary points generated by the bulk of the spectrum are expected to give rise to chaotic dy-

namics. We therefore turn to the DMF theory for chaotic states, which are described by an additional variable that quantiﬁes temporal

ﬂuctuations. For the case studied here of connectivity vectors m and n overlapping only along the unitary direction, Equation 77

become

Z

 pﬃﬃﬃﬃﬃ 

m = Fðm; D0; DNÞ = MmMn Dzf m + D0 z

D0 = Gðm; D0; DNÞ =

(Z D2N + 2g2

 pﬃﬃﬃﬃﬃ  Z DzF2 m + D0z À

Dz

Z

 DxF m

+

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ D0 À DNx

+

pﬃﬃﬃﬃﬃﬃﬃ !2' DNz

+

M2n S2m h½fi i2 ðD0

À

i1 DNÞ 2

Z

Z

 pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pﬃﬃﬃﬃﬃﬃﬃ !2

DN = Hðm; D0; DNÞ = g2 Dz Dxf m + D0 À DN x + DN z + M2nS2mh½fii2:

(86)

As the system to be solved is now three-dimensional, graphical approaches have only limited use. Similarly to the stationary state, a practical and stable way to ﬁnd numerically the solutions is to iterate the dynamical system given by

m_ = À m + Fðm; D0; DNÞ

D_ 0 = À D0 + Gðm; D0; DNÞ

(87)

D_ N = À DN + Hðm; D0; DNÞ:

where the double Gaussian integrals from Equation 86 can be evaluated numerically as two nested Gauss-Hermite quadratures. Note that stationary states simply correspond to solutions for which D0 = DN.
As for stationary solutions, different types of chaotic solutions appear depending on the values of the structure strength MmMn and the disorder strength g. If g > 1 and MmMn < 1, a single chaotic state exists corresponding to m = 0 and DN = 0, meaning that the temporally averaged activity of all units vanishes, so that ﬂuctuations are only temporal (Figure 1B red). As MmMn crosses unity,

e13 Neuron 99, 609–623.e1–e29, August 8, 2018

two symmetric states appear with non-zero values of m and DN. These states correspond to bistable heterogeneous chaotic states (Figure 1B orange) that are analogous to bistable heterogeneous stationary states.

The critical disorder strength gB at which heterogeneous chaotic states emerge (gray boundary in the phase diagram of Figure 1) is computed by evaluating the linear stability of the dynamics in 87 around the central solution ð0; D0; 0Þ. A long but straightforward

algebra reveals that the stability matrix, evaluated in, is simply given by

0 BBBB@

Mm

Mn 0

f0



0 g2Àf2 + Ff0  À hFif0 Á
D0

1

0

0

CCCCA;

(88)

0

0

g2f0 2

such that gB corresponds to the value of the random strength g for which the largest of its three eigenvalues crosses unity.

Spontaneous dynamics: structures overlapping on an arbitrary direction In the previous section, we focused on the simpliﬁed scenario where the connectivity vectors m and n overlapped only in the unitary direction. Here, we brieﬂy turn to the opposite case where the overlap along the unitary direction u vanishes (i.e., Mm = 0; Mn = 0), but the overlap r along a direction orthogonal to u is non-zero. As we will show, although the equations describing the network activity present some formal differences, they lead to qualitatively similar regimes. The same qualitative results apply as well to the general case, where an overlap exists on both the unitary and an orthogonal direction.
The network dynamics can be studied by solving the DMF Equations 40 and 77 by setting m = 0. Stationary solutions are now determined by:

Dk =0 =rkgS2mÂSfn2i hð½0f;0

iDð00Þ; ÃD0+ÞiS:2m

=F k2 :

ðk; D0Þ = Gðk;

D0

Þ:

(89)

Note that, in this more general case, the relevant ﬁrst-order statistics of network activity is given by the overlap k, which now can

take non-zero values even when the population-averaged activity h½fii vanishes.

As in the previous case, the stationary solutions can be analyzed in terms of nullclines (Figure S2A). The main difference lies in the k

nullcline given by k = rkSmSnh½f0i ð0;D0Þi. As both sides of the ﬁrst equation are linear and homogeneous in k, two classes of solutions exist: a trivial solution (k = 0 for any D0), and a non-trivial one (D0 = D~0 for any k), with D~0 determined by:

Âf0i

À 0;

D~ 0ÁÃ

=

1=ðrSm

Sn

Þ:

(90)

Because 0 < f0ðxÞ < 1, Equation 90 admits non-trivial solutions only for sufﬁciently large overlap values: r > 1=SmSn. In consequence, the k nullcline takes qualitatively different shapes depending on the value of r: (i) for r < 1=SmSn, it consists only of a vertical branch k = 0 (ii) for r > 1=SmSn an additional horizontal branch D0 = D~0 appears (Figure S2A).
The D0 branch is qualitatively similar to the previously studied case of m and n overlapping along the unitary direction, with a qualitative change when the disorder parameter g crosses unity.
The stationary solutions are given by the intersections between the two nullclines. Although the shape of the k nullcline is distinct from the shape of the m nullcline studied in the previous case, qualitatively similar regimes are found. The trivial stationary state k = 0, D0 = 0 exists for all parameter values. When the structure strength rSmSn exceeds unity, two symmetric heterogeneous states appear with non-zero k values of opposite signs (but vanishing mean m). Finally for large g an additional state appears with k = 0, D0 > 0.
Similarly to Figure 1, the solutions of Equation 89, which correspond to stationary activity states, are shown in blue in Figures S2B–S2D.
In Figure S2B we address their stability properties: again we ﬁnd that when non-centered stationary solutions exist, the central ﬁxed point becomes unstable. The instability is led by the outlier eigenvalue of the stability eigenspectrum. Similarly to Figure 1, furthermore, the DMF theory predicts an instability to chaotic phases for high g values. As for stationary states, both heterogeneous and homogeneous chaotic solutions are admitted (Figures S2C and S2D); heterogeneous chaotic states exist in a parameter region where the values of g and r are comparable.

Response to external inputs
In this section, we examine the effect of non-vanishing external inputs on the network dynamics. We consider the situation in which every unit receives a potentially different input Ii, so that the pattern of inputs at the network level is characterized by the N-dimensional vector I = fIig. The network dynamics in general depend on the geometrical arrangement of the vector I with respect to the connectivity vectors m and n. Within the statistical description used in DMF theory, the input pattern is therefore characterized by the ﬁrst- and second-order statistics MI and SI of its elements, as well as by the value of the correlations SmI and SnI with the vectors m and n. In geometric terms, MI quantiﬁes the component of I along the unit direction u, while SmI and SnI quantify the overlaps with m

Neuron 99, 609–623.e1–e29, August 8, 2018 e14

and n along directions orthogonal to u. For the sake of simplicity, here we consider two connectivity vectors m and n that overlap solely on the unitary direction (r = 0). The two vectors thus read (see Equation 34):

m = Mm + Smx1 n = Mn + Snx2:

(91)

The input pattern can overlap with the connectivity vectors on the common (u) and on the orthogonal directions (x1 and x2). It can moreover include further orthogonal components of strength St. The most general expression for the input vector can thus be written as:

I

=

MI

+

SmI Sm

x1

+

SnI Sn

x2

+

St

h

(92)

where h is a standard normal vector. We ﬁrst focus on the equilibrium response to constant inputs, and then turn to transient
dynamics.
The mean-ﬁeld equations in presence of external inputs can be derived in a straightforward fashion by following the same steps as in the input-free case. We start by considering the statistics of the effective coupling term, which is given by xiðtÞ = hiðtÞ + IiðtÞ, with hiðtÞ deﬁned as in Equation 20. We can then exploit the statistics of hiðtÞ which have been computed in the previous paragraphs to obtain the equation for the mean activity:

mi = ½xi = mik + Ii:

(93)

Equation 93 indicates that the direction of the average network activity is determined by a combination of the structured recurrent connectivity and the external input pattern. The ﬁnal direction of the activation vector in the N-dimensional population space is controlled by the value of the overlap k, which depends on the relative orientations of m, n and I. Its value is given by the self-consistent equation:

k = (hni½fZii 

qﬃﬃﬃﬃﬃ )

= ni Dzf mik + Ii + DI0 z

(94)

=

Mn

h½fi

i

+

SnI

Âf0i

Ã ;

as both vectors m and I share non-trivial overlap directions with n.

The second-order statistics of the noise are given by:

Â xi

ðtÞxj

ðt

+

Ã tÞ

=

dij

g2

h½fi

ðtÞfi

ðt

+

tÞi

+

mi

mj

k2

+

ðmi

Ij

+

mj

Ii

Þk

+

Ii

Ij

:

(95)

Averaging across the population we obtain:

h½xi

ðtÞxi

ðt

+

tÞi

À

h½xi

ðtÞi2

=

g2

Â f2i

Ã

+

S2m

k2

+

2SmI

k

+

S2I

:

(96)

The ﬁrst term of the r.h.s. represents the quenched variability inherited from the random connectivity matrix, while S2m = S2mk2 + 2SmIk + S2I represents the variance induced by the structure, which is inherited from both vectors m and I (Equation 93). From Equation 92, the variance of the input reads:

S2I

=

S2mI S2m

+

S2nI S2n

+

S2t :

(97)

The ﬁnal DMF equations to be solved are given by the following system:

mDk€===MMDnmÀhk½fÈ+igMi2+hI½fSinðItÞÂffð0itÃ+ tÞi + S2mk2 + 2SmIk + S2I É

(98)

which, similarly to the cases we examined in detail so far, admits both stationary and chaotic solutions. As for spontaneous dynamics,

the instabilities to chaos are computed by evaluating the radius of the eigenspectrum of the stability matrix Sij (Equation 44). The sta-

bility matrix can admit an outlier eigenvalue as well, whose value can be predicted with a mean-ﬁeld stability analysis. Extending the

arguments already presented in the previous paragraphs allows to show that the effective stability matrix M is given by:

M

=

0 @

22bgg22Â0Âffifif0i Ã0i Ã

bgg22ÈÈÂÂffi0 2i0 2ÃÃ0++ÂÂffi fi f0i00iÃ0 ÃÉÉ

Mm

1

bÀ22SS2m2mkk00++22SSmmIÁI + a A;

(99)

e15 Neuron 99, 609–623.e1–e29, August 8, 2018

with:

a = MmMnÂf0i Ã + MmSnIÂf0i0Ã

b

=

1 2

ÈMnÂf}i

Ã

+

SnI

Âf0i00

ÃÉ :

(100)

As in the input-free case, when the stability eigenspectrum contains one outlier eigenvalue, its position is well predicted by the largest eigenvalue of M.
In the following, we refer to Figure 2 and analyze in detail the contribution of every input direction to the ﬁnal network dynamics. In Figure 2D (left), we consider a unit-rank structure whose vectors m and n are orthogonal: Mm = Mn = 0. The input direction is orthogonal to the connectivity vectors: SmI = SnI = 0, so that the input strength is quantiﬁed by the amplitude of the component along h ðStÞ. In this conﬁguration, because of Equation 94, the amount of structured activity quantiﬁed by k systematically vanishes. In Figure 2D (center), we consider again orthogonal connectivity vectors, but we take an input pattern which overlaps with n along x2. We keep St = 1 ﬁxed and we vary the component of the input along n by increasing SnI. As can be seen from the equation for k (Equation 98), the overlap SnI between the input and the left vector n has the effect of increasing the value of k, which would otherwise vanish since the structure has null strength ðMn = 0Þ. In response to the input, a structured state emerges. From the same equation, furthermore, one can notice that the SnI term has the effect of breaking the sign reversal symmetry ðx/ À xÞ that characterizes the mean-ﬁeld equations in the case of spontaneous dynamics. In Figure 2D (right), we include strong non-vanishing structure strengths ðMmMn = 3:5Þ. In absence of external activity, the network dynamics thus admit two bistable solutions (Figure 1). We consider an input pattern that correlates with n but is orthogonal to the structure overlap direction (MI = 0, SnI > 0). In this conﬁguration, the external input has the effect of disrupting the symmetry between the two stable solutions. For sufﬁciently strong input values, one of the two stable solutions disappears by annihilating with the unstable one. In Figure S4C, we show that the value of the critical input strength for which one of the two stable solution disappears can be controlled by an additional external input that overlaps with n on a different, orthogonal direction. Speciﬁcally, in Figure S4C, we tune the additional input along the direction of the structure overlap u. This input component can be thought as a modulatory signal which controls the way the network dynamics process the input stimulus along x2. In models of computational tasks that employ nonlinear input responses (Figure 4), a modulatory input along the structure overlap can regulate the threshold value of the input strength that the network has learnt to detect. Similarly, in Figures 5 and 6, modulatory inputs are used to completely block the response to the non-relevant input stimulus, so that the readout can produce context-dependent outputs.

Asymmetric solutions A major effect of external inputs is that they break the sign reversal symmetry ðx/ À xÞ present in the network dynamics without inputs. As a consequence, in the parameter regions where the network dynamics admit bistable structured states, the two stable solutions are characterized by different statistics and stability properties.
To illustrate this effect, we focus on the simple case where the external input pattern I overlaps with the connectivity vectors m and n solely on the unitary direction (MIs0, SmI = SnI = 0). The solutions of the system of equations corresponding to stationary states can be visualized with the help of the graphical approach, which unveils the symmetry breaking of network dynamics induced by external inputs (Figure S4D).
Similarly to the input-free case, the D0 nullcline consists of a symmetric V-shaped curve. In contrast to before, however, the vertex of the nullcline is no longer ﬁxed in ð0; 0Þ, but takes positive ordinate values also at low g values. The value of Gð0;D0Þ, indeed, does not vanish, because of the ﬁnite contribution from the input pattern S2I .
The nullcline curves of m are instead strongly asymmetric. For low MmMn values, one single m nullcline exists. In contrast to the input-free case, this nullcline is no longer centered in zero. As a consequence, it intersects the D0 nullclines in one non-zero point, corresponding to a unique heterogeneous stationary solution. As MmMn increases, a second, separated branch can appear. In contrast to the input-free case, the structure strength at which the second branch appears is not always equal to unity, but depends on the mean value of the input. If MmMn is strong enough, the negative branch of the nullcline can intersect the D0 nullcline in two different ﬁxed points, while a third solution is built on the positive m nullcline. As g increases, the two intersections on the negative branch become closer and closer and they eventually collapse together. At a critical value gB, the network activity discontinuously jumps from negative to positive mean solutions.
As they are no longer symmetrical, the stability of the positive and the negative ﬁxed points has to be assessed separately, and gives rise to different instability boundaries. Computing the position of the outlier reveals that, when more than one solution is admitted by the mean-ﬁeld system of equations, the centered one is always unstable.
As the stability boundaries of different stationary solutions do not necessarily coincide, in presence of external input patterns the phase diagram of the dynamics are in general more complex (Figures S4A–S4C). Speciﬁcally, hybrid dynamical regimes, where one static solution co-exists with a chaotic attractor, can be observed.

Neuron 99, 609–623.e1–e29, August 8, 2018 e16

Transient dynamics We now turn to transient dynamics evoked by a temporal step in the external input (Figure 2B). We speciﬁcally examine the projection of the activation vector and its average onto the two salient directions spanned by vectors m and I.
The transient dynamics of relaxation to a stationary solution can be assessed by linearizing the mean-ﬁeld dynamics. We compute the time course of the average activation vector mi, and we ﬁnally project it onto the two orthogonal directions which are indicated in the small insets of Figure 2B.
Similarly to Equation 45, the time evolution of mi is governed by:

m_ iðtÞ = À miðtÞ + mikðtÞ + IiðtÞ

(101)

so that, at every point in time:

miðtÞ = mi~kðtÞ + eIi ðtÞ;

(102)

where ~kðtÞ and eIi ðtÞ coincide with the low-pass ﬁltered versions of kðtÞ and IðtÞ. When the network activity is freely decaying back to an equilibrium stationary state, eIi ðtÞ coincides with a simple exponential relax-

ation to the pattern Ii. The decay timescale is set by the time evolution of activity (Equation 6), which is taken here to be equal to unity:

eIi ðtÞ = Ii + ÀIiic À IiÁeÀt:

(103)

The timescale of ~kðtÞ is inherited from the dynamics of kðtÞ. We thus refer to our mean-ﬁeld stability analysis, and we compute the

relaxation time of the population statistics kðtÞ as the largest eigenvalue of the stability matrix M. The eigenvalue predicts a time con-

stant tr, which is in general larger than unity. As a consequence, the relaxation of kðtÞ obeys, for small displacements:

kðtÞ = k0 + Àkic À k0ÁeÀttr ;

(104)

where the asymptotic value of k0 is determined from the equilibrium mean-ﬁeld equations (Equations 98). Finally, the time course of ~kðtÞ is derived as the low-pass ﬁlter version of Equation 104 with unit decay timescale.

Rank-two connectivity structures In the following paragraphs, we provide the detailed analysis for network models with rank-two connectivity structures. The structured component of the connectivity can be written as:

Pij

=

mði 1Þnðj 1Þ N

+

mði 2Þnðj 2Þ; N

(105)

where the vector pairs mð1Þ and mð2Þ, nð1Þ and nð2Þ are assumed to be linearly independent. As in the case of unit-rank structures, we determine the network statistics by exploiting the link between linear stability analysis and
mean-ﬁeld description. The study of the properties of eigenvalues and eigenvectors for the low-dimensional matrix Pij helps to predict the complex behavior of activity above the instability and to restrict our attention to the cases of interest.
The mean activity of the network in response to a ﬁxed input pattern Ii is given by:

mi = k1mði 1Þ + k2mði 2Þ + Ii:

(106)

The ﬁnal direction of the population activity is thus determined by the overlap values k1 = hnði 1Þ½fii and k2 = hnði 2Þ½fii. The expression of the mean-ﬁeld equations for the ﬁrst- and second-order statistics are determined by the geometrical arrange-
ment of the connectivity and the input vectors. Similarly to the unit-rank case, the simplest mean-ﬁeld solutions correspond to sta-
tionary states, which inherit the structure of the most unstable eigenvectors of the connectivity matrix Jij. The stability of the heterogeneous stationary states can be assessed as before by evaluating separately the value of the radius (Equation 44) and the

position of the outliers of the linear stability matrix Sij. Similarly to the unit-rank case, it is possible to compute the position of the outlier eigenvalues by studying the linearized dynamics

of the network statistics close to the ﬁxed point, that is given by:

0 m1 1

0 m1 1

0 m1 1

d dt

BB@

D10 k11

CCA

=

À

BB@

D10 k11

CCA

+

MBB@

D10 k11

CCA:

k12

k12

k12

(107)

Note that, in klk, the subscript k = 1; 2 refers to the left vector nðkÞ with which the overlap is computed, while the superscript l = 0; 1 indicates the order of the perturbation away from the ﬁxed point.
In order to compute the elements of the linear stability matrix M, we follow and extend the reasoning discussed in details for the unit-rank case. We start by considering the time evolution of the linearized activity m1i , which similarly to Equation 45 reads:

m_ 1i ðtÞ = À m1i + mði 1Þk11 + mði 2Þk12:

(108)

e17 Neuron 99, 609–623.e1–e29, August 8, 2018

At every point in time, we can write: mti = mði 1Þ~kt1 + mði 2Þ~kt2, where ~ktk is the low-pass ﬁltered version of ktk: ð1 + d=dtÞ~ktk = ktk. In the case of orthogonal (zero mean), random connectivity vectors, we get:

m_ 1ðtÞ = À m1;

(109)

so that the elements in the ﬁrst row of M vanish. In analogy with Equation 64, the linearized dynamics of D0 gives instead:

D_ 10 =

À

D10

+

2g2

Â fi

f0i

Ãm1

+

g2ÈÂfi02Ã

+

Â fi

f0i0

ÃÉD10

+ 2S2mk01k11 + 2S2mk02k12:

(110)

Similarly to the unit-rank case (Equation 47), in order to determine the linear response of k1 we need to compute:

k11

=

D nði 1Þ

Âxi1

f0

Àxi0ÁÃE

=

D nði 1Þ

mi

Âf0i

ÃE

+

D10 2

À

m1i m0i 

À

m1i m0i Dnði 1ÞÂf}i ÃE

(111)

A similar expression can be derived for k12. In general, the integrals in the r.h.s. can be expressed in terms of the perturbations ~k11, ~k12 and D10, leading to expressions of the form:

k11 = a11~k11 + a12~k12 + b1D10 k12 = a21~k11 + a22~k12 + b2D10:

(112)

Applying the operator ð1 + d=dtÞ to the Equation 111 allows to reshape the results in the ﬁnal matrix form:

M

=

0 BB@

222bbg12gg222Â0fÂÂffifiiff0i Ã0i0iÃÃ

bbg12 gg2 È22 ÈÈÂfÂÂff0i20i0iÃ22ÃÃ0+++ÂfÂÂffi fii ff0i0 Ã0i0i00ÃÃÉÉÉ

0
2S2mk01 2b1S2mk01 + a11 2b2S2mk01 + a21

0

1

2b1

2S2mk02 S2mk02 +

a12

CCA;

2b2S2mk02 + a22

(113)

where the values of the constants a and b depend on the geometric arrangement of the structure and the input vectors. In the following, we consider several speciﬁc cases of interest. Note that the non-linear network dynamics is determined by the
relative orientation of the structure and input vectors, but also by the characteristics of the statistical distribution of their elements. In contrast to the cases we analyzed so far, the precise shape of the distribution of the entries in the connectivity vectors can play an important role when the rank of Pij is larger than unity. In the following, we focus on the case of broadly, normally distributed patterns.

Rank-two structures with null overlap The simplest case we consider consists of rank-two matrices whose four connectivity vectors mð1Þ, mð2Þ, nð1Þ, and nð2Þ are mutually
orthogonal. From the point of view of responses to inputs, networks with this structure behave as superpositions of two independent
unit-rank structures. Similarly to the unit-rank case, if the connectivity vectors are orthogonal, the network is silent in absence of external inputs: k1 =
k2 = 0. A single homogeneous state – stationary or chaotic – is the unique stable attractor of the dynamics. Consistently, the eigenspectrum of Jij does not contain any outlier, since every eigenvalue of Pij vanishes.
In order to compute the eigenspectrum of Pij, we can rotate the matrix onto a basis deﬁned by an orthonormal set of vectors, and compute its eigenvalues in the transformed basis. For simplicity, we consider an orthonormal set whose ﬁrst four vectors are built
from the connectivity vectors:

u1 = a1mð1Þ u2 = a2mð2Þ
u3 = a3nð1Þ u4 = a4nð2Þ;

(114)

where the coefﬁcient ak ðk = 1; .; 4Þ denote the normalization factors. In this basis, the ﬁrst four rows and columns of the rotated

matrix P0ij read:

0

1

1

P0ij

=

1 N

BBBBBBBB@

0 0 0

0 0 0

a1a3 0 0

0
1 a2a4
0

CCCCCCCCA;

(115)

00 0 0

all the remaining entries being ﬁxed to 0. From the present matrix form, it easy to verify that all the eigenvalues of P0ij, and thus all the eigenvalues of Pij, vanish. Note that rewriting Pij in an orthonormal basis simpliﬁes the search for its eigenvalues also in more complex cases where the connectivity vectors share several overlap directions. In those cases, a proper basis needs to be built starting from
the connectivity vectors through a Gram-Schmidt orthonormalization process.

Neuron 99, 609–623.e1–e29, August 8, 2018 e18

As a side note we observe that, even though P0ij (and thus Pij) admits only vanishing eigenvalues, its rank is still equal to two. Indeed, the rank can be computed as N minus the dimensionality of the kernel associated to P0ij, deﬁned by any vector x obeying P0 x = 0. As P0ij contains N À 2 empty rows, the last equations impose two independent contraints on the components of x. As a consequence, the

dimensionality of the kernel equals N À 2, and the rank is equal to two.

We turn to responses that are obtained in presence of external inputs. We examine the network dynamics in response to a normal-

ized input I which partially correlates with one of the left-connectivity vectors, here nð1Þ:

sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ

I

=

nð1Þ

SnI S2n

+

x

S2I

À

S2nI S4n

:

(116)

Similarly to the unit-rank case, we ﬁnd that I elicits a network response in the plane I À mð1Þ. The overlap values are given by:

k1 = SnIÂf0i Ã k2 = 0;

(117)

and they can be used to close the mean-ﬁeld equations together with the equation for the ﬁrst (m = 0) and second-order statistics. In

the case of stationary states we have:

D0 = g2Âf2i Ã + S2mÀk21 + k22Á + S2I :

(118)

Similar arguments allow to derive the two equations needed for the chaotic states. In order to assess the stability of the stationary states, we evaluate the position of the outliers in the stability eigenspectrum by computing the eigenvalues of M (Equation 113). In the case of orthogonal structures and correlated input patterns I, a little algebra reveals that all the a values vanish, while we have:

b1 = 12SnIÂf0i0Ã b2 = 0:

(119)

We conclude that the ﬁrst and the last row of M always vanish. Furthermore, the second and the third rows are proportional one to the other. As a consequence, the stability analysis predicts at most one outlier eigenvalue, which is indeed observed in the spectrum (not shown). The outlier is negative, as the effect of introducing inputs in the direction of the left vector nð1Þ is to further stabilize the dynamics. As it will be shown, more than one outlier can be observed in the case where the low-dimensional structure involves overlap directions.

Rank-two structures with internal pairwise overlap

As a second case, we consider structured matrices where the two connectivity pairs mð1Þ and nð1Þ, mð2Þ and nð2Þ share two different

overlap directions, deﬁned by vectors y1 and y2. We set: qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
mð1Þ = qSﬃﬃﬃﬃ2ﬃﬃﬃÀﬃﬃﬃﬃﬃrﬃﬃ21ﬃ x1 + r1y1 mð2Þ =qﬃﬃSﬃﬃﬃ2ﬃﬃﬃÀﬃﬃﬃﬃﬃrﬃﬃ22 x2 + r2y2 nð1Þ = qSﬃﬃﬃﬃ2ﬃﬃﬃÀﬃﬃﬃﬃﬃrﬃﬃ21ﬃ x3 + r1y1 nð2Þ = S2 À r22 x4 + r2y2:

(120)

where S2 is the variance of the connectivity vectors and r21 and r22 quantify the overlaps along the directions y1 and y2. By rotating Pij onto the orthonormal basis that can be built from mð1Þ and mð2Þ by orthogonalizing the left vectors nð1Þ and nð2Þ, one
can easily check that the two non-zero eigenvalues of Pij are given by l1 = r21 and l2 = r22. They correspond, respectively, to the two right-eigenvectors mð1Þ and mð2Þ. In absence of external inputs, an instability is thus likely to occur in the direction of the mðkÞ vector

which corresponds to the strongest overlap.

We speciﬁcally focus on the degenerate condition where the two overlaps are equally strong, r1 = r2 = r, and any combination of mð1Þ and mð2Þ is a right-eigenvector. The mean-ﬁeld equations for the ﬁrst-order statistics read:

k1 k2

= =

r2 r2

k1 k2

ÂÂff0i0i

Ã Ã
:

(121)

Similarly to Equation 89, the two equations admit a silent ðk1 = k2 = 0Þ and a non-trivial state, determined by two identical conditions

which read:

1

=

r2

Âf0i

ð0;

D0

Ã Þ:

(122)

The equation above determines the value of D0. Note that the non-trivial state exists only for r > 1.

e19 Neuron 99, 609–623.e1–e29, August 8, 2018

A second condition is imposed by the equation for the second-order momentum which reads, for stationary solutions: D0 = g2Âf2i Ã + S2Àk21 + k22Á:

(123)

As the value of D0 is ﬁxed, the mean-ﬁeld set of equations ﬁxes only the sum k21 + k22, but not each singqle ﬃcﬃﬃoﬃﬃﬃmﬃﬃﬃﬃpﬃﬃﬃonent. The meanﬁeld thus returns a one-dimensional continuum of solutions, the shape of which resembles a ring of radius k21 + k22 in the mð1Þ À mð2Þ

plane (see Figures S5D and S5E). Similarly to the unit-rank case, the value of the radius can be computed explicitly by solving numer-

ically the two mean-ﬁeld equations (three in the case of chaotic regimes), and depends on the relative magnitude of r2 compared to g

(Figure S5F). Highly disordered connectivities have the usual effect of suppressing non-trivial structured solutions in favor of homo-

geneous and unstructured states. For sufﬁciently high g values, furthermore, structured solution can display chaotic dynamics (Fig-

ures S5E and S5F, red).

A linear stability analysis reveals that the one-dimensional solution consists of a continuous set of marginally stable states. Similarly

to the orthogonal vectors case, the position of the outliers in the eigenspectra of Sij can be evaluated by computing the reduced stability matrix M, which reads:

M

=

0 BB@

222bbg12gg222Â0fÂÂffifiiff0i Ã0i0iÃÃ

bbg12 gg2 È22 ÈÈÂfÂÂffi0 2ii00Ã22ÃÃ0+++ÂfÂÂffi fii ff0i0 Ã0i0i00ÃÃÉÉÉ

0
2S2m k01 2b1S2mk01 + a11
2b2 S2m k01

0

1

2S2m k02 2b1 S2m k02

CCA;

2b2S2mk02 + a22

(124)

with:

a11 = r2Âf0i Ã b1 = 12r2k01Âf0i00Ã

(125)

and

a22 = r2Âf0i Ã

b2

=

12r2k02

Âf0i00

Ã :

(126)

As shown in Figure S5G, diagonalizing the stability matrix M returns the values of two distinct outlier eigenvalues. The third non-

zero eigenvalue of M lies instead systematically inside the compact component of the spectrum, and corresponds to an average

measure of the timescales inherited by the random modes. One of the two outliers is tuned exactly to the stability boundary for every

value of the parameters which generate a ring solution. This marginally stable eigenvalue is responsible for the slow dynamical time-

scales which are observed in numerical simulations of the network activity (Figures S5D and S5E).

The DMF predictions formally hold in the limit of inﬁnite-size networks; in simulations of ﬁnite-size networks, the dynamics instead

always converge on a small number of equilibrium spontaneous states located on the ring (see Figures S5D and S5E). The equilibrium

reached in a given situation is determined by the corresponding realization of the random part of the connectivity, and the initial con-

ditions. Different realizations of the random connectivity lead to different equilibrium states, which all however lie on the predicted ring

(see Figures S5D and S5E). For a given realization of the random connectivity, transient dynamics moreover show a clear signature of

the ring structure. Indeed the points on the ring are close to stable and form a slow manifold. The convergence to the equilibrium

activity is therefore very slow, and the temporal dynamics explore the ring structure.

We next examine how the structured, ring-shaped solution is perturbed by the injection of external input patterns.

We consider an input pattern I of variance S2I . When I does not share any overlap direction with the left vectors nð1Þ and nð2Þ, the mean-ﬁeld equations are affected solely by an extra term SI which needs to be included in the equation for the second-order statistics

(Equation 123). As the equations for the ﬁrst-order statistics do not change, the one-dimensional degeneracy of the solution persists.

The extra term S2I however decreases the value of the radius of the ring. When the input contains a component which overlaps with one or both left vectors nð1Þ and nð2Þ, the degeneracy in the two equa-

tions for k1 and k2 is broken. As a consequence, the one-dimensional solution collapses onto a unique stable point. Consider for

example an input pattern of the form:

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pﬃﬃﬃ  I = SI 1 À a x3 + a x4 :

(127)

The equations for the ﬁrst order become:

k1

=

 r2

k1

+

SI

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1Àa

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ S2 À r2

Âf0i

Ã

k2

=

 r2

k2

+

SI

pﬃﬃﬃ a

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ S2 À r2

Âf0i

Ã

(128)

Neuron 99, 609–623.e1–e29, August 8, 2018 e20

or, alternatively:

k1 = SIpﬃ1ﬃﬃﬃÀﬃﬃ1ﬃﬃﬃaÀﬃﬃqr2ﬃSﬃﬃﬃ2ﬃÂﬃﬃfÀﬃﬃﬃ0iﬃÃﬃrﬃﬃ2ﬃﬃÂf0iÃ k2 = SIpﬃa1ﬃﬃqÀﬃSﬃrﬃﬃ2ﬃ2ﬃﬃÀﬃﬃÂﬃﬃfﬃrﬃﬃ0i2ﬃﬃÃÂf0iÃ:

(129)

The values of k1 and k2 are thus uniquely speciﬁed, and can be computed by iterating the two equations together with the expres-

sion for the second-order statistics:

D0 = g2Âf2i Ã + S2Àk21 + k22Á + S2I :

(130)

In a similar way, the presence of correlated external inputs affect the values of the entries of the reduced stability matrix M:

b1

=

1 2

 r2

k01

+

SI

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1Àa

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ S2 À r2

Âf0i00

Ã

b2

=

1 2

 r2

k02

+

SI

pﬃﬃﬃ a

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ S2 À r2

Âf0i00

Ã :

(131)

In Figures S5H and S5I, we focus on the case of an external input pattern aligned with x3 (and thus nð1Þ). We ﬁx a = 0, that implies k2 = 0.
Solving the mean-ﬁeld equations reveals that, according to the strength of the input SI, one or three ﬁxed points exist. When the input is weak with respect to the structure overlap r2, two ﬁxed points appear in the proximity of the ring, along the direction deﬁned by the axis k2 = 0 (Figure S5H, top). In particular, when I positively correlates with nð1Þ, only the ﬁxed point with positive value of k1 gets stabilized. The remaining two solutions are characterized by one outlier eigenvalue which lays above the instability boundary, and are
thus unstable. On the other hand, when the input is sufﬁciently strong, solely the stable ﬁxed point survives (Figure S5H, bottom). Activity is then robustly projected in the direction deﬁned by the right vector mð1Þ.

Rank-two structures for oscillations We ﬁnally consider the following conﬁguration:

mð1Þ = ax1 + ry1
mð2Þ = ax2 + ry2 nð1Þ = ax3 + ry2 + gry1 nð2Þ = ax4 À ry1;

(132)

where the right- and the left-connectivity vectors share two cross-overlap directions y1 and y2. Note that the vectors in one of the two pairs, mð1Þ À nð2Þ, are negatively correlated. A second overlap is introduced internally to the mð1Þ À nð1Þ pair, and scales with the

parameter g. The directions xj, with k = 1; .; 4, represent uncorrelated terms. Note that different values of a affect quantitatively

the network statistics, but they do not change the phase diagram in Figure S8A.

By rotating Pij on a proper orthonormal basis, one can check that its eigenvalues are given by:

sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ !

l±

= gr2 2

1±

1À 4 g2

;

(133)

and they are complex conjugate for g < 2. In this case, the internal overlap g has the effect of returning a non-vanishing real part. The

two complex conjugate eigenvectors are given by:

e±

 =

À

gmð1Þ 2

 + mð2Þ ±

isﬃﬃ1ﬃﬃﬃﬃÀﬃﬃﬃﬃﬃgﬃ4ﬃ2ﬃﬃﬃﬃﬃmð1Þ:

(134)

The eigenspectrum of Jij = gcij + Pij inherits the pair of non-zero eigenvalues of Pij. When g < 1 and g < 2, the trivial ﬁxed point thus undergoes a Hopf bifurcation when the real part of l crosses unity (Figure S8A, blue). When g > 2, instead, the two eigenvalues are
real. One bifurcation to bistable stationary activity occurs when the largest eigenvalue l + crosses unity (Figure S8A, gray). On the boundary corresponding to the Hopf bifurcation, the frequency of instability uH is determined by the imaginary part of Equa-
tion 133. At the instability, the oscillatory activity of unit i can be represented as a point on the complex plane. Since close to the bifur-
cation we can write:

mi = ei+ eiuHt + c:c: ;

(135)

e21 Neuron 99, 609–623.e1–e29, August 8, 2018

its coordinates are given by the real and the imaginary part of the ith component of the complex eigenvector e + . The phase of oscil-

lation can then be computed as the angle deﬁned by this point with respect to the real axis. Note that the disorder in the elements of

the eigenvector, which is inherited by the random distribution of the entries of the connectivity vectors mð1Þ and mð2Þ, tends to favor a

broad distribution of phases across the population.

In the limit case where the real and the imaginary parts of the complex amplitude of the oscillators are randomly and independently

distributed, the population response resembles a circular cloud in the complex plane. In this case, the phase distribution across the

population is ﬂat. Note that a completely ﬂat phase distribution can be obtained for arbitrary frequency values by adopting a rank-two

structure where an internal overlap of magnitude gr2 exists between vectors mð2Þ and nð2Þ as well.

In the present case, for every ﬁnite value of g, the real and the imaginary part of ei+ are anti-correlated through mð1Þ (Equation 134). Correlations tend to align the network response on two main and opposite phases, as shown in the phase histograms of Figures S8C

and S8D. The distribution of phases becomes sharper and sharper in the g/2 limit, as the distribution in the complex plane collapses

on the real axis.

The phase distribution across the population is reﬂected in the shape of the closed orbit deﬁned by activity on the mð1Þ À mð2Þ plane,

whose components are given by k1 and k2. The phase of the oscillations in k1 (resp. k2) can be computed by projecting the eigen-

vector e + on the right-connectivity vectors nð1Þ and nð2Þ:

D

E

k1

=

jk1

j

eiðF1

+

uH tÞ

+

c:c:

=

Dnði 1Þ½fi

 E

k2 = jk2 j eiðF2 + uHtÞ + c:c: = nði 2Þ½fi

(136)

By using Equations 134 and 135 we get, in the linear regime:

k1

=

"D nði 1Þ

mði 2Þ

E

À

g 2

D nði 1Þ

mði 1Þ

E

+

D i nði 1Þ

mði 1Þ

Esﬃﬃ1ﬃﬃﬃﬃÀﬃﬃﬃﬃﬃgﬃ4ﬃ2ﬃﬃﬃﬃﬃ

# eiuH

t

+

c:c:

=

" r2 1

À

g2 2



+

igr2sﬃﬃ1ﬃﬃﬃﬃÀﬃﬃﬃﬃﬃgﬃ4ﬃ2ﬃﬃﬃﬃﬃ

# eiuH t

+

c:c:

(137)

while:

k2

=

"D nði 2Þ

mði 2Þ

E

À

g 2

D nði 2Þ

mði 1Þ

E

+

D i nði 2Þ

mði 1Þ

Esﬃﬃ1ﬃﬃﬃﬃÀﬃﬃﬃﬃﬃgﬃ4ﬃ2ﬃﬃﬃﬃﬃ

# eiuH

t

+

c:c:

=

" r2 g 2

À

ir2sﬃﬃ1ﬃﬃﬃﬃÀﬃﬃﬃﬃﬃgﬃ4ﬃ2ﬃﬃﬃﬃﬃ

# eiuH t

+

c:c:

(138)

When g is close to 2, the complex amplitudes of k1 and k2 vanish. However, their real parts have different signs. We thus get:

F2 = 0, F1 = p. As a consequence, at large g values, the oscillatory activity in k1 and k2 tends to be strongly in anti-phase.

Stationary solutions can be instead easily analyzed with the standard mean-ﬁeld approach. The equations for the ﬁrst order sta-

tistics read:

k1 k2

= =

ÀÀgrr22kk11+Ârf20ikÃ2Á: Âf0i Ã

(139)

The two equations can be combined together to give the following condition on h½f0ii, which in turn determines the value of D0:

r4Âf0i Ã2 À gr2Âf0i Ã + 1 = 0:

(140)

The mean-ﬁeld equations thus admit two solutions, given by:

sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ !

Âf0i Ã ±

g = 2r2

1+ ±

4 1 À g2

(141)

which, similarly to ﬁeld solutions are

aEcqcueaptitoanbl1e3o3n, ltyakifehr½efa0iliva<lu1e.sAfosritgc>a2n.bBeeecaasuislyecohfetchkeecdo,ntshteraciontnsdoitniotnheh½fsi0igimÀ o<id1acloaicntcividaetisonwiftuhnicmtipoons, itnhge

meanl + > 1.

We conclude that two stationary solutions exist above the instability boundary of the trivial ﬁxed point (Figure S8A, gray). A second

pair of solutions appears for h½f0ii + < 1, which coincide with lÀ > 1 (Figure S8A, dashed), where the second outlier of Jij becomes

unstable. This second pair of solutions is however always dynamically unstable, as it can be checked by evaluating the outliers of

Neuron 99, 609–623.e1–e29, August 8, 2018 e22

their stability matrix through Equation 113. The coefﬁcients of the reduced matrix M read: a11 = gr2Âf0i Ã a12 = r2Âf0i Ã

b1

=

1r2 2

Àk20

+

gk10ÁÂf0i0Ã

(142)

and a21 = À r2Âf0i Ã

a22 = 0

b2

=

À

1r2k10 2

Âf0i0

Ã :

(143)

On the phase diagram boundary corresponding to g = 2, the stable and the unstable pair of stationary solutions annihilate and disappear. At slightly smaller values of g ðg(2Þ, the network develops highly non-linear and slow oscillations which can be thought of as smooth jumps between the two annihilation points (Figure S8D).

IMPLEMENTATION OF COMPUTATIONAL TASKS

Go-Nogo discrimination
Here we describe and analyze the unit-rank implementation of the Go-Nogo discrimination task (Figure 3). The network receives inputs speciﬁed by N-dimensional vectors Ik. In every trial, the input vector coincides with one among the two
vectors IA and IB, representing respectively the Go and the Nogo stimuli. The components of the two input patterns are generated
independently from a Gaussian distribution of mean zero and variance SI. As the components of the inputs are uncorrelated, the two vectors are mutually orthogonal in the limit of large N.
The network activity is readout linearly through a vector w generated from a Gaussian distribution of mean zero and variance S2w. The readout value is given by:

z

=

1 N

XN
i=1

wi fðxi Þ:

(144)

We ﬁx the connectivity vectors m and n such that: (i) the readout is selective, i.e., zs0 if the input is IA and z = 0 for the input IB; (ii) the readout is speciﬁc to the vector w, i.e., it is zero for any readout vector uncorrelated with w. The simplest network architecture which satisﬁes these requirements is given by:

m=w n = IA;

(145)

i.e., the right-connectivity vector m corresponds to the readout vector, and the left-connectivity vector corresponds to the preferred stimulus IA.
The response of the network can be analyzed by referring to the stationary and chaotic solutions of Equation 98. In the case
analyzed here, the connectivity vectors have no overlap direction, so we set Mm = Mn = MI = SmI = 0, which implies m = 0. The ﬁrst-order network statistics are determined by the overlap SnI between the left-connectivity vector and the input vector. As the left-connectivity is given by IA, SnI is the overlap between the current input pattern I and the preferred pattern IA, and it takes values SnI = S2I during the Go stimulus presentation and SnI = 0 otherwise. From Equation 94 we have:

k

= =

hnIAi i ½½ffiii:

(146)

As a consequence, when the Go stimulus is presented ðI = IAÞ:

k

=

S2I

Âf0i

Ã ;

(147)

while the ﬁrst-order statistics k vanishes in response to any orthogonal pattern IB. When activity is read out by the speciﬁc decoding vector w, the readout value is:

e23 Neuron 99, 609–623.e1–e29, August 8, 2018

z = h(wi½fZii 

qﬃﬃﬃﬃﬃ )

= wi Dzf mik + Ii + DI0 z

(Z 

qﬃﬃﬃﬃﬃ )

= wi Dzf wik + Ii + DI0 z

=

kS2wÂf0i

Ã ;

(148)

while we trivially obtain z = 0 for any decoding set orthogonal to both connectivity vectors m and n.

In Figure 3C, we display the transient dynamics predicted by the mean-ﬁeld theory within the m À I plane. In order to compute the

predicted trajectory, we use Equations 103 and 104, where the slowest time-scale of k is computed by diagonalizing the reduced

stability matrix in Equation 99.

In Figure 3G, we test the generalization properties of a network which responds to two Go patterns IA1 and IA2 . We examine the response to a normalized mixture input deﬁned as:

I

=

pﬃaﬃﬃIA1

+

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1 À aIA2

;

(149)

so that the variance of the total input is ﬁxed and equal to S2I . We set n = IA1 + IA2 , so that the equation for the ﬁrst-order statis-

tics reads:

k

= =

IpA1i½ﬃafﬃﬃ i+p+ﬃ1ﬃﬃﬃIÀﬃA2ﬃﬃiﬃ½ﬃfaﬃﬃiS2I

Âf0i

Ã :

(150)

Detection of a continuous noisy stimulus
In Figure 4, we construct a network model which performs a Go-Nogo detection task on a one-dimensional continuous stimulus. The stimulus consists of an input of time-varying amplitude cðtÞI. As in Figure 3, the input direction I is a centered Gaussian vector of
variance S2I . The strength value cðtÞ includes a stationary component c together with additive white noise of standard deviation s. Less importantly, we include in the input an orthogonal component of quenched noise of unitary variance. The network output is deﬁned at the level of an orthogonal readout as in Equation 144, and the task consists in responding to the stimulus when the strength of the input c is larger than a given threshold.
We obtain highly non-linear readout responses by considering non-vanishing overlaps between the connectivity vectors m and n. The simplest setup consists of taking:

m = w + rmy n = I + rny;

(151)

where y is a standard Gaussian vector which deﬁnes a direction common to m and n, but orthogonal both to w and I.

For this conﬁguration, as in Equation 94, the mean-ﬁeld equation for the ﬁrst-order statistics includes two terms, generated respec-

tively by the input and the rank-one structure:

k

=

À rm

rn

k

+

c

S2I

ÁÂf0i

Ã :

(152)

Before the stimulus presentation (c = 0, s = 0), the structure overlap rmrn is strong enough to generate two bistable solutions (Figure 1). We set the negative k solution to represent the Nogo condition, and we initialize the network in this state. To have a zero output in this condition, we add an offset to the readout.
When an input along the preferred direction is presented ðc > 0Þ, two asymmetric solutions exist only when the strength of the input c is not too large (Figure 2D, right). When the correlation c is large, instead, only the positive branch of the solution is retrieved (Figure 2D, right). As a consequence, the average value of k (and thus the readout signal) jumps to positive values, which deﬁne the Go
output condition. More generally, in order to compute the network performance (Figure 4G), the network is said to respond to the stimulus if the
readout z at the end of the stimulus presentation takes values larger than one half of the readout value expected for the upper state.
The threshold value for c at which the bistability disappears is mostly determined by the strength of the structure overlap, but depends also the input and readout parameters SI and Sw. For practical purposes, in order to obtain the model implementation illustrated in Figure 4, we ﬁrst ﬁx the values of SI = 1:2, Sw = 1:2 and rn = 2. We then tune the value of rm in order to obtain a threshold value for c close to 0.5. This leads to rm = 2.
In Figure 4F we vary rm and we show that the value of the threshold decreases to zero as the structure strength rmrn decreases from its original value (rmrn = 4). Rank-one structures characterized by different strengths thus correspond to different thresholds, but also induce different dynamical time-scales in the network. As a rough estimate of this time-scale, we compute the inverse of the outlier eigenvalue from the stability matrix of the ﬁxed point corresponding to the Go resting state ðc = 0Þ. The value of the outlier can be computed from the linearized mean-ﬁeld equations (Equation 71). We show that arbitrarily large time-scales are only obtained

Neuron 99, 609–623.e1–e29, August 8, 2018 e24

by decreasing the value of the structure strength to the critical point where the two bistable branches of the solution emerge from the trivial ﬁxed point. In this conﬁguration, the threshold detected by the network is arbitrarily small.

Contextual modulation of threshold value

Here we brieﬂy illustrate how the threshold of detection can be controlled by an additional modulatory input (Figure 5B). Modulatory

inputs are used in Figures 5 and 6 to implement more complex tasks which require context-dependent responses to stimuli. Any input

direction which overlaps with the left-connectivity vector n and is orthogonal to the stimulus axis I can serve as modulatory input. For

simplicity, we consider modulatory inputs which are aligned with the overlap direction y (see Equation 151). The total external input to

the network contains the modulatory component gy together with the stimulus term cðtÞI, where g is a scalar which controls the

strength of the modulation. The mean-ﬁeld equation for the ﬁrst-order statistics reads:

k

=

À rm rn k

+

rn

g

+

c

S2I

ÁÂf0i

Ã :

(153)

Equation 153 indicates that the modulatory component of the input acts as a constant offset to the stimulus strength. Its net effect is to shift the response curve of the network along the x axis (Figure 5B) by an amount directly regulated by the parameter g. Varying g thus results in network models which detect variable threshold values.

Rank-two structures for context-dependent computations
Here we provide details on the rank-two implementation of the context-dependent tasks. The same model has been used for both
tasks in Figures 5 and 6. The stimuli consist of combinations of two different features A and B that correspond to inputs along two directions IA and IB,
generated as Gaussian random vectors of variance S2I . Contextual cues are represented as additional inputs along directions IctxA and IctxB of unit variance. The total input pattern to the network on a given trial is therefore given by:

IðtÞ = cAðtÞIA + cBðtÞIB + gAIctxA + gBIctxB:

(154)

The values cA and cB express the strength of the stimulus along the two feature directions. They are given by the sum of stationary average values (cA, cB), and temporary ﬂuctuating components generated from independent realizations of white noise with standard deviation s. In the simple discrimination version of the task (Figure 5), inputs are noise-free ðs = 0Þ and consist of a single feature in each trial (cA = 1 and cB = 0 or vice versa). In the evidence integration version of the task (Figure 6), inputs are noisy ðs > 0Þ and include non-zero average components along both feature directions. Finally, the parameters gA and gB control the two modulatory inputs which are taken in the directions deﬁned by IctxA and IctxB.
In order to implement context-dependent computations, we deﬁne a unique readout signal zðtÞ by using a common readout set w of unit variance (Equation 144), to which we add an offset so that the baseline Nogo output is set to zero. The network is said to respond to the stimulus if the value of the total readout at the end of the stimulus presentation takes values larger than one half of the largest predicted value for the upper state.
The rank-two connectivity matrix we consider is given by:

mð1Þ = yA + rmIctxA + bmw nð1Þ = IA + rnIctxA + bnw mð2Þ = yB + rmIctxB + bmw nð2Þ = IB + rnIctxB + bnw;

(155)

where vectors yA and yB represent the orthogonal components of the right-connectivity vectors and are generated as Gaussian vectors of ﬁxed variance (for simplicity, we set Sy = SI).
For our choice of the parameters, the network solves the two different tasks by relying on the strongly non-linear responses generated by the interplay between the recurrent connectivity and the feed-forward inputs (details given below).
For weak input values, the network dynamics is characterized by two stable attractors (Figure 6F). As in Figure 4, we initialize the network in the state characterized by negative k1 and k2 values before the stimulus presentation. This dynamical attractor corresponds to the Nogo state. For strong input strengths, the network can jump to the Go state, deﬁned as the stable attractor characterized by positive k1 and k2 values.
The rank-two connectivity matrix has been designed as an extension of the unit-rank recurrent connectivity employed in Figure 4. We started by setting:

mð1Þ = yA + rmIctxA nð1Þ = IA + rnIctxA mð2Þ = yB + rmIctxB nð2Þ = IB + rnIctxB:

(156)

Note that, because the only overlap directions (IctxA and IctxB) are internal to the mð1Þ À nð1Þ and mð1Þ À nð1Þ pairs, Equation 156 describes a rank-two structure which generates a continuous ring attractor as in Figures S5D–S5I (gray circles in Figure 6F).

e25 Neuron 99, 609–623.e1–e29, August 8, 2018

The readout zðtÞ should detect the presence of both stimuli directions. As a consequence, it should be sensitive to both overlap

values k1 and k2. For this reason, we introduce a common term in the four connectivity vectors that is aligned to the common readout (Equation 155).

Introducing a common overlap direction has the effect of destabilizing the continuous attractor dynamics along the direction k1 = k2

(dashed line in Figure 6F), where two stable and symmetric ﬁxed points are generated. The equations for the ﬁrst-order spontaneous

dynamics read indeed:

k1 k2

= =

nnðð12ÞÞ

½fi ½fi

  

= =

rm rm

rn rn

k1 k2

ÂÂff0i0i

Ã Ã

+ +

bm bn bm bn

ðk1 ðk1

+ +

k2 k2

ÞÞÂÂff0i0i

Ã Ã

(157)

from which the value of k1 = k2 = k can be derived by dividing and multiplying together the two equations. The ﬁnal readout signal

contains a contribution from both ﬁrst-order statistics:

zðtÞ

=

hwi

½fi

i

=

bm

ðk1

+

k2

ÞÂf0i

Ã :

(158)

The input-driven dynamics of the network are determined by the interplay between the structure strength and the contextual and

stimulus inputs. Crucially, the modulatory inputs along IctxA and IctxB are used to gate a context-dependent response. Similarly to Figure 5B, a strong and negative gating variable along IctxA can completely suppress the response to stimulus IA, so that the readout signal is left free to respond to IB.

The overall effects of the inputs on the dynamics can be quantiﬁed by solving the mean-ﬁeld equations. For the ﬁrst-order statistics,

we obtain:

k1 k2

= =

ÂÂff0i0i

ÃÈ ÃÈrm
rm

rnk1 rnk2

+ +

bmbnðk1 bmbnðk1

+ +

k2 Þ k2 Þ

+ +

cA S2I cB S2I

+ +

É rn gA É rngB

(159)

while the second-order gives, in the case of stationary regimes: D0 = g2Âf2i Ã + S2wÀk21 + k22Á + b2mÀk21 + k22Á + S2I Àc2A + c2BÁ + ðrmk1 + gAÞ2 + ðrmk2 + gBÞ2:

(160)

Figures S5L–S5M displays the values of the ﬁrst-order statistics and the readout response in the two contexts. Note that, when the response to IA (resp. IB) is blocked at the level of the readout, the relative ﬁrst-order statistics k1 (resp. k2) does not vanish, but actively contributes to the ﬁnal network response.

The average activation variable of single neurons contains entangled contributions from the main directions of the dynamics, which

are inherited both from the external inputs and the recurrent architecture:

mi

=

½xi 

=

À yA;i

+

rm IctxA;i

+

Á bmwi k1

+

À yB;i

+

rm IctxB;i

+

Á bmwi k2

+

cA IAi

+

cB IBi

+

g1 IctxA;i

+

g2 IctxB;i :

(161)

In Figures 5E and 6D, we project the averaged activation mi in the directions that are more salient to the task. The projection along w, which reﬂects the output decision, is proportional to the readout value (Equation 158). The input signals affect instead the average
activity through the values of k1 and k2, but can be also read out directly along the input directions. Note that the projection on the input direction IA (resp. IB) is proportional to the signal cA (resp. cB) regardless of the conﬁguration of the modulatory inputs selecting one input channel or the other.
In practical terms, in order to obtain the network architecture that has been used in Figures 5 and 6, we ﬁxed the parameters step by step. We ﬁrst considered input patterns only along IA (cB = 0), and we ﬁxed two arbitrary values of bm and bn. In particular, we considered intermediate values of b. Large values of b tend to return large activity variance, which requires evaluating with very high precision the Gaussian integrals present in the mean-ﬁeld equations. Small values of b bring instead the network activity closer to a
continuous-attractor structure, and turn into larger ﬁnite-size effects. In a second step, we ﬁx rm and rn such that the network detects normalized input components along IA only when they are larger than a threshold value, that is taken around 0.5. We then looked for a pair of gating variables strengths ½gA; gB which completely suppresses the response to IA by extending the range of bistable activity. The opposite pattern can be used to block the response in IB and allow a response in IA.
Once the response in IA has been blocked, it can be veriﬁed that the network solely responds to inputs which contain a response along IB that is larger than a threshold close to 0.5. Note that, as in Figures S5L–S5M, different values of cA only minimally affect the exact position of the threshold.
To conclude, we remark that this procedure leaves the freedom of ﬁxing the network parameters in many different conﬁgurations.
The complex rank-two architecture leads to larger ﬁnite-size effects than the respective unit-rank setup which acts as a single de-
tector of correlations. In particular, the error at the level of the readout is larger but it decays with the system size, as expected for
deviations induced by ﬁnite-size effects (Figure S5N). Finally, note that when the noise in the input stimuli becomes extremely large,
the network loses its ability to respond in a totally context-dependent fashion, as strong ﬂuctuations in the non-relevant stimulus
become likely to elicit a response.

Neuron 99, 609–623.e1–e29, August 8, 2018 e26

METHOD DETAILS FOR MAIN FIGURES
Figure 1 In this ﬁgure, Sm = Sn = 1:0. Note that the precise position of the instability to chaos depends on the value of Sm. The connectivity vectors m and n were generated from bivariate Gaussian distributions (means Mm and Mn, variances Sm and Sn, correlation r). Here we display the case where m and n overlap only along the unitary direction (Mm > 0; Mn > 0, r = 0, see STAR Methods). As shown in Figure S2, qualitatively similar regimes are obtained when the overlap is deﬁned on an arbitrary direction. C-D: Network simulations were performed starting from initial conditions centered around m and À m. Activity is integrated up to T = 800. In simulations, N = 5000, and statistics are averaged over 15 different connectivity realizations. The error bars, when visible, correspond to the standard deviation of the mean (as in every other ﬁgure, if not differently speciﬁed).
Figure 2 In this ﬁgure, g = 0:8. Other parameters are set as in Figure 1. B: The asymptotic input parameters are indicated by gray dots in D (middle). The simulation results (dark gray traces) correspond to 20 trajectories for different network realizations (different trajectories strongly overlap). We simulated Ntr = 20 different networks, each consisting of N = 3500 units. In every network realization, the random part of the connectivity cij is varied, while the low-rank part minj is kept ﬁxed. I (resp. m) scale: 0.7 (resp. 0.25) units. D: The external input is increased along nt, the component of n that is perpendicular to the overlap direction.
Figure 3 The input and the readout vectors are Gaussian patterns of standard deviation S = 2. C (right): Colored traces: 20 trajectories from different network realizations (different trajectories strongly overlap). We simulated Ntr = 20 different realizations of the network, each consisting of N = 2500 units. In every network realization, the random part of the connectivity cij is generated independently, while the low-rank part minj is kept ﬁxed. IA, IB and m scale: 1.5 units. D: Here, and in every plot if not differently stated, r indicates the Pearson correlation coefﬁcient. F: The PC axis are determined by analyzing separately the trials corresponding to the Go (top) and the Nogo (bottom) stimuli. Connectivity is measured as the average reciprocal synaptic strength; it includes both the random and the unit-rank components and it is averaged across network realizations. Note that the value of the correlation coefﬁcient r increases with the number of realizations Ntr and the structure strength.
Figure 4 The input and the readout vectors are Gaussian patterns of standard deviation S = 1:2. The overlap between the connectivity vectors m and n leading to non-linear responses is quantiﬁed by rm = rn = 2:0. B: The input is generated as white noise of mean c = 0:6 and standard deviation s = 0:4 (the noise trace in the ﬁgure is only for illustration purposes). The red dashed line indicates the threshold in the implemented network. C: The gray bar indicates the time point at which the network output is measured. Here and in the following ﬁgures, the readout includes an offset, so that the baseline value is set to zero. D: We simulated many input noise traces for Ntr = 4 different realizations of the network, each consisting of N = 2500 units. In every network realization, the random part of the connectivity cij is varied, while the low-rank part minj is kept ﬁxed. Trajectories are smoothed with a Gaussian ﬁlter of standard deviation equal to one normalized time unit. I (resp. m) scale: 0.5 (resp. 3.5) units. F: The structure strength corresponds to the overlap rmrn. The effective timescale is measured as the inverse of the value of the outlier eigenvalue of the stability matrix for c = 0. G: The psychometric curve was measured across Ntr = 100 different realizations. The network produces an output to the stimulus if at the end of the stimulus presentation (vertical gray line in B) the value of the readout z is larger than one half of the largest readout value predicted by the theory. H: Details as in Figure 3F.
Figure 5 The stimuli vectors are Gaussian patterns of standard deviation S = 1:2. We furthermore set: g = 0:8, bm = 0:6, bn = 1, rm = 3, rn = 1:6. The amplitudes of the two context directions are ﬁxed to ½0:08; À0:14 (resp. ½ À 0:14; 0:08) during the context A (resp. context B) trials. B: We consider in this case a unit-rank network as in Figure 2D, and we show in the two panels the network response for two different values of the input strength along the overlap axis (we set, respectively, MI = À 0:3 and 0.6). Details on the effect of contextual modulation on the full rank-two model are further illustrated in Figures S5L–S5N. E: We simulated Ntr = 4 different realizations of the network, each consisting of N = 3000 units. In every network realization, the random part of the connectivity cij is varied, while the low-rank part minj is kept ﬁxed. IA and IB (resp. w) scale: 1.0 (resp. 2.0) units. F: The network performance was measured across Ntr = 50 different network realizations of size N = 7500. The network produces an output to the stimulus if at the end of the stimulus presentation (vertical gray line in D) the value of the readout z is larger than one half of the largest readout value predicted by the theory. G: Details as in Figure 3F.
Figure 6 The stimuli vectors are Gaussian patterns of standard deviation S = 1:2. We furthermore set: g = 0:8, bm = 0:6, bn = 1, rm = 3, rn = 1:38. The amplitudes of the two context directions are ﬁxed to ½0:08; À0:18 (resp. ½ À 0:18; 0:08) during the context A (resp. context B) trials. B: Here cA = 0:6 and cB = 0:1, while the standard deviation of the noise in the input is s = 0:3 (the noise trace in
e27 Neuron 99, 609–623.e1–e29, August 8, 2018

the ﬁgure is only for illustration purposes). D: We simulated many noisy input traces for Ntr = 5 different realizations of the network,
each consisting of N = 4000 units. In every network realization, the random part of the connectivity cij is varied, while the low-rank part minj is kept ﬁxed. For the sake of clarity, only correct trials have been included. IA and IB (resp. w) scale: 1 (resp. 1.5) units. E: Network
performance was measured across Ntr = 50 different network realizations of size N = 7500.

QUANTIFICATION AND STATISTICAL ANALYSIS

In this section, we brieﬂy describe the analysis techniques that have been applied to the datasets generated from direct simulations of activity in ﬁnite-size networks (Figures 2, 3, 4, 5, and 6).

Dimensionality reduction
In order to extract from the high-dimensional population activity the low-dimensional subspace which contains most of the relevant dynamics, we performed dimensionality reduction via a standard Principal Component (PC) analysis.
To begin with, we constructed the activation matrix X. In X, every column corresponds to the time trace of the activation variable xiðtÞ for unit i, averaged across trials. We indicate as trials different network simulations, where different noisy inputs, or different quenched noise in the random connectivity matrix have been generated (details are speciﬁed in the ﬁgure captions). The activation matrix X is normalized through Z-scoring: to every column, we subtract its average over time, and we divide by its standard deviation. Note that Z-scoring distorts the shape of the population trajectory in the phase space. For this reason, in order to facilitate the comparison with the trajectory predicted by the mean-ﬁeld theory, in Figure S3 we more simply consider the mean-subtracted matrix X. Applying the PCA analysis to one of the two data formats impacts the results from a quantitative point of view, but does not change their general validity.
The principal components (PC) are computed as the normalized eigenvectors felgl = 1;.;N of the correlation matrix C = XT X. The PC are sorted in decreasing order according to the corresponding real eigenvalue ll. The activation matrix X can be projected on the orthonormal basis generated by the PC vectors by computing: X0 = XE, where E is the N3N matrix containing the PC eigenvectors ordered as columns. The variance explained by the l-th PC mode el can be computed as the l-th entry on the diagonal of the rotated correlation matrix C0 = X0T X0 .
While in our network models the low-rank part of the connectivity determines a purely low-dimensional dynamics (Figure S3A), the random part of the connectivity generates a continuum of components whose amplitude is determined by strength of the random connectivity g with respect to the connectivity and input vectors. In Figure 2, where g = 0:8, the low-dimensional nature of the dynamics is revealed by considering averages across several ðNtr = 20Þ realizations of the random connectivity. In Figure S3B, we illustrate the result of performing PCA on the activity generated by a single network. In this case, even if more PC components contribute to the total variance, the two ﬁrst axis bear a strong resemblance with the directions predicted with the theory. In Figure S3C we show that, in the same spirit, a PCA analysis can be used to extract the relevant geometry of the network model also when activity is strongly chaotic.
In order to more easily connect with the theoretical predictions, we systematically applied dimensionality reduction on datasets constructed from the activation variable xi. We veriﬁed that our results still hold, from a qualitative point of view, when the analysis is performed on the non-linearly transformed variables fðxiÞ. In the network models we considered, the activation variables fðxiÞ indeed form a non-linear but dominantly low-dimensional manifold in the phase space. The axes predicted by the mean-ﬁeld theory determine the dominant linear geometry of this manifold, and can be still captured (although less precisely) by looking at the ﬁrst PC components.

Linear regression
In order to estimate how single units in the network are tuned to different task variables (such as input stimuli or decision variables), we used a multi-variate linear regression analysis.
To this end, we considered the full population response xikðtÞ, where k = 1; .; Ntr indicates the trial number. Following (Mante et al., 2013), our aim was to describe the network activation variables as linear combinations of the M relevant task variables. In Figure 3, the two variables we considered were the strength of the Go and of the Nogo inputs, that we indicate here with cGo and cNogo:

xik ðtÞ = bGi;tocGoðkÞ + bNi;togocNogoðkÞ:

(162)

In a Go, or in a Nogo trial, only one of the two strength coefﬁcients is non-zero. In Figure 4, the two relevant task variables are assumed to be the input strength along I, quantiﬁed by c, and the network output, quantiﬁed as the value of the readout z at the end of the stimulus presentation:

xik ðtÞ = biin;tputcðkÞ + bci;thoicezðkÞ:

(163)

Neuron 99, 609–623.e1–e29, August 8, 2018 e28

In Figures 5 and 6, the relevant variables are four: the strength of stimuli A and B, the trial context and the network output. We thus have:

xik ðtÞ = bAi;tcAðkÞ + bBi;tcBðkÞ + + bci;ttxyðkÞ + bci;thoicezðkÞ:

(164)

where the context variable is represented by a unique symbolic variable y, which takes value y = 1 in context A and y = À 1 in

context B.

More generally, we indicate with bni;t the regression coefﬁcient of unit i with respect to the task feature n at time t. The vector bi;t = fbni;tgn = 1;::;M indicates the collection of the M variables regressors for a given unit at the time point t. We compute the regression coefﬁcients by deﬁning a matrix F of size M 3 Ntr, where every row contains the value of the M relevant task variables across trials.

The regression coefﬁcient vectors are then estimated by least-square inversion:

bi;t

=

À FF

T

ÁÀ1

Fxi;t

(165)

where the vector xi;t is constructed by collecting across trials the value the activation variable of unit i at time t. In order to get rid of the time dependence of our result, we simply consider the coefﬁcients bi;t at the time point where the two-
dimensional array bi;t for every i has maximal norm (Mante et al., 2013). The resulting set of M-dimensional vectors bi contains the regression coefﬁcients of unit i with respect to the M relevant task variables. The N-dimensional regression axis for a given task variable n is ﬁnally constructed by collecting the n-th components of bi across different population units: fbni gi = 1;::;N.

DATA AND SOFTWARE AVAILABILITY

Software was written in the Python (http://python.org) programming languages. Implementations of algorithms used to compute quantities presented in this study are available at: https://github.com/fmastrogiuseppe/lowrank/.

e29 Neuron 99, 609–623.e1–e29, August 8, 2018

Neuron, Volume 99
Supplemental Information
Linking Connectivity, Dynamics, and Computations in Low-Rank Recurrent Neural Networks
Francesca Mastrogiuseppe and Srdjan Ostojic

A

B

C

D

E

F

G MmMn > 1

g<1

MmMn < 1

g>1

1

Figure S1 (previous page): Dynamical Mean-Field description of rank-one networks whose right- and leftconnectivity vectors overlap solely on the unitary direction (ρ = 0, see Methods). Related to Figure 1.
(A-B-C) Eigenspectrum of the partially structured connectivity matrix Jij, related to the stability matrix Sij of the homogeneous ﬁxed points through: Sij = φ (x¯)Jij. A. Eigenspectrum of Jij in the complex plane. Red dots: eigenspectrum of a single realization Jij of size N = 1000. In black: theoretical prediction. Every matrix Jij consists of a sum of a random and of a ﬁxed unit-rank structure. In the large N limit, the spectrum of the full matrix is given by the sum of the eigenspectra of the two parts. The black circle has radius equal to the total random strength g, and the black star indicates the position of the non-zero eigenvalue of the rank-one structure Pij. B. Mismatch between the statistics measured in ﬁnite-size networks (xsim) and the theoretical prediction (xth) as the network size N is increased. The error is normalized: |xsim − xth|/xth. Averages over 100 realizations. The error bars (as in every other ﬁgure, if not diﬀerently speciﬁed) correspond to the standard deviation of the mean. Dashed lines: power-law best ﬁt (y ∝ N γ). The values of γ are indicated in the legend. C. Pearson correlation coeﬃcient between the connectivity eigenvector m and the eigenvector eˆ which corresponds to the outlier eigenvalue. Choice of the parameters: ρ = 0, MmMn = 1.43, Σm = 0.33, Σn = 1. In A and B, g = 0.7.
(D-E-F) Analysis of the eigenspectrum of the linear stability matrix Sij = φ (x¯)Jij for heterogeneous stationary solutions. D. Eigenspectrum of Sij in the complex plane. Red dots: eigenspectrum of a single, ﬁnite-size realization of Sij, N = 2500. The radius of the black circle corresponds to the theoretical prediction r = g [φi2] . The black star indicates the position of the non-zero eigenvalue of the rank-one structure miφ (x0j )nj/N , which deviates signiﬁcantly from the position of the outlier eigenvalue. We thus address the problem of evaluating the position of the outlier eigenvalue through a mean-ﬁeld stability analysis (Eq. 71), the prediction of which is indicated by the blue star. E. Mismatch between the results from simulations and mean-ﬁeld predictions for the radius and the outlier position. The error is measured as an average over Ntr = 30 ﬁnite size matrices, and decays as the system size is increased. Details as in B. F. Radius and outlier of the stability eigenspectrum for increasing random strength values. The dots indicate the results of numerical simulations of networks with N = 2500 units, averaged over Ntr = 30 realizations of the random and structured connectivities. In grey: radius of the compact bulk (continuous line: mean-ﬁeld prediction r). In blue: position of the outlier eigenvalue (continuous dark and light lines: ﬁrst and second eigenvalue of matrix M given in Eq. 71). In black: position of the outlier when χij is shuﬄed (continuous line: mean-ﬁeld prediction for the outlier of the structured part miφ (x0j )nj/N ). Choice of the parameters: ρ = 0, MmMn = 2.2, Σm = 0.4, Σn = 1. In D and E, g = 0.5.
(G) Graphical analysis of stationary solutions. Large ﬁgures: nullcline plots for the population-averaged DMF equations in Eq. 83. Black dots indicate the solutions that are stable with respect to the outlier eigenvalue. Four set of parameters (two values for MmMn, two for g) have been selected. Note that the shapes of the µ and the ∆0 nullcline depend only on the structure strength MmMn and the disorder strength g. For the ﬁgures in the ﬁrst (resp. second) row, the structure strength MmMn = 0.55 (resp. MmMn = 2.0) is weak (resp. strong). For the ﬁgures in the ﬁrst (resp. second) column: the random strength g = 0.7 (resp g = 2.0) is weak (resp. strong). Note that the stationary states at large g values (right column) are always unstable with respect to the continuous component of their stability eigenspectra (Fig. 1 C-D). The small side ﬁgures associated to every row and column show how the µ (for the rows) and ∆0 (for the columns) nullclines have been built. We solve µ = F (µ) (resp. ∆0 = G(∆0)) for diﬀerent initial values of ∆0 (resp. µ). Diﬀerent initial conditions are displayed in gray scale. Dark grey refers to ∆0 = 0 (resp. µ = 0). The dots indicate the solutions for diﬀerent initial values, which together generate the nullcline curves. Choice of the parameters: Σm = 1.
2

A ρΣmΣn > 1 ρΣmΣn < 1

g<1

g>1

B

C

D

Stationary Chaotic
3

Figure S2 (previous page): Dynamical Mean-Field description of rank-one networks whose right- and leftconnectivity vectors overlap onto an arbitrary direction y (Mm = Mn = 0, ρ = 0, see Methods). Related to Figure 1. (A) Graphical analysis of stationary solutions. Large ﬁgures: nullcline plots for the population-averaged DMF equations in Eq. 89. Black dots indicate the solutions that are stable with respect to the outlier eigenvalue. Four set of parameters (two values for ρΣmΣn, two for g) have been selected. Note that the shapes of the κ and the ∆0 nullcline depend only on the structure strength ρΣmΣn and the disorder strength g. For the ﬁgures in the ﬁrst (resp. second) row, the structure strength ρΣmΣn (resp. ρΣmΣn) is weak (resp. strong). For the ﬁgures in the ﬁrst (resp. second) column: the random strength g = 0.5 (resp. g = 1.7) is weak (resp. strong). Note that the stationary states at large g values (right column) are always unstable with respect to the continuous circular component of their stability eigenspectra (see B-C-D). The small ﬁgures associated to every row and column show how the κ (for the rows) and ∆0 (for the columns) nullclines have been built. We solve κ = F (κ) (resp. ∆0 = G(∆0)) for diﬀerent initial values of ∆0 (resp. κ). Diﬀerent initial conditions are displayed in gray scale. Dark grey refers to ∆0 = 0 (resp. κ = 0). The dots indicate the solutions for diﬀerent initial values, which together generate the nullcline curves. (B-C-D) Bifurcation diagram of the activity statistics as the random strength g is increased. Details as in Fig. 1 C-D. B. Stability eigenspectrum of stationary solutions, mean-ﬁeld prediction for the radius of the compact part and the outlier position. C. Overlap κ = ni[φi] . D. Individual second order statistics. The DMF solutions are displayed as continuous (resp. dashed) lines if they correspond to a stable (resp. unstable) state. In C-D, top panels display statistics for stationary solutions and bottom panels display statistics for chaotic solutions. Dots: we measured activity statistics in ﬁnite-size networks, starting from globally positive and negative initial conditions. Activity is integrated up to T = 400. N = 3500, average over 8 diﬀerent network realizations. Choice of the parameters: Σm = Σn = 1.5, ρ = 2.0/ΣmΣn.
4

A

B

C

5

Figure S3: Two-dimensional dynamics in networks with unit-rank structure and external inputs. Related to Figure 2. We consider a unit-rank network as in Fig. 2 B-C. The connectivity vectors m and n are orthogonal, but the external input vector contains a component along n, whose strength (quantiﬁed by ΣnI , see Methods) undergoes a step increase from 0.2 to 2.0. We simulate data from networks of size N = 3500. We analyze the dimensionality of the dynamics by comparing the relevant low-dimensional trajectory predicted by the mean-ﬁeld theory with the strongest modes extracted through dimensionality reduction (Principal Component analysis, see Methods). A. Analysis for a purely structured network (g = 0). Left top: the mean-ﬁeld theory predicts that the lowdimensional network dynamics x = {xi} lies in the plane deﬁned by the right-connectivity vector m and the external input I. We thus projected the high-dimensional population activity (dark grey trajectory) on this plane. Left bottom: we projected the network dynamics (continuous), along with the two vectors m and I (dashed), on the plane deﬁned by the ﬁrst two PC axis e1 and e2. Right top: Pearson correlation coeﬃcient between vectors m and I and the ﬁrst eight PC. Right bottom: strength of the ﬁrst eight PC, measured as the fraction of the standard deviation of activity that they explain (see Methods). Note that when the network connectivity is fully structured (g = 0) as in this case, activity is exactly two-dimensional. The ﬁrst two PC axis span the m − I plane, but they deﬁne a rotated set of basis vectors. B. Analysis for a network which includes a random term in the connectivity matrix (g = 0.8). While in Fig. 2 we performed the PC decomposition on trial-averaged data (Ntr = 20), here we considered a single trial, deﬁned as a single realization of the random connectivity matrix. Details as in A. Note that the random component of the connectivity adds noisy contributions in a continuum of PC directions, whose strength depends on the value of g with respect to the amplitude of input and connectivity vectors, and becomes weaker and weaker when averaging with respect to diﬀerent realizations of χij. When g > 0, vectors m and I are not fully contained in the e1 − e2 plane, so their projections on the PC plane are not orthogonal. C. Analysis for a network which includes a strong random term in the connectivity matrix (g = 1.8), such that spontaneous activity is chaotic. In the left-most column, similarly to Fig. 2, we plot the time trajectories of four randomly selected units. The center and the right columns are as in A and B, with PCA performed on trial-averaged activity (Ntr = 20). The scale of the projections panels is here set arbitrarily.
6

A S C
D

B

C

SS

SC

CC

S

C

SS

S

Low g

High g

High MmMn

Low MmMn

7

Figure S4 (previous page): Dynamical Mean-Field description of input-driven dynamics for rank-one networks whose right- and left-connectivity vectors overlap solely on the unitary direction (ρ = 0, see Methods). Related to Figure 2.
(A-B-C) Dynamical regimes of the network activity as function of the structure connectivity strength mT n/N , the random strength g and the input strength. Grey shaded areas indicate the parameter regions where the network activity is bistable. Red shaded areas indicate the phase space regions where network dynamics are chaotic. When two stable solutions exist, the yellow and the red letter indicate whether each of them is stationary (S) or chaotic (C). Note that stationary and chaotic dynamics can coexist (SC region). In A, as in Fig. 2 D center, the two connectivity vectors m and n are orthogonal. We varied the external input strength by increasing the amplitude of the component along n (quantiﬁed by ΣnI , see Methods) and of the orthogonal one (quantiﬁed by Σ⊥). Note that inputs along both directions contribute to suppressing the amplitude of chaotic ﬂuctuations. In B, as in Fig. 2 D right, the two connectivity vectors m and n are not orthogonal, but they share an overlap component along the unitary direction. We varied the structure strength (quantiﬁed by MmMn) and the strength of the input along the direction of n that is perpendicular to the structure overlap, n⊥ (again quantiﬁed by ΣnI ). Similarly to Fig. 1, strong structure overlaps can lead to the appearance of two bistable solutions. In presence of non trivial external inputs, however, such solutions are not symmetric, and can loose stability on diﬀerent parameter boundaries. In particular, we observe that external inputs tend to suppress bistable regimes, by favouring one solution over the other. In C, ﬁnally, the network conﬁguration is similar to B, but we consider external inputs which include a second component along the direction of n that is shared with m, n (quantiﬁed by MI ). We observe that both input directions play similar roles in reducing the extent of the bistable regime. Choice of parameters: g = 2.2, Σm = Σn = 1.0, ΣmI = 0.
(D) Graphical analysis of stationary solutions. In this example, the external input vector overlaps with n on the unitary overlap direction n (MI = 0.13), and includes orthogonal components quantiﬁed by ΣI = 0.3. Large ﬁgures: nullcline plots for the stationary form of the population-averaged DMF equations in Eq. 98. Black dots indicate the solutions that are stable with respect to the outlier eigenvalue. Four set of parameters (two values for MmMn, two for g) have been selected. Note that the shape of the µ and the ∆0 nullcline depends only, respectively, on the structure strength MmMn and the disorder g together with the input statistics. For the ﬁgures in the ﬁrst (resp. second) row, the structure strength MmMn = 0.55 (resp. MmMn = 2.0) is weak (resp. strong). For the ﬁgures in the ﬁrst (resp. second) column: the random strength g = 0.7 (resp. g = 2.0) is weak (resp. strong). The small ﬁgures associated to every row and column show how the µ (for the rows) and ∆0 (for the columns) nullclines have been built. We solve µ = F (µ) (resp. ∆0 = G(∆0)) for diﬀerent initial values of ∆0 (resp. µ). Diﬀerent initial conditions are displayed in gray scale. Dark grey refers to ∆0 = 0 (resp. µ = 0). The dots indicate the solutions for diﬀerent initial values, which together generate the nullcline curves. Choice of the parameters: Σm = 1.
8

A

B

C

D

E

F

G

Stationary Chaotic

H

I

L

M

N

9

Figure S5 (previous page): Dynamical Mean-Field description of low-rank networks designed for solving computational tasks. Related to Figures 3, 5 and 6.
(A-B-C) Rank-one networks can robustly perform computations also when their dynamics is chaotic due to large random connectivities. Here, we show an example from the Go-Nogo task (Fig. 3). We focus on large random strength values (g = 2.5), so that spontaneous network dynamics is chaotic. A. Left: response of three randomly selected units to the Go pattern IA (top, blue) and to the Nogo pattern IB (bottom, green). Right: time trace of the readout z(t) for the Go (blue) and the Nogo (green) stimulus. B. Absolute, normalized distance between the theoretical prediction and the value of the readout z obtained from ﬁnite-size r√ealizations. As expected, the magnitude of the average normalized error decays with the network size as ∼ 1/ N . In grey: g = 0.8, in black: g = 2.5. Averages over 200 network realizations. Details as in Fig. S1 B. C. As in Fig. 3 F, we consider pairs of units and we compute the correlation coeﬃcient between their weights onto the ﬁrst PC axis and their average reciprocal connectivity strength. The PC axis is computed separately for data corresponding to Go (blue) or the Nogo (green) trials. The correlation coeﬃcient for the Go trials decreases with the amplitude of the random connectivity, although the error in the readout is only weakly aﬀected (panel B.). For every entry of the connectivity matrix Jij, indeed, the random part gχij has larger amplitude then the structured one Pij. As a consequence, the random noise can hide a fraction of the strong correlations existing between the PC weights and the rank-one connectivity Pij. Note that the absolute value of the correlation coeﬃcient depends on the variance of the rank-one connectivity. Finally, the correlation coeﬃcient increases as the connectivity gets averaged on more and more realizations of the random part. Choice of the parameters as in Fig. 3.
(D-E-F-G-H-I) Ring attractor from rank-two connectivity structures with connectivity vectors characterized by strong internal overlaps (see Methods). D. Sample of activity from a ﬁnite-size realization (N = 4000) of the rank-two network. Activity is initialized in two diﬀerent initial conditions (light and dark blue), indicated by the small arrows. Left: time traces of the activation variables for three randomly selected network units. Note the long time range on the x axis. Right: population activation x = {xi} projected on the plane spanned by the right vectors m(1) and m(2). The ring attractor predicted by the mean-ﬁeld theory is displayed in light gray. The strength of the disorder is g = 0.5, so that the network is in a stationary regime. In the small inset, we reproduce the theoretical prediction together with the ﬁnal state of additional Ntr = 20 networks realizations, that are displayed as grey dots. E. Sample of activity for two ﬁnite-size realizations (N = 4000) of the structured connectivity matrix (dark and light red). Details as in D. The strength of random connections is g = 2.1, so that the network is in a chaotic regime. Chaotic ﬂuctuations can occur together with a slow exploration of the ring (dark red). If two speciﬁc states on the ring appear to be more stable, chaotic ﬂuctuations can induce jumps between the two of them (light red). F-G. Mean-ﬁeld characterization of the ring structure: radius of the ring attractor and stability eigenvalues. Details as in Fig. 1. Dots: numerical results from ﬁnite-size (N = 4000) networks, averaged over 6 realizations of the connectivity matrix. H-I. Input patterns which correlate with the left vector n(1) reduce the ring attractor to a single stable state. Activity is thus projected in the direction spanned by the right vector m(1). In H, we show the input response for two ﬁnite-size networks. The grey ring displays the mean-ﬁeld solution in absence of external inputs (g = 0.5, as in D). In the top panel, the input is weak (ΣI = 0.2, see Methods). The transient dynamics as well as the equilibrium state lie close to the ring structure. In the bottom panel, the input is strong (ΣI = 0.6), and the ring structure is not anymore clearly apparent. In I, we plot the values of the overlaps κ1 (blue) and κ2 (azure) as a function of the structure strength parameter ρ, for ﬁxed input strength. Stable solutions are plotted as continuous lines, unstable ones as dashed. Solid (resp. transparent) lines refer to weak (resp. strong) external inputs: ΣI = 0.2 (resp. 0.6). The vertical gray line indicate the value of ρ that has been used in H. Dots: numerical results as in F-G. Choice of the parameters (see Methods): Σ = 2.0, ρ1 = ρ2 = 1.6.
(L-M-N) Rank-two structures for implementing non-linear stimuli detection in a context-dependent fashion (Fig. 6): theoretical mean-ﬁeld predictions. L. Values of the ﬁrst-order statistics κ1 (continuous) and κ2 (dashed) as a function of the overlap strength along the stimulus IA. Results are shown for four increasing values of the overlap strength along the second stimulus IB. Top (resp. bottom): the contextual gating inputs are such that a response to IA (resp. IB) is selected. M. Readout value, built by summing the values of κ1 and κ2 (Eq. 158). Note that although κ1 and κ2 vary with input strength, on each branch their sum is approximately constant. Details as in L. N. Average normalized error between the DMF predictions and the simulated readout, in the two gating conditions as a function of the network size N . Average over 60 network realizations, details as in Fig. S1 B. Parameters as in Fig. 6.
10

A B

C

D

E

F

11

Figure S6 (previous page): Dynamics of unit-rank networks of ﬁnite-size are characterized by two distinct time-scales. Related to Figure 1.
In Fig. 1 we have shown that, when the structure strength is large, the DMF theory predicts the existence of two bistable states, which can display chaotic activity. For those states, the population-averaged statistics of the activation variable xi are stationary. In the chaotic regime, indeed, irregular temporal ﬂuctuations are decorrelated from one unit to the other, so that the central limit theorem applies at every time step, and the network statistics are constant in time. In ﬁnite-size networks, however, the network statistics are not stationary: their dynamics display instead two diﬀere√nt time-scales. The instantaneous population-averaged activity undergoes small ﬂuctuations of amplitude O(1/ N ), whose time-scale is given by the relaxation decay of chaotic activity. Because of bistability, furthermore, the ﬁrst-order statistics displays also sharp transitions from positive to negative values and viceversa, which are made possible by the self-sustained temporal ﬂuctuations. In the following, we focus on rank-one structures where the overlap direction is deﬁned along the unitary vector. As a consequence, the relevant ﬁrst-order statistics is simply the population-average of the activation vector µ (see Methods). A. Sample from a ﬁnite-size network: activation time traces for randomly chosen units displaying attractors jumps. Dashed blue line: time-dependent population average. B. Time-dependent population average in a longer trial. C-D. We consider transition events as point processes, and we measure the average transition rate. We arbitrarily deﬁne a transition point as the time step at which the populationaveraged activation crosses zero (grey points in B). In C, we show that the transition rate decays to zero as the network size N is increased. Details as in Fig. S1 B. Note that the transition rate depends on the amplitude of ﬁnite-size ﬂuctuations measured with respect to the average phase space distance between the two attractors. As a consequence, the transition rate depends on the architecture parameters and on the network size, but also varies strongly from one realization of the connectivity matrix to the other. D. Fano factor of the point process for diﬀerent values of the network size N , which noisily oscillates around 1. For every realization of the network, the jumps count is measured in diﬀerent windows of the total integration time T = 15.000. The Fano factor is measured for every realization and then averaged over Ntr = 30 diﬀerent networks. E-F. Analysis of the two time-scales displayed by the network dynamics. The ﬁrst time-scale is measured as the relaxation time constant τr, which can be derived within the DMF framework by computing the time decay of the full autocorrelation function ∆(τ ). The persistence time scale, indicated by τp, coincides instead with the average time interval which separates two attractors transitions. In E, we show that both time scales depend on the network architecture parameters. Here, we ﬁx the random strength g = 3 and we increase the structure strength. When the structure is weak (left), the network is in the classical homogeneous chaotic state. The persistence time scale coincides here with the relaxation time constant of chaotic ﬂuctuations. When the structured and the random components have comparable strengths, instead, two heterogeneous chaotic phases co-exist (center). In this regime, the average persistence time increases monotonically with the structure strength, and reaches arbitrarily large values. Note that the relaxation time undergoes a very slow increase before sharply diverging at the boundary with stationary states, but the increase takes place on a much smaller scale. Finally, if the structure is too strong (right), the two bistable states become stationary. In this region, τr is formally inﬁnite, while τp coincides with the total duration of our simulations. Pink continuous line: DMF prediction, measured as the full width half maximum of the auto-correlation function ∆(τ ). Pink dots: a rough estimate of τr from ﬁnite size networks is obtained by rectifying the population average signal and we computing the full width half maximum of its auto-correlation function. F. We compare the average transition rate with the average overlap between the two attractors in the phase spac√e. For ev√ery unit, the typical overlap between its positive and its negative tra√jectories is given by πi = 2(−µ − ∆∞z + ∆0 − ∆∞). We average across the population, yielding: π = 2(−µ+ ∆0 − ∆∞). We then normalize π through dividing by its value in the unstructured chaotic regimes (2∆0). When positive, π returns an overlap; when negative, it measures a distance between the two orbits. For every set of the architecture parameters, the theoretical expected value of the overlap can be computed within the DMF framework. We show that, in ﬁnite-size networks, the transition probability between the two chaotic attractors monotonically increases with the attractors overlap in the phase space. In the ﬁgure, the points returned by simulations are ﬁtted with an error function of which we evaluate numerically the amplitude, the oﬀsets and the gain: f (x) = p0 + p1 erf(p2(x − p3)). Choice of the parameters: ρ = 0, g = 3.0, Σm = 0.
12

A

B

C

D

E

F

LH

H

H

L LL

L

LL

L

13

Figure S7 (previous page): Dynamics of unit-rank networks with positively-deﬁned activation functions. Related to Figure 1.
In the main text, we performed our analysis of low-rank networks by adopting a completely symmetric network model, whose input-free solutions are invariant under the sign transformation xi(t) → −xi(t). Such symmetry is broken when a more biologically-plausible, positively-deﬁned activation function φ(x) is adopted. Here, we investigate the eﬀect of changing the transfer function to: φ(x) = 1 + tanh(c(x − γ)). Note that adding a shift γ is equivalent to including an external and constant negative input. The parameter c, instead, rescales the slope of φ(x) at the inﬂection point. For simplicity, we ﬁx γ = 1 and c = 1.5. We furthermore restrict the analysis to the case of unit-rank structures whose right- and left-connectivity vectors solely overlap on the unitary direction (ρ = 0, see Methods). The Dynamical Mean Field (DMF) sets of equations were derived for an arbitrary activation function, so they can directly be adapted to the present scenario. A. We start by graphically analysing the stationary solutions (Eq. 83), and we plot the two nullclines of the system for diﬀerent values of the architecture parameters. The top panel displays the µ nullclines for diﬀerent MmMn values. At MmMn = 1, the unstable branch coincides with µ = 1, and the stable ones are symmetric. Around MmMn = 1, the perfect pitchfork is broken in one or the other direction, generating a ﬁrst stable continuous branch and a second one, where one unstable and one stable solution merge at low or high ﬁring rate. For extremely low (resp. high) MmMn values, ﬁnally, there’s just one nullcline at low (resp. high) µ values. The ∆0 nullcline (bottom panel) displays a more complex behaviour compared to the symmetric φ(x) = tanh(x) case. When g is suﬃciently large, indeed, it can become a non-monotonic function of the mean input µ, transforming into a S -shaped nullcline. As it is shown in the following, this more complex shape is able to induce bistable activity even when the µ nullcline is reduced to a single continuous branch. This situation is reminiscent of the ﬂuctuations driven bistable regime in [Renart et al, 2007]. B. Stationary stable solutions plotted as color maps on the parameter space deﬁned by the random and the structure strengths. The mean-ﬁeld system admits two classes of stable solutions. The ﬁrst one, illustrated in the top row, takes large mean and variance values. It suddenly disappears on the leftmost grey boundary of the plot, in a parameter region which co-exists with the second solution. The second solution, plotted in the bottom row, takes typically small values of µ and ∆0, and disappears on the right-most boundary with a ﬁrst-order transition as well. C-D-E. In order to dissect more systematically the nature of those solutions, we ﬁx the value of the structure strength (dashed lines in in B), and we gradually increase the random strength g. In C, we ﬁx the structure strength to high values: MmMn = 1.2. The bifurcation pattern occurring in this case resembles what we observed in the original case with φ(x) = tanh(x). At low values of g, two stable ﬁxed points are built, respectively, on the high and on the low branches of the µ nullcline. For that reason, we call this state LH (cfr with F). When the random connectivity is too strong, the low ﬁring rate ﬁxed point annihilates, and only one high ﬁring solution survives (H state). In D, MmMn is exactly equal to unity. At small g values, similarly to the previous case, network activity is bistable and admits one L and one H stationary state. As g increases, the ∆0 intersect the high ﬁring rate branch at smaller and smaller values of µ. Finally, the H state is lost, and the second stable ﬁxed point is realized on the intermediate branch at µ = 1. This bistable state is thus formally a LI state. Finally, at large g values, the two intersections on the low rate branch collapse together and disappear. Bistability is lost and only one intermediate (I) state exists. In E, we consider slightly smaller values of MmMn. A classical LH state exists at small g values, the bistable state at large random strengths involves two stable solutions which originate both a low ﬁring rates (LL state). The two states strongly diﬀer in the value of their variance. When g is suﬃciently large, one unique low ﬁring rate, high variance state (L) survives. F. The diﬀerent activity states are ﬁnally sketched in the phase diagram. Note that I states separate the phase diagram in F in two macro areas: below the dashed line, every stationary and chaotic solution is built on the same low ﬁring rate branch of the µ nullcline, and is thus formally a L state. Finally, the exact shape of the phase diagram depends on the value of the parameters c and γ. Choice of the parameters: ρ = 0, Σm = 0.
14

A 0

B Struct. stationary Oscillations
C

D F
E

15

Figure S8 (previous page): Oscillatory activity from rank-two structures that include a cross overlap between left- and right-connectivity vectors. A. Top: phase diagram for the rank-two structure with negative cross-overlap (see Methods). For diﬀerent values of the internal and the cross overlaps, the trivial ﬁxed point can lose stability and give rise to oscillatory or stationary structured activity. The Hopf bifurcation is indicated in blue, the instability to stationary activity in grey. The light-blue parameter region corresponds to sustained non-linear oscillations. Bottom: frequency of oscillations along the Hopf bifurcation boundary, in units deﬁned by the implicit time scale of the network dynamics. B-C-D-E. Samples of activity for diﬀerent connectivity parameters. From left to right: stability eigenspectrum of the trivial ﬁxed point (theory and simulations), sample of activation trajectories (the population average is indicated in dashed black), and population dynamics obtained by projecting the population activation x on the right-connectivity vectors m(1) and m(2). The parameters that have been used for every sample are indicated in A. B: Oscillatory transients in the ﬁxed point regime. C: Stable oscillations above the Hopf instability. The elongated shape of the closed trajectory on the m(1) − m(2) plane is inherited by the phase distribution across the population, and can be tuned by slightly modifying the parameters of the rank-two structure (see Methods). D: Highly non-linear oscillations close to the boundary with bistable activity. E: Oscillatory activity at high g values (g = 1.35), where dynamics include a chaotic component. F. When oscillations are strongly non-linear, their spectrum includes a large variety of frequencies that can be used to reproduce highly non-linear periodic patterns. We designed three random readout vectors and we linearly decoded activity from the dynamical regime in D to generate periodic non-linear outputs, which are displayed in grey.
16

